LYAPUNOV EXPONENT FOR PRODUCTS OF RANDOM ISING
TRANSFER MATRICES: THE BALANCED DISORDER CASE

arXiv:2105.00018v1 [math.PR] 30 Apr 2021

GIAMBATTISTA GIACOMIN AND RAFAEL L. GREENBLATT
Abstract. We analyze the top Lyapunov exponent of the transfer matrix for the nearest
neighbor Ising chain with random external field. This Lyapunov exponent coincides
with the free energy of the Ising chain with random external field, but it also plays a
central role in the analysis of the two dimensional Ising model with columnar disorder
and of the quantum chain with transverse random field. We obtain its sharp behavior
in the large interaction limit when the external field is centered: this balanced case
turns out to be critical in many respects. From a mathematical standpoint we precisely
identify the behavior of the top Lyapunov exponent of a product of two dimensional
random matrices close to a diagonal random matrix for which top and bottom Lyapunov
exponents coincide. In particular, the Lyapunov exponent is only log-Hölder continuous.
AMS subject classification (2010 MSC): 82B44, 60K37, 82B27, 60K35
Keywords: disordered systems, transfer matrix, singular behavior of Lyapunov exponents.

1. Introduction and results
1.1. Background. The simple two dimensional matrix of the form


1 ε
M = M (ε, Z) :=
,
εZ Z

(1.1)

with ε ≥ 0 and Z ≥ 0 repeatedly appears in physics, notably in the Ising model context
[13, 23]:
(I1) If d = 1 the transfer matrix of the Ising model with external magnetic field h and
nearest neighbor interaction potential J is equal to eh+J times (1.1), with ε = e−2J
and Z = e−2h . Therefore the free energy density of the model is h + J plus the largest
among the two eigenvalues of M .
(I2) If d = 2 on the square lattice and for h = 0, one can still express the partition
function of the model in terms of the largest eigenvalue of M with an adequate
choice of ε(θ, J1 ) and Z = Z(J1 , J2 ) where J1 and J2 are the horizontal and vertical
nearest neighbor interactions and θ is a phase arising from a Fourier transform in the
nonrandom direction. In particular the free energy density can be expressed as the
integral of such an eigenvalue, for fixed J1 and J2 , over θ ∈ [−π, π], with the small θ
(long wave-lengths) being relevant for the critical behavior. In this limit, i.e. θ small,
ε(θ, J1 ) becomes proportional to |θ| and we point out also that the θ, or ε, small limit
clearly corresponds to large J in ((I1)).
(I3) The ground state analysis of the d = 1 quantum Ising field with traverse field leads
once again to the same matrix problem and it is directly connected to the d = 2
classical model discussed in ((I2)), see e.g. [20].
For the d = 1 (non quantum) case there is no phase transition, i.e. the free energy
is a real analytic function, and this is just the regularity of the leading eigenvector of a
1

2

G. GIACOMIN AND R. L. GREENBLATT

matrix with positive entries. But for d = 2, or in the d = 1 quantum case, there is a
phase transition; this is possible due to the fact that ε(θ, J1 ) = 0 for θ = 0 and π, so we
are no longer in the context of matrices with positive entries and the two eigenvalues may
coincide (when J1 and J2 are positive, this can happen only if θ = 0). These solutions
are to a certain extent robust with respect to introduction of (some types of) disorder.
For example in d = 1 if h varies from site to site and it is the realization of a sequence
(hj )j∈Z of random variables (for example, IID), then the free energy expression still holds,
provided we replace the leading eigenvalue of the single matrix with the (top) Lyapunov
exponent of the sequence:
1
L(ε) := lim E log kM (ε, Z1 )M (ε, Z2 ) * * * M (ε, Zn )k ,
(1.2)
n→∞ n
where k * k is an arbitrary norm of matrices and of course Zj = e−2hj , ε = e−2J . Analogously, in the d = 2 and with columnar disorder – J2 is replaced by an IID sequence – the
free energy is equal to the Lyapunov exponent averaged over the phase [22, 23, 27].
The large interaction limit for the disordered external field one dimensional Ising model,
or zero phase limit for the d = 2 of quantum case, has been repeatedly analyzed. Notably
in [12] B. Derrida and H. Hilhorst – origin of the acronym DH – claimed that when
E[log Z]
 6= 0 and E[Z] > 1 , unless Z is restricted to a discrete set of values of the form
{0} ∪ . . . , z −1 , 1, z, z 2 , . . . ,
ε&0

L(ε) − (E[log Z])+ ∼ CZ ε2|α| ,

(1.3)

where α is the only nonzero real solution of E[Z α ] = 1 and CZ is a positive constant. The
assumption on the values of Z is important: in [12] an example without this property
is given in which CZ must be replaced by a log-periodic function is given. In [14] (1.3)
has been proven under the stronger assumption that the law of Z has a compact support
bounded away from zero and that Z has a C 1 density. Beyond the application, this result
identifies a singular behavior of the Lyapunov exponents and enters the field of inquiry into
the regularity of Lyapunov exponents (e.g. [19, 2, 28]) as a case in which the singularity
is sharply identified.
In this work we focus on E[log Z] = 0: since we assume also Z non trivial, we have that
E[Z α ] = 1 is solved only by α = 0. We will discuss in § 1.4 what is known if E[log Z] 6= 0
and E[Z] ≤ 1: in this case |α| ≥ 1 as opposed to |α| ∈ (0, 1) of (1.3).
The E[log Z] = 0, or α = 0, case has been considered in [24, (4.34) and pp. 1218-1220]
where the authors claim, via formal expansions, that for ε & 0

CZ,1
+ O ε2 ,
(1.4)
L(ε) =
| log ε| + CZ,2
for a specific class of laws of Z (superposition of one (bi-)exponential and one Dirac delta).
Moreover, the 2-scale approach of [12] can be generalized and leads to a prediction in the
spirit of (1.4) for arbitrary distributions of Z [11]. See also the discussion on weak disorder
limits in Section 1.4.
Our aim is obtaining a mathematical control on L(ε) for ε & 0 for a wide class of laws
for Z. We anticipate that our results will hold asymptotically under the assumption that
Z has a suitably regular density and without requiring the support to be bounded (but
both Z and 1/Z are in Lp for a p > 1). As in [14], we will start from the 2-scale idea of
[12]. What is done in [12] is to guess a probability measure – we call it the DH probability
– that should be sufficiently close to the Furstenberg probability (on the projective space,

ISING TRANSFER MATRIX WITH BALANCED DISORDER

3

i.e. simply the sector [0, π/2] because we work with positive matrices in dimension two)
with which one can explicitly express the Lyapunov exponent. The measure essentially
concentrates near 0, but a much finer description is needed. The 2-scale idea is about
gluing together an ε & 0 limit problem near 0 and another limit problem near π/2. Both
problems are non trivial and they require a quantitative understanding of the invariant
measure of a chain that appears in the context of one dimensional Random Walks in
Random Environment (RWRE) [7, 17, 16] and in random affine iterations [6]. Even if the
problems at 0 and π/2 are in a sense dual problems, when E[log Z] 6= 0 they are actually
different in nature: one of the two chains is positive recurrent, the other is transient. In
[14] we first gave a rigorous construction of the DH probability – this is essentially an
asymptotic matching problem – and then we showed that this probability is sufficiently
close to the Furstenberg probability to yield (1.3). In the key second step we exploited a
contraction property, under the random matrix action, of a suitable norm that depends
on a parameter β ∈ (0, α): the contraction factor is precisely E[Z β ] < 1.
When E[log Z] = 0 there are two major changes with respect to the E[log Z] 6= 0 case :
(1) The two limit problems change nature: they become two qualitatively identical problems – if Z has the same law as 1/Z they are the same problem – and they are both
null recurrent. These chains are associated to a balanced RWRE, also known as Sinai
walk, and they are a particular critical random difference equation: these chains do not
have an invariant probability, but they do have a unique σ-finite measure on which we
need a sharp control in order to perform the gluing procedure. The gluing procedure
builds the DH measure, which is a probability measure and is close to the invariant
probability of the chain we are interested in (which is positive recurrent).
(2) E[log Z] = 0 means α = 0, so there is no room to apply the contractive procedure
of [14]. As a matter of fact, this problem is critical in the sense of [1, 4] and the
contraction property exploited in [14] is simply not there. We take this occasion
to stress that E[log Z] = 0 is (rather, is expected to be, since mathematically the
problem is open) also the criticality condition for the two dimensional Ising model
with columnar disorder ([22, 23] and [8, App. A]).
We will nevertheless take the DH path: we are going to explain in § 1.3 how we do it,
notably how we deal with the lack of the contraction property. But first let us introduce
more formally the model and state our main result.
1.2. The main result. For what follows it is more natural to work with zj := log Zj and
let z be a variable that is distributed like the zj 's. On z we assume:
(1) exponential integrability, namely that there exists δ > 0 such that
P (|z| > x) = O (exp (−δx)) .

(1.5)

(2) z has a density ζ which is uniformly θ-Hölder continuous, for a θ ∈ (0, 1], i.e.
sup
x6=y

|ζ(x) − ζ(y)|
< ∞.
|x − y|θ

(1.6)

We remark that these hypotheses imply that ζ is locally bounded and that ζ(x) =
O(exp(−δ 0 θ|x|/(1 + θ)) for every δ 0 ∈ (0, δ) (see Lemma B.1(1)). In particular, ζ ∈ Lp for
every p ∈ [1, ∞].
Of course we assume E[z] = 0.

4

G. GIACOMIN AND R. L. GREENBLATT

Theorem 1.1. There exist three constants κ1 > 0, κ2 ∈ R and η ∈ (0, 1) such that, for
ε → 0,
κ1
+ O (|ε|η ) .
(1.7)
L(ε) =
log(1/|ε|) + κ2
κ1 , κ2 and η depend only on the law of Z. For κj we have a semi-explicit expression,
see (3.12), and for η we give an explicit lower bound. Note that Theorem 1.1 is stated also
for ε < 0: we can pass from ε to −ε by conjugation via the diagonal matrix with (1, −1)
on the diagonal.
Theorem 1.1 shows in particular that L(*) is not Hölder continuous: this is not in
contrast with [19, 2] because we are looking at the neighborhood of random matrices in
which there is no separation between the two Lyapunov exponents (in the Osedelec sense
[3, Ch. IV]). We take this occasion also to point out that with respect to the well known
Halperin example in the context of Anderson localization [28, App. 3], the singularity
identified by Theorem 1.1, as well as (1.3), is present also if ζ is C ∞ or even analytic and
it is not related to low regularity of the law of z, like in the Halperin case.
1.3. A walk through our approach: the DH strategy for α = 0. In order to explain
our approach we make a change of perspective on the problem by setting
k := − log ε ,

(1.8)

so that the original matrix M of (1.1) becomes


1
exp(−k)
,
(1.9)
exp(−k + z) exp(z)
and if we parametrize P (R2 ) ∼
= R with the coordinates (1, exp(x))T we readily see that the
action of the matrix (1.9) is

 −k
e + ex
= z + hk (x) .
(1.10)
x 7→ z + log
1 + ex−k
We observe that hk (*) is odd and x 7→ x − hk (x), which is also odd, is small if x ∈ (−k, k)
and far from the boundary points ±k:






1 + ex−k
−(k−x)
−(k+x)
x − hk (x) = log
=
O
e
+
O
e
,
(1.11)
1 + e−x−k
so hk (x) is very close to x on an interval that approaches R in the limit k → ∞ (see
Fig. 2).
Denote by X = (Xn )n=0,1,... the Markov chain generated by the map (1.10), that is
Xn+1 = zn+1 + hk (Xn ) .

(1.12)

Since the image of hk is (−k, k) the process (Xn ) hardly leaves (−k, k) and when the
process is in (−k, k) and far from the boundaries it is close to being a random walk with
centered increments.
X is an irreducible positive recurrent Markov chain (see the beginning of Appendix A)
and via its invariant probability νk one has that
Z
a.o.n.
L(k) = L(exp(−k))) =
log(1 + exp(−k − x))νk ( dx) ,
(1.13)
R

where the first equality is an abuse of notation (a.o.n.) that we will systematically commit.
The formula (1.13) will be further explained later on (see in particular (2.7)), but it is

ISING TRANSFER MATRIX WITH BALANCED DISORDER

5

what follows by specializing the Furstenberg formula for the top Lyapunov exponent [3,
Th. 3.6] to our context.
Since X is close to being a symmetric random walk in the bulk and because of the strong
containing effect at the boundary it is natural to expect that the invariant probability is
going to be close to the Lebesgue measure times a suitable constant 1/Ck in the bulk,
and this measure should decay quickly outside (−k, k). If this is the case Ck ∼ 2k in the
k → ∞ limit. If we insert this guess into (1.13) we readily see that the leading contribution
comes from x close to −k on the scale k: by this we mean an interval centered in −k of
diverging length o(k). We are therefore interested in focusing on how the process looks
from −k. So we consider (Xn + k) and we readily see that the process that appears for
k → ∞ is
Yn+1 = zn+1 + h (Yn )
with h(y) = y + log(1 + exp(−y)) .
(1.14)
This new Markov chain, of which we will give a detailed treatment, has a strong repulsion
at zero, forcing Y to live most of time in the positive semi-axis. But there is no mechanism
that bounds Y from above: in fact, Y is null recurrent and the non normalizable invariant
measure does approach a multiple of the Lebesgue measure far from the origin. The
random iteration (1.14) is the critical random difference equation that emerges in the
analysis of Sinai walks [1, 4, 6, 29]; consequently, the literature is extensive, notably on
the behavior of the invariant measure of the Y process at +∞. However the focus for us
is twofold:
(1) characterizing the local part of the invariant measure and the behavior at −∞;
(2) obtaining a sharp estimate on the behavior at +∞.
While point (1) is central because it determines the leading behavior of (1.13), point
(2) is as important because the invariant measure of Y just provides the DH guess on
the negative and positive semi-axes, separately (they are essentially symmetric problems).
But we need a sharp asymptotic analysis at +∞ of the two measures, i.e. at the origin
which is very far from both +k and −k, to glue them together. The results available on
this problem are obtained in too general a context and they are too weak for our purposes.

Figure 1. The solid line gives density of the invariant probability for k = 10 and
z = ξ(−1/2 + 3η/10) + (1 − ξ)(η + 1), with ξ a Bernoulli with success probability 2/3 and
η a standard Gaussian, with ξ and η independent. The distribution of z is asymmetric
and bimodal. The densities of the two non normalizable measures that build the DH
measure are (essentially) given by the prolongation of the plateau to the right for one
measure, and to the left for the other one (dashed lines).

Once the DH probability νkDH (explicit!) is built, its closeness to the invariant probability
νk (not explicit!) has to be established. What is directly accessible is the action of the
(one step) Markov Kernel T = Tk on νkDH and we certainly want T νkDH − νkDH small. In

6

G. GIACOMIN AND R. L. GREENBLATT

[14] this closeness is estimated for α > 0 in terms of a norm |||*|||β indexed by β ∈ (0, α).
And the key point is that νkDH − νk β is bounded above by cβ T νkDH − νkDH β , with
cβ = 1/(1 − E[Z β ])+ (if α < 0, i.e. E[Z α ] > 1, we work with E[Z −β ]). |||*|||0 is well defined
too: it is simply the L1 norm of the primitive of * which is in L1 . And in fact |||*|||0 is, or
could be, a good norm for our purposes: the problem is that c0 = +∞.
To get around this problem we take an approach that avoids using contraction properties. In fact we show that the bound holds with c0 = c0 (k) which is O(k 2 ) (up to
logarithmic corrections). The divergence of c0 (k) can be overcome if T νkDH − νkDH 0
decays faster that k −2 . With our hypotheses, this decay is exponential in k.
Remark 1.2. At this stage the dominant role of −k with respect to +k may appear strange.
But this is just an artifact of the choice of (1.13) which is the formula that one obtains
when looking at the exponential growth of the (1, 1) entry of the matrix. But +k takes the
leading stand if we consider the formula stemming out of the exponential growth of the
(2, 2) entry, see (2.7). Of course, the Lyapunov exponent does not depend on the choice
of entry. This is discussed further in Remark 3.2.
1.4. Generalizations, more on the literature and organization of the paper.
The case E[log Z] 6= 0 and E[Z] ≤ 1. Under these hypotheses E[Z α ] = 1 has a solution for
|α| ≥ 1 unless P(Z ∈ (0, 1]) = 1 and this last case can be looked upon as α = ±∞ case
(we assume in any case that Z is not a constant). One can find in [12] an argument in
Pb|α|c
favor of the fact that L(ε) = j=0 Cj ε2j + Cα ε2|α| + o(ε2|α| ), at least for α 6∈ Z. Results
in this direction can be found in [15] that we cite also for more complete results. Let us
point out, however, that identifying the singular behavior, that is the presence of Cα ε2|α| ,
is an open problem and the approach we employ does not seem to be appropriate when
this is a subleading term (i.e., |α| ≥ 1).
Weak disorder limits. The weak disorder limit corresponds in dynamical terms to a very
slow dynamics. This allows to rescale time and the arising dynamics is a two dimensional
system of stochastic ODEs that can be exactly solved. This limit dynamics has the
remarkable property that the Furstenberg probability has an explicit formula which leads
to an expression for the Lyapunov exponent in terms of Bessel functions (to our knowledge
this appeared first in the works of McCoy and Wu [22], but similar computations for related
systems were already in the literature, see[8, 9, 10, 20, 26] for recent works on weak disorder
limits and historical overviews). Deriving the full asymptotic expansion of the Lyapunov
exponent is then a somewhat cumbersome, but straightforward, exercise [8, Prop. 1.3].
Particularly relevant for us is the ε & 0 limit behavior of the Lyapunov exponent for
α = 0 [8, (1.11)] which can be refined to [8, end of Sec. 4]

1
+ O ε2 ,
(1.15)
4 (log(1/ε) − log 2 − γ)
where γ is the Euler-Mascheroni constant and the O(ε2 ), which can be explicitly expressed,
is non zero. Of course the last formula should be compared to our main result (1.7).
As already pointed out in [8], the extremely sharp matching of the finite disorder case
and the infinitesimal disorder case is mathematically quite surprising: not only the leading
order matches, but the structure of the subheading terms is the same (to the extent of
what is known). However there does not appear to be any way to recover finite disorder
results from infinitesimal disorder computations. This is particularly unfortunate for the

ISING TRANSFER MATRIX WITH BALANCED DISORDER

7

d = 2 columnar disorder case [22, 23, 27]: McCoy and Wu used the weak disorder limit to
infer results about the model and setting forth a precise and highly non trivial prediction
for the critical behavior that represents a challenge for mathematicians (see [8, App. A]
and references therein).
What about lower regularity of the distribution of the disorder? Our approach does not
capture only the leading behavior, but also the main subleading correction and gives a
very small error term, much like (1.15) which stems instead from an exact computation.
Clearly in estimating the error term there seems to be a lot of room: for example, it would
be sufficient to show (the one step estimate) that kT νkDH − νkDH k1 = O(k −c ), for a c > 2.
But our approach naturally yields an exponential estimate, see (3.10). This approach (i.e.,
estimating the inverse Laplace transform) strongly relies on an appropriate regularity of ζ,
i.e. asymptotic decay in Fourier space (the imaginary direction for the Laplace transform),
and yields leading term, subleading correction and exponential bounds. It is really not
clear whether our one step bound procedure could go through with less regularity.
As a matter of fact, we are unable to treat the exactly solvable case of [24]: in this case
the Laplace transform of the invariant σ-finite measures are more explicit, but they have
a poor decay in the imaginary direction.
More general matrices. All we did can be greatly generalized (and this discussion applies
to [14] too). Matrices of the form




e
1
εZ 0
Ze εZ
and
,
(1.16)
εZ 00 Z
εZ Z
can be dealt with in a straightforward way under suitable hypotheses: for the first example,
e can be factored
if Ze > 0 too, with no independence requirement with Z, simply because Z
e
out and the key variable becomes Z/Z. For the second example, the analysis does not
contain much novelty if Z, Z 0 and Z 00 are independent and positive and if we require on
the marginal laws of Z 0 and Z 00 the same integrability and regularity we have required for
Z. A complete analysis of two dimensional matrices with positive entries approaching for
ε & 0 a diagonal matrix might be of interest (but it is probably rather cumbersome and
the Ising model motivation is lost). Some higher dimension matrices generalizations are
treated in [15, App. A].
Organization of what follows. In Section 2 we study the main Markov chain X. We state
in Proposition 2.2 the crucial bound that we call reduction to one step estimate. We also
study the auxiliary chain Y that is central in the construction of the DH probability.
In Section 3 we construct the DH probability and perform the one step bound: the
proof of our main result, Theorem 1.1, is in this section (right after Proposition 3.3).
In Section 4 we prove the reduction to one step estimate, i.e. Proposition 2.2.
2. The underlying Markov chains
In general, if Z is a random variable, we use GZ (x) = P(Z > x) and FZ (x) = 1 −
GZ (x). This notation is used also for finite measures μ: Gμ (x) = μ((x, ∞)) and Fμ (x) =
μ((−∞, x]). We will work also with non normalizable (σ-finite) measures, notably with
measures μ such that μ((−∞, x]) < ∞ for every x, and the notation Fμ (x) will be used
also in this case.

8

G. GIACOMIN AND R. L. GREENBLATT

2.1. About the main Markov chain X. We start the analysis of the X chain defined
by (1.12).
Lemma 2.1. We have
Z
T GXn (x) := GXn+1 (x) = Gz (k + x) +

GXn (y)h0k (y)ζ (x − hk (y)) dy ,

(2.1)

FXn (y)h0k (y)ζ (x − hk (y)) dy .

(2.2)

R

and
Z
T FXn (x) := FXn+1 (x) = Fz (−k + x) +
R

Proof. By using that hk (*) is an increasing bijection from R to (−k, k) we see that
GXn+1 (x) = P (zn+1 + hk (Xn ) > x)
= P (x − zn+1 ≤ −k) + P (hk (Xn ) > x − zn+1 ; |x − zn+1 | < k)

= P (z ≥ x + k) + P Xn > h−1
k (x − zn+1 ) ; |x − zn+1 | < k
Z k+x

= Gz (k + x) +
GXn h−1
k (x − z) ζ(z) dz ,

(2.3)

−k+x

and by the change of variable z = x − hk (y) we complete the proof of the first identity.
The proof of the second identity is of course exactly analogous (or can be directly derived
from the first).

The map T can then be extended by linearity to act on G defined by G(x) = Gν+ (x) −
Gν− (x), ν± finite measures (so |G(−∞)| < ∞):
Z k+x

(2.4)
T G(x) = G(−∞)Gz (k + x) +
G h−1
k (x − z) ζ(z) dz .
−k+x

In the applications we consider ν is the difference of two probability measures, so G(−∞) =
0 and T reduces to T0 :
Z
(2.5)
T0 G(x) :=
G(y)h0k (y)ζ (x − hk (y)) dy ,
R

which is well defined also in the slightly different set-up of G ∈ L1 because (1.6) implies
that kζk∞ < ∞.
The following bound will be crucial; Section 4 is devoted to its proof.
Proposition 2.2. There exists C > 0 and k0 such that for every k ≥ k0 and G ∈ L1 (R; R)
∞
X

kT0n Gk1 ≤ k 2 (log k)C kGk1 .

(2.6)

n=0

It is well known that the Lyapunov exponent can be expressed in terms the invariant
probability for the action of the random matrix on the projective circle [3, Ch. 2] and
for two by two matrices with positive entries we can work on (0, π/2) or, considering the
tangent of this angle, on (0, ∞) (see [14, (1.6)] or [12, Sec. 2]). Our parametrization choice
P (R2 ) ∼
= R, recall (1.9)-(1.10), just corresponds to applying the logarithm to the tangent
of the angle and it suffices to apply this change of variables to the expressions in [12, 14]
(which take advantage of the specific form of the matrix under consideration to obtain a

ISING TRANSFER MATRIX WITH BALANCED DISORDER

9

simpler expression than the standard Furstenberg formula): we obtain that, by writing νk
for the invariant measure on R, the Lyapunov exponent L(k) is equal to Lk [Gνk ] with
Z
Z
1
1
Lk [G] :=
G(x) dx =
(1 − G(x)) dx ,
(2.7)
k−x
k+x
R 1+e
R 1+e
which will be used for G(x) = ν((x, ∞)) and ν a probability. We readily see that
|Lk [G1 ] − Lk [G2 ]| ≤ kG1 − G2 k1 .

(2.8)

We thus have the following important corollary to Proposition 2.2:
Corollary 2.3. With C and k0 as in Proposition 2.2, for k ≥ k0 and any probability γ
such that T Gγ − Gγ ∈ L1
|L(k) − Lk [Gγ ]| ≤ kGνk − Gγ k1 ≤ k 2 (log k)C kT Gγ − Gγ k1 .

(2.9)

Proof. Since the Markov chain (Xn ) is irreducible and positive recurrent (see the beginning
of Section A), limn T n Gγ (x) = Gνk (x) for every x ∈ R which is a continuity point of Gνk (*)
(we will see that Gνk (*) is continuous, see Remark 2.8, but at this stage this is not needed
since the set of discontinuities of Gνk (*) is countable, hence of Lebesgue measure zero).
Therefore, by Fatou's Lemma, we have that
kGνk − Gγ k1 ≤ lim inf T N Gγ − Gγ
N

≤

∞
X

1

kT0n (T Gγ − Gγ )k1 ≤ k 2 (log k)C kT Gγ − Gγ k1 , (2.10)

n=0

and the proof is complete by (2.8).



2.2. Looking from the edge: the reduced chain Y . If we sit on −k, that is if we
make it our new origin, in the limit as k → ∞ the Markov chain becomes
Yn+1 = zn+1 + h (Yn )

-��

-��

with h(y) = y + log(1 + exp(−y)) .

��

���

��

���

�

���

-�

�

��

���

��

���

-�
-��

(2.11)

-��

-��

-�

�

��

��

Figure 2. For k = 10: on the left the plot of hk (*) (solid line) and of x 7→ h(x + k) − k
(dashed line); on the right the derivative of the same functions.

Proposition 2.4. The Markov chain Y has a unique invariant measure ν with
ν((−∞, x]) < ∞ for every x ∈ R, but ν(R) = ∞.

10

G. GIACOMIN AND R. L. GREENBLATT

For non-normalizable measures uniqueness is of course meant up to a multiplicative
constant and we note that ν is characterized by
Z
Z Z
g(z)ν( dz) =
g(z + h(y))ζ(z) dz ν( dy) for every measurable g ≥ 0 . (2.12)
The proof of Proposition 2.4 is given for completeness in the Appendix: the Markov
chain Y is a (very) particular case of the critical random difference equation problem,
see [1, 4, 6] and references therein. We sketch a concise proof of Proposition 2.4 because
the context of [1, 4, 6] is really much wider and Proposition 2.4 can be proven with
substantially easier arguments.
We conclude this section by studying the asymptotic behavior of Fν . From the results
of [4, 6] one can extract that Fν (x) = ν((−∞, x]) ∼ cx for x → ∞; however our techniques
require a stronger result, see (2.13) below, which requires substantially more constraining
requirements than the hypotheses in [4, 6].
Here is a preliminary result that allows the use of the Laplace transform. Besides being
an immediate consequence of the analysis in [4, 6], it can also be extracted from [14,
Lemma 3.3]: since context and notations here are slightly different, we give the proof in
the appendix. We make a definite choice of Fν by stipulating that Fν (x0 ) = 1 for an x0
which is (strictly) inside the support of ν.
Lemma 2.5. There exists c > 0 such that Fν (x) ≤ c exp(cx) for every x ≥ x0 .
Here is the main result of this section:
Proposition 2.6. There exist three constants mζ > 0, cζ ∈ R and %ζ ∈ (0, 1) such that
for x → ∞
Fν (x) = mζ x + cζ + O (exp(−%ζ x)) .
(2.13)
0
Moreover, for every δ ∈ (0, δ) (recall (1.5) for δ) we have for x → −∞

Fν (x) = O exp(δ 0 x) .
(2.14)
The constants carry the subscript ζ because they are primarily determined by ζ, but
mζ and cζ depend also on the arbitrary choice of x0 : cζ /mζ and %ζ instead depend only
on ζ.
Proof. We start with (2.13). We are going to use the Laplace transform for (non
R
negative) functions and measures supported on [0, ∞): fb(u) := R exp(−ux)f ( dx) =
R
R∞
b
[0,∞) exp(−ux)f ( dx) and, if f is absolutely continuous, f (u) = 0 exp(−ux)f (x) dx. If
Ff (x) ≤ exp(cx) for x large, then fb(u) is analytic in the complex half plane <u > c and
|fb(u)| ≤ fb(<u).
In order to use the Laplace transform to study the right-tail of ν we introduce Yen :=
log(1 + exp(Yn )); then (Yen ) is a Markov chain with




 

Yen+1 = log 1 + exp zn+1 + Yen
= zn+1 + Yen + log 1 + exp − zn+1 + Yen
,
(2.15)
e
analogous to (2.11). The chains (Yn ) and (Yn ) are equivalent and the invariant measure μ
of (Yen ) is directly related to ν. In particular μ is σ-finite, Fμ (x) = 0 for every x ≤ 0 and,
by Proposition 2.4, Fμ (x) < ∞ for every x. In fact, we have Fν (x) = Fμ (log(1 + exp(x)))

ISING TRANSFER MATRIX WITH BALANCED DISORDER

11

so we can replace Fν (x) with Fμ (x) in (2.13) obtaining an equivalent expression. Note the
following characterization of μ: for measurable g : (0, ∞) → [0, ∞)
Z
Z Z
g dμ =
g (log (1 + exp(−z − x))) ζ( dz)μ( dx) .
(2.16)
By Lemma 2.5 we have that Fbμ (u) is analytic in the domain <u > c > 0. We recall also
that, in the same domain, μ
b(u) = uFbμ (u). We call bμ the optimal value of c:
bμ := inf{u ∈ R : μ
b(u) < ∞} ,

(2.17)

so bμ ≤ c. On the other hand Proposition 2.4 tells us that bμ ≥ 0, because ν(R) =
μ([0, ∞)) = ∞. What we are going to show now is that bμ = 0.
By (2.16) we readily see that
Z Z
(1 + exp(x + y))−u ζ( dx)μ( dy) .
(2.18)
μ
b(u) =
We now use the identity [25, (5.13.1)] (with a = 0, b = u and s = w, a, b and s are the
notations in [25])
Z w0 +i∞
1
−u
(1 + z)
=
Γ(w)Γ(u − w)z −w dw ,
(2.19)
2πiΓ(u) w0 −i∞
which holds if w0 ∈ (0, <u) and for every z ∈ C \ i(−∞, 0). We recall also the generalized
Stirling formula [25, (5.11.9)]:
|y|→∞ √
|Γ(x + iy)| ∼
2π|y|x−1/2 exp(−π|y|/2) ,
(2.20)
for |y| → ∞, uniformly for x in bounded intervals. Therefore by inserting (2.19) into
(2.18) and by using the Fubini-Tonelli Theorem we obtain that for bμ < w0 < <u we have
Z w0 +i∞
1
b μ(w) dw .
μ
b(u) =
Γ(w)Γ(u − w)ζ(w)b
(2.21)
2πiΓ(u) w0 −i∞
The next step is moving w0 to the right of <u: more precisely to (<u, <u + 1). Given the
decay for large imaginary part of the Gamma function, this can be done, but the stripe
{z ∈ C : w0 < <z < w0 + 1} contains the simple pole of Γ(w − u) at w = u. Since the
b μ(u) and by remarking that we are
residue of the integrand (times the prefactor) is −ζ(u)b
performing the integral in the clockwise sense, we obtain that for w0 ∈ (<u, <u + 1)
Z w0 +i∞
1
b
b μ(w) dw ,
μ
b(u) = ζ(u)b
μ(u) +
Γ(w)Γ(u − w)ζ(w)b
(2.22)
2πiΓ(u) w0 −i∞
which we rewrite as
μ
b(u) =

1
b
2πiΓ(u)(1 − ζ(u))

Z

w0 +i∞

b μ(w) dw =: P (u)I(u) ,
Γ(w)Γ(u − w)ζ(w)b

(2.23)

w0 −i∞

with P (*) the pre-factor and I(*) the integral. We remark that this allows to analytically
continue I(u) to smaller values of <u, even down to <u < w0 − 1 and w0 is chosen larger
(but arbitrarily close to bμ ). But P (u) has a singularity at u = 0 as we are going to
explain:
• Γ(u) ∼ 1/u has a simple pole at u = 0 (like for all the negative integers) and these
are the only singularities; moreover the Γ function has no zero.

12

G. GIACOMIN AND R. L. GREENBLATT

R
b
b
• u 7→ 1 − ζ(u)
is an entire function with 1 − ζ(u)
∼ −σ 2 u2 /2, σ 2 := x2 ζ( dx) > 0:
b
so, 1 − ζ(u)
a double zero at the origin. We remark also that (with standard
b
probabilistic notation) ζ(−it)
= φz (t), i.e. it is the characteristic function of a
b
continuous random variable which yields |ζ(it)|
< 1 for t 6= 0 (|φz (t)| = 1 for a
t 6= 0 directly implies that z is discrete). By the Riemann-Lebesgue Lemma we
b
b
have |ζ(it)|
= o(1) for |t| large. Therefore there exists % > 0 such that 1 − ζ(u)
=0
for <u > −% only for u = 0. We assume from now on that % ≤ 1 (in any case, I(*)
has been analytically continued only up to <u > −1).
So P (*) does contribute a simple pole at z = 0: a priori there is still the possibility that
I(0) = 0 making this singularity removable, but in fact this is not possible because we
have already remarked that bμ ≥ 0.
We have therefore proven not only that bμ = 0, but also that μ
b can be meromorphically
extended to {z ∈ C : <z > −% } and the only singularity in this region is the simple pole
at zero
mζ
+ cζ + uH(u) ,
(2.24)
μ
b(u) =
u
with mζ > 0 because μ
b(u) > 0 for u > 0, cζ ∈ R and H(*) is an analytic function on the
domain {z ∈ C : <z > −% }. This of course tells us that also Fbμ can be meromorphically
extended to the same domain and
cζ
mζ
+ H(u) .
(2.25)
Fbμ (u) = 2 +
u
u
We now use the classical Mellin-Bromwich formula for the inverse Laplace transform:
for every b > bμ = 0
Z b+iR
1
Fμ (x) =
lim
Fbμ (u) exp(ux) du .
(2.26)
2πi R→∞ b−iR
We introduce the rectangular contour CR made of the segment of integration in (2.26),
the segment {u : <u = −%ζ and =u ∈ [−R, R]} and by the two segments {u : =u = ±R
and <u ∈ [−%ζ , a]}. The orientation is counter-clockwise and %ζ is chosen in (0, %). The
integration along this contour is equal to the residue of the double pole at the origin:

m
cζ
mζ cζ + mζ x
ζ
+
O(1)
(1 + xu + O(u2 )) = 2 +
+ O(1) , (2.27)
Fbμ (u) exp(ux) =
+
2
u
u
u
u
hence the residue is cζ + mζ x and
Z
1
Fbμ (u) exp(ux) du = cζ + mζ x .
(2.28)
2πi CR
We are therefore left with showing that:
(1) the contribution of the integrals along the segments {u : =u = ±R and <u ∈
[−%ζ , a]} vanishes as R → ∞;
(2) the contribution of the integrals along {u : <u = −%ζ and =u ∈ [−R, R]} is
O(exp(−%ζ x)) for x → ∞.
Both estimates rely on

ISING TRANSFER MATRIX WITH BALANCED DISORDER

13

Lemma 2.7. We consider w0 > 0 and a closed interval I ⊂ (w0 −1/2, w0 ). For |w1 | → ∞
and uniformly in <u ∈ I
Z w0 +i∞


1
|Γ(w)| |Γ(u − w)| dw = O |=u|1/2
(2.29)
|Γ(u)| w0 −i∞
Lemma 2.7 is [14, Lemma 4.4], but the proof can also be obtained directly from (2.20).
We remark that the dominant contribution comes from integrating for w in a neighborhood
of the real axis.
Moreover for both (1) and (2) the crucial formula is (2.23) because, in conjunction with
(2.29) and recalling that Fbμ (u) = μ
b(u)/u, it tells us that


b
|=u|−1/2 ,
(2.30)
|Fbμ (u)| = O ζ(u)
for |=u| large and uniformly for <u in the interval we consider.
Therefore the contour integration in point (1) is O(1/R1/2 ) and for what concerns point
(2) we have that for every R
Z −%ζ +iR
Z ∞
b
Fμ (u) exp(ux) du ≤ exp(−%ζ x)
Fbμ (−%ζ + iy) dy ,
(2.31)
−%ζ −iR

−∞

with the integral in the right-hand side that converges because of (B.1). This completes
the proof of (2.13).
For (2.14) we use (2.12) with g = 1(−∞,x] , that is
Z Z
Fν (x) =
1(−∞,x] (z + h(y))ν( dy)ζ( dz) .
(2.32)
By splitting the integral in the y variable into positive and negative values and by using
that h(y) ≥ 0 for y negative and h(y) ≥ y for y positive we see that
Z Z
Fν (x) ≤ Fν (0)Fζ (x) +
1(−∞,x] (z + y)1(0,∞) (y)ν( dy)ζ( dz)
Z Z
≤ Fν (0)Fζ (x) +
1(−∞,x−z] (y)1(0,∞) (x − z)ν( dy)ζ( dz)
Z
Z ∞
yζ(x − y) dy ,
= Fν (0)Fζ (x) + Fν (x − z)1(0,∞) (x − z)ζ( dz) ≤ Fν (0)Fζ (x) + C
0

(2.33)
where inR the last step C is a positive constant and we have used (2.13). But
x
y) dy = −∞ Fζ (z) dz and (2.14) is established.
The proof of Proposition 2.6 is therefore complete.

R∞
0

yζ(x −


Remark 2.8. (2.32) also characterizes νk if we replace h(*) with hk (*). From this we also
have the characterization
Z
Fνk (x) =
Fζ (x − hk (y))νk ( dy) ,
(2.34)
R

C 1,

from which we see that Fνk is
so νk has a density, because 0 ≤ ζ(x − hk (y)) ≤ kζk∞ .
Similarly ν has a density, but the argument needs to be refined. Equation (2.34) still
holds also without the subscripts k, but because ν(R) = ∞ we need an appropriate upper
bound for ζ(x−h(y)) in order to take the derivative under the integral. Since ν((−∞, x]) <

14

G. GIACOMIN AND R. L. GREENBLATT

∞ for every x, ζ ∈ L∞ reduces the problem to finding and upper bound for y > 0 and
large (of course we can focus on x in a compact set). But this follows because there exists
ε > 0 and z0 such that ζ(z) ≤ exp(εz) for z ≤ z0 . In order to show this and to make the
argument more readable, let us replace ζ(z) with ζ(−z). If ζ(z) ≤ exp(−εz) for z larger
than some value z0 is false, then there exists a sequence (zj ) with limj zj = ∞ and ζ(zj ) >
exp(−εzj ). Uniform θ-Hölder continuity Rimplies that ζ(z) ≥ exp(−εzj )/2 for |z − zj | ≤
∞
((e−εzj )/2C)1/θ . So, possibly for j large, zj /2 ζ(z) dz ≥ exp(−εzj )((e−εzj )/2C)1/θ . Since
the left-hand side decays exponentially, by choosing ε > 0 small we reach a contradiction.
So we have shown that ζ vanishes exponentially fast: using that ν((−∞, x]) has linear
asymptotic growth, Fν ∈ C 1 follows.
3. The DH probability, the one step bound and the proof the main result
We now define the DH probability, i.e. the measure that we expect (and will prove) to
be close to the invariant probability νk . It is built by gluing together the invariant measure
for the edge process at k and for the one at −k. Unless ζ is symmetric, the two edge limit
problems are not the same, since the one on the left involves x 7→ ζ(x) as jump density
probability and the one one the right involves x 7→ ζ(−x). So the cumulative function on
the left (respectively, right) edge will be denoted by FC (*) (respectively, FB (*)). Moreover
we choose to normalize the cumulative functions so that they are, for x → ∞, equivalent
to x. Proposition 2.6 than says that there exist cs ∈ R, s is C or B, and % ∈ (0, 1) such
that for x → ∞
Fs (x) = x + cs + O (exp(−%x)) .
(3.1)
We define the DH probability by giving its integrated tail probability Gk (x) = νkDH ((x, ∞))
(cf. Sec. 1.3):
(
FB (k − x)/Ck
if x ≥ 0 ,
Gk (x) :=
(3.2)
1 − (FC (x + k)/Ck ) if x ≤ 0 ,
where the fact that the definition must be consistent at x = 0 fixes the value of Ck > 0
which, therefore, by (3.1), satisfies
Ck = 2k + cC + cB + O(exp(−%k)) .
We register also that for the cumulative probability Fk (*) = 1 − Gk (*) we have
(
1 − (FB (k − x)/Ck ) if x ≥ 0 ,
Fk (x) :=
FC (x + k)/Ck
if x ≤ 0 .

(3.3)

(3.4)

The next fact is straightforward computation, but it is of course central for us:
Lemma 3.1. For k → ∞
Z
Z
FB (y)
1
FC (y)
1
dy
+
O(exp(−k))
=
dy + O(exp(−k)) .
Lk [Gk ] =
Ck R 1 + ey
Ck R 1 + e y
Proof. By the first equality in (2.7) we have
Z ∞
Z 0
1
FB (k − x)
Gk (x)
Lk [Gk ] =
dx
+
dx .
k−x
k−x
Ck 0 1 + e
−∞ 1 + e

(3.5)

(3.6)

ISING TRANSFER MATRIX WITH BALANCED DISORDER

15

Using Gk (x) ∈ [0, 1] we readily see that the second addendum is smaller than 2 exp(−k).
Moreover
Z ∞
Z
Z
Z ∞
FB (y)
FB (y)
FB (y)
FB (k − x)
dy
−
dy
=
dy + O (k exp(−k)) ,
dx
=
y
y
y
k−x
1
+
e
1
+
e
1
1
+
e
k
R +e
R
0
(3.7)
where in the last step we used (3.1).
Repeating the same argument starting with the second equality in (2.7) gives the same
expression with FC replacing FB .

R
Remark 3.2. (3.5) of course implies (1 + ey )−1 Fs (y) dy does not depend on s. Equivalently, by integration by parts:
Z
Z


−y
log 1 + e−y νC ( dy) .
(3.8)
log 1 + e
νB ( dy) =
R

R

R
One way to see this directly is to observe that by (1.12) and (1.10) we have R (x −
hk (x))νk ( dx) = 0, with νk the invariant probability of the Markov chain defined by (1.12).
But this is equivalent to
Z
Z




log 1 + ek−x νk ( dx) =
log 1 + e−k−x νk ( dx) ,
(3.9)
R

R

and (3.8) follows by exploiting the strong form of convergence – see Remark 3.4 – we have
−1
for 2kνk Θ−1
−k towards νC and for 2kνk Θk towards νB : here Θa (x) := x + a.
Proposition 3.3. There exists η > 0 such that
kT Gk − Gk k1 = O (exp(−ηk)) .

(3.10)

The proof is written so that the constant η in (3.10) can be chosen arbitrarily in
(0, min(δ/2, θ)/2, %), with δ in (1.5), θ in (1.6) and % is the constant %ζ in Proposition 2.6:
a slight modification of the proof yields that we can choose η ∈ (0, min(δ, θ, %)).
Before proving Proposition 3.3 we remark in an official way that it is the last brick
needed for our main result.
Proof of Theorem 1.1. Recall that νk is the invariant probability of the main chain X. It
suffices to choose Gγ = Gk and apply Corollary 2.3. By using Proposition 3.3 we obtain
that


|L(k) − Lk [Gk ]| ≤ kGνk − Gk k1 = O k 2 (log k)C exp(−ηk) .
(3.11)
We then conclude by Lemma 3.1 and by exploiting the expression (3.3) for Ck . We obtain
Z
Z
1
FC (y)
FB (y)
1
1
κ1 =
dy =
dy and κ2 = (cC + cB ) .
(3.12)
y
y
2 R 1+e
2 R 1+e
2

−1
Remark 3.4. We remark that a byproduct of the L1 control in (3.11) is that 2kνk θ−k
converges vaguely for k → ∞ towards νC and 2kνk θk−1 converges vaguely towards νB .
Vague convergence, i.e. with test functions that are compactly supported and C 0 , is an
elegant statement, but (3.11) is much stronger and, in particular, it solves the issue raised
in Remark 3.2.

16

G. GIACOMIN AND R. L. GREENBLATT

Proof of Proposition 3.3. Of course
kT Gk − Gk k1 = (T Gk − Gk )1(−∞,0)

1

+ (T Gk − Gk )1(0,∞)

1

,

(3.13)

and, even if the two terms on the right may be (and typically are) different unless ζ is
symmetric, they can be treated in the same way because both of them can be written as
Z 0 Z
Ak,1 :=
F (y)h0k (y)ζ(x − hk (y)) dy − F (x) dx ,
(3.14)
−∞

R

with F (*) which is Fk (*), given in (3.4), for the first addendum in the right-hand side of
(3.13), and F (*) that is instead replaced by the right-hand side in (3.4) with C and B
exchanged. Arbitrarily, we choose to work with the first addendum and the bound we are
after is achieved in two steps.
The first step in controlling Ak,1 is to remark that we can avoid the nuisance of the fact
that Fk (y) has two different expressions according to the sign of y. Namely, we want to
switch to:
Z 0 Z
1
(3.15)
FC (y + k)h0k (y)ζ(x − hk (y)) dy − FC (x + k) dx .
Ak,2 :=
Ck −∞ R
We can do this because
Z 0 Z
|Ak,2 − Ak,1 | ≤
−∞

0

∞

FC (k + y) + FB (k − y)
− 1 h0k (y)ζ(x − hk (y)) dy dx , (3.16)
Ck

and, by (3.1) and (3.3), for y ≥ 0 we have
FC (k + y) + FB (k − y)
C
−1 ≤
×
Ck
Ck

(
exp(−%(k − y))
y−k+1

if y ∈ [0, k] ,
if y > k ,

for a suitably chosen C > 0. So |Ak,2 − Ak,1 | is bounded above by
Z k
Z ∞
exp(−%(k − y))Fζ (−hk (y))h0k (y) dy +
(y − k + 1)Fζ (−hk (y))h0k (y) dy ,
0

(3.17)

(3.18)

k

times C/Ck . So, by keeping in mind that Fζ (−x) = O(exp(−δx), that h0k (y) = O(exp(−(y−
k))) for y−k → ∞, that h0k (y) ∈ (0, 1) and by remarking that y−hk (y) ≤ log 2 for y ∈ [0, k],
with adequate choice of C
Z k

Z ∞


C
−%(k−y) −δy
−δk
−(y−k)
|Ak,2 − Ak,1 | ≤
e
e
dy + e
(y − k + 1)e
dy = O e−(δ∧%)k ,
Ck
0
k
(3.19)
with δ ∧ % = min(δ, %).
The second step is the control of Ak,2 . For this we introduce
hk (x) := h(x + k) − k

(3.20)

and remark that
(
O(exp(x − k)) for x − k → −∞ ,
0 ≤ hk (x) − hk (x) = log(1 + exp(x − k)) =
(3.21)
O(x − k)
for x − k → +∞ ,
(
O(exp(x − k)) for x − k → −∞ ,
0
0
0 ≤ hk (x) − hk (x) = 1/(1 + exp(k − x)) =
(3.22)
≤1
for every x and k .

ISING TRANSFER MATRIX WITH BALANCED DISORDER

17

Moreover we stress that h0k (x) and h0k (x) are in (0, 1) for every x and, with reference
to Figure 2, the two functions almost coincide for x  k. They start to differ when x
approaches k from the left because h0k (x) keeps being very close to one and hk (x) ∼ x.
Since by the characterizing property of FC (*)
Z
Z
0
FC (y + k)hk (y)ζ(x − hk (y)) dy − FC (x + k) =
FC (y)h0 (y)ζ(x − h(y)) dy − FC (x) = 0,
R

R

(3.23)

we have
1
=
Ck

Ak,2

0

Z

Z

−∞


FC (y + k) h0k (y)ζ(x − hk (y)) − h0k (y)ζ(x − hk (y)) dy dx , (3.24)

R

We split the integral over R according to y < −3k/2, y ∈ [−3k/2, k/2] and y > k/2. We
have
0

Z

Z

−∞

∞

k/2


FC (y + k) h0k (y)ζ(x − hk (y)) − h0k (y)ζ(x − hk (y)) dy dx ≤
0

Z

Z

∞

C
−∞
∞

Z
C


(y + k) h0k (y)ζ(x − hk (y)) + ζ(x − hk (y)) dy dx =

k/2


(y+k) h0k (y)Fζ (−hk (y)) + Fζ (−hk (y)) dy ≤ C 0

Z

∞



δ
y e(k−y)∧0− 2 k + e−δy dy,

k/2

k/2

(3.25)
which is O(k 2 exp(−δk/2)). Similarly with δ 0 ∈ (0, δ) (recall (2.14))
0

Z

Z

−∞

−3k/2

−∞
−3k/2

Z


FC (y + k) h0k (y)ζ(x − hk (y)) − h0k (y)ζ(x − hk (y)) dy dx ≤
0

eδ (y+k) ey+k (Fζ (−hk (y)) + Fζ (−hk (y)) dy ≤ 2C

C
−∞

Z

−3k/2

0

eδ (y+k) ey+k dy ,

−∞

(3.26)
which is O(exp(−k(1 + δ 0 )/2)). We have therefore obtained that Ak,2 is equal to
1
Ck

Z

0

Z

−∞

k/2

−3k/2




δ0
FC (y + k) h0k (y)ζ(x − hk (y)) − h0k (y)ζ(x − hk (y)) dy dx + O ke− 2 k .
(3.27)

For the last estimate we use
Z

k/2

−3k/2

Z


FC (y + k) h0k (y)ζ(x − hk (y)) − h0k (y)ζ(x − hk (y)) dy ≤

k/2


FC (y+k) h0k (y) − h0k (y) ζ(x − hk (y)) + h0k (y) |ζ(x − hk (y)) − ζ(x − hk (y))| dy ,
{z
}
|
{z
} |
−3k/2
1

2

(3.28)

18

G. GIACOMIN AND R. L. GREENBLATT

and we write the last line as T1 (x) + T2 (x) in the obvious way. Since FC (y + k) ≤ 2k for
y ≤ k/2
Z k/2
Z 0
FC (y + k) h0k (y) − h0k (y) Fζ (−hk (y)) dy
T1 (x) dx =
−3k/2

−∞

k/2

Z
≤ 2k

−3k/2

h0k (y) − h0k (y) dy ≤ Ck

Z

k/2

ey−k dy ≤ O (k exp(−k/2)) .

−3k/2

(3.29)
Moreover
Z
Z 0
T2 (x) dx =

0

−∞

−∞

Z

k/2

Z

0

−3k/2

Z

k/2

≤
−2k

−3k/2

Z

FC (y + k)Cζ |hk (y)) − hk (y)|θ dy dx

k/2

+C
−3k/2
0

FC (y + k)h0k (y) |ζ(x − hk (y)) − ζ(x − hk (y))| dy dx

FC (y + k) (Fζ (−2k − hk (y)) + Fζ (−2k − hk (y))) dy

k/2


 
 
θ
2
≤Ck
dx
exp(θ(y − k)) dy + 8Ck Fζ (−k) = O k exp −
∧δ k
,
2
−2k
−3k/2
(3.30)
0

Z

Z

2

where in the second step we have exploited the regularity of ζ for x ∈ (−2k, 0) – the
constant Cζ is the left-hand side in (1.6) – and, for smaller x's we have just bounded
|ζ(x − hk (y)) − ζ(x − hk (y))| with ζ(x − hk (y)) + ζ(x − hk (y)) and we have performed the
integral over x, using once again FC (y + k) ≤ 2k in the range we consider.
The proof of Proposition 3.3 is therefore complete.

4. Diffusion estimate: the proof of Proposition 2.2
In this section we essentially estimate the speed of convergence to stationarity of the
chain X. The process is essentially a symmetric random walk far from the boundary, but
the positive recurrent character crucially depends on the visits to the boundary. By a
preliminary manipulation we reduce the estimate we need to estimating the expectation
of the time of first exit from (−k, k) and this is achieved by diffusion approximation on
(−k + 2 log log k, k − 2 log log k) and by a rough estimate on the remaining portions of
length 2 log log k.
Proof of Proposition 2.2. In order to work with more explicit constants we give the proof
under the assumption that both P(z > log 2) > 0 and P(z < − log 2) > 0: we explain in
Remark 4.2 how the proof can be easily generalized.
Our proof involves two auxiliary Markov chains, closely related to X, which we now
introduce.
The first one is X ∼ = (Xn∼ )n∈N0 with state space R ∪ {C}. For X ∼ the state C is
∼ = C for every m larger than n. On
absorbing (cemetery), that is Xn∼ = C implies Xm
∼
∼
the other hand if Xn = y ∈ R the probability that Xn+1
= C is 1 − h0k (y). Therefore
if Xn∼ = y ∈ R the chain survives with probability h0k (y) and, in this case, it jumps to
x with transition density ζ(x − hk (y)). Note that h0k (x) is even, decreasing for x > 0,

ISING TRANSFER MATRIX WITH BALANCED DISORDER

19

h0k (x) = O(exp(−k + x)) + O(exp(−k − x)) and
h0k (±k) =

1
1
1
< ,
− 2k
2 e +1
2

(4.1)

so h0k (x) < 1/2 if |x| ≥ k, and h0k (x) ≤ h0k (0) = 1 for every x.
By the observations we just made it is easily seen that the chain X ∼ can be coupled
with a simpler chain X ≈ = (Xn≈ )n∈N0 that has a smaller death probability. The transition
kernel for X ≈ from y ∈ (−k, k) to x ∈ R is given by ζ(x − hk (y)) and it is therefore a
probability: transitions from (−k, k) to C are forbidden. On the other hand the kernel from
y ∈ R \ (−k, k) to x ∈ R is ζ(x − hk (y))/2 and the probability of going from y ∈ R \ (−k, k)
to C is 1/2. C is absorbing also for this chain.
Of course these two chains are dominated by the even simpler chain X = (Xn )n∈N0 for
which the death probability is zero: its natural state space is just R, but of course we can
keep R ∪ {C} as state space and the absorbing state does not communicate with R, and
the transition kernel from y to x is the probability density ζ(x − hk (y)). In fact X is just
the main Markov chain we consider in this walk, i.e. (1.12).
We denote by Py the (joint) law of the chains X, X ∼ and X ≈ and the index y denotes
the (common) initial condition.
Let us assume that G(*) is non negative (the general result is recovered by linearity).
Of course
Z
Z
T0 G(x) dx =
h0k (y)ζ(x − hk (y))G(y) dy ,
(4.2)
R

R

and a direct computation shows that for n = 2, 3, . . .


Z
Z
n−1
Y
T0n G(x) dx =
h0k (y1 ) 
ζ(yj − hk (yj+1 ))h0k (yj+1 ) dy1 . . . dyn .
Rn

R

(4.3)

j=1

Therefore for n ∈ N we have the probabilistic representation and bound:
Z
Z
Z
n
∼
T0 G(x) dx =
G(y)Py (Xn 6= C) dy ≤
G(y)Py (Xn≈ 6= C) dy
R
R
ZR
h
i
=
G(y)Ey 2−H(n) dy ,

(4.4)

R

with H(n) := |{j = 0, 1, . . . , n : |Xj | ≥ k}|. Let us set τ0H = 0 and, for j ∈ N, τjH :=
inf{n ∈ N : H(n) = j}, so τjH is the time of the j th entry of X into R \ (−k, k). With
these notations, by the Fubini-Tonelli Theorem, we have
"∞
#
Z
∞ Z
h
i
X
X
−H(n)
−H(n)
G(y)Ey 2
dy =
G(y)Ey
2
dy ,
(4.5)
n=0 R

R

n=0

and, again by the Fubini-Tonelli Theorem and using the definitions:
"∞
#
"∞
#
∞
∞
X
X
X
X
 H

 
−H(n)
−j
Ey
2
=
2 Ey
1H(n)=j =
2−j Ey τj+1
− τjH ≤ 2 sup Ey τ1H .
n=0

j=0

n=0

j=0

y

(4.6)

20

G. GIACOMIN AND R. L. GREENBLATT

For every r > 0 we set τr := inf{n ≥ 0 : Xn ∈ R \ (−r, r)}. So τ1H = τk and the steps we
have just developed yield that for G(*) ≥ 0
Z
∞ Z
X
(4.7)
T0n G(x) dx ≤ 2 sup Ey [τk ] G(x) dx ,
y

n=0 R

R

which readily implies for G ∈ L1
∞
X

kT0n Gk1

≤

∞ Z
X

T0n |G(x)| dx ≤ 2 sup Ey [τk ] kGk1 .

n=0 R

n=0

(4.8)

y

We are going to show:
Lemma 4.1. There exists C > 0 and k0 > 0 such that for every k ≥ k0
sup Ex [τk ] ≤
x

1 2
k (log k)C .
2

Lemma 4.1 and (4.8) imply (2.6) and Proposition 2.2 is proven.

(4.9)


Remark 4.2. The purpose of the assumption that P(z > log 2) and P(z < − log 2) are
non zero is to guarantee that the chain X can exit (−k, k). Under this assumption in fact
P(Xn+1 ≥ k|Xn = x) > 0 for x sufficiently close to k (and analogous statement at −k):
note in fact that hk (±k) = ±(k − log 2 + log(1 + e−2k ). If either P(z > log 2) = 0 or
P(z < − log 2) = 0, the definitions of the chain X ≈ should be modified by stipulating that
the chain may step to the absorbing state C only when |x| ≥ k − c, with c > 0 such that
(k − c) − hk (k − c)(> 0) and (k − c) + hk (−(k − c))(< 0) are in the interior of the support
of ζ. Of course in this case the probability of jumping to C is no longer 1/2: since
h0k (±(k − c)) = 1 −

1
1
1
−
< 1− c
,
ec + 1 e2k−c + 1
e +1

(4.10)

we can choose this probability equal to 1 − 1/(ec + 1). These changes do not affect Proposition 4.9 because they only affect the constants C and k0 , whose precise values are unimportant.

R
Proof of Lemma 4.1. We recall that ζ is centered and σ 2 = x2 ζ(x) dx > 0. We claim
that it suffices to show that for k ≥ k0

inf Px τk ≤ k 2 ≥ 2(log k)−C =: pk .
(4.11)
x∈R

In fact (4.11) implies that for j ∈ N and every x


Px τk > (j + 1)k 2 ≤ Px τk > jk 2 (1 − pk ) ,
and therefore (also for j = 0)
l τ m


k
Px
>
j
= Px τk > jk 2 ≤ (1 − pk )j ,
2
k

(4.12)

(4.13)

ISING TRANSFER MATRIX WITH BALANCED DISORDER

21

which implies
Ex [τk ] ≤ k 2 Ex

hl τ mi
k
k2

= k2

∞
X

Px τk > jk 2



j=0

≤ k2

∞
X

(1 − pk )j =

j=0

1
k2
= k 2 (log k)C . (4.14)
pk
2

Let us prove (4.11): it suffices to prove that both


inf Px τk ≤ k 2 , Xτk > 0 and inf Px τk ≤ k 2 , Xτk < 0
x≥0

x≤0

(4.15)

are bounded below by 2(log k)−C . These two estimates are equivalent up to the fact that
ζ is not symmetric (but this affects only the choice of the constants and in an obvious
way). So we just treat the first expression, i.e. when the chain exits on the right.
We take this occasion also to remark that if we take two chains X and X 0 , with X0 = x
and X00 = x0 , both satisfying (1.12), coupled by the common randomness (zn ), we have
0
Xn+1 − Xn+1
= hk (Xn ) − hk (Xn0 ), with hk (*) increasing, so the sign of Xn − Xn0 does not
depend on n. Therefore P0 τk ≤ k 2 , Xτk > 0 ≥ Px0 τk ≤ k 2 , Xτk > 0 for x ≥ x0 . So it
suffices to check that

P0 τk ≤ k 2 , Xτk > 0 ≥ 2(log k)−C .
(4.16)
Moreover, what the process does outside (−k, k) is irrelevant, so, when |Xn | ≥ k, we
replace hk (Xn ) in (1.12) with Xn . This means that hk (x) is redefined as hk (x)1(−k,k) (x) +
x1R\(−k,k) (x). Note that this corresponds to choosing a different increasing function hk (*).
To show that (4.16) holds we choose δ > 0 so that inf x∈[0,k) Px (X1 − X0 ≥ δ| X0 = x) =
pδ > 0. Note that this infimum is achieved at x = k and, since we have trivialized the drift
outside (−k, k), the infimum over x ∈ [0, k) can be taken over x ∈ [0, ∞). We introduce
also the stopping time
τk0 := inf {n ≥ τk−2 log k : |Xn − (k − 2 log k)| ≥ 2 log k − 2 log log k} ,

(4.17)

and we remark that


P0 τk ≤ k 2 , Xτk > 0 ≥ P0 τk−2 log k ≤ k 2 /2 and Xτk−2 log k ≥ k − 2 log k ,
τk0 − τk−2 log k ≤ (2 log k)2 and Xτk0 ≥ k − 2 log log k ,

Xτk−2 log log k +j − Xτk−2 log log k+j−1 ≥ δ for j = 1, 2, . . . , d(2/δ) log log ke . (4.18)
Let us read this complicated-looking expression: each item in the list that follows corresponds to the three lines of the right-hand side of (4.18).
(1) We ask that in less then k 2 /2 steps the chain exits the interval (−k + 2 log k, k −
2 log k) from the right: we will show that, by Brownian motion approximation,
this probability is positive uniformly in k. For this to work we need the drift to be
negligible on the relevant (diffusive) time scale, which given the form of hk requires
us to stop sufficiently far (i.e. 2 log k) from the boundary.
(2) We ask that the chain exits on the right the interval of length 2(2 log k −2 log log k)
centered in k−2 log k in 4(log k)2 steps or less. Note that in the unlikely (favorable)
event that τk0 = τk−2 log k then Xτk−2 log k ≥ k − 2 log log k is verified, so we can
assume Xτk−2 log ∈ [k − 2 log k, k − 2 log log k) and again we can treat this step by
Brownian motion approximation. The drift is larger, but still negligible because

22

G. GIACOMIN AND R. L. GREENBLATT

the time span is much shorter than what we considered before. This will cost a
probability factor bounded away from zero, like for (1). We also remark that, on
this event, τk0 = τk−2 log log k .
(3) Once the chain is at a distance at most 2 log log k from the right boundary point
k, we ask that it goes straight out of the domain by making steps of length at
least δ > 0: this will cost a factor smaller than one (but bounded below by
pδ > 0) at each step. Since the number of steps is proportional to log log k this
costs a probability factor that vanishes like a power of 1/ log k. Note that we have
modified the chain outside of (−k, k), by removing the boundary repulsion, so that
this ballistic strategy can be performed also once the chain is beyond k.
Before starting the lower bound estimate for the right-hand side of (4.18) we anticipate
that, by the Strong Markov Property, we will be able to perform three separate estimates:
each estimate corresponding to one of the three items in the list.
Corresponding to the first item we aim at showing that

P0 τk−2 log k ≤ k 2 /2 and Xτk−2 log k ≥ k − 2 log k
(4.19)
is bounded away from zero, uniformly in k ≥ k0 . The event does not change if we replace
hk (x) with x for |x| ≥ k − 2 log k in defining the Markov chain X: so we will do so.
Moreover we define the process Xt,k := Xtk2 /k if tk 2 is an integer and otherwise we define
Xt,k by affine interpolation so the trajectory is continuous. We are going to show, via
a diffusion approximation, that the sequence of processes (X*,k )k∈N , with X*,k a random
element of C 0 ([0, T );
R R) (with T > 0 arbitrary), converges in law to a Brownian motion
with variance σ 2 = x2 ζ(x) dx. This is formally stated in the following Lemma.
Lemma 4.3. For every continuous and bounded function F from C 0 ([0, T ); R) to R we
have that
lim |E0 [F (X*,k )] − E0 [F (B*σ )]| = 0 ,
(4.20)
k→∞

where, under P0 , B*σ = σB* with B* a standard Brownian motion.
Proof of Lemma 4.3. We apply the diffusion approximation procedure detailed in [31, pp.
266–272]. By [31, Assumptions (2.4)-(2.5)-(2.6), Theorem 11.2.3] it sufficed to check three
conditions:
(1) The (vanishing) drift condition:
Z
lim
sup
k
(x − y)ζ(x − hk (y)) dx = 0 .
k→∞ y: |y|≤k−2 log k

(4.21)

Note that we can restrict to |x| ≤ k − 2 log k because the process has been modified
so to be centered outside of this interval. (4.21) holds because, using first
R
xζ(x) dx = 0 and then (1.11), we obtain
Z
1
(4.22)
(x − y)ζ(x − hk (y)) dx = |hk (y) − y| ≤ exp(−2 log k) = 2 ,
k
and (4.21) is verified.
(2) The control of the variance:
Z
lim
sup
(x − y)2 ζ(x − hk (y)) dx − σ 2 = 0 ,
k→∞ y: |y|≤k−2 log k

(4.23)

ISING TRANSFER MATRIX WITH BALANCED DISORDER

23

and this is a direct consequence of the fact that, in the range of y that we consider,
|y − hk (y)| is bounded by the boundary case y = k − 2 log k and the resulting
expression vanishes as k → ∞.
(3) The control on large jumps: for every ε > 0

lim k sup P |z1 + (hk (x) − x) 1[−(k−2 log k),+(k−2 log k)] (x)| > εk = 0 ,
(4.24)
k→∞

x∈R

which is (largely) verified because hk (x) − x = O(1/k 2 ) for |x| ≤ k − 2 log k and
because z1 has finite exponential moments.
This completes the proof of Lemma 4.3.



Lemma 4.3 implies
 k→∞

P0 τk−2 log k ≤ k 2 /2 and Xτk−2 log k ≥ k − 2 log k −→ P0 tσ ≤ 1/2, Btσσ = 1 =: pσ ,
(4.25)
and tσ is the hitting time of (−1, 1){ by B*σ . Hence for k sufficiently large

pσ
.
P0 τk−2 log k ≤ k 2 /2 and Xτk−2 log k ≥ k − 2 log k ≥
2

(4.26)

We now restart from time τk−2 log k and use the Strong Markov Property. Xτk−2 log k ∈
[k − 2 log k, k − 2 log log k) and we apply a Brownian motion approximation to the chain
(Xτk−2 log k+j )j=0,1,... on the time scale (log k)2 : the steps are analogous to the ones of the
previous step. Also in this case it is more concise to resort to the comparison argument
explained right before (4.16), so that the starting point of our chain can and will be
chosen equal to k − 2 log k. Therefore, by recentering, i.e. by translating the system so
that k − 2 log k becomes the origin (of course we have to shift accordingly the repulsion),
it suffices to show that


lim P0 τ2 log k−2 log log k ≤ (2 log k)2 , Xτ2 log k−2 log log k > 0 = P0 τ ≤ tσ , Btσσ = 1 > 0 ,
k→∞

(4.27)
But (4.27) is just a close analog of Lemma 4.3 and the key point is that the proof is
essentially the same up to replacing k with log k: note notably that now the repulsion
is much stronger, O(exp(−2 log log k)) = O(1/(log k)2 ) uniformly in the interval, but this
yields a negligible drift because time is O(log k).
Therefore we arrive at: there exists p0σ > 0 and k0 such that

inf P0 τk−2 log k ≤ k 2 /2 and Xτk−2 log k ≥ k − 2 log k ,
k≥k0

τk0 − τk−2 log k ≤ (2 log k)2 and Xτk0 ≥ k − 2 log log k ≥ p0σ (4.28)
Now we apply again the Strong Markov Property using the fact that Xτk −2 log log k ≥
k − 2 log log k and the last estimate is just a product estimate that leads to (recall (4.18))

d(2/δ) log log ke
P0 τk ≤ k 2 , Xτk ≥ k ≥ p0σ pδ
≥

1
,
(log k)C

(4.29)

with C = (3/δ) log(1/pδ ) and k sufficiently large. This completes the proof of Lemma 4.1.


24

G. GIACOMIN AND R. L. GREENBLATT

Appendix A. Complementary results on the auxiliary chains
The analysis of the basic properties of the X chain can of course be found for example
in the first two chapters of [3]: by basic properties we mean existence and uniqueness of an
invariant probability, that follow from irreducibility and positive recurrence. We choose
not to discuss these issues in detail because we do give below more details about the Y
chain that is a bit more delicate to deal with – in particular, the invariant measure is not
normalizable – and because for the Y chain we need a few results that depend on our
restricted framework. For Y we are going to exploit [21] and we will in particularly give
a Lyapunov function to show recurrence: for X we have positive recurrence because of
the rather evident confinement properties that can be made explicit using for example the
Lyapunov function x 7→ (|x| − k)2+ ) and [21, Th. 11.3.4].
Proof of Proposition 2.4. We distinguish here among the case in which the support of ζ is
bounded away from −∞ and when it is not.
In the first case it is straightforward to see that if −c = inf{x : ζ(x) > 0} < 0 then
the process eventually enters (− log(exp(c) − 1), ∞) and does not leave this set: in fact
− log(exp(c) − 1) is the fixed point of y 7→ h(y) − c. Moreover Y is irreducible, more
precisely ψ-irreducible in the terminology for example of [21], with ψ any probability with
a density that is positive on (− log(exp(c) − 1), ∞) and zero on the complement: this is a
direct consequence of the fact that z has a density, of (2.11) and of the fact that −c is the
left edge of the support of the transition probability. Recurrence of Y follows for example
by applying the criterion in [18, Th. 3.1], or we can apply [21, Th. 8.0.2] with a Lyapunov
function equal to log((x − M )+ + 1), with M = M (σ) > 0 suitably chosen.
If the support of ζ is not bounded below, then the process is still ψ-irreducible and
this time ψ is any probability with positive density. Recurrence can be established in the
same way: the repulsion from the left is very strong. An explicit Lyapunov function is
x2 1(−∞,0)(x) + 1[0,∞) (x) log((x − M )+ + 1) [21, Th. 8.0.2] .
So in both cases ν exists, it is σ-finite and it is unique [21, Th. 10.4.9]). Of course ν is
characterized by (2.12). In particular for every bounded Borel set B

Z Z
ν(B) =
ζ(z − h(y))ν( dy) dz ,
(A.1)
B

R

and we use this formula to show that ν(B) < ∞ for every bounded Borel set. To show
this we use the fact that ζ ≥ ε1(a,b) for suitably chosen positive constants ε, a and b.
Assume that there exists a bounded Borel set B1 with ν(B1 ) = ∞. Then there exists
x
R 0 ∈ B1 such that ν((x0 − 1/n, x0 + 1/n)) = ∞ for every n and this directly yields
R ζ(z − h(y))ν( dy) = ∞ for every z such that z + h(x0 ) ∈ (a, b). But in this case, by
(A.1), ν(B) = ∞ for every B ⊂ (a − h(x0 ), b − h(x0 )) of positive Lebesgue measure, which
is impossible because ν is σ-additive.
To establish ν(R) = ∞ we suppose that ν(R) < ∞, and we assume that ν is a probability. In this case we remark that the process defined recursively by
√ Sn
√ Sn+1 := zn+1 +
and by S0 := Y0 satisfies Sn ≤ Yn for every n. Hence P(Sn ≥ n) ≤ P(Yn ≥ n).
But the Central Limit Theorem implies that the left-hand side converges as n → ∞ to a
positive number, while the right-hand side vanishes in the same limit because (Yn ) is tight
by the Ergodic Theorem applied to our (supposedly) positive recurrent process. Hence
ν(R) = ∞.
For the left tail property we start by remarking that using (2.12) with g = 1(a,b+1) ,
a < b, and restricting the integral in the right-hand side to y ∈ (−∞, log(e − 1)), i.e.

ISING TRANSFER MATRIX WITH BALANCED DISORDER

25

h(y) ∈ (0, 1), and to x ∈ (a, b)
ν((a, b + 1)) = ν × ζ ({(y, z) : a < z + h(y) < b + 1}) ≥ ν ({y : h(y) ∈ (0, 1)}) ζ((a, b)) ,
(A.2)
that is ν((a, b+1)) ≥ ν((−∞, log(e−1)))ζ((a, b)). By choosing a and b such that ζ((a, b)) >
0 we see that ν((−∞, log(e − 1))) < ∞. Therefore ν((−∞, x]) < ∞ for every x and the
proof is complete.

Proof of Lemma 2.5. the measure. By setting g = 1(−∞,x] in (2.12) we see that for every
x∈R
Z
Fν (x) =
Fζ (x − h(y))ν( dy) .
(A.3)
Since both h(*) and Fζ (*) are non decreasing we have that y 7→ Fζ (x − h(y)) is non
increasing. So, by (A.3), we have that for every x and every z
Z z
Fν (x) ≥
Fζ (x − h(y))ν( dy) ≥ Fζ (x − h(z))Fν (z) .
(A.4)
−∞

Set −e
c := inf{y : Fζ (y) > 0} ∈ [−∞, 0): one extracts form (1.14) that inf{y : Fν (y) >
0} = − log(exp(e
c) − 1). Now consider the sequence (xj ), x0 the value we have chosen
for Fν (x0 ) = 1, and, for j ∈ N, xj = xj−1 + ρ with ρ > 0 such that x0 − h(x1 ) =
x0 − h(x0 + ρ) > −e
c, so that q := Fζ (x0 − h(x1 )) > 0. Such a choice of ρ > 0 is possible
because x0 > − log(exp(e
c) − 1) so
x0 − h(x0 ) = − log(1 + exp(−x0 )) > −e
c.

(A.5)

Note moreover that for j ∈ N we have xj − h(xj+1 ) = −ρ − log(1 + exp(−xj+1 )) ≥
−ρ − log(1 + exp(−x1 )) = x0 − h(x1 ). So Fζ (xj − h(xj+1 )) ≥ q for every j. Therefore from
(A.4) we infer that
1
(A.6)
Fν (xj+1 ) ≤ j .
q
Since Fν is non decreasing this yields the claim.

Appendix B. About the Fourier transform of ζ
b
With our Laplace transform notation ζ(−it)
= φz (t) is the characteristic function of z,
that is the Fourier transform of the law of z. In this Appendix we show how our hypotheses
(1.5) and (1.6) on the distribution ζ imply that the characteristic function satisfies the
bound
Z
|φz (t)|
dt < ∞ ,
(B.1)
1/2
R (1 + |t|)
and Lemma B.1 directly yields (B.1), thanks to our hypotheses (1.5) and (1.6). We use
R
fˆ(t) := fb(−it) = R f (x) exp(itx) dx.
θ
Lemma B.1. Assume that there exists θ > 0 such
R that |f (x) − f (y)| ≤ |x − y| for every
x 6= y ∈ R and that there exists c > 0 such that R |f (x)| exp(c|x|) dx < ∞. Then
(1) fR (x) = O(exp(−b|x|)) with b = cθ/(1 − θ);
(2) R (|fˆ(t)|/(1 + |t|)1/2 ) dt < ∞.

Proof. It suffices to consider the case x → ∞. We proceed by contradiction: assume
that there exists a sequence of positive numbers (xn ) with limn xn = +∞ and |f (xn )| ≥

26

G. GIACOMIN AND R. L. GREENBLATT

2 exp(−bxn ) for every n. Then for every x ∈ [xn , xn + exp(−bxn /θ)] we have |f (x)| ≥
exp(−bxn ). Therefore
Z xn +exp(−bxn /θ)
|f (x)| exp(cx) dx ≥ exp(−bxn /θ) exp(−bxn ) exp(cxn ) = 1 ,
(B.2)
xn

R
which is impossible because R |f (x)| exp(c|x|) dx < ∞. Therefore (1) is established.
For (2) we start by remarking that (1) implies that the continuous
function f is in Lp
R
2
for every p ≥ 1, in particular for p = 2. Letting (ω2 (h)) := R (f (x + h) − f (x))2 dx, for
h ∈ [−1/2, 1/2] and K > 2 we have
Z ∞
Z K
2
2
exp(−2bx) dx
(f (x + h) − f (x)) dx + C
(ω2 (h)) ≤
(B.3)
K−h
−K
2θ
0
≤ 2K|h| + C exp(−2bK) ,
so if we choose K = (θ/b) log(1/|h|) – if (θ/b) log(2) ≤ 2 we just choose a smaller value
for b – we have that there exists C > 0 such that for |h| ≤ 1/2.
p
ω2 (h) ≤ C|h|θ log(1/|h|) ;
(B.4)
√
also evidently suph ω2 (h) ≤ 2kf k2 . These estimates imply that for every α ∈ (0, θ)
Z
(ω2 (h))2
dh < ∞ ,
(B.5)
1+2α
R |h|
which implies [30, Ch. 5: (46) and paragraph after (47)] that
Z
2
fˆ(t) (1 + |t|)2α dt < ∞ .

(B.6)

R

Since
Z
R

!2
Z
Z
fˆ(t)
1
2
2α
ˆ
f (t) (1 + |t|) dt
dt
≤
dt ,
1+2α
1/2
(1 + |t|)
R
R (1 + |t|)

the proof of part (2) is complete.

(B.7)


Acknowledgments
We are very grateful to Leonardo Colzani for the proof of Lemma B.1 and to
Bernard Derrida for several enlightening discussions. G.G. is partially supported by
ANR–19–CE40–0023 (PERISTOCH). The work of R.L.G. was carried out in the Department of Mathematics and Physics of Roma Tre University (Italy) and supported by
the European Research Council (ERC) under the European Union's Horizon 2020 research
and innovation programme (ERC CoG UniCoSM, grant agreement No. 724939 and ERC
StG MaMBoQ, grant agreement No. 802901).
References
[1] M. Babillot, P. Bougerol and L. Elie, The random difference equation Xn = An Xn−1 + Bn in the
critical case, Ann. Probab. 25 (1997), 478-493.
[2] A. Baraviera and P. Duarte, Approximating Lyapunov exponents and stationary measures, J. Dynam.
Differential Equations 31 (2019), 25-48.
[3] P. Bougerol and J. Lacroix, Products of random matrices with applications to Schrödinger operators.
Progress in Probability and Statistics 8, Birkhäuser Boston, Inc., Boston, MA, 1985.
[4] S. Brofferio, D. Buraczewski and E. Damek, On the invariant measure of the random difference
equation Xn = An Xn−1 + Bn in the critical case, Ann. Inst. Henri Poincaré Probab. Stat. 48 (2012),
377-395.

ISING TRANSFER MATRIX WITH BALANCED DISORDER

27

[5] S. Brofferio and D. Buraczewski, On unbounded invariant measures of stochastic dynamical systems,
Ann. Probab. 43 (2015), 1456-1492.
[6] D. Buraczewski, E. Damek and T. Mikosch, Stochastic models with power-law tails. The equation
X=AX+B, Springer Series in Operations Research and Financial Engineering. Springer, 2016.
[7] C. de Calan, J. M. Luck, Th. M. Nieuwenhuizen and D. Petritis, On the distribution of a random
variable occurring in 1D disordered systems, J. Phys. A 18 (1985), 501-523.
[8] F. Comets, G. Giacomin and R. L. Greenblatt, Continuum limit of random matrix products in statistical mechanics of disordered systems, arXiv:1712.09373
[9] A. Comtet, J. M. Luck, C. Texier and Y. Tourigny, The Lyapunov Exponent of Products of Random
2 × 2 Matrices Close to the Identity. J. Statist. Phys. 150 (2013), 13-65.
[10] A. Crisanti, G. Paladin and A. Vulpiani, Products of random matrices in statistical physics, volume
104 of Springer Series in Solid-State Sciences. Berlin: Springer-Verlag, 1993.
[11] B. Derrida, Private communication 2017.
[12] B. Derrida and H. J. Hilhorst, Singular behaviour of certain infinite products of random 2×2 matrices.
J. Phys. A 16 (1983), 2641-2654.
[13] S. Friedli and Y. Velenik, Statistical mechanics of lattice systems. A concrete mathematical introduction, Cambridge University Press, Cambridge, 2018.
[14] G. Genovese, G. Giacomin and R. L. Greenblatt, Singular behavior of the leading Lyapunov exponent
of a product of random 2×2 matrices, Commun. Math. Phys. 351 (2017), 923-958.
[15] B. Havret, Regular expansion for the characteristic exponent of a product of 2 × 2 random matrices,
Math. Phys. Anal. Geom. 22 (2019), no. 2, Paper No. 15, 30 pp..
[16] H. Kesten, M. V. Kozlov and F. Spitzer, A limit law for random walk in a random environment,
Compositio Math. 30 (1975), 145-168.
[17] M. V. Kozlov, Random walk in a one-dimensional random medium, Theory Probab. Appl. 18 (1974)
387-388.
[18] J. Lamperti, Criteria for the recurrence or transience of stochastic process. I, J. Math. Anal. Appl. 1
(1960), 314-330.
[19] E. Le Page, Régularité du plus grand exposant caractéristique des produits de matrices aléatoires
indépendantes et applications, Ann. Inst. H. Poincaré Probab. Statist. 25 (1989), 109-142.
[20] J. M. Luck, Critical behavior of the aperiodic quantum Ising chain in a transverse magnetic field, J.
Statist. Phys. 72 (1993), 417-458.
[21] S. Meyn and R. L. Tweedie, Markov chains and stochastic stability, Second edition, Cambridge University Press, Cambridge, 2009.
[22] B. M. McCoy and T. T. Wu, Theory of a two-dimensional Ising model with random impurities. I.
Thermodynamics. Phys. Rev. 176 (1968), 631-643.
[23] B. M. McCoy and T. T. Wu, The Two-Dimensional Ising Model. Harvard University Press, 1973.
[24] Th. M. Nieuwenhuizen and J. M. Luck, Exactly soluble random field Ising models in one dimension.
J. Phys. A 19 (1986), 1207-1227.
[25] F. W. J. Olver, A. B. Olde Daalhuis, D. W. Lozier, B. I. Schneider, R. F. Boisvert, C. W. Clark,
B. R. Miller, B. V. Saunders, H. S. Cohl, and M. A. McClain, (Editors) NIST Digital Library of
Mathematical Functions. http://dlmf.nist.gov/, Release 1.0.28 of 2020-09-15.
[26] C. Sadel and H. Schulz-Baldes, Random Lie group actions on compact manifolds: a perturbative
analysis. Ann. Probab. 36 (2010), 2224-2257.
[27] R. Shankar and G. Murthy, Nearest-neighbor frustrated random-bond model in d=2: Some exact
results. Phys. Rev. B 36 (1987), 536-545.
[28] B. Simon and M. Taylor, Harmonic analysis on SL(2, R) and smoothness of the density of states in
the one-dimensional Anderson model, Commun. Math. Phys. 101 (1985), 1-19.
[29] Ya. G. Sinai, The Limiting Behavior of a One-Dimensional Random Walk in a Random Medium,
Theory Probab. Appl., 27 (1982), 256-268.
[30] E. M. Stein, Singular integrals and differentiability properties of functions, Princeton Mathematical
Series, 30, Princeton University Press, 1970.
[31] D. Stroock and S. Varadhan, Multidimensional diffusion processes. Classics in Mathematics. SpringerVerlag, Berlin, 2006.

(GG) Université de Paris, Laboratoire de Probabilités, Statistiques et Modélisation, UMR
8001, F-75205 Paris, France

28

G. GIACOMIN AND R. L. GREENBLATT

(RLG) Scuola Internazionale Superiore di Studi Avanzati (SISSA), Mathematics Area, via
Bonomea 265, 34136 Trieste, Italy

