speeding up pythonbased lagrangian fluidflow
particle simulations via dynamic collection data
structures
arxiv210500057v1 physicscompph 30 apr 2021
christian kehl  erik van sebille  and angus gibson
abstract arraylike collection data structures are widely established in pythons scientific
computingecosystem for highperformance computations the structure maps well to regular gridded lattice structures that are common to computational problems in physics and geosciences high
performance is however only guaranteed for static computations with a fixed computational domain we show that for dynamic computations within an actively changing computational domain
the arraylike collections provided by numpy and its derivatives are a bottleneck for large computations in response we describe the integration of naturallydynamic collection data structures eg
doublelinked lists into numpy simulations and ctypesbased cbindings our benchmarks verify
and quantify the performance increase attributed to the change of the collection data structure our
application scenario a lagrangian oceanic fluidflow particle simulation within the parcels framework demonstrates the speedup yield in a realistic setting and demonstrates the novel capabilities
that are facilitated by optimised collection data structures
key words collection data structures python justintime compilation performance analysis
performance optimisation numerical methods numpy scientific computing
ams subject classifications 68p05 68q25 18b05 70b05
1 introduction during the past decade the rise of python as a versatile programming platform for scientific research particularly in physics and geoscience has
led to the development of various domainspecific numerical simulation and modelling platforms for subsurface modelling eg gempy 8 plate tectonics eg
gplate 17 eulerian oceanic flow simulations eg veros 10 and others 16 and
lagrangian particle tracing within ocean currents eg parcels 9 simulation and
modelling with python is advantageous for its coding simplicity for novice developers
and domain experts its versatility due to its typeless interpreterbased processing and
its computational efficiency via its links to establish highperformance linear algebra
libraries eg blas 4 and lapack 2 through packages such as numpy 25 11
scipy 27 and scikits 26 21 additionally pythons embedding in modern devops
tools eg git and its wide application in opensource and openscience projects are
additional reasons for its high acceptance in the scientific community
the development of parcels 14 9 which is our application platform in this
paper relies on the highperformance computation within python in this regard
it already employs ctypes 20 as justintime jit cinterface to execute highlyadaptive kernels for the advection and tracing of particles on ocean general circulation
model ogcm fields those kernels are provided as elementary python functions by
the developers as well as the user community and are compiled at runtime into ccode
for rapid field interpolation and advection evaluation the available highperformance
packages such as numpy handle densematrix data organisation with high efficiency
on the other hand problems that cannot be translated into a densematrix algebra
 submitted
to the siam journal on scientific computing editors 20210125
funding this work was funded by the european research council erc under topios project grant no 715386 simulations were carried out on the surfs dutch national einfrastructure
project no 16371 and 2019034
 institute for marine and atmospheric research utrecht university the netherlands
ckehluunl evansebilleuunl
 the australian national university angusgibsonanueduau
c kehl e van sebille and a gibson
problem or data that are dynamically altered during the simulation pose a challenge
to efficient calculations because the reorganisation of the data container ie arraylike collection can in some cases consume more runtime as memory overhead than
is required for the particle advection itself this issue also arises in other physics
simulations that concern large sets of dynamicallychanging physics entities eg the
common nbody simulation where bodies are onthefly added and deleted
after careful benchmarking section 3 we identified this data collection reorganisation as the major barrier to increased performance for particle simulations
in fluid flows in this paper we propose the introduction of alternative data structures for the organisation of dynamic data entities in python furthermore those data
structures are expressed in a jitcompliant format for increased performance the
major contribution of this article is a detailed performance evaluation and comparison
of various data structures and data organisation patterns
the starting point of our investigation is a common arrayofstructure arrangement of the particles within a dense array the performance benefit of a structureofarray arrangement which allows for higher cache coherence and better data locality
in vectorized computations is evident for static data arrays on the other hand a
listofarray arrangement with sublists of defined maximum size limiting the memory
reordering overhead to a fixed upper bound only provides marginal improvements despite the theoretical upper bound on computational complexity see subsection 41
in scenarios with dynamic data arrays where particle entities are insertedinto and
removedfrom the particle collection arbitrarily during the simulation the runtime
expense for data reordering memory copies and poor data locality becomes evident
in that context doublelinked list containers show a distinct performance advantage
especially for large particle collections with more than 215 entities
2 related work in this section we first review the current state of efficient
collection data structures in pythons scientific computing environment as this is
a comprehensive topic with abundant literature we concentrate mainly on nonarraylike collections and their access potential in python  c wrappers
afterwards we present the background knowledge on our physics application of
lagrangian oceanic flow simulations via particles that section mainly serves as an introduction to our workflow and provides background information to the performanceand runtime bottleneck concerns
21 efficient data structures in scientific computing native python is
known for its hampered processing performance in order to circumvent the compute
bottleneck numpy 25 was introduced as fast cwrapper to blas and lapack
which arranges its data efficiently as blockwise matrices or dense arrays ie array collections those arraylike collections such as dense arrays and matrices
are common highperformance data structures especially in python for modern scientific computing the data structures main objective which is random access of
items is rarely required in modern computing frameworks such as scipy 27 scikitimage 26 scikitlearn 21 and other machinelearning frameworks 3 1 6 build on
top of numpys densearray and matrix data structures a common fundamental
issue with arraylike collections is that random insertions and deletions of items in
the collection requires array copies and merges see 24 this slows down the operation and requires an additional data buffer of the size of the collection which can be
prohibitively expensive in modern big datacomputing
the pythoninternal bisect module facilitates a more listlike collection behaviour
via binary searches of objects in arraylike collections that said the binary search
speeding up pythonbased lagrangian particle simulations
only simplifies object search and object access but it provides no functionality to
speedup native internal item insertion or deletion this functionality is provided by
the collectionsdeque module which is a linked nodelist in terms of item insertions
or deletions performance it exhibits the expected constant runtime complexity ie
logarithmic complexity when including item localisation that said working with
the collectionsdeque module is difficult because the cinterface and data structure
access eg via ctypes is poorly documented moreover the connection cannot
reliably be established because the cinterface is a doublelinked list whereas the
python interface is a unilinked list
the sortedcollections is another python module that promises a ccomparable
performance behaviour for dynamic collections such as the sortedlist for stdlist
equivalence the module internally wraps python lists ie arraylike collections in
an internallymanaged item indexing scheme the indexing scheme provides constantcomplexity insertions and deletions while the use of the bisect module internally
facilitates fast object access all together the module provides all performancerelevant features and advantages of listlike collections the drawback of the module
is the lack of python  c interfaces for fast execution of simulations and analyses
the gap we address in this paper is the development and benchmark of dynamic
collection data structures in python with an integrated cinterface for rapid computations on dynamicallyorganised data
22 lagrangian particle tracing in oceanic flows with parcels our
physicsmotivated prime application scenario is the tracking of virtual particles in
oceanic flows these virtual particles can for example represent plastic 15 plankton 18 nutrients 5 or fish 23 the tracing is done by lagrangian advection where
particles follow a given background eulerian advective flow field 9 and whose trajectories are numerically integrated over time as given in eq 21 implemented in
eg an eulerforward or rungekutta scheme 19
xt  t  xt 
vx   d  dxb 
with xt  x representing an individual particles position at time t advected
in t timesteps and discretized over   vxt is the gridded fluid flow from a
numerical simulation and dxb is an optional position change because of particle
behaviour swimming sinking etc the partial integral is required to match up the
temporal resolutions of the underlying flow field and the particle simulation
considering the computational complexity in this particle advection scheme there
is a distinction between the theoretical and practical computational workload theoretically the operations that majorly contribute to the compute load are i the
r tt
velocity interpolation t
vx   d on the gridded eulerian flow field ii the
position integration xt  t with a fixed or adaptive timestepping and iii the
sum of partial integration results ie fused multiplyadd fma operations next to
those plain calculations for the particle trajectory the lagrangian simulations main
purpose is the tracking of particle properties be they of physical biochemical or behavioural nature over time those functions are executed in parcels by the help of
userdefined kernels depending on the complexity of the particle model those operations represent the majority of calculations in a practical setting on the other hand
as those kernels are casespecific and hardly generalizable their compute workload is
not considered in this papers experiments
c kehl e van sebille and a gibson
in terms of performance and benchmarking the runtime of the whole simulations
as well as each integration iteration individually is the sum of the compute time as
analysed above and the inputoutput io time the latter is further split into
memoryio fileio and plotting time which is the special case for device output
happening not exclusively to files nor inmemory
in realistic practical lagrangian fluid flow simulations the compute time is by
far exceeded by the fileio as the oceanic flow fields need to be dynamically loaded
from large file databases hence from a practical perspective lagrangian fluid flow
simulations are typically strictly iobound tasks parcels 9 already implements multiple ioenhancing techniques eg deferred loading dynamic and selective loading
of exclusively occupied cells and thus we are hereby more concerned about improving
the particle integration itself for large particle sets ie particle swarms therefore
all experiments in this paper utilize synthetic preloaded perlinnoise flow fields 22
that remain resident in memory over the whole simulation neglecting fileio time
of the flow fields
an indepth performance analysis is required for the case of large particle sets because effects from hardware caching memory layout and particle set data organisation
have a potentially measurable impact on the overall runtime from the perspective
of lagrangian fluid flow simulations as a particle system a particle set can be statically allocated and remains fixed in its data entities over the the whole simulation
alternatively in order to circumvent limitedprecision floatingpoint errors and to
comply with casespecific simulation requirements particles can be dynamically reemitted and removed from the particle set both cases potentially behave differently
in terms of performance and runtime due to data access and alteration patterns for
different collection data structures that can be used to store a particle set
the potential performance difference between static and dynamic particle sets
its quantification its rationale its impact on different oceanic flow simulation cases
and its exploitation for speed improvement are the main experimental subjects of this
article
3 statusquo benchmarks the investigation into performance enhancing of
lagrangian particle simulations starts with the following research questions what are
the most costly functions and functionalities within the existing particle simulation
in order to answer this question we needed to i determine the most costly functions
within the simulation framework ii quantify the controls ie impact parameters
on the computational runtime and iii test the different computational settings of
the simulation such as python and cbased kernel evaluation the impact of task
parallelisation via message passing mpi and the cost of dynamic array operations
for this quantification the parcels code1 in version 215 was profiled using the cprofile package the outcome were linebyline code timings which were analysed with
snakevis7 plotting a percentage runtime as in figure 1
figure 1 shows the total runtime blue bars of a 365day particle simulation
on a synthetic flow field generated by perlin noise see figure 2 advected with a
fixed temporal resolution of 1 hour 13000 seconds for the removalscenario 265000
seconds for the insertionscenario in both presented cases the simulation targets
an average number of particles in the arraycollection of 218  at the third code
level one can see a first significant difference the active garbage collector darker
green bar consumes a significant amount of the runtime 56 for dynamic removal
1 oceanparcels
website  httpoceanparcelsorg
speeding up pythonbased lagrangian particle simulations
a dynamic particle insertion
b dynamic particle removal
figure 1 plot of percodeline runtime distributions for different dynamic array operations
the average particle number target in both cases is 218  those plots show runs with no mpiparallelisation and cevaluated kernels
which is not the case for the dynamicinsertion scenario furthermore in the third
and fifth level we see that both scenarios spend large amounts of time in localising
erroneous 3046 for removalscenario 2973 for insertionscenario and deleted
3115 for removalscenario 3128 for insertionscenario particles the removal
scenario where particles are actually deleted also spends approximately twice the
time in the removal function than the insertion scenario 448 compared to 211
those extra runtime expenses for item removal and garbage collection result in less
time being spent in the actual particle advection only 2546 for the removal scenario
compared to the 3589 for the insertionscenario the remaining percentages are
smaller helper functions listcomprehensions and memory management we note
here again that the synthetic flow field on which particles are advected is held entirely
inmemory and that particle positions are not written to external storage so that
extensive fileio is not a limit in those cases
the following top5 most expensive execution functions emerge from an extensive
benchmark analysis of the different scenarios described above ie staticvsdynamic
datasets varying particle set sizes etc furthermore these are the mostexpensive
functions as their detrimental impact on performance scales with the size of the
particle set and hence are overpronounced even when running large particle sets
in parallel to reduce the common fileio performance malus in realworld ocean
simulations
1 list comprehension in kernel execution that gathers all unadvected particles
computational complexity on 
2 delete function for removing individual particles from the particle set computational complexity on 2 
3 arraycopy operation that flips the densearray arrangement in memory from
columnmajor order fortranpythonnative orientation to rowmajor
order cc native orientation for cinternal kernel evaluation computational complexity on 
4 the actual particle advection computation function which would ideally be
at the top of this list 34 of the runtime
5 another array transposeandcopy operation that performs the nddensearray concatenation for periodicallyreleased particles computational com
c kehl e van sebille and a gibson
figure 2 probabilistic synthetic fluid flow field generated from perlin noise 22 the background displays absolute flow speeds in range 0  10 m
in a metrescale flow field whereas the
green particle traces depict the particles trajectory within the flow
plexity o2n 
therefore the clear target of this research is to reduce the impact of the abovelisted superlinearly runtime scaling of collection reordering and alteration functions to the point that the advection function becomes the major runtime expenditure
of the simulation this would as traditionally intended transform the whole simulation process into a computebound application which can subsequently be sped
up by common highperformance computing approaches such as simd processing
parallelisation and load distribution
the reason for the runtime malus in the abovelisted operations is that densearrays are randomaccess data structures that exhibit a high computational and
memory complexity for random alterations in other words adding and removing
entities in an arraycollection requires splitting the collection in multiple distinct
subsets that exclude removed and include inserted elements and concatenate them
together into a newlyallocated result collection ie they are not inplace operations
thus we need to exploit alternative data structures that exhibit a sublinear scaling
on those operations
4 highperformance collection data structures for pythonbased numerical simulations in this paper we propose to supplement python with dynamic collections such as doublelinked nodelists bisectsearch augmented arrays
with explicit cbindings to perform highspeed kernel executions in c those structures attempt to remedy the performance malus rooted in slow randomalteration
speeding up pythonbased lagrangian particle simulations
methods on arraycollections
our comparison baseline is a common arrayofstructure aos arraycollection
data structure which is a simple array housing complex multidimensional objects see
figure 3 and the supplement material for details the simplicity is advantageous for
the conversion of the data structure into c but it suffers from the severe speed malus
for dynamic alterations
figure 3 arrayofstructure aos collection memory layout  a collection data structure with
an itemlinearized memory alignment the linear item order is annotated by the black major arrow
in order to address the trivial and most common request to do engineering optimisations such as speedy list comprehensions reduced memory copies and so forth
we created a first comparison case of engineeringoptimised aos to showcase the
potential as well as the limitations of simplistic and trivial improvements
41 listofarrays ofstructures loaos collection a first attempt
to provide an upper bound to the scaling complexity of random array alterations is
to split up the global array into a list of fixedlength arrays we refer to this data
structure as listofarray loa collection in the text remainder though highlighting
that the underlying array structure is aos by itself
such a splitting setup subsequently requires an address translation when randomly accessing and retrieving items of the collection hence slowing down excessivelynumerous item lookups the advantage is that the complexity of item insertion and
deletion is reduced from on   k n n 1n 1 with k being the number
of added or removed items and n being the total number of items ie particles to
on   k  d  m  m  1  m  1  d  2 m
2  with d  m and m being
the maximum size of the subarrays and a loose bound that n  m  the latter
additional accounts for the effort of merging two subarrays when their occupancy is
below 50 the layout is illustrated in figure 4
42 structureofarrays soa collection the soa representation is a
transposition of the memory layout of aos rather than using a single instance of the
container datatype at the toplevel whose elements are particle structures there is
only one instance of the particle structure with an array of values in each field eg
velocity position a particle is then denoted by an index and can be reconstructed
by extracting the element at that index from all the constituent fields
c kehl e van sebille and a gibson
figure 4 listofarray loa collection memory layout  a nodelist container with fixedsize
preallocated aosorganised subarrays
due to its layout soa lends itself to a vectorized instruction flow as common
to modern linearalgebra software frameworks ie blas lapack numpy 12
when accessed from python a loop over all particles may be turned into a loop of
numpyaccelerated vectorized operations over the particle fields this is particularly
apparent in the case of multiple particle initialization where the allocation of many
independent objects can be replaced by the instantiation of just a few arrays soa
may also be more cachefriendly for the justintime advection calculation a field
value for multiple particles may be loaded into a single cache line see figure 5 thus
an optimizing compiler may choose to use vectorized cpu instructions to operate on
multiple particles at once see sm1 in the supplementary material
the linearized inmemory layout of soa is a tradeoff of speed for dynamic flexibility cache coherence and high data locality in vectorized processing are prioritized
over efficient deletion of arbitrary particles accessing and manipulating individual
particles also becomes unwieldy as all the fields are accessed separately
figure 5 structureofarray soa collection memory layout  a collection data structure with
an attributelinearized memory alignment
43 nodal doublelinked list collection the proposed loacollection enforces an upper bound on the computational complexity of dynamic collection alterations while the soacollection inherently only improved cache access during advection
execution hence not underpinning tackling the dynamic collection alteration issue
in order to reduce that computational complexity the actual dynamic collection alteration process needs to be addressed
speeding up pythonbased lagrangian particle simulations
we here introduce a doublelinked list implementation into the parcels particle
framework the novel aspect here is its straightforward implementation with ctypes
which allows direct access of the data collection within the highspeed ckernels
figure 6 nodebased doublelinked list collection memory layout
in this implementation the node objects are a complementary of python classobjects and their crepresentation as a structure each node stores a reference to
its predecessor its successor and its data container the data container in this case
is a particleobject reference in python and its numpyobject pointer reference in
c this numpyobject reference of a particle is just a pointer to a numpymapped
cstruct reference of one particle not a pointer to a numpy array
during runtime particles can dynamically be added and removed with the standard complexity of o for linked lists particles are bound to a node and insertion
and removal is performed by plain relinking of the predecessor and successor references in the collection in contrast to pythons internal deque implementation the
list can commonly be traversed by beginenditerators or foreachelementiterations
in python and c equivalently the traditional drawback remaining here too is the
lack of fast randomaccess which usually requires an on  list traversal compared
to the o randomaccess in arrays thus it is expected that the nodesbased
doublelinked list outperforms alternative implementations in scenarios with abundant dynamic particle operations
for abbreviation purposes the remainder of the paper refers to the doublelinked
list as nodal list as our benchmark logs and plots refer to the collection as nodes in
the context of this paper it is hence implied that nodes in a nodal list are doublelinked
5 results in order to assess compare and quantify the effectiveness of the
individual data collection designs we run benchmarks on the same setup as in section 3 in contrast to the initial benchmark timings do not need to be resolved down
to individual atomic operations which costs considerable runtime and memory in and
of itself for evaluation purposes the parameters of interest are listed as follows and
measured at the highest distinct code level
 compute time the bare time required to execute flow advection kernels and
field interpolations
 io time the combined time require for fileio and memory io operations
 plotting time operation time required for console and image output ie
image rendering
 total time the time between the start of the simulation and the end of
c kehl e van sebille and a gibson
the simulation excluding preprocessing setup of flowfields and initial
collections startup phase result collection and writeback cleanup postprocessing and shutdown approximately equal to the cumulative compute
io and plotting time with minor roundup deviations possible
 fileio time first component of iotime that measures all filerelated operation times ie data transfer between external storage and memory the
demonstrated benchmarks also include memoryio of the gridded flow fields
in this measure because the use of synthetic flow fields is a special case and
the interface commonly accesses files instead of preloaded memory sections
 memoryio second component if iotime that measures memory management operation times eg array and variable copies memory mapping
between c and python storage
 kerneltimes average compute io and total processing time among all
iterations of the simulation
 perparticle time average compute io and total processing time for an
individual particle per simulation iteration
 memory consumption the amount of memory acquired by the simulation
process for instruction data and stack segment as well as the size of shared
memory page blocks
furthermore to enable appropriate comparisons static and dynamic scenarios are
setup to cover in each simulation approximately the same average number of particles
and hence the same amount of workload during the simulation time while avoiding
irrelevant setups eg simulations with  4 particles singleparticle additions and removals each iteration etc in order to achieve that with dynamic scenarios we start
items and insert 128 items with a periodicity tp  tsim  128
 74 
with n  target
while removing items by
q defining a randomized uniformlydistributed life expectancy
with b  2 23 tsim  t and a  0
px  ba
in order to cover a wide range of scenarios we benchmark each experiment with
n  2x particles with x  1016
51 static datasets a static setup with a fixed particle number within the
collections shows the runtime behaviour in figure 7
a total time
b average runtime per particle
c memory consumption
figure 7 runtime behaviour of different collection data structures for a static simulation setup
all adapted collection types consistently improve upon the runtime when compared to the aos case however we observe rather diverging runtime behaviours
loa and the engineering case show a linear time increase which is just negatively
offset to the aos curve this is rather unsurprising as both collection structures are
speeding up pythonbased lagrangian particle simulations
conceptually or structurally similar to the aos starting case the soa structure
shows a strong logarithmic time behaviour whereas the doublelinked list shows a
weak logarithmic behaviour both of those behaviours agree with the approximations
of the computational complexity for data access see subsection 43
the overall consistently fastest collection implementation is the nodal list this
statement is not only supported by the overall runtime figure 7a but even more
so by the average runtime per individual particle figure 7c the plot shows the
limited constant offset in time for setting up and managing the data structure as
the perparticle runtime even for small datasets is significantly lower than the other
structures this beneficial runtime behaviour comes however with a significantlyincreased memory demand as each item record ie each node with its particle data
package needs to store the predecessor and successor reference
a compute vs io ratio
b speedup relative to aos 
figure 8 derivative performance metrics of different collection data structures for a static
simulation setup
in figure 8b we see that the speedups for minor data structure modifications
compared to aos eg loa and engineered improvements is limited which is at
no point exceeding the 2x speedup different collection structures such as soa and
nodal lists yield higher speedups figure 8b also shows that soas starts with
less speedup for small collections and a consistently increasing speedup for larger
collections in contrast the speedup of nodal lists is highest with smaller collections
then depreciates for intermediatesized collections while then increasing again for very
large datasets the runtime and hence also speedup is less reliable and stable for
nodal lists than for soa collections
furthermore figure 8a shows that for most collections more time is spent in
memory management tasks the larger the dataset becomes meaning that the simulation process is increasingly iobound  more specifically memoryio bound the
single exception to this behaviour is the soa data structure which inherently requires
minimal memory management when static dataset sizes are concerned leading to an
increasing computetoio ratio
52 fullydynamic datasets in the dynamic case when combining the dynamic collection operations of insertion and removal the performance figures significantly change as seen in figure 9
here we first separate the analysis in two regimes small collections ie low
particle count with n  25 000 and large collections with n  25 000 in small
collections we see two speed regimes namely the arraylike structures ie aos engineering improvements loa and soa and the doublelinked list arraylike structures
experience a hyperbolic rise in runtime for small collections because computational
c kehl e van sebille and a gibson
a total time
b average runtime per particle
c memory consumption
figure 9 runtime behaviour of different collection data structures for fully dynamic collection
operations during simulation
effort still outweighs the additional memoryio workload see also figure 10a
furthermore the fixed memory allocation requires more setup time than the dynamics of a nodal lists which clearly outperforms the other collections
in the second regime of large collections we see further divergence of the data
structures into three categories category a row major orderaligned arraylike collections ie aos and loa category b column major orderaligned arraylike
collections ie engineering improvements and soa and category c the nodal list
for large collections cat a collections scale linearly and with a steep slope relative
to n  due to their disadvantageous memory layout which complicates memory operations via numpy cat b collections exhibit a more logarithmic runtime scale for
large collections that is because despite their fixed memory arrangements numpy
internally concatenates column major order arrays faster than the row major order
arrays for arraylike data structures in terms of runtime both insertions and removals result in large number of array concatenation operations as expected the
doublelinked list more favourably deals with dynamic collection operations when
comparing the results in figure 12 and subsection 52 doublelinked lists are the only
collections structures that exhibit approximately equal runtimes for both cases the
regime split as well as the performance argumentation is further supported by the
computetoio ratio in figure 10a as well as an observation of the speedup in
figure 10b
a compute vs io ratio
b speedup relative to aos 
figure 10 derivative performance metrics of different collection data structures for fully
dynamic collection operations during simulation
speeding up pythonbased lagrangian particle simulations
in comparison to previous experiments the soa structure seems particularly
impeded in performance from random operations as it is consistently slower than the
engineering improvements furthermore the specific advantages and disadvantages
balance the performance of aos loa and soa for intermediate dataset sizes for
very large collections the perparticle runtime is approximately equal for nodal lists
engineering improvements and soa collections
the final point of note and also the most interesting observation for extrapolating the results to massive dataset sizes with n  100 000 are the observations
of memory consumption the fully dynamic operations and their subsequent concatenation operations leave temporary data blocks as traces in memory leading to an
exponential memory demand for row major orderaligned arraylike collections cat
a collections the fixedsize buckets of loa prevent such an excess in memory consumption for soa and nodal lists their memory requirements remain linear in our
measurements
hence combining the insights in runtime perparticle runtime and memory behaviour the only two viable collection data structures for massive dynamicallyaltering
datasets ie huge particle swarms in the fluid simulation are doublelinked nodal
lists and soa the doublelinked list yields the highest performance in terms of speed
but requires five times more memory than soa due to the supplemental node information if memory capacity is the major limiting factor then the soa data structure
delivers the maximum speed for the least memory demands
6 discussion the runtime and memory consumption results above demonstrate the diverse performance impacts that the choice of a specific collection data
structure has on largescale simulations traditionally the choice of the collection
data structure depended exclusively on
 the dataset size
 the available computerelated technologies eg simd parallelization assembly compilers
 the available memory
our benchmark analysis demonstrates that the choice of the ideal collection data
structure depends on the scenario and the operations actually performed on the dataset
in order to judge potential future improvements we analyse the computetoio
ratio here while the static and dynamic case appear very different their trends on
large data are similar all new collection types show an increased computational load
for n  25 000 items larger collections experience a dropoff in the ratio on very
large collections with n  50 000 items only the soa collection experiences another
increase in compute load in conclusion all collections with a bad data locality show
an iobound processing behaviour whereas the cacheoptimised soa collection has
a computebound processing behaviour the data locality issue also explains the
large dropoff in the computetoio ratio for nodal lists items of the collection are
sparsely distributed in the memory in contrast to any arraylike collection this
impacts the ideal use of the collections as nodal list perform well with dynamic
collection operations and simple compute kernels such as the simple advection used
in our experiments the soa collection is expected to conversely outperform the
other collections in static and dynamic experiments complex computational kernels
and an increasing amount of field interpolations furthermore the high degree of
data locality is beneficial to sharedmemory parallelization in the future we shall
combine the doublelinked list collection with an improved data locality scheme
c kehl e van sebille and a gibson
our findings directly impact studies in ocean physics which can be split into the
following casestudy categories
1 mass  momentumconserving particle studies lagrangian particles are once
released and traced over stretched time periods the set of particles stays
fixed ie static
2 endofstate particle studies particles are once released their state is traced
over multiple iterations and they are removed upon reaching a given end
state the simulation either ends after removal of the last particle or releases
particles anew when a particle converges on its end state the simulation
either just includes dynamic data removal or is fully dynamic a special
case of such an endofstate simulation is particle aging where particles are
deleted after a defined maximum lifetime
3 density and flux approximations particles are continuously released and
their position and attribute modalities are traced at regular intervals a
regulargridded map of those attributes is created by averaging the the particle
attributes over all particles covered within a gridcell ie stencil bufferlike
operation in order to facilitate a high particle density particles are constantly created and inserted to the collection in order to prevent accuracy
errors in the accumulation longtraced particles are regularly removed from
the dataset this is a traditional case of large dynamics in the collection data
structure that contains the particles
according the our findings each of those simulation cases shall employ its optimal collection structure qualitatively the data structures enable developing new
dynamic operations that are driven by the particle behaviour itself eg split or
merge of particles such particleinherent dynamics are beyond the capabilities of
geophysical lagrangian oceanicflow simulations until now as they require i a versatile definition of particle behaviour and ii an efficient memoryconserving fast
data structure that facilitates random insertions and removals specific example applications are the individual modelling of fractured particulate matter in the oceans
or the aggregation of smaller particulates into larger clusters
the performance results on the different data structures extend further to computational cases outside physics simulations general data analysis requires iterative
attribute derivation and subsequent costfunction computation as well as information condensation by removing or merging attributes on large datasets those data
operations map to the different experiments for static datasets dynamic data insertion dynamic data removal and fully dynamic operations in section 5 therefore
geophysical data science applications and frameworks can benefit from improved
data organisation this is particularly important where computation speed is crucial
such as in emergency response systems and forecast systems that are connected to
realtime sensor networks
regarding the conceptual and development approach we acknowledge that performance optimisation and especially performance measurement are hardly possible
in python the challenge for measurement and improvement is that pythons native
core eg typeless codes class hierarchy interpreter and introspection support is
too slow to measure runtimes and memory consumption below a certain resolution
empirically tm  20ms thus temporary resource allocations that may happen
within numpy or scipy cannot be measured in python because its measurement
function calls are too slow this can be observed in the numbers see 13 and the
nearlyflat memory consumption graph of soa furthermore we acknowledge that
the manual provision of justintime ccompiler interfaces with ctypes itself consumes
speeding up pythonbased lagrangian particle simulations
resources ie runtime and memory that may either be hidden reduced or removed
with other cbinding implementations eg numba
in order to check the results or apply the outlined structures to other modellingand simulation problems the program codes for the individual structures are available
on github
 aos  httpsgithubcomoceanparcelsparcelstreebenchmarking
 engineering optimizations httpsgithubcomoceanparcelsparcelstree
engineering optim trials
 loa httpsgithubcomoceanparcelsparcelstreelist of pset array trials
 doublelinked list httpsgithubcomoceanparcelsparcelstreesorted pset
trials
 soa httpsgithubcomoceanparcelsparcelstreesoa benchmark
7 conclusion this paper aimed at quantifying the detrimental performance
impact of using static arraylike collection data structures for dynamic collection
operations eg item insertion and removal in geophysics and especially oceanic
applications following the expected negative impact of detrimental data locality
new highperformance collection data structures within python were designed and
developed that also interfaces easily with ckernels that is because in actual physics
calculations the performance gain from optimised data structures could vanish if
calculations then require a slow native python processing the whole process is
specifically analysed for large datasets cases n  215  for which those data structure
performance limits are increasingly impacting the overall simulation runtime
we have demonstrated and quantified the negative performance impact in our
statusquo experiments section 3 listing the five most runtimecostly code operations
that increasingly limit performance with growing datasets qualitatively calls to the
insertion ie concatenate and deletion ie remove functions as well as related
array reordering functions are prime performance barriers
consequently the paper introduces new collection data structures to circumvent
the performance bottlenecks of which two collections are arraylike structures ie
soa loa and one is a doublelinked list structure our experiments showed that
using those newly proposed data structures yield significant runtime improvements
the paper shows the two major experiment cases a static dataset a and a fully
dynamic dataset with insertion and removal b in all cases the doublelinked list
is the fastestcomputing data structure whereas soa is the most efficient concerning
runtime improvement and memory consumption in combination
the results and insights gained from the specific lagrangian oceanic and fluid
flow simulation are generalisable to other problems in geophysics as well as general
data science as demonstrated in section 6 the developed data structures and the
gained insight on performance facilitate new application scenarios and modelling cases
in physical oceanography which demonstrates the impact of our research
appendix a removalonly and insertiononly experiments
separate experiments were conducted for removalonly figure 11 and insertiononlyfigure 12 cases
the removalonly experiment responds similar to the static case with two exceptions firstly the loa collections is partially slower than the aos collection which
can be attributed to the mergeoperation of nodes with sublists of length n  nlim 
secondly the nodal list does not provide the expected speedup compared to arraylike collections especially soa this is because with ctypes as jitinterface for the
simulations the node removal and relinking requires additional operations to the
c kehl e van sebille and a gibson
a total time
b memory
consumption
c compute vs io
ratio
d speedup relative
to aos 
figure 11 runtime behaviour of different collection data structures for a dynamic removal of
elements during simulation
traditional replacement of the next and previous pointer
a total time
b memory
consumption
c compute vs io
ratio
d speedup relative
to aos 
figure 12 runtime behaviour of different collection data structures for a dynamic insertion
of elements during simulation
the insertiononly case is similar to the fullydynamic case showing that particle
insertion represents a major runtime expense the distinct fastinsertion complexity
of nodal lists is visible in its improved runtime the soa performance is comparable
to the nodal list because the particles are unordered thus a particle insertion is
performed by simple array concatenation
acknowledgments this work is part of the tracking of plastic in our seas
topios project supported through funding from the european research council erc under the european unions horizon 2020 research and innovation programme grant agreement no 715386 simulations were carried out on the dutch
national einfrastructure with the support of surf cooperative project no 16371
and 2019034
references
1 m abadi p barham j chen z chen a davis j dean m devin s ghemawat
g irving m isard et al tensorflow a system for largescale machine learning in
12th usenix symposium on operating systems design and implementation osdi
16 2016 pp 265283
2 e anderson z bai c bischof j demmel j dongarra j du croz and a greenbaum linear algebra package 1999
3 j bergstra o breuleux f bastien p lamblin r pascanu g desjardins
j turian d wardefarley and y bengio theano a cpu and gpu math expression compiler in proceedings of the python for scientific computing conference scipy
vol 4 austin tx 2010 pp 17
speeding up pythonbased lagrangian particle simulations
4 l s blackford a petitet r pozo k remington r c whaley j demmel j dongarra i duff s hammarling g henry et al an updated set of basic linear algebra
subprograms blas acm transactions on mathematical software 28 2002 pp 135151
5 p cetinaheredia e van sebille r j matear and m roughan nitrate
sources supply and phytoplankton growth in the great australian bight
eulerianlagrangian modeling approach journal of geophysical research oceans 123
2018 pp 759772 httpsdoiorghttpsdoiorg1010022017jc013542 https
agupubsonlinelibrarywileycomdoiabs1010022017jc013542 httpsarxivorgabs
httpsagupubsonlinelibrarywileycomdoipdf1010022017jc013542
6 f chollet et al keras the python deep learning library ascl 2018 pp ascl1806
7 m davis e bray n schlmer y xiong et al snakeviz 2014 httpsjiffyclubgithub
iosnakeviz
8 m de la varga a schaaf and f wellmann gempy 10 opensource stochastic geological
modeling and inversion geoscientific model development 12 2019 pp 132 https
doiorg105194gmd1212019 httpsgmdcopernicusorgarticles1212019
9 p delandmeter and e van sebille the parcels v20 lagrangian framework new field interpolation schemes geoscientific model development 12 2019 pp 35713584 https
doiorg105194gmd1235712019 httpsgmdcopernicusorgarticles1235712019
10 d hfner r l jacobsen c eden m r b kristensen m jochum r nuterman and b vinter veros v01  a fast and versatile ocean simulator in pure python geoscientific model development 11 2018 pp 32993312 httpsdoiorg10
5194gmd1132992018 httpsgmdcopernicusorgarticles1132992018
11 c r harris k j millman s j van der walt r gommers p virtanen
d cournapeau e wieser j taylor s berg n j smith r kern m picus
s hoyer m h van kerkwijk m brett a haldane j fernndez del ro
m wiebe p peterson p grardmarchant k sheppard t reddy w weckesser h abbasi c gohlke and t e oliphant array programming with numpy
nature 585 2020 p 357362 httpsdoiorg101038s4158602026492
12 c intel intel 64 and ia32 architectures optimization reference manual intel corporation
sept 2014 pp 212  215 sec 45 improving memory utilization
13 c kehl performance measurement data for speeding up lagrangian fluidflow
particle simulations in python via dynamic collections data repository 1 utrecht
university 2021 httpsdoiorg1024416uu01cv3oeh httpsdoiorg1024416
uu01cv3oeh
14 m lange and e van sebille parcels v09 prototyping a lagrangian ocean analysis framework for the petascale age geoscientific model development 10 2017
pp 41754186 httpsdoiorg105194gmd1041752017 httpsgmdcopernicusorg
articles1041752017
15 lm lebreton s greer and j borrero numerical modelling of floating debris in the
worlds oceans marine pollution bulletin 64 2012 pp 653  661 httpsdoiorg
httpsdoiorg101016jmarpolbul201110027 httpwwwsciencedirectcomscience
articlepiis0025326x11005674
16 p lemenkova processing oceanographic data by python libraries numpy scipy and pandas aquatic research 2 2019 pp 7391 httpsdoiorg103153ar19009 https
halarchivesouvertesfrhal02093491
17 r mller j cannon x qin r watson m gurnis and s williams gplates building
a virtual earth through deep time geochemistry geophysics geosystems 19 2018 https
doiorg1010292018gc007584
18 p d nooteboom p k bijl e van sebille a s von der heydt and h a dijkstra
transport bias by ocean currents in sedimentary microplankton assemblages implications for paleoceanographic reconstructions paleoceanography and paleoclimatology 34
2019 pp 11781194 httpsdoiorghttpsdoiorg1010292019pa003606 https
agupubsonlinelibrarywileycomdoiabs1010292019pa003606 httpsarxivorgabs
httpsagupubsonlinelibrarywileycomdoipdf1010292019pa003606
19 t nordam and r duran numerical integrators for lagrangian oceanography
geoscientific model development 13 2020 pp 59355957 httpsdoiorg105194
gmd1359352020 httpsgmdcopernicusorgarticles1359352020
20 t e oliphant python for scientific computing computing in science  engineering 9
2007 pp 1020
21 f pedregosa g varoquaux a gramfort v michel b thirion o grisel
m blondel p prettenhofer r weiss v dubourg et al scikitlearn machine
learning in python journal of machine learning research 12 2011 pp 28252830
c kehl e van sebille and a gibson
22 k perlin an image synthesizer siggraph comput graph 19 1985 p 287296 https
doiorg101145325165325247 httpsdoiorg101145325165325247
23 j scutt phillips a sen gupta i senina e van sebille m lange p lehodey
j hampton and s nicol an individualbased model of skipjack tuna katsuwonus
pelamis movement in the tropical pacific ocean progress in oceanography 164 2018
pp 63  74 httpsdoiorghttpsdoiorg101016jpocean201804007 httpwww
sciencedirectcomsciencearticlepiis0079661117302896
24 r sedgewick algorithms in c parts 14 fundamentals data structure sorting
searching algorithms in c pearson education 1998 httpsbooksgooglenlbooksid
zcchaeprwvyc
25 s van der walt s c colbert and g varoquaux the numpy array a structure for
efficient numerical computation computing in science  engineering 13 2011 pp 2230
26 s van der walt j l schnberger j nuneziglesias f boulogne j d warner
n yager e gouillart and t yu scikitimage image processing in python peerj 2
2014 p e453
27 p virtanen r gommers t e oliphant m haberland t reddy d cournapeau
e burovski p peterson w weckesser j bright et al scipy 10 fundamental
algorithms for scientific computing in python nature methods 17 2020 pp 261272
supplementary materials speeding up pythonbased
lagrangian fluidflow particle simulations via
dynamic collection data structures
arxiv210500057v1 physicscompph 30 apr 2021
christian kehl  erik van sebille  and angus gibson
sm1 memory access patterns for contiguous arrays n xm arrays
arrays can be aligned in two patterns in a usual and simple matter 1 the major
axis represents the structure attributes aligned contiguously in memory while the
minor axis represents the multiple array items alternatively 2 the major axis can
represent the array items contiguously aligned in memory whereas the attributes
follow the minor axis formally with n  i i  x0  x1   xn  and m  a
for xai a  a with a representing or item structure attributes the arrays can
structured as n xm 1 or as 2 m xn  this has performance implications for the
functional evaluation of the array differing between a traditional sequential and a
modern vectorized evaluation this is shown in figure sm1 for n xm arrays named
array of structures aos and in figure sm2 for m xn arrays named structure of
arrays soa
comparing the memory access patterns in terms of performance we see that
in traditional sequential processing the soa pattern is faster as it only requires 1
cache update for each iteration whereas it requires 4 updates with an soa pattern
in modern computing architectures each iteration actually executes between 2 for
multithreaded cpus and 32 for gpu warps iterations simultaneously thus a view
on periteration ie peritem computations is invalid this simultaneous computation is referred to as vectorization on cpus or simultaneous multiprocessing
smp on general processors comparing the numbers for a 2item evaluation
aos requires 2 cache updates and soa requires 8 cache updates when assuming a
sequential processing for vectorized processing though evaluating 2 items requires
14 updates for aos and just 4 updates for soa furthermore the number of cache
changes is approximately m  pnu  where p u  is the number of processing units
hence scaling favourably with the number of processing units and threads in software this naturally makes soa superior in performance for modern processors for
m  n  conversely where n  m one can just switch both array axes lastly
in practice the layout change can be easily achieved by changing from fortrancontiguous to ccontiguous order in python arrays or vice versa which equally is
achieved by matrix transposition obviously establishing the correct item order on
allocation and keeping the order static is vital as a periteration matrix transposition
consumes vastly more processing cycles than is saved by the soa evaluation
 submitted
to the siam journal on scientific computing editors 20210125
funding this work was funded by the european research council erc under topios project grant no 715386 simulations were carried out on the surfs dutch national einfrastructure
project no 16371 and 2019034
 institute for marine and atmospheric research utrecht university the netherlands
ckehluunl evansebilleuunl
 the australian national university angusgibsonanueduau
sm1
sm2
c kehl e van sebille and a gibson
figure sm1 memory access patterns related to a simple particle advection function on an
n xm array of structures for sequential and vectorized evaluation listed as assembly mnemonics
supplementary materials speeding up pythonbased lagrangian particle simulations
sm3
figure sm2 memory access patterns related to a simple particle advection function on an
m xn structure of arrays for sequential and vectorized evaluation listed as assembly mnemonics
