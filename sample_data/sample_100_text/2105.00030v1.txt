Leveraging Machine Learning to Detect Data
Curation Activities
Sara Lafia∗ , Andrea Thomer† , David Bleckley∗ , Dharma Akmon∗ , and Libby Hemphill∗†
∗

ICPSR, University of Michigan, Ann Arbor, MI, USA
School of Information, University of Michigan, Ann Arbor, MI, USA
Email: slafia@umich.edu, athomer@umich.edu, dbleckle@umich.edu, dharmrae@umich.edu, libbyh@umich.edu

arXiv:2105.00030v1 [cs.CL] 30 Apr 2021

†

of controlled vocabularies or standards; and so on. These tasks
vary and depend on the type of data being curated; the scope
and focus of the organization doing the curation; and the
designated community the curators seek to serve [6]–[8]. Chao,
Cragin and Palmer [9] derive a typology of data curation
concepts, activities, and terms through a qualitative study of
earth science researchers and show how the typology can
support cost analysis of different curatorial activities. However,
further work is needed to examine the efficacy of these tasks
across different data types and to empirically demonstrate the
costs and benefits of the activities to a repository and a user
community.
The use of project management systems (e.g., Jira, Asana,
Trello) in large-scale curatorial settings presents us with a
rich potential data source to study curatorial actions. Through
routine use of systems such as Jira, data curators at ICPSR
generate a corpus documenting data curation work and the
frequency of specific curatorial activities. These data offer an
abundant source of information about the impact and efficacy
of different curatorial processes, and, given their scale and
structure, computational methods are useful for analyzing
them. ICPSR adopted Jira to facilitate existing work practices,
I. I NTRODUCTION
and the content staff generated in the system documented
Data curation – the work needed to make a dataset fit- the "articulation work" [10] of curation. Their comments
for-use over the long-term – is critical to eScience [1]–[5]. make visible the details of curation tasks and hint at the
Datasets are almost never analysis- or preservation-ready related organizational processes. Analysis of these work logs is
upon initial collection, and extensive pre-processing, cleaning, important in revealing the often hidden ways in which people,
transformation, documentation, and preservation actions are data, and data processing algorithms are brought together
required to support data's usability, sharing, and management to produce refined datasets ready for analysis. These work
over time. However, despite extensive development of data logs can thereby complicate accounts of data production as
curation best practices, the impacts that specific curatorial a straightforward pipeline, and instead show the importance
activities have on data use and reuse are unclear. We use of the "humans-in-the-loop". Additionally, identification and
supervised machine learning techniques to analyze a corpus analysis of curatorial activities is an important first step in
of Jira tickets documenting data curation activities at a large showing the long-term impact and value of these activities.
The eScience community has called for improved data
social sciences data archive, the Inter-university Consortium
for Political and Social Research (ICPSR) at the University of curation tools and analysis to support new ways of doing
Michigan. We ask: 1) What are the main tasks of curatorial work science [11]. Digital data archives like ICPSR are critical
at a large scale data science repository?; 2) How do curators components of knowledge infrastructures [12]. As sites of
document their curation work?; and 3) How do curatorial scientific data curation, they play a key role in managing,
preserving, and disseminating knowledge. ICPSR is a wellactions vary across projects and requests?
We define curatorial actions as the specific steps taken to established social science archive, curating data at scale across
improve data products' fitness-for-use or preservation readiness. a breadth of sub-disciplines ranging from criminal justice
These include tasks such as data normalization; creating and to early child care and education. Over 3,000 studies were
improving metadata and other documentation; the application deposited at ICPSR from 2016 to 2020 [13]. Human effort

Abstract-This paper describes a machine learning approach
for annotating and analyzing data curation work logs at ICPSR,
a large social sciences data archive. The systems we studied
track curation work and coordinate team decision-making at
ICPSR. Repository staff use these systems to organize, prioritize,
and document curation work done on datasets, making them
promising resources for studying curation work and its impact
on data reuse, especially in combination with data usage analytics.
A key challenge, however, is classifying similar activities so that
they can be measured and associated with impact metrics. This
paper contributes: 1) a schema of data curation activities; 2)
a computational model for identifying curation actions in work
log descriptions; and 3) an analysis of frequent data curation
activities at ICPSR over time. We first propose a schema of
data curation actions to help us analyze the impact of curation
work. We then use this schema to annotate a set of data curation
logs, which contain records of data transformations and project
management decisions completed by repository staff. Finally, we
train a text classifier to detect the frequency of curation actions
in a large set of work logs. Our approach supports the analysis of
curation work documented in work log systems as an important
step toward studying the relationship between research data
curation and data reuse.
Index Terms-data curation, research infrastructures, machine
learning, text classification, workflows

to curate data products is often rendered invisible to those
outside of such archives out of an impulse to create a "pristine"
dataset that deliberately obscures the curator's mark on it [14].
However, curators' specialized disciplinary knowledge and
labor is essential to the enterprise of data curation [15]. Our
research values curators' specialized knowledge by making
their labor explicit and measurable.
This paper describes a study of Jira work logs to better
understand common curation activities as a first step toward
connecting curation activities with data reuse and impact.
We recognize that, like any documentation system, ICPSR's
curation work logs are incomplete and vary in specificity;
even with the adoption of systems like Jira, some curation
work goes undocumented or is obscured by the level of
recorded detail. Therefore, our results tell a partial story about
curation activities; we likely underestimate how often actions
are performed. However, these estimates serve as a baseline
for defining categories of curation activity and measuring the
frequency and effort time of various categories of curatorial
actions performed on social science datasets.
We identified eight main categories of tasks that are frequently recorded in curation work logs, two of which were
non-curation or other kinds of activities (e.g., creating training
materials, attending staff meetings). Excluding these, quality
checks, initial review and planning, and data transformation
were the most frequent and time consuming curatorial activities
recorded across all of the studies in our analysis. On average,
curators spent more time on studies assigned higher levels
of curation, confirming that applying more intensive sets of
curation actions requires more staff time.
Our analysis covers a period of transition as ICPSR standardized its curation work; we observed changes in curation actions
over this time across levels of curation and between ICPSR
archives. The average amount of time spent curating studies
has been decreasing since 2017, signaling possible increases in
efficiency in ICPSR's curation practices. For example, fewer
instances of data transformation were performed over time on
deposits in topical archives while initial review and planning
became more common; in the ICPSR General Archive however,
the frequency of data transformation remained constant while
initial review and planning were performed more often.
For intensively curated studies, initial review and planning
was recorded more frequently than it was for non-intensive
studies; communication was also recorded more frequently for
intensively curated studies, although this decreased over time.
To analyze data curation work, we contribute:
1) a schema of data curation activities;
2) a computational model for identifying curation actions
in work log descriptions; and
3) an analysis of the frequency and effort associated with
particular curation activities.

as several researchers in this area have noted [17], [18],
the term is often ill-defined with little explication of the
specific activities that comprise "data curation." In an effort
to clarify what is meant by "data curation," a number of
researchers and practitioners have defined it by describing
what data curation enables. They define data curation as
"the management and promotion of data from the point of
its creation, ensuring the fitness of data for contemporary
purposes, and making data available for discovery and re-use"
[17], [19]; as "actions taken on data sets at any stage of their
existence...that enhance their use or reuse value" [20]; and as
"the process of managing research data throughout its lifecycle
for long-term availability and reusability" [21]. Johnston and
colleagues [22] call attention to repository curators as the key
actors in data curation, describing it as "work and actions taken
by curators of a data repository in order to provide meaningful
and enduring access to data."
To better get at what, precisely, this work and these actions
are, several studies have developed lists and taxonomies of
specific curation tasks; however, we have found that existing
vocabularies of curatorial actions are not readily applicable to
workflow documentation at ICPSR. The language of curation
documented in the literature differs from the terminology that
curators we studied used in practice. For instance, Johnston
and colleagues [22] create a ranked list of 47 individual
curation activities based on focus groups with researchers to
identify the curation activities they most valued, resulting in
"documentation" (ranked first); "chain of custody" (second);
and "secure storage" (third). Many terms describe automated
actions applied to all data in a large-scale repository like ICPSR
(e.g., "terms of use," "use analytics," and "secure storage")
and, hence, are not useful in studying the day-to-day work of
human data curators. Furthermore, while many of the activity
definitions (e.g., "Use metadata to link the data set to related
publications, dissertations, and/or projects that provide added
context for how the data were generated and why"), would be
readily recognized by the curators we studied, the terms (e.g.,
"contextualizing") are not consistent with the language ICPSR
curators use to characterize their work.
The Data Practices and Curation Vocabulary (DPCVocab) [9]
was derived from interviews with data managers in the earth
sciences. The DPCVocab links data curation practices with
products, curation workflows, and curation roles and functions
including stakeholders and stewards. One of the key aims
was to create a practical vocabulary for curators that could
be used in the curation of datasets from all fields of science.
Within the "curatorial actions and functions" category, the
DPCVocab includes categories of activities that are synonymous
with the function of an archive like ICPSR (e.g., ingest,
representation, provenance management, data storage, policies,
and preservation). However, it places the hands-on work with
data that is often part of curation work (e.g., validating data) as
II. BACKGROUND
"research data practices" performed primarily by the scientists
A. Data curation activities
who created the data. Lee and Stvilia [21] identify 14 research
Data curation plays a critical role in enabling accessibility, data activities and actions in an institutional repository, which
discovery, re-use, preservation, and data sharing [16]. Yet, span communication about data curation needs, managing and

sharing data, ensuring data accessibility, and re-evaluating
data for long term preservation. But again, these do not align
with ICPSR workflows. Finally, the RDA/TDWG Curatorial
Metadata and Attribution Model [23] proposes an abstract data
model for describing and citing curatorial work; rather than
prescribing specific curatorial tasks, it allows curators to receive
credit for work. However, this model is meant to be applied
in conjunction with an existing taxonomy of curatorial work,
and does not outline curatorial tasks.
This breadth of vocabularies regarding curatorial actions
reflects the diversity of data curation contexts and domains,
and points to a need for further research on the nature
of data curation work. These vocabularies were developed
through interview-based methods, not by examining existing
documentation; therefore they are not ideal for supporting text
extraction and classification of curatorial work logs. Though
there appear to be high-level commonalities between these
vocabularies, more specific accounts of data curation are needed
to render this important work visible, and to support institutions
in assessing the efficacy of their own curatorial pipelines. In
the section that follows, we outline data curation pipelines at
ICPSR so as to foreground the development of our own data
curation taxonomy.
B. Data curation at ICPSR
Prior studies suggest that the adoption of standards can make
implicit institutional practices, like data curation, more explicit
so that efforts and funds can be prioritized [24]. In 2017, ICPSR
took a significant step toward standardizing curation work by
centralizing curation staff into an organizational unit. ICPSR's
entire repository of social science data is organized around
several thematic collections or "topical archives" with each
archive corresponding to a particular social science research
audience (e.g., the National Addiction & HIV Data Archive
and the National Archive of Computerized Data on Aging).
Prior to the 2017 reorganization, each topical archive within
ICPSR employed its own team of curators, leading to unique,
project-specific approaches to curating data. Following the
reorganization, ICPSR also established a set of written curation
standards that grouped specific curatorial actions and outputs
into three different curation levels that vary with respect to
the amount, intensiveness, and complexity of effort required
as well as the end product.
All studies deposited at ICPSR receive a base level of
curation called "Level 1": curators conduct a disclosure risk
review and create a study webpage with subject terms and
relevant study information including a description, title, authors,
and relevant notes. Level 1 curation also includes an ICPSR
codebook and data files for all major statistical software
packages. "Level 2" builds on Level 1, further improving
the usability of data by ensuring missing values are identified
and documented, acronyms and abbreviations are spelled out,
spelling is checked and corrected, and labels are checked
for completeness and readability. "Level 3" is ICPSR's most
intensive level of curation; it develops custom documentation
for the data and adds survey question text to variables so that

they can be indexed and searched. This level is also applied to
non-tabular or non-numeric data such as GIS and qualitative
data. In developing our curatorial schema in Section III-B,
we include actions that are performed on every study across
curation levels but which vary in intensiveness; for example,
disclosure risk review is applied to all studies but Level 3
curation tends to involve more steps, such as reviewing all
variables and survey question text for disclosive information.
C. Recording curation activities
In an additional move to systematize the curation workflow,
ICPSR adopted Jira to document, prioritize, and communicate
curation work. Jira is a highly-customizable web-based tool,
most commonly used by software developers to plan, track, and
release software. Previous studies have performed text mining
on issue tracking systems to glean behavioral insights into the
communication styles of developers [25]. Systems like Jira offer
new ways to document and analyze the data curation process.
Given the need to rationalize return on investments for data
curation [26], we are interested in studying work management
systems to better understand the effects of particular curation
activities on data.
At ICPSR a curation "ticket" or "issue" is synonymous with a
"curation request"; therefore, we use the terms interchangeably.
Jira tickets are the primary means for making the request to
curate a study (often made up of multiple files); the tickets
list the necessary curation tasks, communicate about curation,
track time-stamped milestones, and document work from start
to resolution (i.e., study release).
III. M ETHOD
Jira tickets contain work logs, which describe the curation
work performed on deposited data (Figure 1). To identify
curation actions in the work logs, we first developed a schema
of curation activities and then manually labeled a subset of work
logs according to the type of data curation activity that each
described. We trained a supervised classifier using the labeled
data to predict curation actions in unlabeled work logs. We
compared results from two classification models (Complement
Naive Bayes and Stochastic Gradient Descent) to a baseline
model that applied labels proportionally. Below, we describe
our corpus, our pre-processing workflow, our annotation schema
and labeling method, and our classifier in more depth.
A. Jira ticket corpus and preprocessing
We analyzed a corpus of Jira tickets created between
February 2017 and December 2019. The start date coincided
with ICPSR's adoption of the Jira system; we omitted tickets
created from 2020 onward as curation was still in progress for
many of these at the time of writing. We included only tickets
with at least one work log entry written by ICPSR staff during
curation. The corpus of 669 Jira tickets corresponded to 566
unique studies.
We deidentified work log text by replacing curators' names
with linked anonymous identifiers. We segmented the work log

Table I
S UMMARY OF CURATORIAL ACTIONS IN ANNOTATION SCHEMA
Curatorial Action
Initial review and planning

Data transformation

Metadata

Documentation

Quality checks

Communication
Figure 1. Anonymized view of a curator's work logged in ICPSR's Jira
ticketing system

Other
Non-curation

Examples
Look at deposited files, determine curation
work needed, compose processing plan,
create processing history syntax
Locate identifiers, revise or add variable/value labels, designate or fix missing values, reorder/standardize/convert variables, create variable-level metadata, collapse categories for disclosure
Draft or revise study description, copy
metadata from deposit system, update collection dates based on dataset, create survey
question text, describe variable level labels
Create a codebook, document major
changes or issues with the data, compile
documentation archived by the data producer
Check all files and metadata for completeness, adherence to standards, alignment
with JIRA request after all data and documentation curation is complete (Self QC,
1QC, 2QC)
Discuss study with project manager, consult
supervisor on curation standards for study,
check how to handle specific variables
Compile folders for study, ambiguous or
overly-general curation work
Staff meetings, timesheets, administrative
work

descriptions into short fragments and applied term frequencyinverse document frequency [27] to identify important curatorial
The first four actions listed occur in succession; quality
phrases. This preliminary analysis suggested that the description
of curation activities within the work logs was not consistent. checks tend to happen at the end of other actions. CommuFor example, descriptions of quality checks included phrases nication for study is done as needed throughout the curation
such as "self-checks", "1QC," and "addressing identified issues." process. While we acknowledge similarities between the two
Many curation actions were also described with generic phrases; terms, we distinguished between documentation and study
for example, "wrapping up study" and "running scripts" imply metadata as the standalone human-readable descriptive files
activities that include quality checks but are not explicit enough (codebooks, record layouts, questionnaires, technical reports,
to classify as such without more context. Work logs tended etc.) and machine-readable descriptive information, respectively.
to overgeneralize work so that broad categories rather than We included other as a category to capture curation-related
specific actions were captured. Descriptions of work also varied actions outside of our designated categories. We also identified
non-curation actions, which we removed from our analysis as
in complexity, ranging in length from 1 to 204 words.
reported in Section IV.
We uploaded a proportional random sample of Jira tickets
B. Manual annotation schema
across curation levels 1-3 to a web-based annotation tool, BRAT
To account for the variety in the work log descriptions, [28]. We applied the schema to manually annotate a set of 789
we developed a schema of curation activities. We focused labeled curation actions from 10 randomly-selected tickets to
on curatorial actions that vary in application across studies use as training data for text classification.
by curation level and relative amounts of time spent. More
background and definitions for ICPSR's curation levels are C. Computational model for text classification
given in Section II-B. We first consulted with ICPSR interOur objective was to classify curation actions in unlabeled
nal documentation and curation supervisors to identify an curation work logs. Methods like supervised classifiers reduce
exhaustive list of curation actions. We then sorted these actions the labor needed to detect and distinguish specialized curatorial
according to how frequently they were performed (i.e., across language in short, unstructured text [29], [30]. We chose a
all studies vs. as needed) and how variable time spent on supervised approach that leveraged the manual annotations to
them was (i.e., a consistent amount of time vs. dependent train a machine classifier to recognize curation activities. Our
on the dataset). We used this initial set of frequent actions labeled data had many instances of quality checks and fewer
with high variability in our first annotation attempts; we then instances of study level metadata (Figure 2). To train a classifier
iteratively revised the schema as a team until we settled on to predict curation actions, we split the labels generated in
eight comprehensive, mutually exclusive categories of curatorial BRAT into 80% training and 20% testing datasets. We removed
actions (Table I).
stopwords from the labeled data and constructed ngrams of

time with respect to data curation levels and archives within
the repository.
A. Defining curatorial actions
The annotation schema of curation activities we developed
(Figure 3) allowed us to control for variation in curator
styles and changing norms in the use of Jira at ICPSR over
time. Given our interest in understanding the variation in
curation work across studies, we included actions that varied
in frequency and effort. We initially tested annotation with
a schema of 25 terms that included frequent instances of
each example in Table I (e.g., designate missing values as
Figure 2. Distribution of labeled curation actions for each schema class
a frequent instance of data transformation); however, given the
variation in detail of work log descriptions, this proved to be
Table II
too granular to apply consistently. We refined the schema
C OMPARISON OF SUPERVISED CLASSIFIER PERFORMANCE SCORES
to focus only on the broad categories of curation actions.
Classifier
Accuracy
F1
Precision
Recall
We also added non-curation and other categories so that we
Baseline
0.15
0.14
0.14
0.14
could differentiate usage of the Jira ticket (e.g., professional
SGD Classifier
0.73
0.72
0.74
0.72
development activities); for the purposes of our analysis, these
Complement NB
0.75
0.74
0.76
0.75
actions were not relevant. For further context on this distinction,
see Section V-A2. The initial categories of "disclosure risk
lengths 1 and 2. We then stored the labeled data as a document remediation" and "processing history" were merged under data
transformation in the revised schema. We also added quality
term matrix for retrieval with our classifier.
We trained a stratified baseline dummy model, which we checks and communication, as we found these actions were
compared to two other supervised approaches appropriate for applied across all studies but complemented the established
classifying large amounts of text: Complement Naive Bayes categories rather than falling within them.
We found that the revised schema was comprehensive enough
(NB) and a linear model, stochastic gradient descent (SGD).
to
annotate
the vast majority of work logs in Jira tickets sampled
Complement NB is suited for text classification tasks with
across
curation
levels. We only used the other category in
imbalanced class distributions [31] while SGD has been found
several
cases
where
an action was clearly supporting curation
to perform well on large, sparse text datasets [32]. Using these
but
fell
outside
of
the
established action types (e.g., "compiling
models, we achieved substantial gains in performance over the
folder")
or
was
ambiguous
(e.g., "curation work"). We also
baseline model. We compared the performance of the classifiers
found
that
many
instances
of
communication were documented
with an ensemble of performance measures including accuracy,
defined as the proportion of correctly predicted ground truth
labels (Table II). By this measure, the Complement NB
classifier gave the best predictions for the test classes.
We applied the trained classifier to identify curatorial actions
in the unseen Jira work logs. Work log syntax showed that
curators used new line and carriage key delimiters along with
line breaks and periods to separate curation actions in their
work logs. We used these delimiters to segment work log
descriptions into short fragments, resulting in 12,995 unseen
curation sentence fragments. We then predicted a curation
action for each fragment using our trained model.
IV. R ESULTS
Our schema defines the main tasks of data curation work at a
large data science repository, ICPSR. We find that the coverage
of the schema is sufficient to characterize and distinguish
general categories of intensive curatorial actions. We use our
computational model to detect curation actions in Jira tickets.
We interpret the model to understand the variety in the language
of curation, including sources of confusion that the classifier
encountered. Finally, we analyze the output of the classifier to Figure 3. Revised schema with activities distinguished by curation levels
characterize the degree to which curatorial work varies over (L1-L3)

Table III
D ESCRIPTION OF J IRA TICKET CORPUS OF CURATION REQUESTS

Table IV
S TUDIES RECORDING CURATION ACTIONS AND PERCENT OF HOURS
LOGGED ACROSS ALL STUDIES

Curation

Archive

Year

Level 1
Level 2
Level 3
BJS
ICPSR
Other
2017
2018
2019

Total
tickets
(n=669)
221
229
219
131
116
422
133
305
231

Total
studies
(n=566)
178
210
178
124
104
338
119
276
171

Average
curation
hours/study
51
79
165
78
105
102
107
99
88

Action

Percent of studies
containing action

Quality checks
Initial review and planning
Data transformation
Metadata
Documentation
Communication
Other

90.1
70.0
67.6
57.7
56.2
54.6
40.9

Percent of total
work log hours
classified as action
31.6
14.0
29.9
6.5
7.5
7.9
2.8

Each predicted curatorial action corresponded to an amount
of time logged in a ticket. To estimate hours associated with
each kind of curation activity, we summed the hours logged
for each kind of curation action, which we divided by the total
curation hours logged across all tickets. We report this as a
percent of total work log hours; we also report the frequency
of curation actions as the percent of studies containing each
type of action (Table IV).
Quality checks, initial review and planning, and data transformation were the most recorded activities across all of the studies
B. Detecting curatorial actions
in our analysis. We find that these curation actions are both
Overall, our model performed well (F1 = 0.74). It revealed frequent and time consuming. Other actions, including initial
several categories of curation work that were straightforward to review and planning, were also recorded frequently across
detect and others that created confusion, indicated by incorrect all studies but were not as time consuming in the aggregate.
predictions. The classifier performed best in detecting commu- Actions like communication were not recorded as frequently
nication, quality checks, non-curation, and data transformation across all studies, suggesting that the Jira ticketing system is
actions, suggesting that the language used to describe these used to record work done directly to data; the substrate of data
is internally consistent. While smaller in size, there was also work, including documentation and communication, does get
complete agreement between all predictions of metadata related recorded but is not logged as frequently or for as long of a
actions. When the model made mistakes, it confused data duration in aggregate as other kinds of actions, like quality
transformation with other classes. We inspected the mislabeled checks and data transformations.
instances and noticed that classes with the least confusion also
Our study covered a period of institutional transition starting
exhibited more homogeneity in the language used (e.g., "quality in 2017. To interpret how curation actions changed over this
checks"). It makes sense that a bag-of-words classifier would period, we examined differences in curatorial actions between
have lower performance when the language is more diverse as levels of curation and archives over time. Proportionally, quality
it is in initial review and planning and data transformation. checks and data transformations were the most frequently
Activities that are part of these general classes of curation recorded actions across curation levels and archives (Figure 4).
activity also overlap (e.g., talking to a supervisor about a data Tickets for Level 2 and 3 curation recorded more project related
transformation), which may explain the model's confusion.
communication than Level 1 curation. BJS, which is a topical
archive, recorded more instances of documentation than the
C. Identifying differences in curatorial actions
ICPSR General Archive and all other topical archives (grouped
We describe our corpus of Jira tickets along with information under "Other topical archives").
about the amount of time spent on curation (Table III).
From 2017 to 2019, data transformation, communication
More time on average was spent curating Level 3 studies, and other related curation actions decreased while initial review
supporting the idea that higher levels of curation tend to be and planning and documentation increased. Despite this, the
more time intensive. Relatively less time was spent curating proportions in which curation actions are applied across studies
studies in one of ICPSR's large topical archives – the Bureau of is consistent in the aggregate across curation levels and archives
Justice Statistics (BJS) within the National Archive of Criminal for our corpus of Jira tickets. In recent years, ICPSR has
Justice Data – compared to the ICPSR General Archive and been emphasizing the automation of repetitive curation tasks
all the other topical archives combined. The average amount to prioritize value-added work that requires human expertise;
of time spent curating studies has also been decreasing since ICPSR curators may be spending more time on actions like
2017, signaling possible changes in curation practices or their initial review of data and quality checks and less time on
efficiency.
data transformation due to this shift in priorities. Additionally,
alongside other actions (e.g., "asked about the content for
string responses... and if any additional information could be
provided") making it difficult in some cases to differentiate
discrete curation actions. In cases where curators described
transforming data or revising documentation in response to a
quality check, we labeled the action as a quality check even
though data transformation and documentation may have also
been relevant.

Figure 4. Proportion of curation actions recorded by study curation level,
archive, and year

thorough initial reviews may expedite data transformation.
V. D ISCUSSION

workflows. Many data curation workflow models [34], [35]
envision curation as a set of easily identifiable, sequential, and
discrete steps. The reality of work is much harder to itemize,
however. In our analysis, we find examples of curators working
in parallel rather than sequentially, which complicates many
accounts of data curation and data science workflows (and
which may explain why some instances of similar work fall into
different categories). Second, the schema of curatorial work
we have developed can be compared to taxonomies rooted
in other contexts [9], and thereby support a more nuanced
understanding of data practices across disciplines.
We also introduced a machine learning classifier that detected
curation activities identified in our schema. The classifier
performed well in detecting the most frequent categories of
curatorial actions (communication for study, quality checks, and
data transformation). However, no classifier is perfect, ours
included. In some cases, the classifier was confused by what
constituted a discrete action or distinctions between categories
of complementary actions, such as data transformations and
quality checks. An example of a mislabeled work log was "went
through processing history files", which was manually labeled
as a quality check because it referred to a person reviewing
the files that curators generate when editing data; however,
the classifier labeled it data transformation, likely because
other work logs with "processing history files" referred to the
process of generating that document instead of reviewing it.
Such examples highlight compound, multi-part, or iterative
decisions recorded by data workers, which are difficult to
render visible without an in-depth understanding of the work
context.
A. Implications for future work

Prior studies of curation work have primarily focused on
1) Measuring curation activities: Our analysis in Secarticulating the activities that fall under the category of data tion III-A focuses exclusively on the work log portion of
transformation [5], [9], [33]. Our results surface other kinds the Jira ticket. We plan to incorporate other parts of the ticket
of curation activities like quality checks and initial review; we such as the comments, which describe curatorial decisions in
additionally show that these meta-level review activities tend greater depth and incorporate the voices of project supervisors
to be even more frequent than data transformation. We also and managers in addition to curation staff. Analysis of the
find that activities like metadata creation, documentation of amount of time logged in Jira tickets will also allow us to
curatorial work, and communication with other stakeholders further understand the intensity of specific curatorial actions
take considerable time regardless of curation level and archive. (e.g., examining the differences between time estimated and
This broader portrait of curatorial work is important in the actual time required for curating studies). We will also
developing data curation pipelines and best practices going triangulate our findings from this study with analysis available
forward. Further work is needed to understand the impact from processing history files and through qualitative interviews
of these separate tasks on increasing the value of deposited with curators. Processing history files are internal documents
data. Accounting for the differential impacts of curation tasks that contain commented syntax with statistical commands
may help to define some of the value of depositing data in a used to transform the deposited data files. They document
trusted repository. For example, it is unclear whether individual all work done and changes made to deposited data, supporting
researchers or teams preparing data for self-deposit attend to reproducibility for data curation as new waves of data are added
quality checks with as much detail as professional curators.
to a study. These files are referenced in work logs and provide
This study contributes a novel schema of data curation more granular information about specific actions including data
activities. Though specific to the workflows of ICPSR, we transformations and documentation steps. This will provide
believe it has several applications beyond this. First, the method richer detail about how work is coordinated within the curation
used to develop the schema may be adapted in other contexts, unit and by staff outside of it.
both to facilitate the analysis of similar collections of work logs,
Adding a second level of granularity to our schema will
and also to foster a better understanding of curatorial work and allow us to detect and differentiate specific curation activities.

This will involve a second round of annotation with a version deposited data. We also recognize that the transcripts curators
of the more detailed schema in Section III-B. We are also generate in Jira are public within the organization; that means
interested in understanding typical sequences of curation that the comments of subordinates (e.g., curators) are visible
actions in workflows and which actions tend to co-occur. to those with power (e.g., supervisors, directors). Prior work
Our computational model in Section III-C incorporates both acknowledges that "what can be part of any public transcript
individual words and bigrams, preserving common sequences is also a matter of struggle" [37]; we must remember that
of terms in work logs rather than modeling work log text some comments or work descriptions will be purposefully
as an unstructured bag of words. To improve the classifier, vague or missing in order to resist preservation, domination, or
we will account for the order in which curation activities other forms of control. Plantin [36] argues that data processors
tend to occur; including factors for order of operations could use micro-resistance such as socializing and communicating
address some of the confusion exhibited by the classifier. For expertise to avoid the alienation that can accompany their work.
example, initial review and planning tends to happen at the Future analysis must take into account the power dynamics at
beginning of curation work while quality checks occur prior play in creating this documentation.
to and immediately after release of a study.
Prior studies of issue tracking systems like Jira have
We will also compare frequent curation actions by archives mitigated similar issues by narrowly constraining their analysis
to characterize the impacts that standardization efforts have had to issue resolution time or by extracting sentiments from
on curation work at ICPSR over the past several years (e.g., developer discourse [25], [38]. The detection of non-curation
how adopted or mandated curation practices have percolated actions, however, gives a more complete picture of a curator
through the organization). Identifying curation work is a first participating in the wide range of activities that an organization
step towards analyzing the relationship between data curation values but that extend beyond traditional curation work (e.g,
and use. The larger goal of our research is to connect curation professional development). ICPSR's practice of capturing these
activities with measures of data use and impact, including trust activities, and our model's ability to detect them, raise larger
in data and in the archive itself [14], [36].
questions about what it means to meaningfully engage human
2) Understanding the impact of organizational structure curators in an increasingly standardized curation pipeline.
on curation work: We believe our analysis has revealed Activities like professional development that are important
the continued impact of legacy organizational structures on to archives and their employees are not readily captured by
curatorial work. ICPSR moved curation work out of topical taxonomies of curatorial action, and it will continue to be
archives and into a central unit in 2017, which changed important to account for these kinds of resources as we support
the relationships between curators, archives, and data; the the humans in the data curation loop. Any model that doesn't
organization then adopted Jira to track and manage curation account for these activities is missing a critical part of what it
requests and work. The work logs available for analysis did means to be an employee in any organization.
not extend far enough back to reveal traces of ICPSR's prior
3) Characterizing data curation – it's not always a pipeline:
organizational structure and heterogeneity between topical Our itemized approach to detecting curation activities fits a
archives. However, we did see evidence of evolving work narrative in which the data processing pipeline is theorized as
practices following the centralization of the Curation Unit, the a factory-like workflow [36]. In interrogating the limits of our
adoption of Jira, and standardized levels of curation. During own approach, we ask what a system like Jira might afford
the initial months of Jira use, curators broke curation requests data workers in resisting both invisibility and accounting. One
into multiple tickets, with one ticket for each phase of curation, possible example of passive resistance to such invisibility is
while in later years, each curation request was a single ticket the variety of ways the Jira system is used. Our analysis of
that covered all phases.
ticket length and complexity of syntax showed differences in
Additional context about Jira implementation, including the ways that curators used the Jira system. The amount of
reporting artifacts, is needed to interpret our findings. Data detail that curators included in their entries, approximated by
transformations likely include work that curators do to make the length of work log entries, varied from the detailed to
data compatible with ICPSR's tools and methods; in such the minimal or vague (e.g., "worked on curating study"). This
cases, there is effort spent that is unique to the institution and also suggests variability in the value individual curators place
indirectly about improving the data. Curators must also track on work logs; some may view work logs as administrative
work by projects, and so they use Jira tickets to indicate all busywork and therefore document the bare minimum while
hours worked, which include non-curation activities. While we others may incorporate work logs into their personal task
removed non-curation actions from our analysis, they accounted management practices, seeing a benefit to adding more detail.
for a sizable portion of logged actions. This high level of non- Worklogs make curators' work more visible, make it possible to
curation activity may not be reported in the same way in other acknowledge contributions, and capture divisions of labor: these
systems where curators are not required to keep track of their goals are in line with Plantin's [36] steps to the emancipation
work hours in this way. Clerical work and other miscellaneous of data workers. Balancing the visibility and exposure that
tasks are a necessary part of daily work in a large organization, work logs create is a critical challenge for organizations that
but capturing these as part of a curation workflow obscures employ them. We look forward to further exploring the benefits
some of the higher value efforts made to improve the quality of and risks of rendering curatorial work visible.

VI. C ONCLUSION
Our annotation schema suggests that curation work is not
limited to data transformation or adequately captured by
pipeline analogies. Instead, curatorial actions include many
types of work such as communication and documentation
that have not been effectively captured in prior descriptions.
Our computational model identifies myriad curation actions
and provides a mechanism for measuring their frequency and
duration. By automating the analysis of curation work logs,
we enable research that studies evolving curation activities.
We illustrate the kinds of insights our model can reveal about
curation work. Many of the activities that are core to the
curators' work (e.g., communicating about a study, reviewing
curation plans) do not fit neatly into pipeline metaphors or
narrow depictions of curation work. Instead, we show that
planning, communication, and quality review are central to the
curation work of data archives.
VII. ACKNOWLEDGMENT
We thank ICPSR curation supervisors including Rujuta
Umarji, Julie Eady, Sharvetta Sylvester, Sara Del Norte, Lindsay
Blankenship, Katey Pillars, Meghan Jacobs, and Scott Liening
who provided feedback on our schema of curatorial actions.
We also thank Amy Pienta (ICPSR) and Jeremy York (UMSI)
for their comments on earlier drafts. This material is based
upon work supported by the National Science Foundation under
grant 1930645. This project was made possible in part by the
Institute of Museum and Library Services LG-37-19-0134-19.
VIII. AUTHOR C ONTRIBUTIONS
Conceptualization, L.H., A.T., and D.A.; Methodology, S.L.,
L.H., A.T., D.A., and D.B.; Resources, D.B.; Data Curation,
D.B.; Writing - Original Draft, S.L., A.T., L.H., D.A., and
D.B.; Supervision, A.T. and L.H.; Project Administration, L.H.
and D.B.
R EFERENCES
[1] P. Lord, A. Macdonald, L. Lyon, and D. Giaretta, "From data deluge to
data curation," in Proceedings of the UK e-science All Hands meeting,
vol. 440, 2004, pp. 371–375.
[2] M. Pennock, "Digital curation: A life-cycle approach to managing and
preserving usable digital information," Library & Archives, vol. 1, no. 1,
pp. 1–3, 2007.
[3] C. Goble, R. Stevens, D. Hull, K. Wolstencroft, and R. Lopez, "Data
curation + process curation=data integration + science," Brief. Bioinform.,
vol. 9, no. 6, pp. 506–517, Dec. 2008.
[4] C. L. Palmer, N. M. Weber, and M. H. Cragin, "The analytic potential of
scientific data: Understanding re-use value," Proceedings of the American
Society for Information Science and Technology, vol. 48, no. 1, pp. 1–10,
2011.
[5] L. Johnston, J. Carlson, P. Hswe, C. Hudson-Vitale, H. Imker, W. Kozlowski, R. Olendorf, and C. Stewart, "Data curation network: How
do we compare? A snapshot of six academic library institutions' data
repository and curation services," Journal of eScience Librarianship,
vol. 6, no. 1, p. e1102, Feb. 2017.
[6] A. M. Pienta, G. C. Alter, and J. A. Lyle, "The enduring value of social
science research: the use and reuse of primary research data," in The
Organisation, Economics and Policy of Scientific Research Workshop,
2010.
[7] M. Daniels, I. Faniel, K. Fear, and E. Yakel, "Managing fixity and
fluidity in data repositories," Proceedings of the 2012 iConference on iConference '12, pp. 279–286, 2012.

[8] E. Yakel, I. Faniel, and Z. Maiorana, "Virtuous and vicious circles in
the data lifecycle," Information Research, vol. 24, no. 2, 2019.
[9] T. C. Chao, M. H. Cragin, and C. L. Palmer, "Data practices and curation
vocabulary (DPCVocab): An empirically derived framework of scientific
data practices and curatorial processes," Journal of the Association for
Information Science and Technology, vol. 66, no. 3, pp. 616–633, 2015.
[10] A. Strauss, "The articulation of project work: An organizational process,"
Sociological Quarterly, vol. 29, no. 2, p. 174, Jun. 1988.
[11] T. Hey, S. Tansley, and K. M. Tolle, Jim Gray on eScience: A transformed
scientific method. Redmond, WA: Microsoft research, 2009, pp. xvii–
xxxi.
[12] C. L. Borgman, A. Scharnhorst, and M. S. Golshan, "Digital data archives
as knowledge infrastructures: Mediating data sharing and reuse," Journal
of the Association for Information Science and Technology, vol. 70, no. 8,
pp. 888–904, Aug. 2019.
[13] "ICPSR Collection Development Report 2016-2020," Internal report:
unpublished, 2020.
[14] J.-C. Plantin, "Data cleaners for pristine datasets: Visibility and invisibility
of data processors in social science," Science, Technology, & Human
Values, vol. 44, no. 1, pp. 52–73, Jan. 2019.
[15] M. H. Cragin, C. L. Palmer, J. R. Carlson, and M. Witt, "Data sharing,
small science and institutional repositories," Philosophical Transactions
of the Royal Society A: Mathematical, Physical and Engineering Sciences,
vol. 368, no. 1926, pp. 4023–4038, 2010.
[16] C. Tenopir, S. Allard, K. Douglass, A. U. Aydinoglu, L. Wu, E. Read,
M. Manoff, and M. Frame, "Data sharing by scientists: Practices and
perceptions," PLoS One, vol. 6, no. 6, p. e21101, Jun. 2011.
[17] J. Carlson, "Demystifying the data interview: Developing a foundation for
reference librarians to talk with researchers about their data," Reference
Services Review, vol. 40, no. 1, pp. 7–23, Jan. 2012.
[18] A. Pham, "Surveying the state of data curation: a review of policy and
practice in UK HEIs," Ph.D. dissertation, University of Strathclyde, Aug.
2018.
[19] L. Carpenter, "Taxonomy of digital curation users," Digital Curation
Centre User Requirements Analysis, Tech. Rep., 2004.
[20] P. T. Darch, A. E. Sands, C. L. Borgman, and M. S. Golshan, "Library
cultures of data curation: Adventures in astronomy," Journal of the
Association for Information Science and Technology, vol. 71, no. 12, pp.
1470–1483, Dec. 2020.
[21] D. J. Lee and B. Stvilia, "Practices of research data curation in
institutional repositories: A qualitative view from repository staff," PLoS
One, vol. 12, no. 3, p. e0173987, Mar. 2017.
[22] L. R. Johnston, J. Carlson, C. Hudson-Vitale, H. Imker, W. Kozlowski,
R. Olendorf, and C. Stewart, "How important are data curation activities
to researchers? Gaps and opportunities for academic libraries," Journal
of Librarianship and Scholarly Communication, 2018.
[23] A. E. Thessen, M. Woodburn, D. Koureas, D. Paul, M. Conlon, D. P.
Shorthouse, and S. Ramdeen, "Proper attribution for curation and
maintenance of research collections: Metadata recommendations of the
RDA/TDWG working group," Data Science Journal, vol. 18, no. 1, p. 54,
Nov. 2019.
[24] M. S. Mayernik, "Research data and metadata curation as institutional issues," Journal of the Association for Information Science and Technology,
vol. 67, no. 4, pp. 973–993, Feb. 2016.
[25] M. Ortu, G. Destefanis, B. Adams, A. Murgia, M. Marchesi, and
R. Tonelli, "The JIRA repository dataset: Understanding social aspects
of software development," in Proceedings of the 11th International
Conference on Predictive Models and Data Analytics in Software
Engineering, no. Article 1. New York, NY, USA: Association for
Computing Machinery, Oct. 2015, pp. 1–4.
[26] C. Parr, C. Gries, M. O'Brien, R. R. Downs, R. Duerr, R. Koskela,
P. Tarrant, K. E. Maull, N. Hoelbelheinrich, and S. Stall, "A discussion
of value metrics for data repositories in earth and environmental sciences,"
Data Science Journal, vol. 18, no. 1, p. 58, Dec. 2019.
[27] K. Spärck Jones, "A statistical interpretation of term specificity and its
application in retrieval," Journal of Documentation, vol. 28, no. 5, pp.
111–121, 1972.
[28] P. Stenetorp, S. Pyysalo, G. Topić, T. Ohta, S. Ananiadou, and J. Tsujii,
"BRAT: a web-based tool for NLP-Assisted text annotation," in Proceedings of the Demonstrations at the 13th Conference of the European
Chapter of the Association for Computational Linguistics. Avignon,
France: Association for Computational Linguistics, Apr. 2012, pp. 102–
107.

[29] S. Bird, E. Klein, and E. Loper, Natural Language Processing with
Python: Analyzing Text with the Natural Language Toolkit. Sebastapol,
CA: O'Reilly Media, Inc., Jun. 2009, pp. 221–233.
[30] L. Hemphill and A. M. Schöpke-Gonzalez, "Two computational models
for analyzing political attention in social media," in Proceedings of the
International AAAI Conference on Web and Social Media, vol. 14, May
2020, pp. 260–271.
[31] J. D. Rennie, L. Shih, J. Teevan, and D. R. Karger, "Tackling the poor
assumptions of Naive Bayes text classifiers," in Proceedings of the 20th
International Conference on Machine Learning (ICML-03), 2003, pp.
616–623.
[32] T. Zhang, "Solving large scale linear prediction problems using stochastic gradient descent algorithms," in Proceedings of the Twenty-First
International Conference on Machine Learning. New York, NY, USA:
Association for Computing Machinery, Jul. 2004, p. 116.
[33] J. Doty, J. Herndon, J. Lyle, and L. Stephenson, "Learning to curate,"
Bulletin of the Association for Information Science and Technology,
vol. 40, no. 6, pp. 31–34, 2014.
[34] S. Higgins, "The DCC curation lifecycle model," pp. 134–140, 2008.
[35] "DDI Lifecycle," https://ddialliance.org/Specification/DDI-Lifecycle/,
accessed: 2021-4-14.
[36] J.-C. Plantin, "The data archive as factory: Alienation and resistance of data processors," Big Data and Society, vol. 8, no. 1, p.
20539517211007510, Mar. 2021.
[37] S. Gal, "Language and the "arts of resistance"," Cultural Anthropology,
vol. 10, no. 3, p. 410, Aug. 1995.
[38] A. Murgia, G. Concas, R. Tonelli, M. Ortu, S. Demeyer, and M. Marchesi,
"On the influence of maintenance activity types on the issue resolution
time," in Proceedings of the 10th International Conference on Predictive
Models in Software Engineering, ser. PROMISE '14. New York, NY,
USA: Association for Computing Machinery, Sep. 2014, pp. 12–21.

