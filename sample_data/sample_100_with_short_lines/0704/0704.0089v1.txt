a general approach to statistical modeling of physical laws
nonparametric regression
igor grabec

arxiv07040089v1 physicsdataan 1 apr 2007

faculty of mechanical engineering university of ljubljana
akereva 6 pp 394 1001 ljubljana slovenia
dated february 14 2013

abstract
statistical modeling of experimental physical laws is based on the probability density function
of measured variables it is expressed by experimental data via a kernel estimator the kernel
is determined objectively by the scattering of data during calibration of experimental setup a
physical law which relates measured variables is optimally extracted from experimental data by
the conditional average estimator it is derived directly from the kernel estimator and corresponds
to a general nonparametric regression the proposed method is demonstrated by the modeling of a
return map of noisy chaotic data in this example the nonparametric regression is used to predict
a future value of chaotic time series from the present one the mean predictor error is used in
the definition of predictor quality while the redundancy is expressed by the mean square distance
between data points both statistics are used in a new definition of predictor cost function from
the minimum of the predictor cost function a proper number of data in the model is estimated
pacs numbers 0250r0705t0545a8990n8435i0620dk



also at amanova kantetova 75 1001 ljubljana slovenia



electronic address igorgrabecfsuniljsi url httpwwwfsuniljsilasin

1

i

introduction

a basic task of physical description of natural phenomena is to express relations between
experimental data about measured variables in terms of physical laws 1 since the corresponding analytical modeling essentially depends on the intuition of the explorer performing
it an ambiguity surrounds this basic task and there thus arises a question how this could be
avoided this problem becomes of fundamental practical importance when developing intelligent electronic systems for automatic modeling of physical laws 2 the ambiguity could
be avoided if a unique objective method of modeling was found that would take into account
common properties of experimental observations and of transitions from experimental data
to models the aim of this article is to show how such a method could be developed from
basic principles of probability and statistics as well as to demonstrate an example of its
applicability
a common property of all experimental explorations is that each experiment corresponds
to a process proceeding from preparation to execution if we want a selected experiment
to yield any information about the phenomenon under observation then the result of the
experiment may not be determined in advance ie several outcomes of the experiment must
be possible the next common property is repeatability of experiments consequently a
correct presentation of experimental observations requires the use of a distribution of experimental results and this must be related to the concept of probability the probability
distribution is therefore a common basis for the description of natural properties in terms
of experimental data 3 while the transition from experimental data to an analytical expression of the corresponding probability distribution function is the crucial problem of
modeling an objective solution of this problem represents statistical modeling of the probability distribution function by a nonparametric kernel estimator if the kernel is determined
by a calibration of the experimental setup 4 5 6 for this purpose the central theorem of
probability theory and the maximum entropy principle provide a quite general route to the
specification of the kernel function of the estimator in this case an experimental physical
law which represents a relation between observed variables can also be generally expressed
by applying the theory of optimal statistical estimators the resulting nonparametric regression is the conditional average ca which can be automatically extracted from the
probability density function pdf of experimental data in a measurement system the
2

complete approach to modeling thus appears objective independent of the intuition of the
observer and consequently generally applicable for automatic execution due to these convenient properties ca is widely applicable in various fields of natural and technical sciences
2
a nonparametric expression of the pdf by the kernel estimator has already been proposed
by parzen 7 8 but weaknesses of his proposal are that the kernel function is arbitrarily
introduced and that there is an assumption that its width should decrease to zero when the
number of data is increased to infinity in order to avoid this weakness we specify the kernel
function objectively by the scattering of the measurement system output during calibration
7 8 the only ambiguity in the expression of the pdf is then related to the number of
experimental data which according to parzens assumption should not be limited since an
infinite number of experiments cannot be performed there arises a fundamental question
how many experiments is it reasonable to perform in order to explore the phenomenon
properly by a given experimental setup intuitively we can conclude that it is reasonable to
repeat experiments for as long as they bring new information however with an increasing
number of experiments the acquired data points become ever more concentrated in the
sample space and consequently the repetition of the experiments becomes redundant this
is observed when distances between data points become comparable to the width of the kernel
function this reasoning led recently to a specification of an information cost function c
4 5 6 9 10 11 12 13 for this purpose the indeterminacy of measurements was first
expressed in terms of information entropy which further led to definition of the experimental
information i and the redundancy r of experiments using these statistics the information
cost function was expressed by the difference c  r  i from the position of its minimum
a proper number of experiments can then be objectively determined 4 5 6
estimation of the information cost function is related to the calculation of integrals which
is inconvenient in a multivariate case therefore another statistic with similar properties
but more simple calculation is sought since it has been shown previously that the predictor quality exhibits similar properties to the experimental information we utilize it here
in the definition of the predictor cost function from its minimum a proper number of
experiments can also be estimated if this is used as a proper number for the adaptation of
the nonparametric regression to data provided by experiments the modeling of the corresponding physical law can be performed automatically on a data acquisition system of the
3

experimental setup to demonstrate this possibility we first briefly describe the nonparametric regression and then turn to the definition of the predictor quality redundancy and
cost function properties of all statistics are subsequently demonstrated in the modeling of
a return map corresponding to a noisy chaotic process

ii

fundamentals of nonparametric modeling
a

description of kernel function

let us consider a phenomenon that can be described by just two joint variables since the
generalization to a multivariate case is straightforward a single result of joint measurement
is represented by the couple z  x y we next assume that the phenomenon can be
characterized statistically by repetition of measurements yielding sample points zn  xn  yn 
in the joint span of a two channel instrument sz  sx  sy 
since the instruments are generally subject to stochastic disturbances the results of
measurements are scattered even during repetition of calibration 9 the scattering can be
described by the data provided by a series of repeated simultaneous calibrations of both
instrument channels for this purpose we have to perform a joint measurement on an
object representing two physical units ux and uy which we denote together by the joint unit
u  ux  uy  the scattering of instrument outputs during calibration is characterized by the
joint pdf zu which we call the scattering function sf 2 4 9 when the interaction
between both channels is negligible the sf is given by the product zu  xux yuy 
without loss of generality we further consider a case with equal channels which are subject
to mutually independent random disturbances that do not depend on u in such cases
the central limit theorem of probability theory as well as the maximum entropy principle
suggest that we express the sf of a particular channel by the gaussian function


1
x  ux 2
gx  ux    
exp 
2
2 

1

the parameters ux   represent the mean value and standard deviation of signal x at the calibration and can be statistically estimated from given data the joint sf is then determined
by the product z  u  gx  ux   gy  uy  
when reporting experimental results experimentalists most often only specify mean values and standard deviations of variables during calibration the maximum entropy principle
4

tells us that in such cases the gaussian function is the best choice for sf 2 9

b

nonparametric estimation of pdf pertaining to experimental data

when we perform a single measurement we get a sample z1  x1  y1 that represents
the mean value of z during measurement and therefore we express the pdf as z  z1  
x  x1 y  y1  when we repeat the measurements n times we get a set of samples
zi  1  i  n by which we model the joint pdf by the statistical average
n
1 x
z  zi
f z 
n i1

2

that represents the kernel estimator
properties of the particular components x y are described by the marginal pdfs
f x f y they are obtained from the joint pdf by integration with respect to one component for example
n
1 x
f zdy 
f x 
x  xi 
n i1
sy

z

3

for modeling natural laws the most important is the conditional pdf of the variable y at
a given value of x defined as
pn
z  zi 
f z
f yx 
 pni1
f x
j1 x  xj 
c

4

estimation of a physical law

distributions of joint experimental data for example that shown in fig 1 often resemble
a ridge along some hypothetical line yo x which we want to extract from the given data
in an optimal way for this purpose we select from a set of joint data only those that
pertain to some selected x these joint data generally exhibit various values of y which we
try to represent by a single value called the predictor of the variable y from a given value x
we consider as an optimal predictor of the hypothetical yo the value yp at which the mean
square prediction error is minimal
eyp  y2x  minyp 

5

5

70
60
50

pdf

40
30
20
10
0
1
08
06
04
02
0

y

0

01

03

02

06

05

04

07

09

08

1

x

fig 1 the joint pdf f z utilized to demonstrate the properties of the conditional average
estimator

here e   x denotes the operation of statistical averaging at given condition x the mini

mum satisfies the equation deyp  y2 xdyp  0 that yields as the optimal predictor yp
the conditional average
yp x  eyx 

z

y f yx dy

6

sy

by using eq 4 we obtain for the conditional average the expansion
yp x 

pn

yi x  xi  
pi1
n
j1 x  xj  



n
x

yi bi x

7

i1

the coefficients of this expansion are sample values yi  while the basis functions are
x  xi  
bi x  pn

x

x


j
j1

8

and satisfy the following conditions
n
x

bi x  1 

i1

0  bi x  1

9

the basis functions bi x can be interpreted as a normalized measure of similarity between
the given value of x and its sample value xi  at a given x the sample value ym contributes
most to the estimated value yp x whose complementary sample value xm is most similar to
x
the calculation of yp x corresponds to an associative recall of memorized items which
is a property of an intelligence therefore the estimator yp x could be treated as a basis
for the development of a machine intelligence based on modeling of natural laws the
6

conditional average given in eq 7 in fact corresponds to a normalized radial basis function
neural network which is equivalent to a multilayer perceptron  the basic paradigm used in
the theory of artificial neural networks 2 21

iii
a

characteristics of the model
predictor quality

a predictor maps the stochastic variable x to a new stochastic variable yp that generally differs from the variable y when the variables x y are related by some hypothetical
physical law yo x and the measurement noise is small the first and second statistical moments ey  yp  ey  yp 2  of the prediction error are also small the second moment

is ey  yp 2   vary  varyp   2covy yp  my  myp 2  where e m var cov

denote statistical average mean value variance and covariance respectively in the case of
statistically independent variables y and yp with equal mean values the last two terms are
zero and we get ey  yp 2   vary  varyp  with respect to this relation we define
the predictor quality relatively by the formula
ey  yp 2 
vary  varyp 
2covy yp 
my  myp 2


vary  varyp  vary  varyp 

q  1

10

the quality is 1 if the prediction is exact yp  y while it is 0 if y and yp are statistically
independent and have equal mean values the quality q may be negative if my 6 myp 
r
for the predictor defined by the conditional average yp x  y f yx dy we analytically
obtain the equalities my  myp  and covy yp  varyp  which yield
q

2varyp 

vary  varyp 

11

from the definition of the conditional average it follows 0  varyp   vary and therefore
0  q  1 this inequality need not be fulfilled exactly if ca is statistically estimated from
a finite number of samples
with an increasing n we generally expect that the ca statistically estimated by eq 7
increasingly better represents the governing physical law and consequently that the corresponding predictor quality q on average increases to a certain limit value as mentioned
7

previously an unlimited increase in the number of experiments is experimentally impossible
and consequently there arises the question how to determine a proper number no of data
that will yield a judicious estimation of the governing law

b

redundancy and predictor cost function

to answer the last question we have analyzed various experimental cases which have
shown us that with an increasing number of experimental samples the value of predictor
quality generally stabilizes when the distance between data points becomes similar to the
width  of the scattering function therefore it is not reasonable to surpass significantly
the corresponding number of data this can be achieved if a ratio of  and a proper
measure of distance  between neighbor data points is considered for this purpose we
introduce  over the mean value of minimum square distance between data points  2 
eminxi  xj 2  yi  yj 2  i  1    n j  1    n  and define a measure of redundancy
of data by the relative variable
r  2n

2
2

12

since  2 is comprised of two terms denoting contributions from x and y components a
factor 2 is utilized in the nominator the fraction 2 2  2 represents an average increase of
redundancy that is assigned to the acquisition of a new data point in order to take into
account acquisition of n data points factor n is further used with respect to this we
introduce the predictor cost function by the sum
c  rq1
2
ey  yp 2 
 2n 2 


vary  varyp 

13

the constant 1 is inserted in the first row in order to obtain a more simple expression
in the second row of eq 13 in the same way as the definition of the information cost
function given in 5 6 the cost function is here expressed in a relative form comprised
of two terms the first corresponds to the redundancy of experiments due to inaccurate
measurements while the second represents the influence of acquisition of information about
the phenomenon by experiments with an increasing number of samples n the redundancy
on average increases while the second term decreases with the decreasing error therefore
the cost function c exhibits a minimum at some no that represents a proper number of data
8

needed for the modeling of the physical law governing the phenomenon explored however
the influence of the first term becomes prevailing when the distance between data points 
becomes essentially smaller than the width  of the scattering function

iv

example

to demonstrate the properties of the ca estimator we utilize the data generated by a
noisecorrupted chaotic return map with the span sx  0 1 this example is used because
similar cases often appear in the analysis of chaotic time series 2 14 the basic problem
in such an analysis is to extract the return map from a given record of time series that
is influenced by additive noise of instrumental origin in our case we apply analytically
determined data to provide for a comparison between the original and extracted physical
law and to make feasible an objective reproduction of the complete method the basic
governing law is here given by the logistic map
n1  38 n 1  n 

14

while the initial value 1 is arbitrary selected from the interval 0 1 using a random generator to the values of generated chaotic series the gaussian noise  of zero mean value and
standard deviation   01 is added to simulate an additive noise of measurement the iterative solution of eq 14 then yields a series of noise corrupted chaotic values xn  n  n 
figure 2 shows two records of such a series that were used in modeling and testing of the
proposed method
from the series xn  n  1    the joint samples of the basic variables x y are obtained
by treating the successive value of xn as the dependent variable yn  xn1  the generator
of data is thus analytically described by the rule
xn  n  n
yn  xn1 

15

while the governing law is given by yo  38 x1x the sample points xn  yn  n  1    n
are distributed along the corresponding parabola in the sample space according to our
previous treatment the standard deviation  corresponds to the width of the instrument
scattering function  the joint pdf shown in fig 1 is determined by the kernel estimator
9

1

09

08

07

xn

06

05

04

03

02

01
x
xt
0

0

5

10

15

20

25

30

n

fig 2 records of the basic  x and the testing  xt noise corrupted chaotic series
testing of ca predictor
18

16

yo
y
yt
yp
er

14

12

y

1

08

06

04

02

0

02

0

02

04

06

08

1

x

fig 3 testing of the ca predictor graphs represent the governing law yo and basic data y 
top two        test yt and predicted data yp  middle two         and prediction
error er  yp  yt  bottom  the upper two parabolas are displaced successively by 035
in the vertical direction for better visualization

eq 2 using 200 data while a reduced set of 30 data is further utilized to demonstrate
the properties of the conditional average estimator the data obtained from the pure chaos
generator are shown by yo   in the top parabola of fig 3 while the basic noisecorrupted
data y   are shown by points scattered around pure data points
the conditional average estimator is obtained by inserting data from the basic data
10

mean square predictor error
008

007

006

 er 2 

005

004

003

002

001

0

0

5

10

15

20

25

30

n

fig 4 mean square prediction error ey  yp 2  as a function of the data number n 

set into eq 7 to demonstrate its performance we additionally generated with different
seeds of random generators a set of nt  60 test data xti  yti based on data xti from
this set the corresponding values of ypi are predicted by the ca estimator the test and
predicted data are shown in fig 3 by the middle two sets of points    and   
the prediction error er  yp  yt  calculated from both data sets is presented by 
at the bottom of fig 3 relatively small differences between predicted and test points
indicate that the properties of the governing law yo x are properly modeled by the ca
estimator to confirm this qualitative conclusion we next analyze the properties of statistics
eye  yt 2  q  2  r c depending on the the number of data n used in modeling the
number of test data is kept constant nt  60 during calculation of these statistics properties
of the statistical model of the governing law depend on sets of samples utilized in modeling
and testing to demonstrate this dependence we repeated the modeling and testing three
times using various statistical sample sets
the mean square predictor error ey  yp 2  is presented in fig 4 versus number of
samples n its value varies statistically but on average it decreases with the increasing
number n statistical fluctuations are largest at small n and significantly depend on samples
used in modeling however with the increasing n the statistical fluctuations are ever less
pronounced if the number of test samples nt is much larger than the number of samples n
changing the testing sample set does not significantly influence the properties of estimated
statistics which is the case in our demonstration this is the reason why we use the value
11

predictor quality
1

09

08

07

q

06

05

04

03

02

01

0

0

5

10

15

20

25

30

n

fig 5 predictor quality q as a function of the data number n 

nt  60
the predictor quality q as determined from the prediction error is presented in fig 5
versus number of samples n for each data set the statistical fluctuations decrease with
increasing n so that qualities calculated from different data sets converge to the same limit
value with increasing n the curves determined from different data sets merge approximately at n  11 the quality is there  097 and rises to  098 at n  30 at n  11
the difference between the curves obtained from different data sets is about two orders
of magnitude smaller than the corresponding quality with respect to these properties we
could conjecture that in the present case about 11 data values already provide for a judicious
modeling of the governing law yo x by the ca predictor
to confirm our last conjecture we turn to the determination of the predictor cost function
for this purpose let us first analyze the properties of the mean square distance between data
points  2  the corresponding graph shown in fig 6 indicates that  2 is rather monotonously
decreasing with the number of samples with the approximate dependence being  1n
consequently the corresponding redundancy r is increasing with n similarly as  n 2 
this conclusion is confirmed by the graph in fig 7
following the definition given by eq 13 we obtain from the estimated error and the
redundancy the predictor cost function c shown in fig 8 its minimum is not very pronounced from various statistical data sets we obtain the estimates of the minimal value
co  00330006 the corresponding number no  102 confirms our previous conjecture
12

msd between data points
1

09

08

07

2

06

05

04

03

02

01

0

0

5

10

15

20

25

30

n

fig 6 mean square distance between data points 2 as a function of the data number n 
redundancy
012

01

r

008

006

004

002

0

0

5

10

15

20

25

30

n

fig 7 redundancy r as a function of the data number n 

stemming from the analysis of predictor quality
with an increasing number of samples n the quality qn of the ca predictor exhibits
a convergence to some limit value q that characterizes hypothetical maximum quality of
proposed nonparametric statistical modeling this limit value generally increases with the
decreasing scattering width  related to this the minimal value of cost function is diminished and takes place at a larger no  for instance at   0005 we get co  0018  0003
and no  14  3 however the limit value of the quality q is less than 1 if 1 and n are
finite this means that it is not possible to exactly determine the governing physical law
13

predictor cost function
1

09

08

07

c

06

05

04

03

02

01

0

0

5

10

15

20

25

30

n

fig 8 predictor cost function c as a function of the data number n 

y  yo x from joint data obtained by an instrument influenced by stochastic disturbances

v

discussion

our method of estimation of natural laws from given data can be simply generalized to
multivariate cases by substituting corresponding vectors for the variables x y such modeling
has already been applied in a variety of examples stemming from physical 15 technical
2 16 economic 2 16 and medical environments 2 17 18 particularly in economic
and medical environments phenomena are often characterized by many variables that could
be either informative or disturbing due to the complexity of such cases there usually
exists little or no information about a possible function that could describe the governing
law in relation to this researchers are faced with the problem of how to define complexity
and to reduce it by extracting informative variables from a given set 19 alongside mutual
information the predictor quality could also be applied for this purpose for instance it has
been recently shown in the field of medicine how an analysis of predictor quality can provide
for an ordering of variables and the extraction of a set that yields an optimal predictor of
the disease healing process 17 18 such an analysis makes feasible further progress towards
the origins of the treated disease
the value of the proper number no  as defined by the minimum of predictor cost function
14

could be interpreted as a measure of the complexity of an adequate predictor model it is
important that this measure depends only on the accuracy of observation and properties of
the phenomenon represented by given experimental data
in relation to the example demonstrated here there emerges an important conclusion
about the description of natural phenomena by physical laws in the form y  yo x as
long as such a law is considered as the only basis for the description of the phenomenon
it is not sufficient for a complete description since no information is provided about the
properties of the sample space of joint data consider a well known example  the law
m  v that relates the mass m the volume v and the density  of an object this
law does not include the restriction m  0 and is in this aspect not complete similar
but much more complex examples are met when treating chaotic phenomena and their
strange attractors 14 for example the law applied here is a special case of the law
n1  a n 1  n  with a being a constant depending on the value of a and the
starting value 1  the series n  n  1    exhibits at large values of parameter n 
 either a discrete or a continuous sample space moreover in the continuous case the
sample space can be comprised of disconnected intervals which could hardly be predicted
analytically similar but still more cumbersome is the situation if we consider chaotic
processes with continuous parameters consequently a governing law y  yo x appears
incomplete for description of the phenomenon the most outstanding deficiency is that it
does not include information about the structure of the sample space corresponding to the
observed phenomenon this deficiency does not appear if we consider as a basis for modeling
the probability density function and estimate it nonparametrically directly from measured
joint data the extraction of a law that describes a relation between variables can then be
generally performed by using the conditional average estimator however applications of
simple parametrical laws like m  v  are of tremendous importance for analytical sciences
and we do not expect that the proposed nonparametric models could substitute for them
although they are convenient for direct applications consequently the question arises of
how to find a univocal link between both paradigms of modeling

15

vi

conclusions

our approach indicates that the objectively introduced kernel estimator provides for a
nonparametric statistical modeling of a quantitatively explored phenomenon since no a
priori information about the form of the governing physical law is required the modeling
can be automatically performed by a computer in a measurement system the proposed
predictor cost function c provides for estimating the proper number no of data needed for
the modeling properties of the predictor cost function resemble those of information cost
function 5 6 but its estimation is much more simple the properties of the extracted
model of the governing law can be quantitatively described by the predictor quality q and
redundancy r of data from which the governing law is extracted this law represents
the distribution of the variable y at a given value x by a single predicted value yp x
such a compressed representation generally corresponds to creation of information about
the explored phenomenon 5 6 this is in contrast to the loss of information caused by
stochastic disturbances in signal transmission channels 20 if the extraction of information
from observations is considered as a basis of natural intelligence 21 22 then a system
capable of estimating a physical law from measured data autonomously must be treated as
an intelligent unit such an interpretation provides a common basis for a unified treatment
of experimental sciences and natural or artificial intelligence 2 21 22

acknowledgments

this work was supported by the ministry of higher education science and technology
of the republic of slovenia and eu  cost

1 r feynman the character of physical law the mit presscambridge ma 1994
2 i grabec and w sachse synergetics of measurement prediction and control springerverlag berlin 1997
3 r e collins found physics 35 734 2005
4 i grabec eur phys j b 22 129 2001
5 i grabec eur phys j b 48 279 2005 doi 101140epjbe2005003910

16

6 i grabec arxivcsit0612027 v1 5 2006
7 e parzen ann math stat 35 1065 1962
8 r o duda and p e hart pattern classification and scene analysis j wiley and sons
new york 1973 ch 4
9 j c g lesurf information and measurement institute of physics publishing bristol 2002
10 j risanen complexity entropy and the physics of information addisonwesley 1990 ed
w h zurek 117125
11 j rissanen ieee trans inf theory 42 40 1996
12 t m cover and j a thomas elements of information theory john wiley  sons new
york 1991
13 a n kolmogorov ire trans inf theory it2 102 1956
14 f c moon chaotic and fractal dynamics john wiley  sons inc new york 1992
15 s mandelj i grabec and e govekar int j bifurcation and chaos 11 2731 2001
16 m thaler i grabec and a poredo physica a 35 46 2005
17 i grabec and d groelj comput methods in biomech biomed engin 6 319 2003
18 i grabec i ferkolj and d groelj proc of 2nd international conference on computational
intelligence in medicine and healthcare lisbon cimed2005 proceedings isbn 0863415202iee 2005 ed j m fonseca 311316
19 c h bennett complexity entropy and the physics of information addisonwesley 1990
ed w h zurek 137148
20 c e shannon and w weaver the mathematical theory of communication univ of illinois
press urbana 1949
21 s haykin neural networks a comprehensive foundation mcmillan college publishing
company new york 1994
22 d j c mackay information theory inference and learning algorithms cambridge university press cambridge uk 2003

17

