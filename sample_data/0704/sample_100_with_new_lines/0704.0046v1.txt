a limit relation for entropy
and channel capacity per unit cost
arxiv07040046v1 quantph 1 apr 2007
imre csiszr14  fumio hiai25 and dnes petz34
alfrd rnyi institute of mathematics
h1364 budapest pob 127 hungary
graduate school of information sciences tohoku university
aobaku sendai 9808579 japan
abstract in a quantum mechanical model disi feldmann and kosloff
arrived at a conjecture stating that the limit of the entropy of certain mixtures
is the relative entropy as system size goes to infinity the conjecture is
proven in this paper for density matrices the first proof is analytic and uses
the quantum law of large numbers the second one clarifies the relation to
channel capacity per unit cost for classicalquantum channels both proofs
lead to generalizations of the conjecture
key words shannon entropy von neumann entropy relative entropy capacity per unit cost holevo bound
email csiszarrenyihu partially supported by the hungarian research grant otka t068258
email hiaimathistohokuacjp partially supported by grantinaid for scientific research
b17340043
email petzmathbmehu partially supported by the hungarian research grant otka t068258
introduction
it was conjectured by disi feldmann and kosloff in 4 based on thermodynamical
considerations that the von neumann entropy of a quantum state equal to a mixture
rn 
  n1      n2      n1  
exceeds the entropy of a component asymptotically by the umegaki relative entropy
sk that is
srn   n  1s  s  sk
as n   here  and  are density matrices acting on a finite dimensional hilbert
space recall that s  tr  log  and
tr log   log 
if supp   supp 
sk 
otherwise
concerning the background of quantum entropy quantities we refer to 10 12
apparently no exact proof of 1 has been published even for the classical case although for that case a heuristic proof is offered in 4
in the paper first an analytic proof of 1 is given for the case supp   supp  using
an inequality between the umegaki and the belavkinstaszewski relative entropies and
the weak law of large numbers in the quantum case in the second part of the paper it
is clarified that the problem is related to the theory of classicalquantum channels the
essential observation is the fact that srn   n  1s  s in the conjecture is a
holevo quantity classicalquantum mutual information for a certain channel for which
the relative entropy emerges as the capacity per unit cost
the two different proofs lead to two different generalizations of the conjecture
an analytic proof of the conjecture
in this section we assume that supp   supp  for the support projections of  and 
one can simply compute
srn kn   trrn log rn  rn log n 
 srn   n  1tr  log   tr  log 
hence the identity
srn kn   srn   n  1s  sk  s
holds it follows that the conjecture 1 is equivalent to the statement
srn kn   0 as n  
when supp   supp 
recall the belavkinstaszewski relative entropy
sbs k  tr log 12 1  12   tr 12 12 
if supp   supp  where t  t log t see 1 10 it was proved by hiai and petz
that
sk  sbs k
see 6 or proposition 711 in 10
theorem 1 if supp   supp  then srn   n  1s  s  sk as n  
proof we want to use the quantum law of large numbers see proposition 117 in
10 assume that  and  are d  d density matrices and we may suppose that  is
invertible due to the gnsconstruction with respect to the limit  of the product
states n a  tr n a on the nfold tensor product md cn  n  n all finite tensor
products md cn are embedded into a von neumann algebra m acting on a hilbert
space h if  denotes the right shift and x  12 12  then rn is written as
rn  12 n
 i x 12 n 
n i0
by inequality 2 we get
0  srn kn   sbs rn kn 
 tr n  12 n rn 12 n
1x i
 x  
n i0
where  is the cyclic vector in the gnsconstruction
the law of large numbers gives
1x i
 x  i
n i0
in the strong operator topology in bh since x  tr 12 12  1
since the continuous functional calculus preserves the strong convergence simply due
to approximation by polynomials on a compact set we obtain
1x i
 x  i  0 strongly
n i0
this shows that the upper bound 3 converges to 0 and the proof is complete
by the same proof one can obtain that for
rmn 
1 m
  n1     m  n2      n1   m 
the limit relation
srmn   n  1s  ms  msk
holds as n   when m is fixed
in the next theorem we treat the probabilistic case in a matrix language the proof
includes the case when supp   supp  is not true those readers who are not familiar
with the quantum setting of the previous theorem are suggested to follow the arguments
below
theorem 2 assume that  and  are commuting density matrices then srn   n 
1s  s  sk as n  
proof we may assume that   diag1      l  0     0 and   diag1      d 
are d  d diagonal matrices 1      l  0 and l  d we may consider   in a matrix
algebra of bigger size if  is invertible if supp   supp  then l1      d  0
this will be called the regular case when supp   supp  is not true we may assume
that d  0 and we refer to the singular case
the eigenvalues of rn correspond to elements i1      in  of 1     dn 
i i    in  i1 i2 i3    in      i1    in1 in 
n 1 2
we divide the eigenvalues in three different groups as follows
a a corresponds to i1      in   1     dn with 1  i1      in  l
b b corresponds to i1      in   1     dn which contains exactly one d
c c is the rest of the eigenvalues
if the eigenvalue 5 is in group a then it is
i1 i1       in in 
i1 i2    in 
first we compute
i1 in
i1 i1       in in 
i1    in 
below the summations are over 1  i1      in  l
x  i i       i i 
i1    in
i i
x i i       i i 
i1    in logi1    in   qn
i i
i1 i2    in log ik 
i1 i2    in log ik
i1 in
i1 in
i1 i2    in log ik
i1 in
n  1
ik log ik 
 n  1s 
ik log ik
 qn
 qn
i log i  qn 
where
qn 
i1 in
i1 i1       in in 
i1    in 
consider a probability space
 p  1     ln  1      l n 
where 1      l n is the product of the measure on 1     l with the distribution
1      l  for each n  n let xn be a random variable on  depending on the
nth 1     l so that the value of xn at i  1     l is i i  then x1  x2     are
identically distributed independent random variables and qn is the expectation value of
x1      xn
the strong law of large numbers says that
x1      xn
i almost surely
 ex1  
since x1      xn n is uniformly bounded the lebesgue bounded convergence
theorem implies that
qn  
as n  
in the regular case
hence we have
i  1 qn  0 and all nonzero eigenvalues are in group a
srn   n  1s  s  
i log i 
i log i  qn  sk  qn
and the statement is clear
next we consider the singular case when we have
  n  1s  o1
and we turn to eigenvalues in b if the eigenvalue corresponding to i1      in  
1     dn is in group b and i1  d then the eigenvalue is
d i2    in 
it follows that
      
x  d  i     i 
d i2
log
i2 in
d x
i2    in  logi2    in  
log
n i i
log 
 n  1s 
when i2  d     in  d we get the same quantity so this should be multiplied with n
  d n  1s  d log
we make a lower estimate to the entropy of rn in such a way that we compute  
when  runs over a and b it is clear now that
srn   n  1s  s 
  n  1s  s
 d n  1s  d log n  o1  
as n  
interpretation as capacity
a classicalquantum channel with classical input alphabet x transfers the input x  x
into the output w x  x which is a density matrix acting on a hilbert space k we
restrict ourselves to the case when x is finite and k is finite dimensional
if a classical random variable x is chosen to be the input with probability distribution
p  px  x  x  then the corresponding output is the quantum state x 
xx pxx  when a measurement is performed on the output quantum system it
gives rise to an output random variable y which is jointly distributed with the input x
if a partition of unity fy  y  x  in bk describes the measurement then
proby  y  x  x  tr x fy
x y  x 
according to the holevo bound we have
ix  y   hy   hy x  ix w   sx  
pxsx 
which is actually a simple consequence of the monotonicity of the relative entropy under state transformation 7 see also 11 ix w  is the socalled holevo quantity or
classicalquantum mutual information and it satisfies the identity
pxsx k  ix w   sx k
where  is an arbitrary density
the channel is used to transfer sequences from the classical alphabet x  x1  x2      xn  
x is transferred into the quantum state w n x  x  x1 x2   xn  a code for
the channel w n is defined by a subset an  x n  which is called a codeword set the decoder is a measurement fy  y  x n  the probability of error is probx 6 y  where
x is the input random variable uniformly distributed on an and the output random
variable is determined by 6 where x and y are replaced by x and y
the essential observation is the fact that srn   n  1s  s in the conjecture
is a holevo quantity in case of a channel with input sequences x1  x2      xn   0 1n
and outputs x1  x2      xn  where 0   1   and the codewords are all
sequences containing exactly one 0 more generally we shall consider holevo quantities
 1 x 
1 x
ia 0  1   s
sx 
a xa
a xa
defined for any set a  0 1n of binary sequences of length n
the concept related to the conjecture we study is the channel capacity per unit cost
which is defined next for simplicity only in the case where x  0 1 the cost of a
character 0  x is 1 while the cost of 1  x is 0
for a memoryless channel with a binary input alphabet x  0 1 and an   0 a
number r  0 is called an achievable rate per unit cost if for every   0 and for any
sufficiently large t  there exists a code of length n  t with at least et r codewords
such that each of the codewords contains at most t 0s and the error probability is at
most  the largest r which is an achievable per unit cost for every   0 is the
channel capacity per unit cost
lemma 1 for an arbitrary a  0 1n 
ia 0  1   cas0 k1 
holds where
ca 
1 x
i  xi  0
a xa
proof let cx  i  xi  0 for x  a since ia 0  1  is a particular holevo
quantity ix w n  we can use the identity 8 to get an upper bound
1 x
1 x
sx kn
cxs0 k1   cas0 k1 
1  
a xa
a xa
for ia 0  1 
lemma 2 if a  0 1n is a code of the channel w n  whose probability of error for
some decoding scheme does not exceed a given 0    1 then
1   log a  log 2  ia 0  1 
proof the righthand side is a bound for the classical mutual information ix y  
hy   hy x where y is the channel output see 7 since the error probability
probx 6 y  is smaller than  application of the fano inequality see 3 gives
hxy    log a  log 2
therefore
ix  y   hx  hxy   1   log a  log 2
and the proof is complete
the above two lemmas shows that the relative entropy s0 k1  is an upper bound
for the channel capacity per unit cost of the channel w 0  0 and w 1  1 with
a binary input alphabet in fact assume that r  0 is an achievable rate for every
  0 and t  0 there is a code a  0 1n for which we get by lemmas 1 and 2
t s0 k1   cas0 k1   ia 0  1 
 1   log a  log 2
 1  t r    log 2
since t is arbitrarily large and   are arbitrarily small r  s0 k1  follows that
s0 k1  equals the channel capacity per unit cost will be verified below
theorem 3 let the classicalquantum channel w  x  0 1  bk be defined as
w 0  0   and w 1  1   assume that an  0 1n is chosen such that
a each element x  x1  x2      xn   an contains at most l copies of 0
b log an  log n  c as n  
can  
1 x
i  xi  0  c
an  xa
as n  
for some real number c  0 and for some natural number l if the random variable xn
has a uniform distribution on an  then
1 x
lim sxn  
sx   csk
an  xa
the proof of the theorem is divided into lemmas we need the direct part of the
socalled quantum stein lemma obtained in 6 see also 2 5 9 12
lemma 3 let 0 and 1 be density matrices for every   0 and 0  r  s0 k1 
if n is sufficiently large then there is a projection e  bkn  such that
n e  tr n
0 i  e  
and for n e  tr n
1 e the estimate
log n e  r
holds
note that n is called the error of the first kind while n is the error of the second
kind
lemma 4 assume that   0 0  r  s0 k1  l is a positive integer and the
sequences x in an  0 1n contain at most l copies of 0 let the codewords be the
nfold repetitions xn  x x     x of the sequences x  an  if n is the integer part
log
and n is large enough then there is a decoding scheme such that the error probability is
smaller than 
proof we follow the probabilistic construction in 13 let the codewords be the nfold repetitions xn  x x     x of the sequences x  an  the corresponding output
density matrices act on the hilbert space kn n  kn n  we decompose this hilbert
space into an nfold product in a different way for each 1  i  n let ki be the
tensor product of the factors i i  n i  2n     i  n  1n so k is identified with
k 1  k2      kn 
for each 1  i  n we perform a hypothesis testing on the hilbert space ki  the
0hypothesis is that the ith component of the actually chosen x  an is 0 based on
the channel outputs at time instances i i  n     i  n  1n the 0hypothesis is
tested against the alternative hypothesis that the ith component of x is 1 according
to the quantum stein lemma lemma 3 given any   0 and 0  r  sk for n
sufficiently large there exists a test ei such that the probability of error of the first kind
is smaller than  while the probability of error of the second kind is smaller than en r 
the projections ei and i  ei form a partition of unity in the hilbert space ki  and
the nfold tensor product of these commuting projection will give a partition of unity in
kn n  let y  0 1n and set fy  ni1 fyi  where fyi  ei if yi  0 and fyi  i  ei
if yi  1 therefore the result of decoding can be an arbitrary 01 sequence in 0 1n 
the decoding scheme gives y  0 1n in such a way that yi  0 if the tests accepted
the 0hypothesis for i and yi  1 if the alternative was accepted the error probability
should be estimated
proby 6 xx  x 
tr n
x fy 
yy6x
x y
tr n
xi fyi
yy6x i1
tr n
xj fyj 
i1 yyi 6xi j1
tr n
xi i  fxi 
if xi  0 then
tr n
xi i  fxi   tr 0 i  ei   
because it is an error of the first kind when xi  1
tr n
xi i  fxi   tr 1 ei  e
from the error of the second kind it follows that l  nen r is a bound for the error
probability the first term will be small if  is small the second term will be small
if n is large enough if both terms are majorized by 2 then the statement of the
lemma holds we can choose n so large that n defined by the statement should be large
enough
proof of theorem 3 since lemma 1 gives an upper bound that is
1 x
lim sup sxn  
sx   csk
an  xa
it remains to prove that
1 x
sx   csk
lim inf sxn  
an  xa
lemma 4 is about the ntimes repeated input x n and describes a decoding scheme
with error probability at most  according to lemma 2 we have
1   log an   1  sx n  
1 x
sxn 
a xa
from the subadditivity of the entropy we have
sx n   nsx 
and
sxn   nsx 
holds due to the additivity for product it follows that
1  
log an 
1 x
sx 
 sx  
an  xa
from the choice of n in lemma 4 we have
log an 
log n
log an 
log n log n  log 2  log 
and the lower bound is arbitrarily close to cr since r  s0 k1  was arbitrary the
proof is complete
references
1 vp belavkin and p staszewski calgebraic generalization of relative entropy and
entropy ann inst henri poincar sec a 371982 5158
2 i bjelakovi j deuschel t krger r seiler r siegmundschultze and a szkola
a quantum version of sanovs theorem comm math phys 2602005 659671
3 t m cover and j a thomas elements of information theory second edition
wileyinterscience hoboken nj 2006
4 l disi t feldmann and r kosloff on the exact identity between thermodynamic
and informatic entropies in a unitary model of friction int j quantum information
42006 99104
5 m hayashi quantum information an introduction springer 2006
6 f hiai and d petz the proper formula for relative entropy and its asymptotics in
quantum probability comm math phys 1431991 99114
7 as holevo some estimates for the amount of information transmittable by a quantum communication channel in russian problemy peredachi informacii 91973
311
8 ma nielsen and il chuang quantum computation and quantum information
cambridge university press cambridge 2000
9 t ogawa and h nagaoka strong converse and steins lemma in quantum hypothesis testing ieee tans inf theory 462000 24282433
10 m ohya and d petz quantum entropy and its use springer 1993
11 m ohya d petz and n watanabe on capacities of quantum channels prob
math stat 171997 179196
12 d petz lectures on quantum information theory and quantum statistics book
manuscript in preparation
13 s verdu on channel capacity per unit cost ieee trans inform theory 361990
10191030
