arXiv:0704.0099v1 [math.FA] 1 Apr 2007

On Ando's inequalities for convex and concave
functions

Koenraad M.R. Audenaert
Institute for Mathematical Sciences, Imperial College London,
53 Prince's Gate, London SW7 2PG, United Kingdom

Jaspal Singh Aujla
Department of Applied Mathematics, National Institute of Technology,
Jalandhar 144011, Punjab, India

Abstract
For positive semidefinite matrices A and B, Ando and Zhan proved the inequalities
|||f (A) + f (B)||| ≥ |||f (A + B)||| and |||g(A) + g(B)||| ≤ |||g(A + B)|||, for any
unitarily invariant norm, and for any non-negative operator monotone f on [0, ∞)
with inverse function g. These inequalities have very recently been generalised to
non-negative concave functions f and non-negative convex functions g, by Bourin
and Uchiyama, and Kosem, respectively.
In this paper we consider the related question whether the inequalities |||f (A) −
f (B)||| ≤ |||f (|A − B|)|||, and |||g(A) − g(B)||| ≥ |||g(|A − B|)|||, obtained by Ando,
for operator monotone f with inverse g, also have a similar generalisation to nonnegative concave f and convex g. We answer exactly this question, in the negative
for general matrices, and affirmatively in the special case when A ≥ ||B||.
In the course of this work, we introduce the novel notion of Y -dominated majorisation between the spectra of two Hermitian matrices, where Y is itself a Hermitian
matrix, and prove a certain property of this relation that allows to strengthen the
results of Bourin-Uchiyama and Kosem, mentioned above.
Key words: Matrix norm inequality, Convex functions, Majorization.
1991 MSC: 15A60

Email addresses: k.audenaert@imperial.ac.uk (Koenraad M.R. Audenaert),
aujlajs@nitj.ac.in (Jaspal Singh Aujla).

Preprint submitted to Elsevier

24 December 2013, 18:22

1

Introduction

In [1], Ando proved the following inequalities for positive semidefinite (PSD)
matrices A, B, and any unitarily invariant (UI) norm. For any non-negative
operator monotone function f (t) on [0, ∞):
|||f (A) − f (B)||| ≤ |||f (|A − B|)|||,

(1)

and, when f (0) = 0 and f (∞) = ∞, and g is the inverse function of f ,
|||g(A) − g(B)||| ≥ |||g(|A − B|)|||.

(2)

In a later paper [2], Ando and Zhan proved the related inequalities (with the
same conditions on f and g)
|||f (A) + f (B)||| ≥ |||f (A + B)|||,

(3)

|||g(A) + g(B)||| ≤ |||g(A + B)|||.

(4)

The conditions on f are satisfied by every operator concave function f with
f (0) = 0.
Inequality (4) was generalised by Kosem [7] to non-negative convex functions
g on [0, ∞), with g(0) = 0. Inequality (3) was generalised very recently to any
non-negative concave function on [0, ∞) by Bourin and Uchiyama [5], who
also gave a simpler proof of Kosem's result.
In the same spirit, we consider the question whether inequalities (1) and (2)
can also be generalised to non-negative concave f and convex g, respectively.
After introducing the necessary prerequisites in Section 2, we present our main
results concerning this question in Section 3. Regrettably, most of our results
are negative answers, and we give counterexamples to this generalisation. The
answer is even negative for the special case A ≥ B, although the apparent
hardness of finding counterexamples had led us temporarily into believing
that in that case the generalisation might actually hold.
All is not bad news, however. In Section 4 we answer the question affirmatively
in the special case when A ≥ ||B||. In Section 5, we introduce the novel notion
of Y -dominated majorisation between the spectra of two Hermitian matrices,
where Y is itself a Hermitian matrix. We prove a certain property of this
relation, namely Proposition 3, which we subsequently use, first in a rather
destructive fashion. To wit, the Proposition has been instrumental in finally
discovering a counterexample to the generalisation of (1) for A ≥ B; this will
2

be reported in Section 6. On the more constructive side, the Proposition also
allows to strengthen the results of Bourin-Uchiyama and Kosem mentioned
above. This is the topic of the final Section, along with a few other applications.

2

Preliminaries

In this Section, we introduce the notations and necessary prerequisites; a more
detailed exposition can be found, e.g. in [4]. We will use the abbreviations LHS
and RHS for left-hand side and right-hand side, respectively.
We denote the vector of diagonal entries of a matrix A by Diag(A).
We denote the absolute value by | * |, both for scalars and for matrices. For
matrices this is defined as |A| := (A∗ A)1/2 . Similarly, we denote the positive
part of a real scalar or Hermitian matrix by (*)+ , and define it by A+ :=
(A + |A|)/2.
In this paper, we are mainly concerned with monotonously increasing convex
and concave functions from R to R. Kosem noted in [7] that any such function
can be approximated by a sum of angle functions x 7→ ax + b(x − x0 )+ , where
a ≥ 0, and b > 0 for a convex angle function (b < 0 for a concave one).
We are also concerned with the unitarily invariant (UI) matrix norms, which
we denote by ||| * |||, and which are defined in terms of the singular values
σj (*) of the matrix only. We adopt the customary convention that the singular
values are sorted in non-increasing order: σ1 ≥ σ2 ≥ . . . ≥ σd . Special cases
of these norms are the operator norm || * ||, which is just equal to the largest
singular value σ1 (*), and the Ky Fan norms || * ||(k) , which are defined as the
sum of the k largest singular values:
||A||(k) :=

k
X

σj (A).

j=1

The famous Ky Fan dominance theorem states that a matrix B dominates
another matrix A in all UI norms if and only if it does so in all Ky Fan norms.
The latter set of relations can be written as a weak majorisation relation
between the vectors of singular values of A and B:
σ(A) ≺w σ(B) :

k
X

j=1

σj (A) ≤

k
X

σj (B), ∀k.

j=1

For PSD matrices, the above domination relation translates to a weak majorisation between the vectors of eigenvalues: λ(A) ≺w λ(B).
3

Weyl's monotonicity Theorem ([4], Corollary III.2.3) states that
λ↓k (A) ≤ λ↓k (A + B),

∀k,

for Hermitian A and positive semi-definite B. Here, λ↓ (A) denotes the (real)
vector of eigenvalues of A sorted in non-increasing order.
Finally, we refer the reader to Chapter 2 of [6] for an exposition of a number
of important functional analytic properties of eigenvalues and corresponding
eigenspaces of a Hermitian matrix, which we will need in the proof of Proposition 2.

3

Main Results

The question we start with is about the straightforward generalisation of inequality (2) to non-negative convex functions.
Question 1 For all A, B, ≥ 0, for all UI norms, and for non-negative convex
functions g on [0, ∞) with g(0) = 0, does the inequality |||g(A) − g(B)||| ≥
|||g(|A − B|)||| hold?
The answer to this question is negative, as shown by the following counterexample. We consider the convex angle function g(x) = x + (x − 1)+ and the
operator norm. For the 2 × 2 PSD matrices


A=


0.9



0

0 0.6

,



B=




0.8 0.5 
,
0.5 0.4

(5)

the eigenvalues of g(|A − B|) are 0.65249 and 0.35249, while those of g(A) −
g(B) are 0.65010 and −0.48862. Thus, ||g(|A − B|)||∞ = 0.65249, which is
larger than ||g(A) − g(B)||∞ = 0.65010. ✷
Under the additional restriction A ≥ B, the absolute value in the argument
of g in the right-hand side vanishes, leading to a simplified statement, and a
second question, with better hopes for success. Introducing the matrix ∆ =
A − B,
Question 2 For all B, ∆ ≥ 0, for all UI norms, and for non-negative convex
functions g on [0, ∞) with g(0) = 0, does the inequality |||g(B +∆)−g(B)||| ≥
|||g(∆)||| hold?
This restricted case also turns out to have a negative answer. Counterexamples, however, were much harder to find, and required a reduction of the prob4

lem based on certain results about a novel majorisation-like relation, which
we call the Y -dominated majorisation. This will be the subject of Sections 5
and 6, where a number of Propositions of independent interest are proven.
It is also very reasonable to ask:
Question 3 For all B, ∆ ≥ 0, for all UI norms, and for non-negative concave
functions f on [0, ∞), does the inequality |||f (B + ∆) − f (B)||| ≤ |||f (∆)|||
hold?
In fact, if this were true, a positive answer to Question 2 would easily follow,
using the same reasoning that was used in [5] to derive the generalisation of
(3) from the generalisation of (4).
Again, this statement is false, as the following counterexample shows. Consider
the concave angle function f (x) = min(x, 1) = x − (x − 1)+ , and the 3 × 3
PSD matrices


0.701816
0.317887
0.198910







B =  0.317887

and

1.014950

0.198910 −0.093826



∆=

 0.192713



0





−0.093826 



0.274236

0
0.446505

0



0



.
0


0 0.455416



One gets
||f (∆)||∞ = 0.455416
while
||f (B + ∆) − f (B)||∞ = 0.455776.
✷
In Section 4, we consider an even more restricted special case, in which the
inequalities (1) and (2) finally do hold. We actually prove that a stronger
relationship holds in this special case:
Proposition 1 For non-negative, monotonously increasing and concave functions g, and A, B ≥ 0 such that A ≥ ||B||, we have
λ↓ (g(A − B)) ≥ λ↓ (g(A) − g(B)).

(6)

An easy Corollary is the corresponding statement for monotonously increasing
convex functions.
5

Corollary 1 Let f be a non-negative convex function on [0, ∞) with f (0) = 0.
Let A, B ≥ 0 such that A ≥ ||B||. Then
λ↓ (f (A − B)) ≤ λ↓ (f (A) − f (B)).

(7)

Proof. Let f = g −1 , with g satisfying the conditions of Proposition 1. Replace
in (6) A by f (A) and B by f (B), yielding
λ↓ (g(f (A) − f (B))) ≥ λ↓ (A − B).
Applying the function f on both sides does not change the ordering, because
of monotonicity of f , and yields validity of inequality (7). ✷
These two results obviously imply the corresponding majorisation relations,
and by Ky Fan dominance, relations in any UI norm.

4

Proof of Proposition 1

We want to prove inequality (6):
λ↓ (g(A) − g(B)) ≤ λ↓ (g(A − B)),
for A, B ≥ 0, A ≥ ||B||, and concave, monotonously increasing and nonnegative g.
W.l.o.g. we will assume ||B|| = 1, since any other value can be absorbed in
the definition of g.
It is immediately clear that if (6) holds for g that in addition satisfy g(0) = 0,
then it must also hold without that constraint, i.e. for functions g(x) + c, with
c ≥ 0. This is because the additional constant c drops out in the LHS, while
λ↓ (g(A − B) + c) ≥ λ↓ (g(A − B)).
Furthermore, (6) remains valid when replacing g(x) with ag(x), for a > 0.
Thus, w.l.o.g. we can assume g(0) = 0 and g(1) = 1. Together with concavity
of g, this implies that, for 0 ≤ x ≤ 1, g(x) ≥ x, while for x ≥ 1, the derivative
g ′ (x) ≤ 1.
Since 0 ≤ B ≤ 11, and for 0 ≤ x ≤ 1, g(x) ≥ x holds, we have g(B) ≥ B,
or −g(B) ≤ −B. By Weyl monotonicity, this implies λ↓ (g(A) − g(B)) ≤
λ↓ (g(A)−B). Thus, statement (6) would be implied by the stronger statement
λ↓ (g(A) − B) ≤ λ↓ (g(A − B)).

(8)
6

Now note that the argument of g in the LHS, A, is never below 1. Thus, in
principle, we could replace g(x) in the LHS by another function h(x) defined
as
h(x) =



 g(x),

 x,

if x ≥ 1

(9)

otherwise.

If we also do that in the RHS, we get a stronger statement than (8). Indeed,
h(x) ≤ g(x) for x ≥ 0 and A − B ≥ 0, and therefore h(A − B) ≤ g(A − B)
holds. By Weyl monotonicity again, we see that (8) is implied by
λ↓ (h(A) − B) ≤ λ↓ (h(A − B)).

(10)

The importance of this move is that h(x) is still a monotonously increasing
and concave function (because g ′ (x) ≤ 1 for x ≥ 1), but now has gradient
h′ (x) ≤ 1 for x ≥ 0.
Defining C = A − B, which is positive semi-definite, we now have to show the
inequality
λ↓k (h(C + B) − B) ≤ λ↓k (h(C)) = h(λ↓k (C)),
for every k. Fixing k, and introducing the shorthand x0 = λ↓k (C), we can
exploit concavity of h to upper bound it as h(x) ≤ a(x − x0 ) + h(x0 ), where
a = h′ (x0 ) ≤ 1. Again by Weyl monotonicity, we find
λ↓k (h(C + B) − B) ≤ λ↓k (a(C + B − x0 ) + h(x0 ) − B)
= λ↓k (aC + (a − 1)B − ax0 + h(x0 ))
≤ λ↓k (aC) − ax0 + h(x0 ) = h(x0 ),
where in the second line we could remove the term (a−1)B because it is negative. This being true for all k, we have proved (10) and all previous statements
that follow from it, including the statement of the Theorem. ✷

5

On Y -dominated Majorisation

To answer Question 2, we have to consider the property that a convex function
f satisfies
λ(f (∆)) ≺w λ(f (B + ∆) − f (B))
7

(11)

for all PSD B and ∆, which is equivalent to the statement
λ(f (A − B)) ≺w λ(f (A) − f (B))

(12)

for all A ≥ B ≥ 0.
The monotone convex angle functions x 7→ ax + (x − 1)+ (a ≥ 0) already
have proven their valour as a testing ground for similar statements, in Section
3. Numerical experiments using angle functions for inequality (11) did not
directly lead to any counterexamples, however. This temporarily increased
our belief that the inequality might actually hold, and led us to investigate,
as an initial step towards a "proof", whether the inequality
k
X

λ↓j (aY + B) ≤

k
X

λ↓j (aY + C)

j=1

j=1

might be true for all a ≥ 0, where B = f (Y ) and C = f (X + Y ) − f (X), and
f (x) = (x − 1)+ . The crucial observation is now that if this holds for all a ≥ 0,
then, actually, a much stronger relationship than just majorisation must hold
between aY + B and aY + C.
To describe this phenomenon, we'll consider a somewhat broader setting. Let
G and C be Hermitian matrices, and let f1 and f2 be monotonously increasing
real functions on R. Suppose that for all a ≥ 0, the following holds:
k
X

λ↓j (aA + B) ≤

k
X

λ↓j (aA + C),

(13)

j=1

j=1

with A = f1 (G) and B = f2 (G).
It is easily seen that if (13) holds for a certain value of a, it also holds for all
smaller positive values. Let b be a scalar such that 0 ≤ b < a. Because both
A and B exhibit their eigenvalues as diagonal elements in the eigenbasis of G,
and both in non-increasing order, we get
k
X

λ↓j (aA + B) =

k
X

λ↓j (bA + B) + (a − b)

λ↓j (A).

j=1

j=1

j=1

k
X

On the other hand, for aA + C we only have the subadditivity inequality
k
X

j=1

λ↓j (aA + C) ≤

k
X

λ↓j (bA + C) + (a − b)

k
X

j=1

j=1

8

λ↓j (A).

As a consequence, we obtain that, indeed,
k
X

λ↓j (bA + B) ≤

k
X

λ↓j (bA + C)

j=1

j=1

follows from (13).
We are therefore led to consider what happens when a tends to infinity, because
P
that value dominates all others. Subtracting kj=1 λ↓j (aA) from both sides, and
substituting a = 1/t, we obtain
k
k
1X
1X
(λ↓j (A + tB) − λ↓j (A)) ≤
(λ↓j (A + tC) − λ↓j (A)).
t j=1
t j=1

In the limit of t going to 0, this yields a comparison between derivatives:
∂
∂t

k
X

∂
∂t

λ↓j (A + tB) ≤

t=0 j=1

k
X

λ↓j (A + tC).

(14)

t=0 j=1

∂
λ↓ (A + tC) are the diagonal
We will show below that the derivatives ∂t
t=0 j
elements of C in a certain basis depending on G and C. Let us first introduce
the vector δ(C; A) whose entries satisfy the following relation:
k
X

δj (C; A) :=

j=1

∂
∂t

k
X

λ↓j (A + tC).

(15)

t=0 j=1

With this notation, relation (14) becomes
k
X

δj (B; G) ≤

k
X

δj (C; G).

j=1

j=1

That is, the entries of δ(B; G) are "majorised" by those of δ(C; G). However, this is a much stronger relation than ordinary majorisation, since the
rearrangement of the entries in decreasing order is absent.
Introducing the symbol ≺dw for weak majorisation with missing rearrangement:
a ≺dw b ⇐⇒

k
X

aj ≤

j=1

k
X

bj ,

(16)

j=1

relation (14) is expressed as
δ(B; G) ≺dw δ(C; G).

(17)
9

To justify these notations, we now show:
Proposition 2 Let A and C be Hermitian matrices. With δ(C; A) defined by
(15), the entries of the vector δ(C; A) are the diagonal entries of C in a certain
basis in which A is diagonal and its diagonal entries appear sorted in nonincreasing order. When all eigenvalues of A are simple (i.e. have multiplicity
1), this basis is just the eigenbasis of A and does not depend on C.
Proof. There are three cases to consider, according to whether A is nondegenerate, A + tC has an accidental degeneracy at t = 0, or A + tC is
permanently degenerate.
1. The most important case is when all eigenvalues of A are simple, i.e. when
they have multiplicity 1. We then show that the derivative is given by
∂
∂t

k
X

λ↓j (A + tC) = Tr[Pk (A) C],

t=0 j=1

where Pk (A) denotes the projector on the subspace spanned by the k eigenvectors of A corresponding to its k largest eigenvalues.
By the simplicity of the eigenvalues of A, the eigenvalues of A + tC are also
simple for small enough values of t. This follows easily from Weyl's inequalities:
λ↓j (A) + λ↓n (tC) ≤ λ↓j (A + tC) ≤ λ↓j (A) + λ↓1 (tC);
thus if t||C|| is strictly less than one half the minimal difference between
all pairs of eigenvalues of A, the difference between all pairs of eigenvalues of
A+tC is bounded away from 0. Therefore, for small enough t, every eigenvalue
of A + tC has a unique eigenvector, and as a result Pk (A + tC) is well-defined
as the sum of the projectors on the eigenvectors pertaining to the k largest
eigenvalues.
It is well-known that the eigenvalues of A+ tC as functions of the real variable
t can be so ordered that they are analytic functions of t (see [6], Chapter 2),
and hence continuous. This implies that the k-th largest eigenvalue of A + tC
is also a continuous function of t, for any k.
If, furthermore, an eigenvalue λ(t) of A + tC is simple in an interval of t,
then the projector P (t) on the eigenvector x(t) associated to it (with P (t) =
x(t)x(t)∗ ) is also analytic, and therefore continuous in t on this interval. We
conclude that Pk (A + tC) is analytic in t, and therefore differentiable.
By the maximality of Pk (A) in the variational characterisation
k
X

j=1

λ↓j (A) = max Tr[A Qk ] = Tr[A Pk (A)],
Qk

10

where Qk runs over all rank-k projectors, we have
∂
∂t

Tr[A Pk (A + tC)] = 0,
t=0

which implies
∂
∂t
=

k
X

λ↓j (A + tC)

t=0 j=1

∂
∂t

Tr[(A + tC) Pk (A + tC)]
t=0

∂
∂
Tr[A Pk (A + tC)] +
∂t t=0
∂t
= Tr[C Pk (A)].

=

Tr[(A + tC) Pk (A)]
t=0

Let U be the unitary that diagonalises A, i.e. UAU ∗ = Λ↓ (A). Then
Tr[C Pk (A)] =

k
X

(UCU ∗ )jj ,

j=1

and the statement of the Proposition follows.
2. When A has degenerate eigenvalues, the situation becomes somewhat more
complicated, but there are no really significant changes. There is no longer
a unique eigenbasis of A, so that Pk (A) is not well-defined for all k. We will
first consider the case where C is such that it removes the degeneracy of the
eigenvalues of A in A + tC for small enough positive t. In that case Pk (A + tC)
will be uniquely defined for all positive t less than some value t0 , which is the
smallest positive t for which A + tC has an accidental degeneracy (which is
what also happens at t = 0).
This occurs, for instance, when C has simple eigenvalues. Indeed, by analyticity of the eigenvalues of A + tC in t, degeneracy is either accidental (for
isolated values of t) or permanent (for all values of t). Since all eigenvalues
are simple for large enough t, they have to remain simple for all values of t
except possibly for some isolated values, such as t = 0, in this case. Let t0
be the smallest positive such value, then A + tC has simple eigenvalues for
0 < t < t0 .
We can therefore define Pk (A) in a unique way as the limit limt→0 Pk (A +
tC). This is an allowed choice because of the continuity of the eigenvalues:
Pk
↓
j=0 λk (A) = Tr[limt→0 Pk (A + tC) A]. Using the same argument as in the
previous case, we obtain δ(C; A) := Tr[limt→0 Pk (A + tC) C].
11

Let λl be the eigenvalues of A (multiplicity not counted), and Ql the projections onto the corresponding eigenspaces of A (with Q∗l the corresponding inclusion operators); the rank of Ql equals the multiplicity of λl , which we denote
by ml . To obtain δ(C; A), we first construct the diagonal blocks Cl := Ql CQ∗l
(of size ml ), then take the eigenvalues λ↓ (Cl ) in non-increasing order of each
block, and then concatenate the obtained sequences of eigenvalues:

δ(C; A) := (λ↓ (C1 ), . . . , λ↓ (Cm )).
If all eigenvalues of A are distinct, this reduces to the vector of diagonal
elements of C in the eigenbasis of A that we encountered in case 1.
For example,if λ↓ (A) 
= (5, 5, 3, 1), then δ(C; A) = (λ↓1 (C1 ), λ↓2 (C1 ), C33 , C44 ),
 C11

where C1 = 

C12 

C21 C22



and all entries of C are taken in the eigenbasis of A.

Let U be a unitary (which, in this case, is not unique) that diagonalises A as
UAU ∗ = Λ↓ , and take the diagonal blocks Cl of UCU ∗ , as above. Each block
can be diagonalised using a unitary Vl . Together with U we obtain the total
L
L
basis rotation W := U( l Vl ). By construction, l Vl leaves Λ invariant, and
resolves the ambiguity in U. We obtain that δ(C; A) is the vector of diagonal
entries of C in the basis obtained by applying the unitary W .
3. Finally, we look at the case when A + tC is permanently degenerate, i.e.
when it has degenerate eigenvalues for all values of t. W.l.o.g. we just have to
look at t in an interval [0, t0 ), where t0 is the smallest positive value for which
A + tC has an accidental degeneracy. Let us denote by λj (t) the eigenvalues
of A + tC in non-increasing order, multiplicity mj not counted, and by Pj (t)
the projectors on the corresponding eigenspaces. In that case Pk (A + tC) is
only well-defined if there is a j ′ such that k = m1 + m2 + . . . + mj ′ ; then we
have Pk (A + tC) = P1 (t) + P2 (t) + . . . + Pj ′ (t).
If there is no such j ′ , let j ′ be the largest integer such that k > m1 + m2 +
. . . + mj ′ =: k ′ . Thus 0 < k − k ′ < mj ′ +1 . Then we have

k
X

λ↓j (A + tC)

j=1
′

=

j
X

mi λi (t) + (k − k ′ )λj ′+1 (t)

i=1

= Tr[(A + tC) (P1 (t) + . . . + Pj ′ (t) +

12

k − k′
Pj ′ +1 (t))]
mj ′ +1

= Tr[(A + tC) (

k − k′
k − k′
Pk′+mj′ +1 (A + tC) + (1 −
)Pk′ (A + tC))].
mj ′ +1
mj ′ +1

Thus, if we define α := (k − k ′ )/mj ′ +1 , kj=1 λ↓j (A + tC) interpolates linearly
P ′
Pk′ +mj ′ +1 ↓
between kj=1 λ↓j (A + tC) and j=1
λj (A + tC) with parameter α.
P

Proceeding in the same way as in the two previous cases, we obtain for the
derivative
∂
∂t

k
X

λ↓j (A + tC) = Tr[C(αPk′+mj′ +1 (A) + (1 − α)Pk′ (A))],

t=0 j=1

where the Pk (A) have to be replaced with the limits limt→0 Pk (A + tC) if in
addition there are accidental degeneracies at t = 0. Let us consider the entries
of C again as before, in an eigenbasis of A in which the eigenvalues of A appear
on the diagonal, in non-increasing order. We get
k ′ +mj ′ +1

′

δ(C; A)k = (1 − α)

k
X
i=1

Cii + α

X

Cii .

(18)

i=1

Because of the permanent degeneracy, an eigenbasis is determined up to "local" rotations within the various eigenspaces. We consider a partitioning of
C in such an eigenbasis corresponding to these eigenspaces. That is, in C we
can single out diagonal blocks, each of which corresponds to the eigenspace of
eigenvalue λj . We can use our freedom to choose the local rotations to make
all diagonal elements of C equal within each diagonal block. This allows us to
get rid of the interpolation in (18), and we finally obtain that, again,
δ(C; A)k =

k
X

Cii ,

i=1

with the entries of C taken in the eigenbasis that we have just chosen. ✷
The upshot of this Proposition is that there exists a unitary U such that
UAU ∗ = Λ↓ (A) and δ(C; A) = Diag(UCU ∗ ). In the generic case that all
λi (A) are distinct, U is unique and does not depend on C.
A number of easy consequences follow immediately from this Proposition:
Corollary 2 Let G and C be Hermitian matrices, f be any monotonously
increasing real function on R, and g any strictly increasing real function on
R, then
(i) δ(f (G); G) = f (λ↓ (G)).
(ii) δ(C; G) obeys Schur's majorisation Theorem: δ(C; G) ≺ λ↓ (C).
13

(iii) δ(C; G) + aλ↓ (f (G)) = δ(C + af (G); G), ∀a ≥ 0.
(iv) δ(C; f (A)) = δ(C; A).
Along with the previously demonstrated equivalence of (13) with (17), the
Corollary immediately leads to the following Proposition:
Proposition 3 For Hermitian G, C, monotonously increasing real functions
f1 , f2 on R, and A = f1 (G), B = f2 (G), the following are equivalent:

λ(aA + B) ≺w λ(aA + C), ∀a ≥ 0
δ(B; G) ≺dw δ(C; G)
δ(aA + B; G) ≺dw δ(aA + C; G), ∀a ≥ 0.

(19)
(20)
(21)

Proof.
(19) implies (20): This is just Proposition 2.
(20) implies (21): Add aλ↓ (A) to both sides and invoke statement (iii) of the
Corollary.
(21) implies (19): By statement (i) of the Corollary, the LHS of (21) is equal
to λ↓ (aA + B), while, by statement (ii) of the Corollary, its RHS is majorised
by λ(aA + C). ✷

6

Counterexample to Question 2

If the answer to Question 2 is to be affirmative, it should at least hold for all
angle functions f (x) = ax + b(x − x0 )+ . By Proposition 3 this is equivalent to
the statement
δ((Y − 11)+ ; Y ) ≺dw δ((X + Y − 11)+ − (X − 11)+ ; Y ).
Consider the 3 × 3 matrices


X=



0.35614 −0.053243 0.10116 





 −0.053243



0.87456 0.40559 


0.40559 0.82474


0.10116

14

and



Y =

 0.53642



0



0
0.42018

0



0



.
0


0 0.094866



The eigenbasis of Y is therefore the standard basis. Then δ((Y − 11)+ ; Y ) =
(0, 0, 0) and


(X + Y − 11)+ − (X − 11)+ =

 −0.00018194


 0.00052449



−0.0016345



0.00052449 −0.0016345 
0.2573

0.12368




0.12368 

0.04



so that δ((X +Y −11)+ −(X −11)+ ; Y ) = (−0.00018194, 0.2573, 0.04). The first
entry is negative, violating the ≺dw relation, and thereby answering Question
2 in the negative. ✷

7

Further Applications of Y -dominated majorisation

One issue we had to address during our attempts at giving a positive answer
to Question 2 dealt with the possibility of reducing the question for convex
functions to convex angle functions. One way of doing so would have been
possible if the set of (monotonously increasing and convex) functions satisfying
(11) were closed under addition. While we were unable to prove this particular
statement (which is most likely false, anyway), Proposition 3 enables us to
prove the corresponding statement for the relation
δ(f (Y ); Y ) ≺dw δ(f (X + Y ) − f (X); Y ).

(22)

Proposition 4 Let all the eigenvalues of Y be distinct. Let f and g be functions from R to R satisfying (22). Then f + g also satisfies (22).
Proof. By the assumption on the eigenvalues of Y , δ(A; Y ) equals Diag(A) in
a basis only depending on Y and is therefore a linear function of A. We can
therefore add up the inequalities (22) for f and g and obtain the corresponding
inequalities for f + g. ✷

A second application of Proposition 3 is a strengthening of the following Proposition, which we also obtained in the course of our attempts at positively
answering Question 2.
15

2

x
, with a ≥ 0, the following
Proposition 5 For X, Y ≥ 0 and ga (x) = ax + x+1
majorisation statement holds:

λ(ga (Y )) ≺w λ(ga (X + Y ) − ga (X)).
Proof. From the proof of Lemma X.1.4 in [4], we have, for X, Y ≥ 0,
λ↓j ((X + 11)−1 − (X + Y + 11)−1 ) ≤ λ↓j (11 − (Y + 11)−1 ).
Defining the function f (x) =

x
x+1

= 1 − (x + 1)−1 , this turns into:

λ↓j (f (X + Y ) − f (X)) ≤ λ↓j (f (Y )).
This implies the majorisation statement
k
X

λ↓j (f (X + Y ) − f (X)) ≤

k
X

λ↓j (f (Y )).

(23)

j=1

j=1

We want to prove a somewhat similar statement for the function ga (x). Since
both f and ga are monotonously increasing over R+ , and noting that ga (x) =
(a + 1)x − f (x), we have
λ↓j (ga (Y )) = ga (λ↓j (Y )) = (a + 1)λ↓j (Y ) − f (λ↓j (Y ))
λ↓j (f (Y )) = f (λ↓j (Y )),
so that
λ↓j (ga (Y )) = (a + 1)λ↓j (Y ) − λ↓j (f (Y )).
This implies in particular
k
X

λ↓j (ga (Y

)) = (a + 1)

k
X

λ↓j (Y

k
X

λ↓j (Y ) −

)−

≤ (a + 1)

λ↓j (f (Y ))

k
X

λ↓j (f (X + Y ) − f (X)),

j=1

j=1

j=1

k
X

j=1

j=1

where we have inserted (23). Exploiting the well-known relation ([4], Th.
III.4.1)
k
X

j=1

λ↓j (A

+ B) ≤

k
X

j=1

λ↓j (A)

+

k
X

λ↓j (B),

j=1

for A = (a + 1)Y − f (X + Y ) + f (X) and B = f (X + Y ) − f (X) then yields
16

k
X

λ↓j (ga (Y )) ≤

j=1

k
X

λ↓j ((a + 1)Y − f (X + Y ) + f (X))

k
X

λ↓j (ga (X + Y ) − ga (X)).

j=1

=

j=1

✷
Proposition 3, with A = G = Y , B = f (Y ), C = f (X + Y ) − f (X), where
f (x) = x2 /(x + 1), then yields the following strengthening of Proposition 5:
Proposition 6 For X, Y ≥ 0, and ga (x) = ax +

x2
,
x+1

with a ≥ 0,

δ(ga (Y ); Y ) ≺dw δ(ga (X + Y ) − ga (X); Y ).
Here we noted that ga (X + Y ) − ga (X) = aY + f (X + Y ) − f (X).

To end this Section, we present a third application of Proposition 3, namely
to the results of Kosem and Bourin-Uchiyama. Consider first inequality (3),
which holds for all non-negative concave functions f (x). In particular, it holds
for all functions f = ax + f0 (x), where f0 is non-negative concave, and a ≥ 0.
Inserting this in the eigenvalue-majorisation form of inequality (3), we get the
(A + B)-dominated majorisation relation
λ(a(A + B) + f0 (A + B)) ≺w λ(a(A + B) + f0 (A) + f0 (B)),
for A, B ≥ 0. Proposition 3 then immediately yields the stronger form
δ(f (A + B); A + B) ≺dw δ(f (A) + f (B); A + B),

(24)

for all non-negative concave functions f . The strengthening of inequality (4)
is performed in a completely identical way and yields the reversed inequality
of (24) for non-negative convex functions g such that g(0) = 0.

Acknowledgements

JSA thanks Professor Moin Uddin, Director of his institute for encouragement and supporting his visit to attend the conference at Nova Southeastern
University, Fort Lauderdale, Florida, USA, which lead to his introduction to
Koenraad M.R. Audenaert and the completion of this work.
17

KA thanks the Institute for Mathematical Sciences, Imperial College London,
for support. His work is part of the QIP-IRC (www.qipirc.org) supported by
EPSRC (GR/S82176/0).

References
[1] T. Ando, "Comparison of norms |||f (A) − f (B)||| and |||f (|A − B|)|||," Math. Z.
197, 403–409 (1988).
[2] T. Ando and X. Zhan, "Norm inequalities related to operator monotone
functions," Math. Ann. 315, 771–780 (1999).
[3] J.S. Aujla and F.C. Silva, "Weak majorization inequalities and convex functions,"
Lin. Alg. Appl. 369, 217–233 (2003).
[4] R. Bhatia, Matrix Analysis, Springer, Heidelberg (1997).
[5] J.-C. Bourin and M. Uchiyama, "A matrix subadditivity inequality for f (A + B)
and f (A) + f (B)," Arxiv.org E-print math.FA/0702475 (2007).
[6] T. Kato, Perturbation theory for linear operators, Reprint of the 1980 edition,
Classics in Mathematics, Springer-Verlag, Berlin (1995).
[7] T. Kosem, "Inequalities between ||f (A + B)|| and ||f (A) + f (B)||," Lin. Alg.
Appl. 418, 153–160 (2006).

18

