applying physicsbased loss functions neural networks improvedgeneralizability mechanics problemssamuel j raymonda david b camarilloabcdepartmentbioengineering stanford university stanford ca 94305 usaneurosurgery stanford university stanford ca 94305 usadepartment mechanical engineering stanford university stanford ca 94305usab departmentarxiv210500075v1 physicscompph 30 apr 2021abstractphysicsinformed machine learning piml gained momentum 5 years scientistsresearchers aiming utilize benefits afforded advances machine learning particularly deeplearning large scientific data sets rich spatiotemporal data highperformance computingproviding large amounts data inferred interpreted task piml ensurepredictions categorizations inferences enforced conform limits imposed physicallaws work new approach utilizing piml discussed deals use physicsbasedloss functions typical usage physical equations loss function requires complex layersderivatives functions ensure known governing equation satisfiedsimilar level enforcement implementing simpler loss functions specific kindsoutput data generalizability approach affords shown examples simple mechanicalmodels thought sufficiently simplified surrogate models wide class problems1 introductionphysicsinformed machine learning piml cuttingedge new field sits intersectionscientific computing machine learning field years old begunproducing valuable insights combined approaches domains particularlyintersection computational mechanics modeling realworld materialsfields deep learningadvanced neural network architecturesimmense rise power artificial intelligence machine learning decadesscientific computing largely looked field application web traffic1 customer habits 2 geospatial information 3 medical imaging 4 far easier apply techniquesdeep learning guiding models areas complex develop principlesdata available training testing attractive fundamentallyprobabilistic endeavor deep learning infer values violate exceed reasonable bounds 5violate laws numerical physics example rendering fundamentally flawedapplication scientific data numerical models perfect errors systematicbiases present approaches bounds expected errors comelearnedmodel similar present scientific model argument relydatadriven approach numerical modeling starting pointdiscretization governing partial differential equation pde form convertedsyntax program pde sense useless tool modelingspecific boundary initial values appropriate parameters set transforms universallycorrespondingauthoremail address sjraystanfordedu samuel j raymondpreprint submitted journal computational physics4 2021applicable pde specific model specific use case corollary manner deep neural networkthought universal function approximator 6 connections neuronsequation transfer function relating variables theoretically constructabledata training neural network like pde completely useless offers contextrelevantinference data fed neural network weights updatedtraining process general function approximator converted far brittle usefulbrittle refers extent network accurate inferencesbounds data train model network relied provide useful informationbounds number useful applications deep learningphysical sciences 7 8 9 10 11 12 13 14 manner application continue developdata generated scientific purposes relying datadriven model provide mechanisticpredictions creates number problems adversarial neural network research 15 shownbrittle models humanimperceptible changes input advantagenumerical models far opaque brittleness generally far better copinglarge range input parameter variations needed remake modelcertainly limit models dangerous prospect presents relying traditionalneural networks provide scientifically relevant inferencesgoals piml address shortcomings traditional neural networks leveragespeed fusion data neural networks affords computational models commonplaceessentially area human endeavor new methods techniques constantly introducedmanage increasingly complex scenarios 16 17 18 19 20 21 22 modelssophisticated horrendously expensive scales climate modeling23 largescalemolecular dynamics models 24 everlarger supercomputers requiring energy run25piml offers energyefficient sustainable approach infuse networks benefitspreexisting domain knowledge years number distinct fields piml startedemerge different approach embed domain knowledge deep learning frameworksinclude physicsinformed neural networks pinns26 synthetic data7 datadriven equationlearning 27fields work primarily interested development use pinns 28 29 30 31 3233 34 26 35 36 37 38 39 approach pinns use inputoutput pair neural networkfundamental variable set variables physical problem use variables constructquantities appear governing equation motion velocity pressure fieldsfluids problem equation motion instance euler navierstokes equationadditional term loss function deep learning architecture additional layeroutput neurons convert variables forms needed equation motion seminal workconducted raissi et al 26 interested reader encouraged read worknew loss function pinns acts similar manner residuals finite element schemetraining performed traditional backpropagation methods loss associatedl2norm equation motion residual minimized work required understandright tradeoff errors weightedloss function pinns deep learning plays crucial role ability networktrain 40 approach pinns leverage understanding governing physics playbuild basis function form neural network solves equationssituations governing equation clear inclusion pde introducescomputational overhead efficiencies afforded use pinn negatedentire pde required sufficiently accurate solution produced toleratederror example viscous flows simple fluid problems 3d navierstokes requiredtraining complex pde unnecessary work introduce new approachproblem utilizing laws physics loss functions ones tend sit governingpdes applied efficiently liberally number different problems presentpaper structured follows observation automatic satisfaction underlyingprinciple action shown arise naturally backpropagation particular models numberexperiments reproduce phasespace trajectories minimized action conducted simplerlossfunction approach presented uses conservation energy guiding principlebuild neural network predict motion pendulum compared traditionaldataonly approach discussion comparison concludes work2 methods21 leastaction conventional loss functionsuppose exists modeled lagrangian l equations motiongovern invoking principle stationary action 41principle states action defined time integral lagrangian stateminimized trajectory moves phasespace equationscome invoking principle known eulerlagrange equations 42 essentiallyreformulation newtons laws motion mathematically described followslagrangian l proposed incorporating kinetic energy potential energiesformed functions generalized positions q velocities q particlesl f q q t vparticles kinetic energy tsys potential energy vsys formed combiningcontributions particlestsysmi q qivsysf qi qiaction formed integral points timez t2lq qtrajectory minimizes action calculus variations invoked leading eulerlagrange equationsz t2lq q 0spacetime trajectory given eulerlagrange equations satisfies principleaction211 minimizing action propagationremarkably approximation procedure training neural networkinputs initial conditions outputs spacetimetrajectories point time combined meansquared errorcommonly measurement error deep learning approximation calculusproducedzk2 ykmean squared error msez f qi qix 0msezk2 ykl lo0summation time step outputs approximates integralanalytical version principle optimizer converges spacetime trajectoryclosely resembles satisfy eulerlagrange equations result impliesfact straightforward use neural net predict motion behavior differentsystems different governing lagrangian possible precise error estimates producerequired conservation properties energy momentum angular momentum needed simulate systemsevolution role propagation use given error term means direct optimizertotal error predicted versus given result small possible optimizationprocess forces output trajectory similar actionobeying trajectory possibleproduces alternative approach learning laws physics network requiresvariables span phasespace mechanics problems key variables positionmoment sufficient span spacetest different mechanical systems simulated generate data neural networktrained learn underlying structures initial starting position phase space positionmomentum spacetime trajectory output networkfocused motion projectile influence gravity effect dragsecond investigated modeling pendulum single pendulum double pendulum212 simple projectile motionprojectile motion simplest forms mechanics problems coveredphysicsstream classes high schools problem useful describes motion objects widerange fields common phenomena incorporate projectile motion fountains sportssimplest form motion constant velocity motion basis projectile motionmotion governed constant acceleration notably gravity acts direction simplestform projectile motion completely decouples motion horizontal vertical directionsmotion horizontal direction xt vertical direction yt described simplyxt dt xt ut dtg dt2ut vt x y velocities respectively g acceleration gravity assumedconstant acting y direction generate data neural network simpleforward euler integrator generate x y positions projectile starting positiongiven starting velocity range values gravity time flightrange starting x y velocities deep learning toolbox matlab r2020a trainfully connected feedforward network neural net consisted input layer 6 neurons 2hiddenlayers 15 neurons layer relu activation functions output layer consisting4x200 time step values resulting motion position velocities 2 directions trainingperformed meansquared error performance criterion levenbergmarquardt algorithmpropagation neural net given starting position velocity gravity strengthtime flight input output set x y positions velocities projectilestime flightyt dt yt vt dt213 nonlinear projectile motionincorporate realistic effects nonlinear drag added equations motionnew equations motion coupled x y directionsxt dt xt ut dt cdv tyt dt yt vt dtg dt2 cdv tcd coefficient drag v t absolute velocity projectile neuralnetwork architecture training procedure second case214 single pendulum motionperiodic motion class commonly occurring motion scientists engineering encounterdetermine efficacy workflow prediction periodic motion similar feedfowardnetwork created learn motion pendulum recurrent networkappropriate period motion learning final desire able model chaoticmotion double pendulums periodicity assumed simple deep fully connected layerssingle pendulum model generated data neural net came simpleforward euler integrator produce angle angular velocity trajectory massinitial starting angle starting angular velocity length mass pendulum setequal 10 models work215 double pendulum motionchaotic motion described solution heavily dependent initial conditions dynamictypical double pendulum exhibits chaotic motion modeling kindchallenging traditional methods follow constraints requiredfollow masses given different initial starting angles angular velocities allowedoscillate periods manner single pendulum reasonchoosing use recurrent network model results pendulum models shownsection neural network attempted predict phasespace trajectory pendulummasses inital conditions22 custom loss function robust time step predictionsprevious section timespace trajectory predicted initial conditionsability neural network minimize correct trajectories satisfy principle actionmethod relies having phasespace solution simple methodscale complex problems instead present approach learns evolutiontime step ensuring energy conserved point timetraditional loss function exposed systems different energy levelstend produce lower energy solution better satisfies minimization process addingextra constraint conservation energy better generalizability introduced networkprepare workflow pendulum simulated initial conditions set 0 4 0 60 8 angular velocity 0 0 case neural network designed inputoutput neurons representing angular position angular velocity times t t 1 respectivelypendulum simulated 6 cycles pairs t t 1 values randomly mixedtrain network loss functions train network standard loss functionlstandard set meansquared error second physicsloss function lphys lstandard ee change energy takene m t mg1 cost m y mg1 cosyt y subscripts refer true networkpredicted values respectively custom lossfunction added training processes matlabs deep learning toolbox determineeffect different loss functions prediction network starting positions 0 2 0 30 6 output step input solutionnumber periods produced results approach shown section3 results31 trajectories actioncompare neural net trajectories solved simulator sample input valuesgiven simulation engine neural net comparisons shown projectilependulum motions linear nonlinear featuresfigure 1 simple projectile motion showing spatial trajectory point 2d space black lines indicatenumerical simulation results circles outputs spacetime trajectories neural network311 projectile motionresults shown figure 1 indicate excellent agreement trajectories numericalprocedures results shown figure 2 difference coefficient dragadded input variable neural net able successfully learn relationships dataprovided input output trajectories lie simulated valuesresults indicate network able input variables initial velocity positiondrag terms produce physically accurate trajectory motion projectile312 pendulum motionresults shown figure 3 network able follow motionperiods oscillation indicating underlying physics correctly inferredtraining figure 4 shows neural network approach capable following complexmulti body motion chaotic systems surprising degree accuracy path phase space showslittle periodicity feedfoward fully connected layers able learn trajectories pendulumpredict masses paths phase space good accuracy32 predicting motion physicsbased loss functioncompare efficacy standard loss functions pimlmotivated loss functions predictionsimple pendulums motion shown conventional physicsbased loss functions differentstarting positions resulting predicted motions different starting angles321 conventional loss functionfigures 5 7 results prediction trajectory pendulum differentstarting positions initial predictions follow phasespace curves time quicklydecay smallest energy phasespacefigure 2 nonlinear projectile motion showing spatial trajectory point 2d space black lines indicatenumerical simulation results circles outputs spacetime trajectories neural networkfigure 3 left phase space trajectory motion right angular velocity black lines indicate numerical simulationresults circles outputs spacetime trajectories neural networkfigure 4 motion double pendulum left phase space trajectory motion blue lower red massesright angular velocities angular positions upper blue lower red masses black lines indicate numericalsimulation results circles outputs spacetime trajectories neural networkfigure 5 prediction motion pendulum starting positionperiodsconventional loss function 4figure 6 prediction motion pendulum starting positionperiodsconventional loss function 4figure 7 prediction motion pendulum starting positionperiodsconventional loss function 4figure 8 prediction motion pendulum starting positionconventional loss function 4 periodscombined physicsbased322 physicsbased loss functionfigures 8 10 results prediction trajectory pendulum differentstarting positions neural network trained custom loss function adding conservationenergy term results preserve energy far better conventional approach decayingpredictions multiple periodsfigure 9 prediction motion pendulum starting positionconventional loss function 4 periodscombined physicsbasedfigure 10 prediction motion pendulum starting positionconventional loss function 4 periodscombined physicsbased4 discussionwork new approach pimlbased neural networks proposed suggests addingsimple universally applicable laws error function enforce physically accurate predictions initially comparison process training solving fundamental principlemechanics principal action appropriate variables shown standard lossfunction deep learning mean squared error combined output variables positionmomentum velocity solve action principle albeit discretely statisticallypredict timespace trajectories number representative mechanics problems linearnonlinear form projectile motion drag single double pendulumtraining network initial conditions projectiles flight network built predict trajectories masses shown figure 1 nonlinear drag figure 2 simpleproblems mechanics benefit offered approach computational speed existing datacomputational data projectile motions commonplace sports military applications quicklypredicting path projectile oftentimes needed additionally pendulum systems modeledtrajectories masses simulated single double pendulumtrained neural network based simple fully connected set neurons capable reproducing periodic behavior single pendulum figure 3 chaotic behavior doublependulum figure 4 understood chaotic systems impossible predictparticular elapsed time approach performed capture nonlinear motionspendulum long shown interesting explore ascertain limitsapproach detailed nonlinear chaotic systems appear nearly aspect humanendeavor second focus work investigate utility adding simple physicsbasedloss term loss function training work neural network constructed predicttime step motion pendulum entire timespace historyintended reflect useful use form piml loss function temporallyneighboring statesneeded data training entire time history pendulum modeledinitially regular loss function meansquared error predict time evolutionpendulum raised height 2 figure 5 3 figure 6 6 figure 7 resultsshowed trained network preferential lowest energy data initial successprediction larger energy systems decayed smallest energy state form mode collapselikely prevalence training minimize magnitude error errorpreferentiating lowest valued data new loss function combined standard meansquarederror measure difference energy input output statesresults drastically different cases figures 8 10 neural networkable predict motion pendulum conserve energy far better original caseworth noting values predicted second neural network case slightlyprecise prediction values lie exact solution likely effect havingbalance extra energy conservation constraint minimization error predicted valuesdespite simplistic models test proposed architecture results promisingnew avenue explore wanting use simulated data andor realworld data physically relevantpredictions inferences adding new constraints conservation energy loss functionfar computationally efficient application traditional pinn approachessentially universally applied energy conserved lossquantified appropriately implies conservation law linear angular momentaforms symmetries embedded loss function input output neuronvariables physical quantities models comprise energy formspresented systems multiple particles complex dynamics pose difficult predictnumber phasespace dimensions increases work simulated datagood measure expected realworld data meaning networksperform managing realworld noisy data need large trainingdata necessitates large computational load performed training networkeffectively creating data choice parameters ranges problemconsidering wide spectrum values need neural networkbased approachuseful networks faster use trained immenseexisting simulated real world data exists oftentimes discardedcostbenefit analysis looks promising form piml moving forward5 conclusionwork present new form physicsbased loss function train neural network predictionevolution comparisons drawn process backpropagation solvingprinciple action governs mechanical systems physics predictions motionprojectiles pendulums linear nonlinear components approachsolving action principle new loss function constructed predict time step evolutionmechanical unlike conventional loss functions consider error predictedtrue value cases systems different energy training dataconventional loss functions fail properly predict different systems instead add additional termerror enforce conservation energy input network resultsdrastically improves prediction evolution different systems differing levelstotal energy simple powerful alteration loss function field physicsinformedmachine learning opens new set opportunities fuse simulation realworld data deeplearning predictions physical systemsreferences1 x bu j rao cz xu reinforcement learning approach online web systems autoconfiguration 2009 pp 211cited 67 doi101109icdcs2009762 s vazquez t muozgarca campanella m poch b fisas n bel g andreu classification usergeneratedcontent consumer decision journey stages neural networks 58 2014 6881 cited 24 doi101016jneunet2014050263 g cheng j han survey object detection optical remote sensing images isprs journal photogrammetryremote sensing 117 2016 1128 cited 505 doi101016jisprsjprs2016030144 g litjens t kooi b bejnordi setio f ciompi m ghafoorian j van der laak b van ginneken c snchezsurvey deep learning medical image analysis medical image analysis 42 2017 6088 cited 3406 doi101016jmedia2017070055 j su d vargas k sakurai pixel attack fooling deep neural networks ieee transactions evolutionarycomputation 23 5 2019 828841 cited 280 doi101109tevc201928908586 k hornik approximation capabilities multilayer feedforward networks neural networks 4 2 1991 251257 cited2585 doi101016089360809190009t7 s j raymond d j collins r ororke m tayebi y ai j williams deep learning approach designed diffractionbased acoustic patterning microchannels scientific reports 10 1 2020 1128 j montgomery s raymond f osullivan j williams shale gas production forecasting illposed inverse problemrequires regularization upstream oil gas technology 5 2020 1000229 x zhan y liu s j raymond h v alizadeh g domel o gevaert m zeineh g grant d b camarillo deeplearning head model realtime estimation entire brain deformation concussion arxiv preprint arxiv201008527202010 g domel s j raymond c giordano y liu s yousefsani m fanton n j cecchi o vovk pirozzi kightb avery boumis t fetters s jandu w m mehring s monga n mouchawar rangel e rice p roy s samih singh l wu c kuo m zeineh g g d b camarillo new openaccess platform measuring sharingmtbi data 11 7501 doihttpsdoiorg101038s4159802187085211 s j raymond j maragh masic j r williams understanding chemomechanical influenceskidney stone failure material point method plos 15 12 2020 e024013312 d collins s raymond y ai j willams r ororke m tayebi acoustic field design microfluidic geometrieshuygensfresnel diffraction deep neural networks journal acoustical society america 148 4 20202707270713 x zhan y li y liu g domel h v alidazeh s j raymond j ruan s barbat s tiernan o gevaert et alprediction brain strain head impact subtypes 18 brain injury criteria arxiv preprint arxiv201210006202014 y liu g domel n j cecchi e rice callan s j raymond z zhou x zhan m zeineh g grant et altime window head impact kinematics measurement calculation brain strain strain rate american footballarxiv preprint arxiv210205728 202115 y ganin e ustinova h ajakan p germain h larochelle f laviolette m marchand v lempitsky domainadversarial training neural networks journal machine learning research 17 cited 1505 201616 s raymond v lemiale r ibrahim r lau meshfree study kalthoffwinkler experiment 3d roomlow temperatures dynamic loading viscoplastic modelling engineering analysis boundary elements 422014 202517 s raymond y aimene j nairn ouenes et al coupled fluidsolid geomechanical modeling multiple hydraulicfractures interacting natural fractures resulting proppant distribution specsur unconventionalresources conference society petroleum engineers 201518 s raymond e aimene ouenes et al estimation propped volume geomechanical modelingmultiple hydraulic fractures interacting natural fractures spe asia pacific unconventional resources conferenceexhibition society petroleum engineers 201519 s j raymond b jones j r williams strategy couple material point method mpm smoothed particlehydrodynamics sph computational techniques computational particle mechanics 2016 11020 s j raymond b d jones j r williams modeling damage plasticity aggregates material point methodmpm computational particle mechanics 6 3 2019 37138221 s wieghold z liu s j raymond l t meyer j r williams t buonassisi e m sachs detection sub500mcracks multicrystalline silicon wafer edgeilluminated darkfield imaging enable solar cell manufacturingsolar energy materials solar cells 196 2019 707722 s j raymond b d jones j r williams fracture shearing polycrystalline material simulations materialpoint method computational particle mechanics 2020 11423 harris p jones t osborn d lister updated highresolution grids monthly climatic observations cru ts310dataset international journal climatology 34 3 2014 623642 cited 3789 doi101002joc371124 m meyers mishra d benson mechanical properties nanocrystalline materials progress materials science 51 42006 427556 cited 3265 doi101016jpmatsci20050800325 r springer d lowenthal b rountree v freeh minimizing execution time mpi programs energyconstrainedpowerscalable cluster vol 2006 2006 pp 230238 cited 66 doi1011451122971112300626 m raissi p perdikaris g karniadakis physicsinformed neural networks deep learning framework solving forwardinverse problems involving nonlinear partial differential equations journal computational physics 378 2019 686707 doihttpsdoiorg101016jjcp201810045url httpswwwsciencedirectcomsciencearticlepiis002199911830712527 l zanna t bolton datadriven equation discovery ocean mesoscale closures geophysical research letters 47 172020 e2020gl088376 e2020gl088376 1010292020gl088376 arxivhttpsagupubsonlinelibrarywileycomdoipdf1010292020gl088376 doihttpsdoiorg1010292020gl08837628 s goswami c anitescu s chakraborty t rabczuk transfer learning enhanced physics informed neural networkphasefield modeling fracture theoretical applied fracture mechanics 106 2020 102447 doihttpsdoiorg101016jtafmec2019102447url httpswwwsciencedirectcomsciencearticlepiis016784421930357x29 e haghighat r juanes sciann kerastensorflow wrapper scientific computations physicsinformed deeplearning artificial neural networks methods applied mechanics engineering 373 2021 113552doihttpsdoiorg101016jcma2020113552url httpswwwsciencedirectcomsciencearticlepiis004578252030737430 e haghighat m raissi moure h gomez r juanes physicsinformed deep learning framework inversionsurrogate modeling solid mechanics methods applied mechanics engineering 379 2021 113741doihttpsdoiorg101016jcma2021113741url httpswwwsciencedirectcomsciencearticlepiis004578252100077331 e j hall s taverniers m katsoulakis d m tartakovsky ginns graphinformed neural networks multiscalephysics journal computational physics 433 2021 110192 doihttpsdoiorg101016jjcp2021110192url httpswwwsciencedirectcomsciencearticlepiis002199912100087532 q d barajassolano g tartakovsky m tartakovsky physicsinformed neural networks multiphysics dataassimilation application subsurface transport advances water resources 141 2020 103610 doihttpsdoiorg101016jadvwatres2020103610url httpswwwsciencedirectcomsciencearticlepiis030917081931164933 x jin s cai h li g e karniadakis nsfnets navierstokes flow nets physicsinformed neural networksincompressible navierstokes equations journal computational physics 426 2021 109951 doihttpsdoiorg101016jjcp2020109951url httpswwwsciencedirectcomsciencearticlepiis002199912030725734 m liu l liang w sun generic physicsinformed neural networkbased constitutive model soft biological tissuesmethods applied mechanics engineering 372 2020 113402 doihttpsdoiorg101016jcma2020113402url httpswwwsciencedirectcomsciencearticlepiis004578252030587935 s wang p perdikaris deep learning free boundary stefan problems journal computational physics 428 2021109914 doihttpsdoiorg101016jjcp2020109914url httpswwwsciencedirectcomsciencearticlepiis002199912030688436 l yang x meng g e karniadakis bpinns bayesian physicsinformed neural networks forward inverse pdeproblems noisy data journal computational physics 425 2021 109913 doihttpsdoiorg101016jjcp2020109913url httpswwwsciencedirectcomsciencearticlepiis002199912030687237 z zhang g x gu physicsinformed deep learning digital materials theoretical applied mechanics letters2021 100220doihttpsdoiorg101016jtaml2021100220url httpswwwsciencedirectcomsciencearticlepiis209503492100025838 n zobeiry k d humfeld physicsinformed machine learning approach solving heat transfer equation advancedmanufacturing engineering applications engineering applications artificial intelligence 101 2021 104232 doihttpsdoiorg101016jengappai2021104232url httpswwwsciencedirectcomsciencearticlepiis095219762100079839 m mahmoudabadbozchelou m caggioni s shahsavari w h hartt g em karniadakis s jamali datadrivenphysicsinformed constitutive metamodeling complex fluids multifidelity neural network mfnn framework journalrheology 65 2 2021 179198 arxivhttpsdoiorg10112280000138 doi10112280000138url httpsdoiorg1011228000013840 j johnson alahi l feifei perceptual losses realtime style transfer superresolution lecture notesscience including subseries lecture notes artificial intelligence lecture notes bioinformatics 9906lncs 2016 694711 cited 2211 doi10100797833194647564341 j larmor direct application principle action dynamics solid fluid systemsanalogous elastic problems proceedings london mathematical society s115 1 1883 170185 cited 5 doi101112plmss115117042 o agrawal formulation eulerlagrange equations fractional variational problems journal mathematical analysisapplications 272 1 2002 368379 doi101016s0022247x02001804