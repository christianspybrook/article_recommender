accepted icassp 2021 2021 ieee international conference acoustics speech signal processing icassppositnn training deep neural networks mixed lowprecision positgonalo raposopedro tomsnuno romainescid instituto superior tcnico universidade lisboa portugalarxiv210500053v2 cslg 4 2021abstractlowprecision formats proven efficient wayreduce memory footprint hardwareresources power consumption deep learning computations premise posit numerical format appearshighly viable substitute ieee floatingpointapplication neural networks training requires research preliminary results shown 8bitsmaller posits inference 16bittraining maintaining model accuracy presented research aims evaluate feasibility train deepconvolutional neural networks posits purpose software framework developed use simulatedposits quires endtoend training inferenceimplementation allows bit size configurationmixed precision suitable different precision requirements stages obtained results suggest 8bit posits substitute 32bit floats trainingnegative impact resulting loss accuracyindex terms posit numerical format lowprecisionarithmetic deep neural networks training inference1 introductiondeep learning dl nowadays hottest topicssignal processing research spanning multiple applications highly demanding computational fieldcases better performance generality resultincreased complexity deeper models 1 example recently published language model gpt3 largesttrained network 175 billion parameters require 355 years 46m train tesla v100 cloud instance 2 increasingly important optimizeenergy consumption required training process algorithmic approaches contribute goalscomputing architectures advances fundamental 3computations involved dl use ieee754 singleprecision sp floatingpoint fp format 432 bits recent research achieved comparableprecision smaller numerical formats novel positformat 5 designed direct dropin replacement floatieee sp fp provides wider dynamic range higheraccuracy simpler hardware posit formatcorresponding exact accumulator named quireparticularly useful frequent dot products dlcontrasting ieee 754 fp posit numerical format size shownable provide accurate operations floatsfewer bits posits use sizes multiples 8 exploited field programmablegate arrays fpga application specific integrated circuits asic obtain optimal efficiency performancepublished studies application posit format deep neural networks dnnsrely inference stage 612 models trainedfloats later quantized positsinference inference phase tendssensitive errors training phase making easierachieve good performance 58bit positscontrast exploiting use posits trainingphase compelling topic computationally demanding stage time positscontext 13 training fully connected neural network fcnn binary classification problem8 10 12 16 32bit posits later 14 15 fcnntrained mnist fashion mnist 16 32bitposits 16 17 convolutional neural networks cnnstrained mix 8 16bit posits relying floats epoch layer computationsrecently 18 cnn trained cifar1016 32bit positspremise previous works researchpresented goes step extending implementation dnns general featurerich approach original contributions paperopensource framework1 natively perform inferencetraining posits precision number bitsexponent size quires developed cadopts similar api pytorch multithread supportadaptation framework support mixedprecisiondifferent stages forward backward gradient optimizer loss operating different posit formatstraining cnns 8 12bit posits impacting achieved model accuracy1 availablehttpsgithubcomhpculisboapositneuralnet2021 ieee personal use material permitted permission ieee obtained uses current future media includingreprintingrepublishing material advertising promotional purposes creating new collective works resale redistribution servers listsreuse copyrighted component work works2 posit numberingtable 1 main properties posit formats according 20nbitsdifferent numbering formatsproposed represent real numbers 19 ieee754 singleprecision floatingpoint float widelyadopted decomposes number sign 1bit exponent8bits mantissa 23bitsobserved application domains need use total accuracywide dynamic range available ieee 754compromising resulting optimization termshardware resources performance energy efficiencydomains dnn training computations zerocenteredovercome issues posit numbering 5recently proposed new alternative ieee 754 positcharacterized fixed sizenumber bits nbitsexponent size es composed following fieldssign 1bit regime variable bits exponent 0esbitsfraction remaining bits 20 decoded eq 2p 1sign222exponent 1 fractionnumber negative twos complementapplied decoding fields regime bitsdecoded measuring k determined runlengthparticular characteristic positinteresting aspect dnn applications refers distribution values resembling lognormal distributionfig 1 similar normal distribution values commonly dnns interesting pointdefinition quire kulischlike large accumulator 21 designed contain exact sums products positstable 1 shows recommended posit quire configurations3 deep learning posit frameworkcurrent dnn frameworks pytorch tensorflowkeras natively support posit data typeresult set functions operators needreimplemented order advantage newnumbering decided develop entirely new framework scratch order ensure bettercontrol inner operations exploit positdata format31 positnn frameworkdeveloped framework named positnn basedpytorch api c libtorch inheriting programfunctions data flow result user familiar pytorch easily port networks models positnn127228128327672120512231 124962048263 1posit8 0log distribution p 0 p 6 narposit8 0distribution100entries2exponent127 mantissaentriessignf 1dynamic rangequire bitsdot product limitvalue102101100101102valuefig 1 distribution posit8 0 linear log scaleexample comparison pytorch proposed framework declaration 1layer modelshown fig 2 left center overall structurefunctions similar difference declaration backward function proposed framework currently support automatic differentiationdespite compared fullfledged frameworklike pytorch proposed framework capable performing dnn inference training commonmodels functions complete list supported functionalities shown fig 2 right allow implementing stages illustrated fig 3 common cnnslenet5 cifarnet alexnet fully supported framework allows user extendcustom functions combine existing onespytorch32 posit variableslibraries available implementposit operators software 22 universal2 library selected thanks comprehensive support posit configuration quires furthermore c classes functiontemplates generically implement different positsdeclaring posit8 0 variable p equal 1simpleinclude universal posit positsw unum posit 8 0 p 1main operations specified posit standard 20 fully supported implemented furthermoreproposed framework adopts bitwise operationspossible avoiding operating intermediate float representations introduce errors native implementation2 availablehttpsgithubcomstillwaterscuniversalinclude torch torch hinclude positnn positnnstruct floatnetimpl torch nn modulefloatnetimpl linear 10 2registermodule linear lineartemplate typename pstruct positnet layer ppositnet linear 10 2registermodule lineartorch tensor forward torch tensor xx linear xreturn torch sigmoid xactivation functionsrelu sigmoid tanhstdtensor p forward stdtensor p xx linear forward xreturn sigmoid forward xstdtensor p backward stdtensor p xx sigmoid backward xreturn linear backward xtorch nn linear lineartorchmodule floatnetlayerslinear convolutionalaverage maximum poolingbatch normalization dropoutloss functionscross entropymean squared error mseoptimizerstochastic gradient descent sgdutilitiesstdtensor convert pytorch tensorsmixed precision tensor save loadmodel scaled gradientslinear p linearsigmoid p sigmoidfig 2 comparison pytorch left proposed framework center implemented functionalities positnn righttraininginferencedatasetforwardpropagationoutputoptimizermodelgradientsbackwardpropagationlosstargetsplitting left operand rows performing computation concatenating results threadsimplemented stdthreadproposed framework adapted supportdata types functions independentposit format use quire accumulate4 experimental evaluationfig 3 dnn training diagram starting datasetpi 15 represent different posit precisions proposed framework33 implementationposit tensors stored stdtensors class implementedc standard library data internally storedonedimensional dynamic vector multidimensional strides automatically accountedgiven stages sensitive numericalerrors proposed framework supports different precisionsstage depicted arrows fig 3illustrated allows model use different precisionslayer accomplish weights stored classmembers copies different posit configurationslayer stage converts posit tensorsappropriate precisions seamlessly updates copies change option use quiresaccumulations significantly improving accuracy matrix multiplications convolutionsorder maximum advantage cpufunctions conveniently parallelized implementedmultithread support dividing minibatchdifferent workers matrix multiplication correspondsmaking use developed framework presentedresearch started studying posit precisiondecreased penalizing dnn model accuracybest configuration train deeper modelcomplex dataset evaluation small accuracydifferences 1 assumed caused solelyrandomness training process exactly lackprecision numerical formatinitial evaluation 5layer cnn lenet5trained fashion mnist complex datasetordinary mnist 10 epochs 15 18posit16 1 decreasedposit8 0 table 2expected posit16 1 achieves floatlike accuracynarrower precisions posit12 1 posit101 usable training incurringaccuracy loss trained posit8 0model accuracy improve fixes 10 equivalent randomly classifying 10class dataset probablynarrow dynamic range seen fig 1 hypothesis subsequently evaluated different exponentsize es quires accumulations table 3table 2 accuracy lenet5 trained fashion mnistposit quire float referencepositfloat16 112 110 18 0accuracy90429087901588151000table 3 accuracy lenet5 trained fashion mnistposit quire posit8 tested different esposit quirefloat10 18 08 18 2accuracy90428840138412861939table 5 accuracy cnns trained mnist fashionmnist cifar10 float posit8 2datasetcnnfloatposit8 2table 4 accuracy lenet5 trained fashion mnist posit quire mixed precision configuration oxlymeans optimizer o positx 2 loss lposity 2 posit8 2floato12l8o12l12o12l10o10l10accuracy90428840900790258808fashion mnistlenet5cifar10cifarnet991999179042902570296865training losslossconfigurationmnistlenet5epochfp32posit16 1posit10 1posit8 0posit8 2testing accuracyaccuracyobtained results confirmed hypothesis showingprecision 8bit model slightly increasesquires especially posit exponent size es 2common problem particularly noted8bit posit precisions vanishing gradient problem gradients smaller smaller modelconverges particularly problematic modelweights updated lowprecision positsresolution small numbers suggested17 16bit posits optimizer loss usually allow models train lowprecision positsobservation mind model traineddifferent precision optimizer lossposit8 2 table 4 posit exponentsize es fixed 2 gave best results simplified conversion posits different nbitsobtained results showcase feasibility 8bit posits achieving accuracy close 32bits ieee754 particular solely computing optimizerposit12 2 achieve floatlike accuracyloss precision increased model abletrain accuracy penalization12bit posits conversely posit10 2optimizer loss final accuracy slightly decreasesconfiguration 12 bits optimizer 10bits loss o12l10 table 4 offers best compromiseterms lowprecision overall model accuracyconfiguration referred posit8 2 lossfunction weight update computed higher precision represent 15 operationsperformed training considered modelsgiven promising results fashion mnistdataset posit8 2 configuration trainlenet5 mnist cifarnet cifar10 validatingproposed mixed configuration resulting accuraciescompared float table 5 plottraining progress lenet5 fashion mnist shownfig 4 comparing different posit configurations float090085080epochfig 4 training loss testing accuracy lenet5 trainedfashion mnist float different posit precisionsposit8 2 corresponds configuration o12l10 table 45 conclusionnew dnn framework positnn supporting traininginference posit precision proposedmixed precision feature allows adjusting posit precisionstage training network achievingresults similar float common cnns trainedmajority operations performed posit8 2showed significant loss accuracy datasetsmnist fashion mnist cifar10future work shall use knowledge framework devise adaptable hardware implementations positunits exploit feasibility implement lowresource lowpower dnn implementations keeping model accuracyacknowledgmentswork supported national funds fundao paracincia e tecnologia fct projectsuidb500212020 ptdceeihac304852017student merit scholarship funded fundao calouste gulbenkian fcg6 references1 neil c thompson kristjan greenewald keeheon leegabriel f manso computational limits deep learning july 2020 arxiv 2007055582 chuan li openais gpt3 language model technical overview june 2020 httpslambdalabscomblogdemystifyinggpt3 accessed 202010133 jrgen schmidhuber deep learning neural networksoverview neural networks vol 61 pp 85117 jan 2015arxiv 140478284 ieee standard floatingpoint arithmetic ieee std7542019 revision ieee 7542008 pp 184 2019 doi101109ieeestd201987662295 john l gustafson isaac yonemoto beating floatingpoint game posit arithmetic supercomputingfrontiers innovations vol 4 2 pp 7186 june 2017doi 1014529jsfi1702066 marco cococcioni emanuele ruffaldi sergio saponaraexploiting posit arithmetic deep neural networksautonomous driving applications 2018 internationalconference electrical electronic technologiesautomotive july 2018 number november ieeedoi1023919eeta201884932337 jeff johnson rethinking floating point deep learningnov 2018 arxiv 1811017218 seyed hamed fatemi langroudi tej pandit dhireeshakudithipudi deep learning inference embedded devicesfixedpoint vs posit 2018 1st workshop energy efficient machine learning cognitive computing embedded applications emc2 mar 2018 pp 1923 ieee arxiv180508624 doi 101109emc22018000129 zachariah carmichael hamed f langroudi char khazanovjeffrey lillie john l gustafson dhireesha kudithipudideep positron deep neural network posit number 2019 design automation test europeconference exhibition date mar 2019 pp 14211426ieee arxiv 181201762 doi 1023919date2019871526210 zachariah carmichael hamed f langroudi char khazanovjeffrey lillie john l gustafson dhireesha kudithipudiperformanceefficiency tradeoff lowprecision numerical formats deep neural networks proceedingsconference generation arithmetic 2019 new yorkny usa mar 2019 vol f1477 pp 19 acm arxiv190310584 doi 1011453316279331628211 hamed f langroudi zachariah carmichael john lgustafson dhireesha kudithipudi positnn framework tapered precision deep learning inferenceedge proceedings 2019 ieee space computing conference scc 2019 pp 5359 july 2019 doi 101109spacecomp20190001112 hamed f langroudi vedant karia john l gustafsondhireesha kudithipudi adaptive posit parameter aware numerical format deep learning inference edge2020 ieeecvf conference vision patternrecognition workshops cvprw june 2020 pp 726727ieee doi 101109cvprw5049820200037113 ral murillo montero alberto del barrio guillermobotella templatebased posit multiplication training inferring neural networks july 2019 arxiv19070409114 hamed f langroudi zachariah carmichael dhireeshakudithipudi deep learning training edge lowprecision posits july 2019 arxiv 190713216v115 hamed f langroudi zachariah carmichael david pastuchdhireesha kudithipudi cheetah mixed lowprecisionhardware software codesign framework dnnsedge pp 113 aug 2019 arxiv 19080238616 jinming lu siyuan lu zhisheng wang chao fang jun linzhongfeng wang li du training deep neural networksposit number sept 2019 arxiv 19090383117 jinming lu chao fang mingyang xu jun linzhongfeng wang evaluations deep neural networkstraining posit numberieee transactions computers vol 14 8 pp 11 2020 doi101109tc2020298597118 raul murillo alberto del barrio guillermo botelladeep pensieve deep learning framework basedposit number digital signal processing vol 102pp 102762 jul 2020 doi 101016jdsp202010276219 leonel sousa nonconventional arithmetic circuitssystems applications ieee circuits systems magazine vol 20 4 pp 126 oct 202020 posit working group posit standard documentation release 32draft 2018 httpsposithuborgdocspositstandardpdf accessed 2020092421 ulrich kulisch arithmetic validity gruyterberlin boston jan 2012 doi 101515978311030179322 nga teamsurvey posit hardware softwaredevelopment efforts unum posit generationarithmetic july 2019 httpsposithuborgdocspdspositeffortssurveyhtml accessed 20201016