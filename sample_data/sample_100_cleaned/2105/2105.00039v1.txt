arxiv210500039v1 csdc 30 apr 2021gpu acceleration 3d agentbasedbiological simulationsahmad hesamlukas breitwieserfons rademakerszaid alarsabs groupdelft university technologydelft netherlandsashesamtudelftnlcern openlabcerngeneva switzerlandlukasbreitwiesercernchcern openlabcerngeneva switzerlandfonsrademakerscernchabs groupdelft university technologydelft netherlandszalarstudelftnlabstractresearchers biology faced toughchallenge developing highperformance simulationsincreasingly complex agentbased models biodynamoopensource agentbased simulation platform aimsalleviate researchers intricaciesdevelopment highperformance computing highlevel interface researchers implement modelsbiodynamos multithreaded core execution engine rapidlydevelop simulations effectively utilize parallel computinghardware biological agentbased modeling type operations typically computeintensiveinvolve agents interacting local neighborhoodwork investigate currently implemented methodhandling neighborhood interactions cellular agentsbiodynamo ways improve performance enablelargescale complex simulations propose replacekdtree implementation iterate neighborhoodagent uniform grid method allowsadvantage massively parallel architecturegraphics processing units gpus implement uniformgrid method cuda opencl address gpusmajor vendors evaluate techniquesimprove performance furthermore analyzeperformance implementations models varyingdensity neighboring agents result performancemechanical interactions method improved ordersmagnitude comparison multithreaded baseline versionimplementations opensource publicly availablegithubindex termsagentbased modeling simulation gpu coprocessing biological models accelerationntroductionagentbased simulation abs powerful tool conducting research complex biological systems absbiological composed number agentsindividually modeled follow fixed set simplerules agents interact neighboring agents respondexternal stimuli individual behavior agentstrivial emerging behavior comes forthbiological researchers valuableinsights 13complexity scale biological agentbasedmodels increases demand computational powerefficiency 2 agentbased simulations inherentlyparallelizable execution agents statesmodified independently modernday hardwareincreasingly parallelized result dennardscaling 4 stagnation moores law 5 pointed6 generalpurpose computing graphicsprocessing units gpus attractive solution improvecomputational efficiency abs applications particular 78 parallel applications general 9 10 portingapplications fully partially run gpuspossible observe speedups orders magnitudecomparison cpuonly execution 11abs frameworks exist achieve significant speedupsgpus field abs significant roomimprovement wish address articlebiodynamo 6 opensource software platform lifescientists simulating biological agentbased modelsagent biodynamo programmed follow specified setrules imposed modeler trigger specifiedactions affecting agents agents biologicalsystems interact local environmentbehavior influenced agents residecertain range example mechanical interactionscellular agent undergoes physically collidesagent local interactions extremely importantconcept biological systems driving forcekey biological processes tissue developmentbiodynamo fully parallelized openmpperformance scales number cpu cores available6 enhance simulations performance want investigate applicability gpusaccelerating computeintensive operations biodynamowork present following contributionsredesign neighborhood search biodynamokdtree method uniform grid method profitparallel architecture gpusport uniform grid implementation gpu codeopencl cuda address major gpu vendorsimprove gpu kernels based domainspecific aspects biological agentbased modelsbenchmark runtime analyze performancegains obtainedorganization paper follows section iifig 1 spheresphere collision force diagram projectedcircles simplicitydiscusses related work section iii define problemsection iv methodologyapproach section v describes hardware softwaresetup section vi present results finallysection vii draw conclusions workii r elated w orkframeworks software packagespossible simulate agentbased models biologicalsystems specialized software solutionsgenerally focus biological processclosely related biological processesgeneral abs frameworks biological systems biocellion13 physicell 14 timothy 15 chaste 16 focusthings computational efficiencysupport gpu acceleration work demonstrategpu acceleration possible generalpurpose agentbasedplatformsworks 7 8 authors present cellularagentbased simulation abs programs run entirelygpu authors report speedups ordersmagnitude abs frameworks targetedcpus findings impressive factsimulation runs entirely gpu majordrawbacks puts lot pressure minimizingmemory consumption gpu memory nonexpandablelimited resource limit complexityagents state scale model workoffload computeintensive operation gpurequires subset agents state data presentgpu memory second operations independentagents extracellular substance diffusionintegral biological systems absent worksbiodynamo simulate extracellular substancediffusion efficiently multicore cpu independentlygpu operations 6iii p roblem d efinitionmechanical interaction operationcomputeintensive operations cellular agentbasedmodel cell agent interacts cellscertain interaction radius cells physicallycontact need compute collisionforces resulting displacement biodynamo cellularfig 2 visualization cell division module biodynamo crosssectional view colors represent diametercellsagents physically modeled spherical objectsscope paper shall consider spheresphereinteractions illustrated fig 1 projected circles equation 1 17 shows calculations involved determiningmechanical forcer1 r2 kp1 p2 kr1 r2r1 r2rp1 p2kp1 p2 kr1 r2 radii spheres p1 p2position vectors repulsion coefficient attractioncoefficient f resulting collision force vectorcollision force computed determinestrong break adherence cell questioncase integrate collision forcecompute final displacement length finaldisplacement vector generally limited upper boundquantify impact improving operationbiodynamo run available benchmarksuse default operations cell division modulebenchmark 3d grid 262144 cells volumespawned proliferate 10 iterations cellsinstantiated iteration operations executed1 cell proliferation 2 neighborhood lookup 3 resolvingmechanical forces visualization cell proliferationbiodynamo fewer cells longer runtime shownfig 2 profile benchmark better understandingcomputational bottlenecks biodynamofig 3 observe mechanical interactionsoperation highlighted blue timeconsumingbenchmark large margin operationrequires iterating agents turnneighboring agents observation matches priorexpectation 51 benchmarks runtime spentmechanical force calculations described 136 spent updating neighborhood listagent updating neighborhood executed steps 1mech force comp 51neighborhood 36cell division 2remainder 11boxgridboxes vectorboxboxlength uint32tgriddimensions arrayuint32t 6successors vectorsohandlestart atomicsohandlelength atomicuint16taddobjectsohandlesohandlefig 3 runtime profile cell division benchmarkbiodynamofig 4 finding neighborhood agent uniformgrid method displayed 2d simplicityforeachneighborwithinradiusf funcinitializeupdategridcleargridtypeidx uint16telementidx uint32tgettypeidx uint16tgetelementidx uint32tfig 5 uml diagram class created uniform gridmethodgrid approach biodynamo c class illustratedfig 5 uml diagram simulation timestepreconstruct uniform grid accountaddition deletion movement agents voxelbox keeps track number agents containsobject added use linked listgridsuccessors iterate objectsinside single box exact implementation detailsgithub repository1b gpu implementationbuilding kdtree 2 searching agents neighborsspecified radiuskdtree methodsradial neighborhood search considering wantoffload mechanical interactions operation gpuappealing method uniform grid methoduniform grid method allows apply different techniquesimprove gpu version mechanical interactionsoperation discuss paperiv m ethodologysection implementationimprovements existingmechanical interactions operation biodynamo usebiodynamo v0098b3d6c7 baseline versionallows benefit gpu acceleration latestversion presented 6 data stored structsofarrays format arraysofstructsuniform grid methoduniform grid method imposes regularlyspaced 3dgrid simulation space voxel gridcontains agents confined subspacefinding neighboring agents particular agenttaking account voxels surroundingparticular agent illustrated 2d fig 4 agentwant neighborhood colored redinteraction radius highlighted red consideragents 9 surrounding voxels 27 3dred line drawn figure implement uniformimplement uniform grid solution gpucuda opencl target gpus majorvendors minimize cpu gpu contextswitches decided port uniform grid algorithmmechanical force computation single gpu kernelgpu thread handles mechanical interactioncell 1 finding cells neighborhood 2 computingmechanical forces cell cellsneighborhood state data agents biodynamostored structsofarrays position dataagents store contiguously memory allowscopy required state data mechanical interactionoperation host dram gpu dramhaving coalesce data agentsc improvement reduction floatingpoint precisionbiodynamo uses doubleprecision floating points fp64data types floatingpoint data consumer gpus perform stronger singleprecision floatingpoint fp32 operations manifestation factgpu vendors primarily target gaming industry fieldartificial intelligence game engines machine learning frameworks rely singleprecision floatingpointoperations gpu manufacturers designed consumergpus fp32 logic units doubleprecisioncounterparts gpu vendors dedicated cardshighperformance scientific computing offer fp64logic units agentbased simulations factors1 httpsgithubcomsenuibiodynamotreepaperfloatsdata requiredthread xb data requiredthread yc data requiredthread zfig 7 exploiting reuse neighboring simulation objectdata usage shared memory resources gpufig 6 path zorder curve 2d adapted 18choosing correct runtime parameters modelinitial agent attribute values number simulation stepsgenerally far outweigh accuracy final resultscomparison imprecision come forthreducing floatingpoint precision double singlebiodynamo extensive set unit tests integrationtests use verify reductionfp32 affects results fp32 data types halfsize fp64 data types memory reduces sizebuffers need copied forthhost device leading potentially significant increasethroughput performanced improvement ii spacefilling curve sortingcuda opencl organize threads groups threadscalled blocks workgroups respectively executionthreads actual hardware warps generallygroups 32 threads warp executinginstruction different data simt execution modelbiodynamo lays agents data memory orderc objects instantiated thread requiresdata neighborhood simulation object processescontiguous memory scattered consequently thread performs numerous scattered memoryaccesses cases end fetching datadram degrade performance significantlyprevented data agentsclose space laid closememory spacefilling curves comespecifically zorder curve 19 spacefilling curvedescribes path multidimensional space passesdata points consecutively local order illustratedfig 6 function implements spacefilling curvemap multidimensional data 3d cartesian coordinatesonedimensional array consecutive elementsarray spatially local zordercurve zvalue data point computedbinary interleaving coordinate values representsindex resulting onedimensional array regardsbiodynamo imply calculating zvaluesagents sorting state data accordinglyanticipate cache line accessing agentcontain data agents neighborhoodreduces number fetches dram reducednumber fetches dram lead datastarvedexecution pipeline higher throughputreduction execution time simulation stepe improvement iii shared memorygpus feature different types onchip memorytexture memory shared memory certain casesstoring data onchip memory drastically reduces latencyfetching data gpu kernel executionimprove overall performance biodynamoconcept letting gpu thread handle mechanicalinteractions agent leaves little room sharedmemory resources gpus reasonreuse data threads cudablock opencl workgroup kernel parallelizesloop agents thread works dataindependent threads block useshared memory need create kernel allows multiplethreads work datareap benefits uniform grid methodimplemented alternative kdtree methodexploit fact cells voxel ug gridshare neighboring voxels sharesimulation object candidates neighborhood insteadparallelizing loop cells consider kernelparallelize loop voxels threadsprocess agents single voxel need reuseneighborhood data stored shared memorylowlatency memory fetches concept illustratedfig 7 state data belonging agentshighlighted region fig 7 stored shared memoryshared memory objects built parallel appendingstate data agents multiple voxels highlightedregion avoid race conditions use atomic operationsrequired building shared memory objects parallelv e xperimental s etuphardware evaluations belongcern department tabulated tablecpus systems consist physical socketsorganized nonuniform memory access numa designtable specifications systems benchmarkinggpu chipgpu rammemorybandwidthsingleprecisionperformancedoubleprecisionperformancenvidia gtx1080 ti11gb484 gbs1134 tflops0354 tflopsbnvidia tesla v10032gb900 gbs157 tflops78 tflopsbaseline serialbaseline 20 threadsugmethod serialugmethod 20 threadsgpu version 0gpu versiongpu version iigpu version iii2581782261449719101039527199274102103104runtime ms105cpu chipcpu coresintel xeone52640 v4intel xeongold 613020 2 sockets40 threads32 2 sockets64 threadsbaseline serial 1baseline 20 threadsugmethod serialugmethod 20 threadsgpu version 0gpu versiongpu version iigpu version iii100cpu dram256gb187gb130101102speedup103fig 8 runtime implementations mechanical interaction operation running benchmark gpuresults obtained cuda runtimefig 9 speedup respect serial baseline versionobtained benchmark gpu results obtainedcuda runtimemitigate crossnuma effects benchmarkresults run benchmarks socketnuma domains practice achievedlinux utility tool taskset section vi explicitlymention benchmarks run single numadomain implementations benchmarksgithub2profile gpu kernel performance metricsuse nvprof cuda sdktoolkit prior recording timing data profiling gpubenchmarks run iterations kernel warmgpu measure necessary following reasons1 gpu initially powersaving stateperform optimally run 2 justintime compilation kernel requires timecompilation 3 additional time taken transferringkernel binary gpu memoryquantify performance solutions performtypes analyses run cell division benchmark benchmark introduced section iiibenchmark quantify performancesolution section iv second created benchmarkbenchmark b analyze performance modelsdifferent local neighborhood densities cell divisionbenchmark fixed average number neighboring agentsagent represents modelsneighborhood density second benchmark varyaverage neighborhood density spawning millionagents random positions variablesized simulation spaceconsequently average number neighboring agentsagent greater simulation space smallermaintain constant neighborhood density simulatedtime set maximum displacement value agentzero neighboring agents stay locked spaceneighborhood density stay constanttiming results benchmarks exclude modelinitialization time creating agents assigning behaviorsfocus simulation performance thirdlyunderstand performance limitations current gpuimplementation perform roofline analysis 20best performing gpu implementation analysisunderstand far current implementationmaximum attainable performance b useempirical roofline tool ert 21 measure empiricalperformance numbers b generate rooflineanalysis plot retrieve performance result gflopsarithmetic intensity flopsbyte gpu kerneluse nvprof2 httpsgithubcomsenuihicombbenchmarksvi r esultsfig 8 shows runtimes obtained running benchmarkimplementations mechanical interaction operation biodynamo fig 9 shows obtainedspeedups comparison serial baseline version notexaxis scaled logarithmically figures orderbar charts follows order versionsintroduced section iv consecutive gpu versionsinclude implementation prior version exampleintel xeon 6130intel xeon 6130intel xeon 6130intel xeon 6130intel xeon 6130tesla v100intelintelintelintelintel250speedupruntime ms1063004 threads8 threads16 threads32 threads64 threads105xeonxeonxeonxeonxeon613061306130613061304 threads8 threads16 threads32 threads64 threads200150104100103number neighbors agentnumber neighbors agentfig 10 runtime benchmark b varying neighborhood density intel xeon entries represent baselineversion tesla v100 entries represent best performinggpu implementation gpu results obtainedcuda runtime bfig 11 speedups respect baseline versionnumbers threads obtained benchmark bvarying neighborhood density gpu results obtainedcuda runtime bgpu version ii includes changes gpu versionresults fig 9 obtained running benchmarkserial uniform grid ug method performs twice fastserial kdtree method 20 coressingle numa domain ug method 82261910 43 timesfaster kdtree method attributedparallel construction uniform grid opposedserial construction kdtreeinitial version gpu implementation gpuversion 0 fig 9 ug method offers82261039 79 speedup compared multithreaded1910baseline version 103918 times fastermultithreaded cpu version kerneloptimized massively parallel architecturegpu able attain significant speedup comparedmultithread cpu versionfig 9 1039527 20 speedup gainedreducing data types define cells statedoubles floats table fp32throughput 32 times greater fp64 throughputspeedup result clear current gpusolution limited memory bandwidth fp32 datatypes 4 bytes fp64 data types 8 bytes expectedspeedup gpu application memory boundheavily relies floatingpoint operations verifiedcorrectness simulations affectedresult reducing floatingpoint precision runningunit tests integration tests included testingsuite biodynamosorting agents state data based spacefilling curveproved reduce execution time significantly527199 26 times comparison previous gpu versionspeedup confirms gpu kernel enjoys spatialdata locality agents state data sorted resultmemory accesses coalesced turn leadsincrease cache hits reduces overall latencyobtaining required neighborhood data memoryredesigning gpu kernel utilize shared memoryresources appears worsen overall performance 28reasons causes kernel performance deteriorate introduction atomic operationskernel use atomics necessary buildshared data structures introduced section iveparallel causes stalling multiple threadstry update shared data object kernelneeds perform boundary checks blocks cudaworkgroups opencl executed gpugives rise thread divergencefig 10 fig 11 summarize results runningbenchmark b b cpu results 32 threadsobtained running single numa domainb fig 10 shows runtimes multithreadedbaseline version 4 8 16 32 64 threads bestperforming gpu version gpu version ii varyingnumber neighboring agents agent figureclear increasing number threadscpuonly runtime reduces runtime marginallygpu coprocessing shows significant reductionruntime fig 11 shows speedup gpu runtimecomparison multithreaded baseline version observespeedup comparison baseline version runningvii c onclusiongoal work perform comparative studyacceleration potential gpu coprocessing biodynamo enable fast simulation largescale complexbiological models understand effectiveway improve performance simulationslargescale complex models implementedprofiled simulations biodynamo currently capablerunning discovered mechanical interactionsoperation computational bottleneck large marginrequired data local neighboring agentsimplemented method alternative kdtree methoduniform grid ug method proved excellentcandidate exploiting parallel architecture gpusperformance gain ug method outperformkdtree method cpu opened possibilitiesexploit advantages gpus offer final gpu kernelv100fp32 147080 gflops104fp64 77193 gflopsperformance gflopsec4 threads lies 160 232 dependingneighborhood density baseline version 64threads speedup lies 71 113 resultsimply simulations densely populated enjoyspeedup orders magnitude acceleratingworkload gpu simulations normallydays multicore cpu completed hourssystems feature gpu significant reductionsimulation runtime allows researchers field biologicalabs scale models obtain results rapidlyfig 11 notice gpu performance gainstagnates decreases neighborhood densityincreases gpu kernel parallelizes mechanical interaction computation agents loop neighboring agents serial consequently bottleneckmodels high neighborhood density likeinvestigate solution exploring dynamic parallelism 22existing gpu programming models hypothesizeparallelizing serial loop neighborhood alleviatesbottleneck manifested fig 11roofline model analysis fig 12best performing gpu implementation ordermagnitude away maximum attainable singleprecisionfloatingpoint performance b data pointsclose roof represents upper bounddevice memory bandwidth hbm indicateskernel close memorybound future improvementskernel focus alleviating strain datatransfer gpu gpu memory investigatingcaching methods bypass hbm bandwidth rooflinemain priority future improvementsobserve kernel able attain higher performancehigher neighborhood density based percentagel2 cache reads relative number total l2 hbmmemory reads obtained nvprof believeresult increased cache reuse neighborhoodstate data agent n 47 percentage 413n 27 406 n 6 394103n47n27100101arithmetic intensity flopsbyte102fig 12 gpu roofline model analysis neighborhooddensities b n number neighborsagentimplementation resulted speedups 71 232comparison multithreaded baseline version dependingnumber neighboring agents agent numberthreads baseline executed result enablesresearchers cellular agentbased models rapidly obtainbiologically insightful simulations biodynamof undingwork supported cern knowledge transferoffice lb cern openlab ahr eferencesc m macal m j north agentbased modelingsimulation proceedings 2009 wintersimulation conference wsc ieee 2009 pp 86982 g q mi j duttamoscato y vodovotzagentbased models translational systems biologywiley interdisciplinary reviews systems biologymedicine vol 1 2 pp 159171 20093 b di ventura c lemerle k michalodimitrakisl serrano vivo silico biologynature vol 443 7111 pp 527533 20064 g fiori f bonaccorso g iannaccone t palaciosd neumaier seabaugh s k banerjee lcolombo electronics based twodimensional materials nature nanotechnology vol 9 10 pp 768779 20145 g e moore et al cramming componentsintegrated circuits 19656 l breitwieser hesam j montigny vvavourakis iosif j jennings m kaiser m mancad meglio z alars f rademakers o mutlur bauer biodynamo general platformscalable agentbased simulation 2021 arxiv 200606775 cscem lysenko r m dsouza frameworkmegascale agent based model simulations graphicsprocessing units journal artificial societiessocial simulation vol 11 4 p 10 2008 issn14607425 online available httpjassssocsurreyacuk11410htmlp richmond d walker s coakley d romanohigh performance cellular level agentbased simulation flame gpu en briefingsbioinformatics vol 11 3 pp 334347 2010publisher oxford academic issn 14675463 doi 101093bibbbp073 online available httpsacademicoup com bib article 11 3 334 225993 visited10082020s ren k bertels z alars efficient acceleration pairhmms forward algorithm gatk haplotypecaller graphics processing units evolutionarybioinformatics vol 14 p 1 176 934 318 760 543 2018pmid 29568218 doi 10 1177 1176934318760543eprint https doi org 10 1177 1176934318760543online available https doi org 10 11771176934318760543g smaragdos g chatzikonstantis r kukreja hsidiropoulos d rodopoulos sourdis z alarsc kachris d soudris c d zeeuw c strydisbrainframe nodelevel heterogeneous acceleratorplatform neuron simulations journal neuralengineering vol 14 6 p 066 008 nov 2017 doi10108817412552aa7fc5 online available httpsdoiorg10108817412552aa7fc5j nickolls w j dally gpu computing eraieee micro vol 30 2 pp 5669 2010p van liedekerke m palm n jagiella d drasdosimulating tissue mechanics agentbased modelsconcepts perspectives novel results computational particle mechanics vol 2 4 pp 4014442015s kang s kahan j mcdermott n flannshmulevich biocellion accelerating simulation multicellular biological models bioinformatics vol 30 21 pp 31013108 2014ghaffarizadeh r heiland s h friedman s mmumenthaler p macklin physicell opensource physicsbased cell simulator 3d multicellularsystems plos computational biology vol 14 2e1005991 2018m cytowski z szymanska largescale parallelsimulations 3d cell colony dynamics computingscience engineering vol 16 5 pp 8695 2014g r mirams c j arthurs m o bernabeu rbordas j cooper corrias y davit sj dunng fletcher d g harvey et al chaste opensource c library computational physiologybiology plos computational biology vol 9 3e1002970 2013hauri selfconstruction context corticalgrowth cell cortex programmingparadigm selfconstructing systems phd dissertation eth 201318 wikimedia commons filefourlevel zsvg wikimedia commons free media repository onlineaccessed 20august2018 201819 g m morton oriented geodetic data basenew technique file sequencing 196620 s williams waterman d patterson rooflineinsightful visual performance model multicorearchitectures communications acm vol 524 pp 6576 200921 c yang r gayatri t kurth p basu z ronaghiadetokunbo b friesen b cook d doerflerl oliker et al empirical roofline methodologyquantitatively assessing performance portability2018 ieeeacm international workshop performance portability productivity hpc p3hpcieee 2018 pp 142322 s jones introduction dynamic parallelism gputechnology conference presentation s vol 338 2012p 2012