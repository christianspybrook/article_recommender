improving response quality backward reasoningopendomain dialogue systemsziming lijulia kiselevamaarten rijkezliuvanluniversity amsterdamamsterdam netherlandsjuliakiselevamicrosoftcommicrosoftredmond united statesmderijkeuvanluniversity amsterdam aholddelhaizeamsterdam netherlandsarxiv210500079v1 cscl 30 apr 2021abstractable generate informative coherent dialogue responsescrucial designing humanlike opendomain dialogue systems encoderdecoderbased dialogue models tend producegeneric dull responses decoding steppredictable response likely noninformative responseinstead suitable alleviate problem propose train generation model bidirectional manneradding backward reasoning step vanilla encoderdecodertraining proposed backward reasoning step pushes modelproduce informative coherent content forward generation steps output infer dialogue contextbackward direction advantage methodforward generation backward reasoning steps trainedsimultaneously use latent variable facilitatebidirectional optimization method improve response quality introducing information pretrained topicmodel proposed bidirectional response generation methodachieves stateoftheart performance response qualityccs conceptsinformation systems chat question answeringkeywordsopendomain dialogue response generationacm reference formatziming li julia kiseleva maarten rijke 2021 improving responsequality backward reasoning opendomain dialogue systemsproceedings 44th international acm sigir conference researchdevelopment information retrieval sigir 21 july 1115 2021 virtualevent canada acm new york ny usa 5 pages httpsdoiorg10114534048353463004introductionrecently developed endtoend dialogue systems trainedlarge volumes humanhuman dialogues capture underlyinginteraction patterns 4 10 14 16 29 35 38 commonlypermission digital hard copies work personalclassroom use granted fee provided copies distributedprofit commercial advantage copies bear notice citationpage copyrights components work ownedauthors honored abstracting credit permitted copyrepublish post servers redistribute lists requires prior specific permissionandor fee request permissions permissionsacmorgsigir 21 july 1115 2021 virtual event canada2021 copyright held ownerauthors publication rights licensed acmacm isbn 97814503803792107 1500httpsdoiorg10114534048353463004hicontextim goodwhats weather tomorrowquerydont know responsereplycloudyresponse bfigure 1 informative response response bfigure provide information helps infer querycontent given dialogue contextapproach designing datadriven dialogue systems useencoderdecoder framework feed dialogue context encoder let decoder output appropriate response buildingfoundation different directions explored designdialogue systems tend interact humans coherentengaging manner 2 15 19 31 34 37 despite significant advances room improvement qualitymachinegenerated responsesimportant problem encoderdecoder dialogue modelstendency generate generic dull responsesdont know im sure 2 9 14 15types methods dealing problem introduces updating signals training modeling futurerewards ease answering applying reinforcement learning 15 19 bringing variants adding constraints decoding step 2 14 31 second type holds dialoguehistory generating highquality responsesinformation taken account topic information 34 35 personal user profiles 37 solutions relyinglarge pretrained language models dialogpt 38classified second familypaper propose train dialogue generation modelsbidirectionally adding backward reasoning step vanillaencoderdecoder training process assume informationflow conversation coherent topicrelevant givendialogue history neighboring turns supposed tighttopical connection infer partial content turn givenprevious turn vice versa inferring turn givenprevious conversation history current turntraditional dialogue generation task extendadding step given dialogue historyturn aim infer content current turnstep backward reasoning hypothesize pushgenerated response informative coherentunlikely infer dialogue topic given generic dull responsebackward direction example shown figure 1 givendialogue context query1 predict reply followingtraditional encoderdecoder dialogue generation setup contrastinfer content query given context replylong reply informative inspired zheng et al 39introduce latent space bridge simultaneously trainencoderdecoder model directions experimentsdemonstrate resulting dialogue generation model calledmirror benefits bidirectional training processoverall work provides following contributionsc1 introduce dialogue generation model mirror generating high quality responses opendomain dialogue systemsc2 define new way train dialogue generation models bidirectionally introducing latent variablec3 obtain improvements terms dialogue generation performance respect human evaluation datasetsrelated workconversational scenarios considered today increasinglycomplex going ability rulebased dialogue systems 30ritter et al 22 propose datadriven approach generate responses building phrasebased statistical machine translationneural networkbased models studied generateinformative interesting responses 23 26 29 serban et al 24introduce latent stochastic variables span variable numbertime steps facilitate generation long outputs deep reinforcement learning methods applied generatecoherent interesting responses modeling future influence generated responses 15 19 retrievalbased methodspopular building dialogue systems learning matchingmodel context predefined response candidatesresponse selection 7 20 28 33 work focuses responsegeneration selectionencoderdecoder models tend generate generic dullresponses li et al 14 propose maximum mutual informationobjective function neural models generate diverseresponses xing et al 35 consider incorporating topic informationencoderdecoder framework generate informativeinteresting responses address dullresponse problem bahetiet al 2 propose incorporating information formdistributional constraints generated responses su et al27 propose new perspective diversify dialogue generationleveraging nonconversational text recently pretrained languagemodels gpt2 21 bert 5 xlnet 36 provedeffective wide range natural language processing tasksauthors use pretrained transformers attainperformance close humans terms automatic humanevaluation 6 32 38 pretrained language modelsperform general dialogue generationeffective data resources support1use query distinguish current dialogue turn contextresponse query necessarily real query question considered searchquestionanswering tasksmodels pretraining work value developingdialogue generation models limited data resourceskey distinction compared previous efforts 2 14work use original training dataset differentiable backward reasoning step external information3 method mirror31 problem settingconversational scenarios dialogue context relativelylong contains lot information reply responseshort different speaker makes difficult predictinformation context relying responsebackward direction decompose dialoguecontext different segments context c query xfigure 1 assuming predicting response turn tdialogue context c consist dialogue turnst m t 2 query x corresponds turn t 1use term query distinguish dialogue turn timestep t 1 context c response y explainedterm query confused query questionsearch questionanswering tasks value m indicatesdialogue turns context c userepresent concatenation c x originalcontext decomposed final goal predictresponse y given dialogue context c query xmirrorgenerative dialogue generationshen et al 25 propose maximize conditional log likelihoodgenerating response y given context log p yintroduce latent variable z group different valid responsesaccording context lower bound log p ygivenlog p y ezq z y log p y zd kl q z yp zeq 1 q z y posterior network p zpriorinstead maximizing conditional log likelihood log p ypropose maximize log p x y c representingconditional likelihood x y appears given dialoguecontext c main assumption underlying changeconversation information flow neighboring turnscoherent relevant connectionbidirectional example possible infer querygeneric noninformative reply dont knowgiven shown figure 1 taking account informationflow different directions hypothesize buildcloser connection response dialogue historygenerate coherent informative responsespropose optimize log p x y c instead log p yfollowing 12 25 choose maximize variational lowerbound log p x y c givenlog p x y c ezq z cxy log p x y c zd kl q z c x yp z cencdecrecnetpriornetencdecdecdecfigure 2 main architecture model mirror consists steps information encoding latent variablegeneration target decodingencz shared latent variable context c query xresponse y explain optimize dialoguemaximizing lower bound shown eq 2 directions321 forward generation dialogue generation respectforward dialogue generation interpret conditional likelihood log p x y c z forward directionlog p x y c z log p y c z x log p x c zrewrite eq 2 forward directionlog p x y cezq z cxy log p y c x z log p x c zd kl q z c x yp z cintroduce q z c x y posterior network referredrecognition net p z c prior network322 backward reasoning dialogue generation forwarddirection decompose conditional likelihood log p x yc z backward direction rewrite eq 2log p x y cezq z cxy log p x c y z log p y c zd kl q z c x yp z c323 optimizing dialogue systems bidirectionally variable z sampled shared latent space forwardgeneration backward reasoning steps regard zbridge connect training different directionopens possibility train dialogue models effectively merging eq 4 eq 5 rewrite lower bound eq 2log p x y c ezq z cxylog p x c z ylog p y c z log p y c z xlog p x c z d kl q z c x yp z clc x yfinal loss function dialogue generation model324 model architecture complete architecture proposed joint training process shown figure 2 consistssteps 1 information encoding 2 latent variable generation3 target decoding respect information encodingstep utilize context encoder encctx compress dialoguecontext c utterance encoder encutt compressquery x response y respectively model latent variable z assume z follows multivariate normal distributionposterior network q z c x y n 2 priornetwork p z c n 2 applying reparameterization trick 12 sample latent variable zestimated posterior distribution n 2 testing useprior distribution n 2 generate variable zkldivergence distance applied encourage approximatedposterior n 2 close prior n 2 accordingeq 6 decoding step right figure 2 consistsindependent decoders dec 1 dec 2 dec 3 dec 4 corresponding log p y c z x log p x c z log p x c z ylog p y c z respectively decoder dec 1 generatefinal response testing stage use variable z attach input decoding stepshared latent vector z bridge training directions independent updating direction definitelyimprove direction end directionscontribute final dialogue generation process4 experimental setup41 datasetsuse datasets movietriples dataset 23 developed expanding preprocessing moviedic corpus 3film transcripts dialogue consists 3 turnsspeakers regard turn dialogue contextsecond query response respectivelyfinal dataset 166k dialogues trainingset 21k validation set 20k test set termsvocabulary table size set 20k frequent wordsdatasetsecond dailydialog dataset 18 highquality multiturndialogue dataset split dialogues original datasetshorter dialogues turns new dialogueturn target response contextquery preprocessing 65k 6k6k dialogs training testing validation sets respectivelylimit vocabulary table size 20k frequentwords dailydialog datasetbaselinesseq2seqatt lstmbased 8 dialogue generation modelattention mechanism 1hred method 23 uses hierarchical recurrent encoderdecoder sequentially generate tokens repliesvhred extension hred incorporates stochastic latentvariable explicitly model generative processes possessmultiple levels variability 24 model trainedeq 1mmi method generates response candidates seq2seqmodel trained direction contexttotarget p y c xreranks separately trained seq2seq modeldirection targettocontext p x y maximizemutual information 14training detailsimplement model mirror 2 pytorch opennmtframework 13 utterance encoder twolayer lstm 8dimension 1000 context encoder architecture utterance encoder parameters shareddecoders design independent parameters twolayer lstm 1000 dimensionsterms dimension hidden vector z set 160dailydialog dataset 100 movietriples wordembedding size 200 datasets use adam 11optimizer initial learning rate 0001 learning rate decayapplied stabilize training processevaluationconduct human evaluation amazon mturk guided 17twoway comparison dialogue responses mirror ask annotators judge responsesappropriate given context method pair mirror baseline dataset randomly sample 200 dialoguestest datasets pair responses annotated 3 annotators2 codebasehttpsgithubcomcszmlimirrorsigirwinslossestiesmirror vs seq2seqattnmirror vs hredmirror vs vhredmirror vs mmimirror vs dcmirror vs dcmmi053041045048050039037040038042033035010019017010017026mirror vs seq2seqattnmirror vs hredmirror vs vhredmirror vs mmimirror vs dcmirror vs dcmmi050049048040045047026032037034038035024019015026017018effectiveness maximizing mutual information improvingresponse quality mirror method treated waymaximize mutual information implicitly advantagetrain dialogue models directions simultaneouslytable 2 example generated responses different modelsdialogue context givencontextspeaker s licensespeaker b m afraid m going askremain apartment narcotics squad arrivingmoment want ask questionsresponseresults analysistable 1 performance comparisons mirrorbaselines different datasets according table 1topsomewhat unexpected hred achieve closeperformance compared mirror dailydialog given mainarchitecture hierarchical encoderdecoder model randomlysample dialogue pairs hred outperforms mirrorannotators prefer hred mirrorcases mirror fails generate appropriate responses hredreturns generic acceptable responses given contextreasoning step mirror expectlead informative generations increasesrisk generating responses incorrect syntax relevantinappropriate responses possible reasonbackward reasoning step dominated joint training processdegenerate forward generation performanceperformance gap mirror approaches including hred large dailydialog dataset table 1bottomspace limitations present dialogue exampletable 2 example typical case response generateddc high embedding scores human evaluation resultpromising example response dc high semantic similarity context words like askapartment questions regardedappropriate meaningful response given context comparing mirror methods use mmi mmi dcmmiperformance gap relatively small evidence showingmethod pairmovietripletable 1 human evaluation movietriple dailydialog datasetsb dailydialogdc method incorporates information form distributional constraints including topic constraints semanticconstraints 2dcmmi method combination mmi dcdecoding step takes account mutual informationproposed distribution constraints method dcreference squad wantt use aspirinseq2seqhred t thinkvhred oh m sorrymmi m sorry m sorry t knowtalking t know m afraiddc m going ask apartmentmoment questionsdcmmi m going askmirror m sure ll waitconclusion future workpresented novel approach generating informativecoherent responses opendomain dialogue systems calledmirror reformulate original response generation tasksides context response sides context queryresponse given dialogue context query predictingresponse exactly like traditional dialogue generation setupmirror step inferring query given dialogue context response incorporating backward reasoning step implicitly push model generate responsescloser connections dialogue history conductingexperiments datasets demonstrated mirrorimproves response quality compared competitivebaselines incorporating additional sources informationcomes additional computational costs complexity future work mirrors bidirectional training approachgeneralized domains taskoriented dialoguesystems questionanswering tasksreferences1 dzmitry bahdanau kyunghyun cho yoshua bengio 2014 neural machine translation jointly learning align translate arxiv preprintarxiv14090473 20142 ashutosh baheti alan ritter jiwei li dolan 2018 generatinginteresting responses neural conversation models distributional constraints arxiv preprint arxiv180901215 20183 rafael e banchs 2012 moviedic movie dialogue corpus researchdevelopment proceedings 50th annual meeting associationcomputational linguistics short papersvolume 2 association computationallinguistics 2032074 siqi bao huang fan wang hua wu haifeng wang 2019 plato pretrained dialogue generation model discrete latent variable arxiv preprintarxiv191007931 20195 jacob devlin mingwei chang kenton lee kristina toutanova 2018 bertpretraining deep bidirectional transformers language understandingarxiv preprint arxiv181004805 20186 sergey golovanov rauf kurbanov sergey nikolenko kyryl truskovskyi alexander tselousov thomas wolf 2019 largescale transfer learning naturallanguage generation proceedings 57th annual meeting associationcomputational linguistics 605360587 jiachen gu tianda li quan liu zhenhua ling zhiming su si weixiaodan zhu 2020 speakeraware bert multiturn response selectionretrievalbased chatbots proceedings 29th acm international conferenceinformation knowledge management 204120448 sepp hochreiter jrgen schmidhuber 1997 long shortterm memory neuralcomputation 9 8 1997 173517809 shaojie jiang maarten rijke 2018 sequencetosequence modelsdull understanding lowdiversity problem chatbots proceedings2018 emnlp workshop scai 2nd international workshop searchoriented conversational ai acl10 chandra khatri behnam hedayatnia anu venkatesh jeff nunn yi pan qing liuhan song anna gottardi sanjeev kwatra sanju pancholi et al 2018 advancingstate art open domain dialog systems alexa prizearxiv preprint arxiv181210757 201811 diederik p kingma jimmy ba 2014 adam method stochastic optimization arxiv preprint arxiv14126980 201412 diederik p kingma max welling 2013 autoencoding variational bayesarxiv preprint arxiv13126114 201313 guillaume klein yoon kim yuntian deng jean senellart alexander mrush 2017 opennmt opensource toolkit neural machine translationproc acl14 jiwei li michel galley chris brockett jianfeng gao dolan 2015diversitypromoting objective function neural conversation models arxivpreprint arxiv151003055 201515 jiwei li monroe alan ritter michel galley jianfeng gao dan jurafsky2016 deep reinforcement learning dialogue generation arxiv preprintarxiv160601541 201616 jiwei li monroe tianlin shi sbastien jean alan ritter dan jurafsky2017 adversarial learning neural dialogue generation proceedings2017 conference empirical methods natural language processing 2157216917 margaret li jason weston stephen roller 2019 acuteeval improveddialogue evaluation optimized questions multiturn comparisonsarxiv preprint arxiv190903087 201918 yanran li hui su xiaoyu shen wenjie li ziqiang cao shuzi niu 2017dailydialog manually labelled multiturn dialogue dataset arxiv preprintarxiv171003957 201719 ziming li julia kiseleva maarten rijke 2019 dialogue generationimitation learning inverse reinforcement learning proceedings aaaiconference artificial intelligence vol 33 6722672920 lisong qiu yingwai shiu pingping lin ruihua song yue liu dongyan zhaorui yan 2020 bots feel moods proceedings 43rd international acm sigir conference research development informationretrieval 1161117021 alec radford jeffrey wu rewon child david luan dario amodei ilyasutskever 2019 language models unsupervised multitask learners openaiblog 1 8 2019 922 alan ritter colin cherry william b dolan 2011 datadriven responsegeneration social media proceedings conference empirical methodsnatural language processing association computational linguistics 58359323 iulian vlad serban alessandro sordoni yoshua bengio aaron c courvillejoelle pineau 2016 building endtoend dialogue systems generativehierarchical neural network models aaai vol 16 3776378424 iulian vlad serban alessandro sordoni ryan lowe laurent charlin joellepineau aaron courville yoshua bengio 2017 hierarchical latent variable encoderdecoder model generating dialogues thirtyfirst aaaiconference artificial intelligence25 xiaoyu shen hui su yanran li wenjie li shuzi niu yang zhao akiko aizawaguoping long 2017 conditional variational framework dialoggeneration arxiv preprint arxiv170500316 201726 alessandro sordoni michel galley michael auli chris brockett yangfeng jimargaret mitchell jianyun nie jianfeng gao dolan 2015 neuralnetwork approach contextsensitive generation conversational responsesarxiv preprint arxiv150606714 201527 hui su xiaoyu shen sanqiang zhao xiao zhou pengwei hu randy zhongcheng niu jie zhou 2020 diversifying dialogue generation nonconversational text arxiv preprint arxiv200504346 202028 chongyang tao wei wu xu wenpeng hu dongyan zhao rui yan2019 multirepresentation fusion network multiturn response selectionretrievalbased chatbots proceedings twelfth acm internationalconference web search data mining 26727529 oriol vinyals quoc le 2015 neural conversational model arxiv preprintarxiv150605869 201530 joseph weizenbaum 1966 elizaa program study naturallanguage communication man machine commun acm 9 1 1966364531 sam wiseman alexander m rush 2016 sequencetosequence learningbeamsearch optimization arxiv preprint arxiv160602960 201632 thomas wolf victor sanh julien chaumond clement delangue 2019transfertransfo transfer learning approach neural network based conversational agents arxiv preprint arxiv190108149 201933 yu wu wei wu chen xing ming zhou zhoujun li 2016 sequentialmatching network new architecture multiturn response selectionretrievalbased chatbots arxiv preprint arxiv161201627 201634 chen xing wei wu yu wu jie liu yalou huang ming zhou weiyingma 2016 topic augmented neural response generation joint attentionmechanism arxiv preprint arxiv160608340 2 2 201635 chen xing wei wu yu wu jie liu yalou huang ming zhou weiying ma2017 topic aware neural response generation thirtyfirst aaai conferenceartificial intelligence36 zhilin yang zihang dai yiming yang jaime carbonell ruslan salakhutdinovquoc v le 2019 xlnet generalized autoregressive pretraining languageunderstanding arxiv preprint arxiv190608237 201937 saizheng zhang emily dinan jack urbanek arthur szlam douwe kielajason weston 2018 personalizing dialogue agents dogpets arxiv preprint arxiv180107243 201838 yizhe zhang siqi sun michel galley yenchun chen chris brockett xianggao jianfeng gao jingjing liu dolan 2019 dialogpt largescalegenerative pretraining conversational response generation arxiv preprintarxiv191100536 201939 zaixiang zheng hao zhou shujian huang lei li xinyu dai jiajun chen2019 mirrorgenerative neural machine translation international conferencelearning representations