data augmentation in high dimensional low sample size setting using a geometrybased variational autoencoder arxiv210500026v1 statml 30 apr 2021 clment chadebec elina thibeausutre ninon burgos and stphanie allassonnire for the alzheimers disease neuroimaging initiative and the australian imaging biomarkers and lifestyle flagship study of ageing abstractin this paper we propose a new method to perform data augmentation in a reliable way in the high dimensional low sample size hdlss setting using a geometrybased variational autoencoder our approach combines a proper latent space modeling of the vae seen as a riemannian manifold with a new generation scheme which produces more meaningful samples especially in the context of small data sets the proposed method is tested through a wide experimental study where its robustness to data sets classifiers and training samples size is stressed it is also validated on a medical imaging classification task on the challenging adni database where a small number of 3d brain mris are considered and augmented using the proposed vae framework in each case the proposed method allows for a significant and reliable gain in the classification metrics for instance balanced accuracy jumps from 663 to 743 for a stateoftheart cnn classifier trained with 50 mris of cognitively normal cn and 50 alzheimer disease ad patients and from 777 to 863 when trained with 243 cn and 210 ad while improving greatly sensitivity and specificity metrics i ntroduction ven though always larger data sets are now available the lack of labeled data remains a tremendous issue in many fields of application among others a good example is healthcare where practitioners have to deal most of the time with very low sample sizes think of small patient cohorts along with very high dimensional data think of neuroimaging data that are 3d volumes with millions of voxels unfortunately this leads to a very poor representation of a given population and makes classical statistical analyses unreliable 1 2 meanwhile the remarkable performance of algorithms heavily relying on the deep learning framework 3 has made them extremely attractive and very popular however such results are strongly conditioned by the number of training samples since such models usually need to be trained on huge data sets to prevent overfitting data used in preparation of this article were obtained from the alzheimers disease neuroimaging initiative adni database httpadniloniuscedu as such the investigators within the adni contributed to the design and implementation of adni andor provided data but did not participate in analysis or writing of this report a complete listing of adni investigators can be found at httpadniloniusceduwpcontentuploadshow to apply adni acknowledgement listpdf data used in the preparation of this article was obtained from the australian imaging biomarkers and lifestyle flagship study of ageing aibl funded by the commonwealth scientific and industrial research organisation csiro which was made available at the adni database httpadniloniuscedu the aibl researchers contributed data but did not participate in analysis or writing of this report aibl researchers are listed at wwwaiblcsiroau clment chadebec and stphanie allassonnire are with the universit de paris inria centre de recherche des cordeliers inserm sorbonne universit paris france elina thibeausutre and ninon burgos are with sorbonne universit institut du cerveau paris brain institute icm inserm u 1127 cnrs umr 7225 aphp hpital de la piti salptrire and inria aramis projectteam paris france or to give statistically meaningful results 4 a way to address such issues is to perform data augmentation da 5 in a nutshell da is the art of increasing the size of a given data set by creating synthetic labeled data for instance the easiest way to do this on images is to apply simple transformations such as the addition of gaussian noise cropping or padding and assign the label of the initial image to the created ones while such augmentation techniques have revealed very useful they remain strongly data dependent and limited some transformations may indeed be uninformative or even induce bias for instance think of a digit representing a 6 which gives a 9 when rotated while assessing the relevance of augmented data may be quite straightforward for simple data sets it reveals very challenging for complex data and may require the intervention of an expert assessing the degree of relevance of the proposed transformations in addition to the lack of data imbalanced data sets also severely limit generalizability since they tend to bias the algorithm toward the most represented classes oversampling is a method that aims at balancing the number of samples per class by upsampling the minority classes the synthetic minority oversampling technique smote was first introduced in 6 and consists in interpolating data points belonging to the minority classes in their feature space this approach was further extended in other works where the authors proposed to oversample close to the decision boundary using either the k nearest neighbor k nn algorithm 7 or a support vector machine svm 8 and so insist on samples that are potentially misclassified other oversampling methods aiming at increasing the number of samples from the minority classes and taking into account their difficulty to be learned were also proposed 9 10 however these methods hardly scale to highdimensional data 11 12 the recent rise in performance of generative models such as generative adversarial networks gan 13 or variational autoencoders vae 14 15 has made them very attractive models to perform da gans have already seen a wide use in many fields of application 16 17 18 19 20 including medicine 21 for instance gans were used on magnetic resonance images mri 22 23 computed tomography ct 24 25 xray 26 27 28 positron emission tomography pet 29 mass spectroscopy data 30 dermoscopy 31 or mammography 32 33 and demonstrated promising results nonetheless most of these studies involved either a quite large training set above 1000 training samples or quite small dimensional data whereas in everyday medical applications it remains very challenging to gather such large cohorts of labeled patients as a consequence as of today the case of high dimensional data combined with a very low sample size remains poorly explored when compared to gans vaes have only seen a very marginal interest to perform da and were mostly used for speech applications 34 35 36 some attempts to use such generative models on medical data either for classification 37 38 or segmentation tasks 39 40 41 can nonetheless be noted the main limitation to a wider use of these models is that they most of the time produce blurry and fuzzy samples this undesirable effect is even more emphasized when they are trained with a small number of samples which makes them very hard to use in practice to perform da in the high dimensional very low sample size hdlss setting in this paper we argue that vaes can actually be used for data augmentation in a reliable way even in the context of hdlss data provided that we bring some modeling of the latent space and amend the way we generate the data hence in this paper we propose the following contributions we propose to combine a proper modeling of the latent space of the vae here seen as a riemannian manifold and a new geometryaware nonpriorbased generation procedure which consists in sampling from the inverse of the riemannian metric volume element the choice of such a framework is discussed motivated and compared to some other vae models we propose to use such a framework to perform data augmentation in the challenging context of hdlss data the robustness of the augmentation method to data sets and classifiers changes along with its reliance to the number of training samples is then tested through a series of experiments we validate the proposed method on several reallife classification tasks on complex 3d mri from adni and aibl databases where the augmentation method allows for a significant gain in classification metrics even when only 50 samples per class are considered variational autoencoder in this section we quickly recall the idea behind vaes along with some proposed improvements relevant to this paper model setting let x x be a set of data a vae aims at maximizing the likelihood of a given parametric model p it is assumed that there exist latent variables z living in a lower dimensional space z referred to as the latent space such that the marginal distribution of the data can be written as p x p xzqzdz where q is a prior distribution over the latent variables acting as a regulation factor and p xz is most of the time taken as a simple parametrized distribution eg gaussian bernoulli etc such a distribution is referred to as the decoder the parameters of which are usually given by neural networks since the integral of eq 1 is most of the time intractable so is the posterior distribution p zx r p xzqz p xzqzdz this makes direct application of bayesian inference impossible and so recourse to approximation techniques such as variational inference 42 is needed hence a variational distribution q zx is introduced and aims at approximating the true posterior distribution p zx 14 this variational distribution is often referred to as the encoder in the initial version of the vae q is taken as a multivariate gaussian whose parameters and are again given by neural networks importance sampling can then be applied to derive an unbiased estimate of the marginal distribution p x we want to maximize in eq 1 p x p xzqz and ezq p p x q zx using jensens inequality allows finding a lower bound on the objective function of eq 1 log p x log ezq p  ezq log p  ezq log p x z log q zx elbo the evidence lower bound elbo is now tractable since both p x z and q zx are known and so can be optimized with respect to the encoder and decoder parameters improving the model literature review in recent years many attempts to improve the vae model have been made and we briefly discuss three main areas of improvement that are relevant to this paper in this section 221 enhancing the variational approximate distribution when looking at eq 2 it can be noticed that we are nonetheless trying to optimize only a lower bound on the true objective function therefore much efforts have been focused on making this lower bound tighter and tighter 43 44 45 46 47 48 one way to do this is to enhance the expressiveness of the approximate posterior distribution q this is indeed due to the elbo expression which can be also written as follows elbo log p x klq zxp zx this expression makes two terms appear the first one is the function we want to maximize while the second one is the kullbackleibler kl divergence between the approximate posterior distribution q zx and the true posterior p zx this very term is always nonnegative and equals 0 if and only if q p almost everywhere hence trying to tweak the approximate posterior distribution so that it becomes closer to the true posterior should make the elbo tighter and enhance the model to do so a method proposed in 49 consisted in adding k markov chain monte carlo mcmc sampling steps on the top of the approximate posterior distribution and targeting the true posterior more precisely the idea was to start from z0 q zx and use parametrized forward resp reverse kernels rzk1 zk x resp rzk zk1 x to create a new estimate of the true marginal distribution p x with the same objective parametrized invertible mappings fx called normalizing flows were instead proposed in 50 to sample z a starting random variable z0 is drawn from an initial distribution q zx and then k normalizing flows are applied to z0 resulting in a random variable zk fxk fx1 z0 whose density writes q zk x q z0 x  det jfxk 1 where jfxk is the jacobian of the k th normalizing flow ideally we would like to have access to normalizing flows targeting the true posterior and allowing enriching the above distribution and so improve the lower bound in that particular respect a model inspired by the hamiltonian monte carlo sampler 51 and relying on hamiltonian dynamics was proposed in 49 and 52 the strength of such a model relies in the choice of the normalizing flows which are guided by the gradient of the true posterior distribution 222 improving the prior distribution while enhancing the approximate posterior distribution resulted in major improvements of the model it was also argued that the prior distribution over the latent variables plays a crucial role as well 53 since the vanilla vae uses a standard gaussian distribution as prior a natural improvement consisted in using a mixture of gaussians instead 54 55 which was further enhanced with the proposal of the variational mixture of posterior vamp 56 in addition other models trying to change the prior distribution and relying on hierarchical latent variables have been proposed 43 57 58 prior learning is also a promising idea that has emerged eg 59 or more recently 60 61 62 and allows accessing complex prior distributions another approach relying on acceptreject sampling to improve the expressiveness of the prior distribution 63 can also be cited while these proposals indeed improved the vae model the choice of the prior distribution remains tricky and strongly conditioned by the training data and the tractability of the elbo 223 adding geometrical consideration to the model in the mean time several papers have been arguing that geometrical aspects should also be taken into account for instance on the ground that the vanilla vae fails to apprehend data having a latent space with a specific geometry several latent space modelings were proposed as a hypershere 64 where vonmises distributions are considered instead of gaussians or as a poincare disk 65 66 other works trying to introduce riemannian geometry within the vae framework proposed to model either the input data space 67 68 or the latent space or both 69 70 71 72 as riemannian manifolds the authors of 73 went further and bridged the gap with sec 221 by combining mcmc sampling and riemannian metric learning within the model they indeed proposed to see the latent space as a riemannian manifold and instead learn a parametrized riemannian metric over this space this idea of riemannian metric learning is attractive since it allows modeling the latent space as desired and was recently reused and combined with prior learning 74 t he p roposed m ethod in this section we present the proposed method which consists in combining a proper latent space modeling with a new nonprior based generation scheme we argue that while the vast majority of works dealing with vae generate new data using the prior distribution which is standard procedure this is often suboptimal in particular in the context of small data sets we indeed believe that the choice of the prior distribution is strongly data set dependent and is also constrained to be simple so that the elbo in eq 2 remains tractable hence the view adopted here is to consider the vae only as a dimensionality reduction tool which is able to extract the latent structure of the data ie the latent space modeled as the riemannian manifold rd g where d is the dimension of the manifold and g is the associated riemannian metric since the latent structure is apriori far from being trivial we propose in this paper to rely on the setting first introduced in 73 where the riemannian metric is directly learned from the data before going further we first recall some elements on riemannian geometry some elements on riemannian geometry in the framework of differential geometry one may define a riemannian manifold m as a smooth manifold endowed with a riemannian metric g that is a smooth inner product g p hip on the tangent space tp m defined at each point of the manifold p m we call a chart or coordinate chart u a homeomorphism mapping an open set u of the manifold to an open set v of an euclidean space the manifold is called a ddimension manifold if for each chart of an atlas we further have v rd that is there exists a neighborhood u of each point p of the manifold such that u is homeomorphic to rd given p u the chart x1 xd induces a basis x1 x on the tangent space tp m hence a local representation of the metric of a riemannian manifold in the chart u can be written as a positive definite matrix gp gij p0ijd h x i xj ip 0ijd at each point p u that is for v w tp m and p u we have huwip u gpw since we propose to work in the ambientlike manifold rd g there exists a global chart encoder metric network decoder fig 1 geometryaware vae framework neural networks are highlighted with the colored arrows and hriemannian are the normalizing flows using riemannian hamiltonian equations represents the parameters of the decoder eg gaussian bernoulli etc given by id hence for the following we assume that we work in this coordinate system and so g will refer to the metrics matrix representation in this chart there are two ways to apprehend manifolds the extrinsic view assumes that the manifold is embedded within a higher dimensional euclidean space think of the 2dimensional sphere s 2 embedded within r3 the intrinsic view which is adopted in this paper does not make such an assumption since the manifold is studied using its underlying structure for example a curves length cannot be interpreted using the distance defined on an euclidean space but requires the use of the metric defined onto the manifold itself the length of a curve between two points of the manifold z1 z2 m and parametrized by t 0 1 such that 0 z1 and 1 z2 is then given by z1 q ktkt dt httit dt curves minimizing such a length are called geodesics and a distance dist between elements of a connected manifold can be introduced as follows distz1 z2 inf l 0 z1 1 z2 the manifold m is said to be geodesically complete if all geodesic curves can be extended to r in other words at each point p of the manifold one may draw a straight line with respect to the formerly defined distance indefinitely and in any direction setting since the latent space is here seen as the riemannian manifold rd g it is in particular characterised by the riemannian metric g whose choice is very important while several attempts have been made to try to put a riemannian structure over the latent space of vaes 70 71 72 75 76 77 the proposed metrics involved the jacobian of the generator function which is hard to use in practice and is constrained by the generator network architecture as a consequence we instead decide to rely on the idea of riemannian metric learning 78 321 the metric as discussed before the riemannian metric plays a crucial role in the modeling of the latent space in this paper we decide to use a parametric metric inspired from 79 having the following matrix representation g1 z  kz c k2 i 2  id li l exp where n is the number of observations li are lower triangular matrices with positive diagonal coefficients learned from the data and parametrized with neural networks ci are referred to as the centroids and correspond to the mean  xi of the encoded distributions of the latent variables zi zi q zi xi n xi xi t is a temperature scaling the metric close to the centroids and is a regularization factor that also scales the metric tensor far from the latent codes the shape of this metric is very powerful since we have access to a closedform expression of the inverse metric tensor which is usually useful to compute shortest paths through the exponential map moreover this metric is very smooth differentiable everywhere p and allows scaling the riemannian volume element det gz far from the data very easily through the regularization factor a similar metric was proposed in 69 but was used in the input data space x and is not learned from the data to be able to refer to geodesics on the entire learned manifold we need the following proposition proved in appendix a in the supplementary materials proposition 1 the riemannian manifold rd g is geodesically complete 322 the model the metric is learned in the same way as proposed in 73 since we rely on riemannian hamiltonian dynamics 80 81 the main idea is to encode the input data points xi and so get the means xi of the posterior distributions associated with the encoded latent variables zi0 n xi xi these means are then used to update the metric centroids ci in the mean time the input data points xi are fed to another neural network which outputs the matrices li used to update the metric the updated metric is then used to sample zik from the zi0 the same way it is done with normalizing flows 50 but riemannian hamiltonian equations are employed instead the zik are then fed to the decoder network which outputs the parameters of the conditional distribution p xz the reparametrization trick is used to sample zi0 as is common and since the riemannian hamiltonian equations are deterministic backpropagation can be performed to update all the parameters a scheme of the geometryaware vae model framework can be found in fig 1 in the following we will refer to the proposed model either as geometryaware vae or rhvae for short an implementation using pytorch 82 is available in the supplementary materials 323 sampling from the latent space in this paper we propose to amend the standard sampling procedure of classic vaes to better exploit the riemannian structure of the latent space the geometryaware vae is here seen as a tool able to capture the intrinsic latent structure of the data and so we propose to exploit this property directly within the generation procedure this differs greatly from the standard fully probabilistic view where the prior distribution is used to generate new data we believe that such an approach remains far from being optimal when one considers small data sets since depending on its choice the prior may either poorly prospect the latent space or sample in locations without any usable information this is discussed and illustrated in sec 324 and sec 33 we instead propose to sample from the following distribution s z det g1 z pz r s z det g1 zdz 100 circles rings u z log ptarget z kv v v these two energies give together the hamiltonian 84 85 shoes 100 100 75 100 25 00 75 100 affine geodesic 100 75 25 00 affine geodesic 75 100 100 hz v u z kv where s is a compact set so that the integral is well defined fortunately since we use a parametrized metric given by eq 4 and whose inverse has a closed form it is pretty straightforward to evaluate the numerator of eq 5 then classic mcmc sampling methods can be employed to sample from p on rd in this paper we propose to use the hamiltonian monte carlo hmc sampler 83 since the gradient of the logdensity is computable given a target density ptarget we want to sample from the idea behind the hmc sampler is to introduce a random variable v n 0 id independent from z and rely on hamiltonian dynamics analogous to physical systems z may be seen as the position and v as the velocity of a particle whose potential energy u z and kinetic energy kv are given by affine geodesic the evolution in time of such a particle is governed by hamiltons equations as follows such equations can be integrated using a discretization scheme known as the stormerverlet or leapfrog integrator which is run l times vt 2 vt z u zt zt zt vt 2 vt vt 2 z u zt where is the integrator step size the hmc sampler produces a markov chain z n with the aforementioned integrator more precisely given z0n the current state of the chain an initial velocity is sampled v0 n 0 id and then eq 6 are run l times to move from z0n v0 to zln vl the  proposal nzl isthen accepted with probability exphzl vl  min 1 exphzn v0 it was shown that the chain z n is timereversible and converges to its stationary distribution ptarget 51 84 86 in our method ptarget is given by eq 4 and eq 5 fortunately since the hmc sampler allows sampling from densities known up to a normalizing constant thanks to the acceptance ratio the computation of the denominator of ptarget is not needed and the hamiltonian follows hz v u z kv log det g1 z v v and is easy to compute hence the only difficulty left is the computation of the gradient z u z needed in the leapfrog integrator which is actually pretty straightforward using the 1 take for instance z z kzk 2 maxi kci k affine geodesic fig 2 geodesic interpolations under the learned metric in two different latent spaces top latent spaces with the log metric volume element presented in gray scale second row the resulting interpolations under the euclidean metric or the riemannian metric third row the learned manifolds and corresponding decoded samples bottom decoded samples all along the interpolation curves chain rule in this paper a typical choice for and l the samplers parameters is 001 005 and l 10 15 we would also like to mention the recent workp of 77 where the authors used the distribution qz 1 det gz1 to sample from a wasserstein gan 87 nonetheless both the framework and the metric remain quite different 324 discussion on the sampling distribution one may wonder what is the rationale behind the use of the distribution p formerly defined in eq 5 by p design the metric is such that the metric volume element det gz is scaled by the factor far from the encoded data points hence choosing a relatively small imposes that shortest paths travel through the most populated area of the latent space ie next to the latent codes as such the metric volume element can be seen as a way to quantify the amount of information contained at a specific location of the latent space the smaller the volume element the more information we have access to fig 2 illustrates well these aspects on the first row are presented two learned latent spaces along with the log of the metric volume element displayed in gray scale for two different data sets the first one is composed of 180 binary circles and rings of different diameters and thicknesses while the second one is composed of 160 samples extracted from the fashionmnist data set 88 geometryaware vae n 0 id vae vamp prior vanilla vae circles rings samples circles rings samples circles rings samples 100 circles rings samples geometryaware vae ours 100 10 8 6 4 2 100 fig 3 vae sampling comparison top the learned latent space along with the means xi of the latent code distributions colored dots and crosses and 100 latent space samples blue dots using either the prior distribution or the proposed scheme for the geometryaware vaes the log metric volume element is presented in gray scale in the background bottom the 100 corresponding decoded samples in the data space the means xi of the distributions associated with the latent variables are presented with the crosses and dots for each class as expected the metric volume element is smaller close to the latent variables since small s were considered 103 resp 101 a common way to study the learned riemannian manifold consists in finding geodesic curves ie the shortest paths with respect to the learned riemannian metric hence on the second row of fig 2 we compare two types of interpolation in each latent space for each experiment we pick two points in the latent space and perform either a linear or a geodesic interpolation ie using the riemannian metric the bottom row illustrates the decoded samples all along each interpolation curve while the third one displays the decoded samples according to the latent space location of the corresponding codes the first outcome of such an experiment is that as expected geodesic curves travel next to the codes and so do not explore areas of the latent space with no information whereas linear interpolations do therefore decoding along geodesic curves produces far better and more meaningful interpolations in the input data space since in both cases we clearly see the starting sample being progressively distorted until the path reaches the ending point this allows for instance interpolating between two shoes and keep the intrinsic topology of the data all along the path since each decoded sample on the interpolation curve looks like a shoe this is made impossible under the euclidean metric where shortest paths are straight lines and so may travel through areas of least interest for instance the affine interpolation travels through areas with no latent data and so produces decoded samples that are mainly a superposition of samples see the red lines and corresponding decoded samples framed in red or crosses areas with codes belonging to the other class see the blue line and the corresponding blue frames this point is even more supported by the plots in the third row of fig 2 where we clearly see that locations with the highest metric volume element are often less relevant this study demonstrates that most of the information in the latent space is contained next to the codes and so if we want to generate new samples that looklike the input data we need to sample next to them and that is why we elected the distribution of eq 5 generation comparison in this section we propose to compare the new generation procedure with other priorbased methods in the context of low sample size data sets 331 qualitative comparison first we validate the proposed generation method on a handmade synthetic data set composed of 180 binary circles and rings of different diameters and thicknesses see appendix c we then train 1 a vanilla vae 2 a vae with vamp prior 56 3 a geometryaware vae but using the prior to generate and 4 a geometryaware vae with the proposed generation scheme and compare the generated samples each model is trained until the elbo does not improve for 20 epochs and any relevant parameter setting is made available in appendix b in fig 3 we compare the sampling obtained with the vanilla vae left column the vae with vamp prior 2nd column the geometryaware vae using a standard normal distribution as prior 3rd column and the geometryaware vae using the proposed sampling method ie sampling from the inverse of the metric volume element the first row presents the learned latent spaces along with the means of the encoded training data points for each class crosses and dots and 100 samples issued by the generation methods blue dots  for the rhvae models the log metric volume element det g is also displayed in gray scale in the background the bottom row shows the resulting 100 decoded samples in the data space table 1 gantrain the higher the better and gantest the closer to the baseline the better scores a benchmark densenet model is trained with five independent runs on the generated data sg resp the real train set strain and tested on the real test set stest resp sg to compute the gantrain resp gantest score 1000 synthetic samples per class are considered for sg so that it matches the size of stest reduced mnist balanced metric gantrain gantest baseline 906 12 vae n 0 id 834 24 671 49 vamp 841 30 749 43 rhvae n 0 id 820 29 631 41 ours 901 14 881 27 reduced mnist unbalanced gantrain gantest 828 07 747 32 528 106 285 89 614 70 693 18 469 84 862 18 838 40 the first outcome of this experiment is that sampling from the prior distribution leads to a quite poor latent space prospecting this drawback is very well illustrated when a standard gaussian distribution is used to sample from the latent space see 1st and 3rd column of the 1st row the prior distribution having a higher mass close to zero will insist on latent samples close to the origin unfortunately in such a case latent codes close to the origin only belong to a single class rings therefore even though the number of training samples was roughly the same for circles and rings we end up with a model overgenerating samples belonging to a certain class rings and even to a specific type of data within this very class this undesirable effect seems even tenfolded when considering the geometrybased vae model since adding mcmc steps in the training process as explained in fig 1 tends to stretch the latent space it can be nonetheless noted that using a multimodal prior such as the vamp prior mitigates this and allows for a better prospecting however such a model remains hard to fit when trained with small data sets 56 another limitation of priorbased generation methods relies in their inability to assess a given sample quality they may indeed sample in areas of the latent space containing very little information and so conduct to generated samples that are meaningless this appears even more striking when small data sets are considered an interesting observation that was noted among others in 75 is that neural networks tend to interpolate very poorly in unseen locations ie far from the training data points when looking at the decoded latent samples bottom row of fig 3 we eventually end up with the same conclusion actually it appears that the networks interpolate quite linearly between the training data points in our case this may be illustrated for instance by the red dots in the latent spaces in fig 3 whose corresponding decoded sample is framed in red the sample is located between two classes and when decoded it produces an image mainly corresponding to a superposition of samples belonging to different classes this aspect is also supported by the observations made when discussing the relevance of geodesic interpolations on fig 2 of sec 324 therefore these drawbacks may conduct to a very poor representation of the actual data set diversity while presenting quite a few irrelevant samples obviously the notion of irrelevance is here disputable but if the objective is to represent a given set of data we expect the generated samples to be close to the training data while having some specificities to enrich it impressively sampling against the inverse of the metric vol reduced emnist gantrain 845 13 753 14 432 44 736 41 826 13 gantest 545 65 581 77 556 50 760 40 ume element as proposed in sec 323 allows for a far more meaningful sample generation furthermore the new sampling scheme avoids regions with no latent code which thus contain poor information and focuses on areas of interest so that almost every decoded sample is visually satisfying similar effects are observed on reduced emnist 89 reduced mnist 90 and reduced fashionmnist data sets and higher dimensional latent spaces dimension 10 where samples are most of the time degraded when the classic generation is employed while the new one allows the generation of more diverse and sharper samples see appendix c finally the proposed method does not overfit the training data since the samples are not always located on the centroids and the quantitative metrics of the following section also support this point 332 quantitative comparison in order to compare quantitatively the diversity and relevance of the samples generated by a generative model several measures have been proposed 91 92 93 94 since those metrics suffer from some drawbacks 95 96 we decide to use the gantrain gantest measure discussed in 95 as it appears to us well suited to measure the ability of a generative model to perform data augmentation these two metrics consist in comparing the accuracy of a benchmark classifier trained on a set of generated data sg and tested on a set of real images stest gantrain or trained on the original train set strain real images used to train the generative model and tested on sg gantest those accuracies are then compared to the baseline accuracy given by the same classifier trained on strain and tested on stest these two metrics are quite interesting for our application since the first one gantrain measures the quality and diversity of the generated samples the higher the better while the second one gantest accounts for the generative models tendency to overfit a score significantly higher than the baseline accuracy means overfitting ideally the closer to the baseline the gantest score is the better to stick to our low sample size setting we compute these scores on three data sets created by downsampling wellknown databases the first data set is created by extracting 500 samples from 10 classes of mnist ensuring balanced classes for the second one 500 samples of the mnist database are again considered but a random split is applied such that some classes are underrepresented the last one consists in selecting 500 samples from 10 classes of the emnist data set having both lowercase and uppercase data augmentation e valuation and r o bustness in this section we show the relevance of the proposed improvements to perform data augmentation in a hdlss setting through a series of experiments data vae model synthetic cnn model cnn model data training trained validation test fig 4 overview of the data augmentation procedure the input data set is divided into a train set the baseline a validation set and a test set the train set is augmented using the vae framework and generated data are then added to the baseline to train a benchmark classifier robustness of the method across these data sets is tested with a standard benchmark classifier then the methods reliability across other common classifiers is stressed finally its scalability to larger data sets is discussed 421 materials the first data set is created by selecting 500 samples from the ten classes of the mnist data set ensuring balanced classes we will refer to it as reduced mnist the second one consists in selecting again 500 samples from the mnist database but applying a random split such that some classes are overrepresented we call it the reduced unbalanced mnist data set then we create another one using the fashionmnist data set and three classes we find hard to distinguish ie tshirt dress and shirt the data set is composed of 300 samples ensuring balanced classes and is referred to as reduced fashion finally we also select 500 samples from ten classes of the emnist these classes are selected such that they are composed of both lowercase and uppercase characters so that we end up with a small database with strong variability within classes the balance matches the one in the initial data set by merge in summary we built four data sets having different class numbers class splits and sample sizes these data sets are then divided such that 80 is allocated for training referred to as the baseline and 20 for validation since the original data sets are huge we decide to use the test set provided in the original databases eg 1000 samples per class for mnist and fashion such that it provides statistically meaningful results while allowing for a reliable assessment of the models generalization power on unseen data setting the setting we employ for data augmentation consists in selecting a data set and splitting it into a train set the baseline a validation set and a test set the baseline is then augmented using the proposed vae framework and generation procedure the generated samples are finally added to the original train set ie the baseline and fed to a classifier the whole data augmentation procedure is illustrated in fig 4 for a convolutional neural network cnn model as classifier train input letters these three data sets are then divided into a baseline train set strain 80 and a validation set sval 20 used for the classifier training since the initial databases are huge we use the original test set for stest so that it provides statistically meaningful results the same generative models as in sec 331 are then trained on each class of strain to generate 1000 samples per class and sg is created for each vae by gathering all generated samples a benchmark classifier chosen as a densenet 97 is then 1 trained on strain and tested on stest baseline 2 trained on sg and tested on stest gantrain and 3 trained on strain and tested on sg gantest until the loss does not improve for 50 epochs on sval for each experiment the model is trained five times and we report the mean score and the associated standard deviation in table 1 as expected the proposed method allows producing samples that are far more meaningful and relevant in particular to perform da this is first illustrated by the gantrain scores that are either very close to the accuracy obtained with the baseline or higher see mnist unbalanced in table 1 the fact that we are able to enhance the classifiers accuracy even when trained only with synthetic data is very encouraging firstly it proves that the created samples are close to the real ones and so that we were able to capture the true distribution of the data secondly it shows that we do not overfit the initial training data since we are able to add some relevant information through the synthetic samples this last observation is also supported by the gantest scores for the proposed method which are quite close to the accuracies achieved on the baseline in case of overfitting the gantest score is expected to be significantly higher than the baseline since the classifier is tested on the generated samples while trained on the real data that were also used to train the generative model having a score close to the baseline illustrates that the generative model is able to capture the distribution of the data and does not only memorize it 95 toy data sets the proposed vae framework is here used to perform da on several downsampled wellknown databases such that only tens of real training samples per class are considered so that we stick to the low sample size setting first the 422 robustness across data sets the first experiment we conduct consists in assessing the methods robustness across the four aforementioned data sets for this study we propose to consider a densenet 97 model2 as benchmark classifier on the one hand the training data the baseline is augmented by a factor 5 10 and 15 using classic data augmentation methods random noise random crop rotation etc so that the proposed method can be compared with classic and simple augmentation techniques on the other hand the protocol described in fig 4 is employed with a vanilla vae and a geometryaware vae the generative models are trained individually on each class of the baseline until the elbo does not improve for 20 epochs the vaes are then used to produce 200 500 2 we used the pytorch implementation provided in 98 table 2 data augmentation with a densenet model as benchmark mean accuracy and standard deviation across five independent runs are reported the first three rows aug correspond to basic transformations noise crop etc in gray are the cells where the accuracy is higher on synthetic data than on the baseline ie the raw data the test set is the one proposed in the entire original data set eg 1000 samples per class for mnist so that it provides statistically meaningful results and allows for a good assessment of the models generalization power mnist mnist unbal emnist unbal 899 06 815 07 826 14 baseline synthetic aug x5 928 04 865 09 856 13 aug x10 882 22 820 24 857 03 aug x15 928 07 858 34 866 08 vae200 885 09 840 20 817 30 vae500 904 13 873 12 834 16 vae1k 912 10 860 25 843 16 vae2k 922 16 880 22 860 02 rhvae200 899 05 823 09 830 13 rhvae500 909 11 840 32 844 12 rhvae1k 917 08 847 18 847 24 rhvae2k 927 14 868 10 849 21 ours200 910 10 841 20 851 11 923 11 877 09 851 11 ours500 ours1k 932 08 897 08 870 10 ours2k 943 08 891 19 876 08 synthetic only vae200 699 15 646 18 657 26 vae500 723 42 694 41 673 24 vae1k 834 24 747 32 753 14 vae2k 865 22 796 38 788 30 rhvae200 760 18 615 29 598 26 rhvae500 800 22 668 33 669 40 rhvae1k 820 29 693 18 736 41 rhvae2k 852 39 773 32 686 23 ours200 872 11 795 16 770 16 ours500 891 13 804 21 802 20 ours1k 901 14 862 18 826 13 ours2k 926 11 875 13 860 10  using a standard normal prior to generate baseline fashion 760 15 775 20 792 06 800 05 786 04 787 03 776 21 793 11 776 13 780 13 793 16 790 14 770 08 785 09 802 08 781 18 739 30 714 85 714 61 767 16 728 36 743 26 760 41 743 31 770 08 785 08 793 06 783 09 1000 and 2000 new synthetic samples per class using either the classic generation scheme ie sampling with the prior n 0 id or the proposed generation procedure referred to as ours finally the benchmark densenet model is trained with five independent runs on either 1 the baseline 2 the augmented data using classic augmentation methods 3 the augmented data using the vaes or 4 only the synthetic data created by the generative models for each experiment the mean accuracy and the associated standard deviation across those five runs is reported in table 2 an early stopping strategy is employed and cnn training is stopped if the loss does not improve on the validation set for 50 epochs the first outcome of such a study is that as expected generating synthetic samples with the proposed method seems to enhance their relevance when compared with other models in particular for data augmentation tasks this is for instance illustrated by the second section of table 2 where synthetic samples are added to the baseline while adding samples generated either by the vae or rhvae and using the prior distribution seems to improve the classifier accuracy when compared with the baseline the gain remains limited since it struggles to exceed the gain reached with classic augmentation methods for instance neither the vae nor the rhvae allows the classifier to achieve a better score on reduced mnist or reduced emnist data sets on the contrary the proposed generation method is able to pro duce very useful samples for the cnn model adding the generated data to the baseline indeed allows for a great gain in the model accuracy which exceeds the one achieved with any other method while keeping a relatively low standard deviation on each data set highlighted in bold secondly the relevance of the samples produced by the proposed scheme is even more supported by the last section of table 2 where the classifier is trained only using the synthetic samples generated by the vaes first even with a quite small number of generated samples 200 per class the classifier is almost able to reach the accuracy achieved on the baseline for instance when the cnn is trained on reduced mnist with 200 synthetic samples per class generated with our method it is able to achieve an accuracy of 872 vs 899 with the baseline in comparison both the vae and rhvae fail to produce meaningful samples when the prior is used since a loss of 15 to 20 points in accuracy is observed combined with a potentially very strong loss in confidence making those samples unreliable the fact that the classifier almost performs as well on the synthetic data as on the baseline is good news since it shows that the proposed framework is able to produce samples accounting for the original data set diversity even with a small number of generated samples even more interesting as the number of synthetic data increases the classifier is able to perform much better on the synthetic data than on the baseline since a gain of 3 to 6 points in accuracy is observed again this strengthens the observations made in sec 331 and sec 332 where it was noted that the proposed method is able to enrich the initial data set with relevant and realistic samples finally it can be seen in this experiment why geometric data augmentation methods are still questionable and remain data set dependent for example augmenting the baseline by a factor 10 where we add flips and rotations on the original data seems to have no significant effect on the reduced mnist data sets while it still improves results on reduced emnist and fashionmnist we see here how the expert knowledge comes into play to assess the relevance of the transformations applied to the data fortunately the method we propose does not require such knowledge and appears to be quite robust to data set changes 423 robustness across classifiers in addition to assessing the robustness of the method to data sets changes we also propose to evaluate its reliability across classifiers to do so we consider very different common supervised classifiers a multi layer perceptron mlp 3 a random forest 99 the k nn algorithm and a svm 100 each of the aforementioned classifiers is again trained either on 1 the original training data set the baseline 2 the augmented data using the proposed method and 3 only the synthetic data generated by our method with five independent runs and using the same data sets as presented in sec 421 finally we report the mean accuracy and standard deviation across these runs for each classifier and data set the results for the balanced resp unbalanced reduced mnist data set can be found in fig 5a resp fig 5b metrics obtained on the two other data sets are available in appendix d but reflect the same tendency 100 100 baseline augmented 200 augmented 1000 synthetic 500 augmented 2000 synthetic 1000 augmented 500 synthetic 200 synthetic 2000 accuracy accuracy baseline augmented 200 augmented 500 augmented 1000 augmented 2000 synthetic 200 synthetic 500 synthetic 1000 synthetic 2000 mlp svm knn random forest mlp svm a reduced mnist balanced knn random forest b reduced mnist unbalanced fig 5 evolution of the accuracy of four benchmark classifiers on reduced balanced mnist left and reduced unbalanced mnist data sets right stochastic classifiers are trained with five independent runs and we report the mean accuracy and standard deviation on the test set 424 a note on the method scalability finally we also quickly discuss the method scalability to larger data sets to do so we consider the mnist data set and a benchmark classifier taken as a densenet which performs well on such data then we downsample the original mnist database in order to progressively decrease the number of samples per class we start by creating a data set having 1000 samples per class to finally reach 20 samples per class for each created data set we allocate 80 for training the baseline and reserve 20 for the validation set a geometryaware vae is then trained on each class of the baseline until the elbo does not improve for 50 epochs and is used to generate synthetic samples 125 the baseline the benchmark cnn is trained with five independent runs on either 1 the baseline 2 the augmented data or 3 only the synthetic data generated with our model the evolution of the mean accuracy on the original test set 1000 samples per class according to the number of samples per class is presented in fig 6 the first outcome of this experiment is that the fewer samples in the training set the more useful the method appears using the proposed augmentation framework indeed allows for a gain of more than 90 points in the cnn accuracy when only 20 samples per class are considered in other words as the number of samples increases the marginal gain seems to decrease nevertheless this reduction must be put into perspective since it is commonly acknowledged that as the results on the baseline increase and thus get closer to the perfect score it is even more challenging to improve the score with the augmented data in this experiment we are nonetheless still able to improve the model accuracy even when it already achieves a very high score for instance with 500 samples per class the augmentation method still allows increasing 100 958 949 931 928 975 967 988 984 977 989 982 985 956 931 accuracy as illustrated in fig 5 the method appears quite robust to classifier changes as well since it allows improving the models accuracy significantly for almost all classifiers the accuracy achieved on the baseline is presented by the leftmost bar in fig 5 for each classifier the methods strength is even more striking when unbalanced data sets are considered since the method is able to produce meaningful samples even with a very small number of training data and so it is able to oversample the minority classes in a reliable way moreover as observed in the previous sections synthetic samples are again helpful to enhance classifiers generalization power since they perform better when trained only on synthetic data than on the baseline in almost all cases 875 876 892 baseline augmented x125 synthetic only 784 100 200 number of samples per class 500 1000 fig 6 evolution of the accuracy of a benchmark cnn classifier according to the number of samples per class in the train set ie the baseline on mnist the vae is trained on each class of the baseline to augment its size by a factor 125 a cnn is then trained 5 times on 1 the baseline blue 2 the augmented baseline orange and 3 only the synthetic data green the curves show the mean accuracy and associated standard deviation on the original test set the model accuracy from 977 to 988 finally for data sets with fewer than 500 samples per class the classifier is again able to outperform the baseline even when trained only with the synthetic data this shows again the strong generalization power of the proposed method which allows creating new relevant data for the classifier validation on m edical i maging with this last series of experiments we assess the validity of our data augmentation framework on a binary classification task consisting in differentiating alzheimers disease ad patients from cognitively normal cn subjects based on t1weighted t1w mr images of human brains such a task is performed using a cnn trained as before either on 1 real images 2 synthetic samples or 3 both in this section label definition preprocessing quality check data split and cnn training and evaluation is done using clinica3 and clinicadl4 two opensource software packages for neuroimaging processing data augmentation literature for ad vs cn task even though many studies use cnns to differentiate ad from cn subjects with anatomical mri 101 we did not 3 httpsgithubcomaramislabclinica 4 httpsgithubcomaramislabaddl fig 7 example of two true patients compared to two generated by our method can you find the intruders answers in appendix f find any metaanalysis on the use of data augmentation for this task some results involving da can nonetheless be cited and are presented in table 4 however assessing the real impact of data augmentation on the performance of the model remains challenging for instance this is illustrated by the works of 102 and 103 which are two examples in which da was used and led to two significantly different results although a similar framework was used in both studies interestingly as shown in table 4 studies using da for this task only relied on simple affine and pixellevel transformations which may reveal data dependent note that complex da was actually performed for ad vs cn classification tasks on pet images but pet is less frequent than mri in neuroimaging data sets 104 as noted in the previous sections our method would apply pretty straightforwardly to this modality as well for mri other techniques such as transfer learning 105 and weak supervision 106 were preferred to handle the small amount of samples in data sets and may be coupled with da to further improve the network performance table 3 summary of participant demographics minimental state examination mmse and global clinical dementia rating cdr scores at baseline materials data used in this section are obtained from the alzheimers disease neuroimaging initiative adni database adniloniuscedu and the australian imaging biomarkers and lifestyle aibl study aiblcsiroau the adni was launched in 2003 as a publicprivate partnership led by principal investigator michael w weiner md the primary goal of adni has been to test whether serial mri pet other biological markers and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment and early ad for uptodate information see wwwadniinfoorg the adni data set is composed of four cohorts adni1 adnigo adni2 and adni3 the data collection of adni3 has not ended yet hence our data set contains all images and metadata that were already available on may 6 2019 similarly to adni the aibl data set seeks to discover which biomarkers cognitive characteristics and health and lifestyle factors determine the development of ad this cohort is also longitudinal and the diagnosis is given according to a series of clinical tests 110 data collection for this cohort is over two diagnoses are considered for the classification task data set label obs age sex mf mmse cdr 403 733 60 185218 291 11 0 403 adni 05 169 1 192 362 749 79 202160 231 21 2 1 0 406 05 22 429 730 62 183246 288 12 1 1 aibl 05 31 1 36 744 80 3343 206 55 2 7 3 2 cn baseline session of participants who were diagnosed as cognitively normal at baseline and stayed stable during the followup ad baseline session of participants who were diagnosed as demented at baseline and stayed stable during the followup table 3 summarizes the demographics the minimental state examination mmse and global clinical dementia rating cdr scores at baseline of the participants included in our data set the mmse and the cdr scores are classical table 4 accuracy obtained by studies performing ad vs cn classification with cnns applied on t1w mri and using data augmentation accuracy study methods participants images baseline augmented valliani and soni 2017 107 rotation flip shift 417 417 788 813 backstrom et al 2018 108 flip 340 1198 901 cheng and liu 2017 109 shift sampling rotation 193 193 855 aderghal et al 2017 102 shift blur flip 720 720 828 837 aderghal et al 2018 103 shift blur 720 720 900 clinical scores used to assess dementia the mmse score has a maximal value of 30 for cognitively normal persons and decreases if symptoms are detected the cdr score has a minimal value of 0 for cognitively normal persons and increases if symptoms are detected 551 hyperparameter choices as for the vae the architecture of the cnn depends on the size of the input then there is one architecture per input size downsampled images and highresolution images see fig 8 moreover two different paradigms are used to choose the architecture first we reuse the same architecture as in 101 this architecture is obtained by optimizing manually the networks on the adni data set for the same task ad vs cn a slight adaption is done for the downsampled images which consists in resizing the number of nodes in the fullyconnected layers to keep the same ratio between the input and output feature maps in all layers we denote these architectures as baseline secondly we launch a random search 117 that allows exploring different hyperperameter values the hyperparameters explored for the architecture are the number of convolutional blocks of filters in the first layer and of convolutional layers in a block the number of fullyconnected layers and the dropout rate other hyperparameters such as the learning rate and the weight decay are also part of the search 100 different random architectures are trained on the 5fold crossvalidation done on trainfull for each input we choose the architecture that obtained the best mean balanced accuracy across the validation sets of the crossvalidation we denote these architectures as optimized preprocessing of t1weighted mri the steps performed in this section correspond to the procedure followed in 101 and are listed below raw data are converted to the bids standard 111 bias field correction is applied using n4itk 112 t1w images are linearly registered to the mni standard space 113 114 with ants 115 and cropped this produced images of size 169208179 with 1 mm3 isotropic voxels an automatic quality check is performed using an opensource pretrained network 116 all images passed the quality check nifti files are converted to tensor format optional images are downsampled using a trilinear interpolation leading to an image size of 8410489 intensity rescaling between the minimum and maximum values of each image is performed these steps lead to 1 downsampled images 8410489 or 2 highresolution images 169208179 evaluation procedure the adni data set is split into three sets training validation and test first the test set is created using 100 randomly chosen participants for each diagnostic label ie 100 cn 100 ad the rest of the data set is split between the training 80 and the validation 20 sets we ensure that age sex and site distributions between the three sets are not significantly different a smaller training set denoted as train50 is extracted from the obtained training set denoted as trainfull this set comprises only 50 images per diagnostic label instead of 243 cn and 210 ad for trainfull we ensure that age and sex distributions between train50 and trainfull are not significantly different this is not done for the site distribution as there are more than 50 sites in the adni data set so they could not all be represented in this smaller training set aibl data are never used for training or hyperparameter tuning and are only used as an independent test set cnn classifiers a cnn takes as input an image and outputs a vector of size c corresponding to the number of labels existing in the data set then the prediction of the cnn for a given image corresponds to the class with the highest probability in the output vector 552 network training the weights of the convolutional and fullyconnected layers are initialized as described in 118 which corresponds to the default initialization method in pytorch networks are trained for 100 epochs for baseline and 50 epochs for optimized the training and validation losses are computed with the crossentropy loss for each experiment the final model is the one that obtained the highest validation balanced accuracy during training the balanced accuracy of the model is evaluated at the end of each epoch experimental protocol as done in the previous sections we perform three types of experiments and train the model on 1 only the real images 2 only on synthetic data and 3 on synthetic and real images due to the current implementation augmentation on fig 8 diagrams of the network architectures used for classification the first baseline architecture a1 is the one used in 101 the second one a2 is a very similar one adapted to process smaller inputs the optimized architectures b1 and b2 are obtained independently with two different random searches for convolution layers we specify the number of channels the kernel size and for the fullyconnected layers we specify the number of input nodes the number of output nodes each fullyconnected layer is followed by a leakyrelu activation except for the last one for the dropout layer the dropout rate is specified highresolution images is not possible and so these images are only used to assess the baseline performance of the cnn with the maximum information available each series of experiments is done once for each training set train50 and trainfull the cnn and the vae share the same training set and the vae does not use the validation set during its training for each training set two vaes are trained one on the ad label only and the other on the cn label only examples of real and generated ad images are shown in fig 7 for each experiment 20 runs of the cnn training are launched the use of a smaller training set train50 allows mimicking the behavior of the framework on smaller data sets which are frequent in the medical domain results results presented in table 5 resp table 6 are obtained with baseline resp optimized hyperparameters and using either the trainfull or train50 data set scores on synthetic images only are given in appendix g experiments are done on downsampled images unless highresolution is specified even though the vae augmentation is performed on downsampled images the classification performance is at least as good as that of the best baseline performance or can greatly exceed it on train50 with baseline hyperparameters the increase of balanced accuracy is of 62 points on adni and 89 points on aibl on trainfull with baseline hyperparameters the increase of balanced accuracy is of 57 points on adni and 47 on aibl on train50 with optimized hyperparameters the increase of balanced accuracy is of 25 points on adni and 63 points on aibl on trainfull with optimized hyperparameters the increase of balanced accuracy is of 15 point on adni and 01 point on aibl then the performance increase thanks to da is higher when using the baseline hyperparameters than the optimized ones a possible explanation could be that the optimized network is already close to the maximum performance that can be reached with this setup and cannot be much improved with da moreover the hyperparameters of the vae have not been subject to a similar search so this places it at a disadvantage for both hyperparameters the performance gain is higher on train50 than on trainfull which supports the results obtained in the previous section see fig 6 the baseline balanced accuracy with the baseline hyperparameters on trainfull 806 on adni and 804 on aibl are similar to the results of 101 with da we improve our balanced accuracy to 863 on adni and 851 on aibl this performance is similar to their result using autoencoder pretraining which can be very long to compute and longitudinal data 1830 cn and 1106 ad images instead of baseline data 243 cn and 210 ad images as we did in each table the first two rows display the baseline performance obtained on real images only as expected training on highresolution images leads to a better performance than training on downsampled images this is not the case for the optimized network on train50 which obtained a balanced accuracy of 721 on adni and 712 on aibl with highresolution images versus 755 on adni and 756 on aibl with downsampled images this is explained by the fact that the hyperparameters choice is made on trainfull and so there is no guarantee that it could lead to similar results with fewer data samples d iscussion contrary to techniques that are specific to a field of application our method produced relevant data for diverse data sets including 2d natural images mnist emnist and fashion or 3d medical images adni and aibl moreover we note that the networks learning on medical table 5 mean test performance of each series of 20 runs trained with the baseline hyperparameters adni training set train50 trainfull data set sensitivity specificity real real highresolution 500 synthetic real 1000 synthetic real 2000 synthetic real 3000 synthetic real 5000 synthetic real 10000 synthetic real real real highresolution 500 synthetic real 1000 synthetic real 2000 synthetic real 3000 synthetic real 5000 synthetic real 10000 synthetic real 703 122 785 94 719 53 698 66 722 44 718 49 747 53 747 70 791 62 845 38 825 34 846 44 854 40 847 36 846 42 842 28 624 115 574 88 670 45 712 37 703 43 734 55 735 48 734 61 763 42 767 40 819 54 843 51 864 59 868 45 869 36 885 29 aibl balanced accuracy 663 24 679 23 694 16 705 21 712 16 726 16 741 22 740 27 777 25 806 11 822 24 844 18 859 16 858 17 857 21 863 18 sensitivity specificity 607 137 572 112 559 68 591 90 666 71 661 93 717 100 691 99 706 67 716 64 760 63 770 70 772 69 772 48 769 52 791 47 738 72 758 70 811 31 821 37 790 41 811 50 805 44 807 51 863 36 892 27 897 33 904 34 904 38 917 29 914 30 910 26 balanced accuracy 672 41 665 30 685 25 706 31 728 22 736 30 761 36 749 32 784 24 804 26 829 25 837 23 838 22 844 18 842 22 851 19 table 6 mean test performance of each series of 20 runs trained with the optimized hyperparameters adni training set train50 trainfull aibl image type sensitivity specificity real real highresolution 500 synthetic real 1000 synthetic real 2000 synthetic real 3000 synthetic real 5000 synthetic real 10000 synthetic real real real highresolution 500 synthetic real 1000 synthetic real 2000 synthetic real 3000 synthetic real 5000 synthetic real 10000 synthetic real 754 50 736 62 732 42 761 53 752 38 765 38 771 37 778 46 825 42 826 45 823 23 825 33 831 42 813 37 819 35 822 34 755 53 706 59 780 33 795 29 786 44 792 42 767 41 782 49 885 66 889 63 898 27 905 41 913 32 904 34 909 25 912 36 images of adni gave similar balanced accuracies on the adni test subset and aibl this shows that our synthetic data learned on adni benefit in the same way to aibl and that it did not overfit the characteristics of adni in addition to the robustness across data sets the usability of synthetic data by diverse classifiers was assessed for toy data sets these classifiers were a mlp a random forest knn algorithm and a svm on medical image data sets two different cnn were studied a baseline one that has been only slightly optimized in a previous study and an optimized one found with a more extensive search random search all these classifiers performed best on augmented data than real data only however we note that the data augmentation was more beneficial to the baseline network than to the optimized one but both networks obtained a similar performance with data augmentation on the largest training set this means that data augmentation could avoid spending time andor resources optimizing a classifier balanced accuracy 755 27 721 31 756 25 778 23 769 24 778 19 769 25 780 21 855 24 857 25 860 18 865 19 872 17 858 26 864 13 867 18 sensitivity specificity 686 85 578 123 692 94 793 58 778 88 809 79 807 61 817 49 751 84 789 54 749 50 764 56 760 47 749 73 741 49 764 42 826 42 846 42 827 41 825 42 822 45 814 42 812 37 819 46 887 90 899 40 914 26 910 34 920 24 923 26 929 19 921 21 balanced accuracy 756 41 712 51 760 42 809 32 800 36 812 37 809 27 819 22 819 32 844 17 832 24 837 20 840 20 836 32 835 22 843 18 the ability of the model to generate relevant data and enrich the original training data was also supported by the fact that almost all classifiers could achieve a better classification performance when trained only on synthetic data than on the real train set our generation framework appears also very well suited to perform data augmentation in a hdlss setting the binary classification of ad and cn subjects using t1w mri in all cases the classification performance was at least as good as the maximum performance obtained with real data and could even be much better for instance the method allowed the balanced accuracy of the baseline cnn to jump from 663 to 743 when trained with only 50 images per class and from 777 to 863 when trained with 243 cn and 210 ad while still improving greatly sensitivity and specificity metrics we witnessed a greater performance improvement than the other studies using a cnn on t1w mri to differentiate ad and cn subjects 102 103 107 108 109 indeed these studies used simple transforms affine and pixelwise that may not bring enough variability to improve the cnn performance though many complex methods now exist to perform data augmentation they are still not widely adopted in the field of medical imaging we suspect that this is mainly due to the lack of reproducibility of such frameworks hence we provide the source code as well as scripts to easily reproduce the experiments of this paper from the adni and aibl data set download to the final evaluation of the cnn performance nonetheless the performance of our classification on synthetic data could be improved in many ways first we chose in this study not to spend much time optimizing the hyperparameters of the vae and hence in sec 5 we chose to work with downsampled images to deal with memory issues more easily we could look for another architecture to train the vae directly on highresolution images leading to a better performance as witnessed in experiments on real images only moreover we could couple the advantages of other techniques such as autoencoder pretraining or weak supervision to our data augmentation framework however the advantages may not stack as observed when using data augmentation on optimized hyperparameters finally we chose to train our networks with only one image per participant but our framework could also benefit from the use of the whole followup of all patients to further improve performance however a long followup is rather an exception in the context of medical imaging this is why we assessed the relevance of our data augmentation framework in the context of small data sets which is a main issue in this field nonetheless a training set of 50 images per class can still be seen as large in the case of rare diseases and so it may be interesting to evaluate the reliability of our method on even smaller training sets 20 or 10 images per class c onclusion in this paper we proposed a new vaebased data augmentation framework whose performance and robustness were validated on classification tasks on toy and reallife data sets this method relies on the combination of a proper latent space modeling of the vae seen as a riemannian manifold and a new generation procedure exploiting such geometrical aspects in particular the generation method does not use the prior as is standard since we showed that depending on its choice and the data set considered it may lead to a very poor latent space prospecting and a degraded sampling while the proposed method does not suffer from such drawbacks the proposed amendments were motivated discussed and compared to other vae models and demonstrated promising results the model indeed appeared to be able to generate new data faithfully and demonstrated a strong generalization power which makes it very well suited to perform data augmentation even in the challenging context of hdlss data for each augmentation experiment it was able to enrich the initial data set so that a classifier performs better on augmented data than only on the real ones future work would consist in building a framework able to handle longitudinal data and so able to generate not only one observation but a whole patient trajectory acknowledgment the research leading to these results has received funding from the french government under management of agence nationale de la recherche as part of the investissements davenir program reference anr19p3ia0001 prairie 3ia institute and reference anr10iaihu06 agence nationale de la recherche10ia institut hospitalouniversitaire6 this work was granted access to the hpc resources of idris under the allocation 101637 made by genci grand quipement national de calcul intensif data collection and sharing for this project was funded by the alzheimers disease neuroimaging initiative adni national institutes of health grant u01 ag024904 and dod adni department of defense award number w81xwh1220012 adni is funded by the national institute on aging the national institute of biomedical imaging and bioengineering and through generous contributions from the following abbvie alzheimers association alzheimers drug discovery foundation araclon biotech bioclinica inc biogen bristolmyers squibb company cerespir inc cogstate eisai inc elan pharmaceuticals inc eli lilly and company euroimmun f hoffmannla roche ltd and its affiliated company genentech inc fujirebio ge healthcare ixico ltd janssen alzheimer immunotherapy research development llc johnson  johnson pharmaceutical research development llc lumosity lundbeck merck co inc meso scale diagnostics llc neurorx research neurotrack technologies novartis pharmaceuticals corporation pfizer inc piramal imaging servier takeda pharmaceutical company and transition therapeutics the canadian institutes of health research is providing funds to support adni clinical sites in canada private sector contributions are facilitated by the foundation for the national institutes of health wwwfnihorg the grantee organization is the northern california institute for research and education and the study is coordinated by the alzheimers therapeutic research institute at the university of southern california adni data are disseminated by the laboratory for neuro imaging at the university of southern california r eferences k s button j p ioannidis c mokrysz b a nosek j flint e s robinson and m r munaf power failure why small sample size undermines the reliability of neuroscience nature reviews neuroscience vol 14 no 5 pp 365376 2013 b o turner e j paul m b miller and a k barbey small sample sizes reduce the replicability of taskbased fmri studies communications biology vol 1 no 1 pp 110 2018 i goodfellow y bengio a courville and y bengio deep learning mit press cambridge 2016 vol 1 issue 2 c shorten and t m khoshgoftaar a survey on image data augmentation for deep learning journal of big data vol 6 no 1 p 60 2019 m a tanner and w h wong the calculation of posterior distributions by data augmentation journal of the american statistical association vol 82 no 398 pp 528540 1987 n v chawla k w bowyer l o hall and w p kegelmeyer smote synthetic minority oversampling technique journal of artificial intelligence research vol 16 pp 321357 2002 h han wy wang and bh mao borderlinesmote a new oversampling method in imbalanced data sets learning in advances in intelligent computing ds huang xp zhang and gb huang eds springer berlin heidelberg 2005 vol 3644 pp 878887 series title lncs h m nguyen e w cooper and k kamei borderline oversampling for imbalanced data classification international journal of knowledge engineering and soft data paradigms vol 3 no 1 pp 421 2011 haibo he yang bai e a garcia and shutao li adasyn adaptive synthetic sampling approach for imbalanced learning in 2008 ieee international joint conference on neural networks ieee world congress on computational intelligence ieee 2008 pp 13221328 s barua m m islam x yao and k murase mwmote majority weighted minority oversampling technique for imbalanced data set learning ieee transactions on knowledge and data engineering vol 26 no 2 pp 405425 2012 r blagus and l lusa smote for highdimensional classimbalanced data bmc bioinformatics vol 14 no 1 p 106 2013 a fernndez s garcia f herrera and n v chawla smote for learning from imbalanced data progress and challenges marking the 15year anniversary journal of artificial intelligence research vol 61 pp 863905 2018 i goodfellow j pougetabadie m mirza b xu d wardefarley s ozair a courville and y bengio generative adversarial nets in advances in neural information processing systems 2014 pp 26722680 d p kingma and m welling autoencoding variational bayes arxiv13126114 cs stat 2014 d j rezende s mohamed and d wierstra stochastic backpropagation and approximate inference in deep generative models in international conference on machine learning pmlr 2014 pp 12781286 x zhu y liu j li t wan and z qin emotion classification with data augmentation using generative adversarial networks in pacificasia conference on knowledge discovery and data mining springer 2018 pp 349360 g mariani f scheidegger r istrate c bekas and c malossi bagan data augmentation with balancing gan arxiv180309655 2018 a antoniou a storkey and h edwards data augmentation generative adversarial networks arxiv171104340 cs stat 20180321 s k lim y loo nt tran nm cheung g roig and y elovici doping generative data augmentation for unsupervised anomaly detection with gan in 2018 ieee international conference on data mining icdm ieee 2018 pp 11221127 y zhu m aoun m krijn j vanschoren and h t campus data augmentation using conditional generative adversarial networks for leaf counting in arabidopsis plants in bmvc 2018 p 324 x yi e walia and p babyn generative adversarial network in medical imaging a review medical image analysis vol 58 p 101552 2019 hc shin n a tenenholtz j k rogers c g schwarz m l senjem j l gunter k p andriole and m michalski medical image synthesis for data augmentation and anonymization using generative adversarial networks in international workshop on simulation and synthesis in medical imaging ser lncs springer 2018 pp 111 f calimeri a marzullo c stamile and g terracina biomedical data augmentation using generative adversarial neural networks in international conference on artificial neural networks springer 2017 pp 626634 m fridadar i diamant e klang m amitai j goldberger and h greenspan ganbased synthetic medical image augmentation for increased cnn performance in liver lesion classification neurocomputing vol 321 pp 321331 2018 v sandfort k yan p j pickhardt and r m summers data augmentation using generative adversarial networks cyclegan to improve generalizability in ct segmentation tasks scientific reports vol 9 no 1 p 16884 2019 a madani m moradi a karargyris and t syedamahmood chest xray generation and data augmentation for cardiovascular abnormality classification in medical imaging 2018 image processing vol 10574 international society for optics and photonics 2018 p 105741m h salehinejad s valaee t dowdell e colak and j barfett generalization of deep neural networks for chest pathology classification in xrays using generative adversarial networks in 2018 ieee international conference on acoustics speech and signal processing icassp ieee 2018 pp 990994 a waheed m goyal d gupta a khanna f alturjman and p r pinheiro covidgan data augmentation using auxiliary classifier gan for improved covid19 detection ieee access vol 8 pp 91 91691 923 2020 l bi j kim a kumar d feng and m fulham synthesis of positron emission tomography pet images via multichannel generative adversarial networks gans in molecular imaging reconstruction and analysis of moving body organs and stroke imaging and treatment ser lncs springer 2017 pp 4351 y liu y zhou x liu f dong c wang and z wang wasserstein ganbased smallsample augmentation for newgeneration artificial intelligence a case study of cancerstaging data in biology engineering vol 5 no 1 pp 156163 2019 c baur s albarqouni and n navab generating highly realistic images of skin lesions with gans in or 20 contextaware operating theaters computer assisted robotic endoscopy clinical imagebased procedures and skin image analysis springer 2018 pp 260267 d korkinof t rijken m oneill j yearsley h harvey and b glocker highresolution mammogram synthesis using progressive generative adversarial networks arxiv preprint arxiv180703401 2018 e wu k wu d cox and w lotter conditional infilling gans for data augmentation in mammogram classification in image analysis for moving organ breast and thoracic images springer 2018 pp 98106 wn hsu y zhang and j glass unsupervised domain adaptation for robust speech recognition via variational autoencoderbased data augmentation in 2017 ieee automatic speech recognition and understanding workshop asru ieee 2017 pp 1623 h nishizaki data augmentation and feature extraction using variational autoencoder for acoustic modeling in 2017 asiapacific signal and information processing association annual summit and conference apsipa asc ieee 2017 pp 12221227 z wu s wang y qian and k yu data augmentation using variational autoencoder for embedding based speaker verification in interspeech 2019 isca 2019 pp 11631167 p zhuang a g schwing and o koyejo fmri data augmentation via synthesis in 2019 ieee 16th international symposium on biomedical imaging isbi 2019 ieee 2019 pp 17831787 x liu y zou l kong z diao j yan j wang s li p jia and j you data augmentation via latent space interpolation for image classification in 2018 24th international conference on pattern recognition icpr ieee 2018 pp 728733 n painchaud y skandarani t judge o bernard a lalande and pm jodoin cardiac mri segmentation with strong anatomical guarantees in international conference on medical image computing and computerassisted intervention springer 2019 pp 632640 r selvan e b dam n s detlefsen s rischel k sheng m nielsen and a pai lung segmentation from chest xrays using variational data imputation arxiv200510052 cs eess stat 2020 a myronenko 3d mri brain tumor segmentation using autoencoder regularization in international miccai brainlesion workshop springer 2018 pp 311320 m i jordan z ghahramani t s jaakkola and l k saul an introduction to variational methods for graphical models machine learning vol 37 no 2 pp 183233 1999 y burda r grosse and r salakhutdinov importance weighted autoencoders arxiv150900519 cs stat 20161107 a a alemi i fischer j v dillon and k murphy deep variational information bottleneck arxiv preprint arxiv161200410 2016 i higgins l matthey a pal c burgess x glorot m botvinick s mohamed and a lerchner betavae learning basic visual concepts with a constrained variational framework iclr vol 2 no 5 p 6 2017 c cremer x li and d duvenaud inference suboptimality in variational autoencoders in international conference on machine learning pmlr 2018 pp 10781086 c zhang j btepage h kjellstrm and s mandt advances in variational inference ieee transactions on pattern analysis and machine intelligence vol 41 no 8 pp 20082026 2018 f ruiz and m titsias a contrastive divergence for combining variational inference and mcmc in international conference on machine learning pmlr 2019 pp 55375545 t salimans d kingma and m welling markov chain monte carlo and variational inference bridging the gap in international conference on machine learning 2015 pp 12181226 d rezende and s mohamed variational inference with normalizing flows in international conference on machine learning pmlr 2015 pp 15301538 r m neal and others mcmc using hamiltonian dynamics handbook of markov chain monte carlo vol 2 no 11 p 2 2011 a l caterini a doucet and d sejdinovic hamiltonian variational autoencoder in advances in neural information processing systems 2018 pp 81678177 m d hoffman and m j johnson elbo surgery yet another way to carve up the variational evidence lower bound in workshop in advances in approximate bayesian inference nips vol 1 2016 p 2 e nalisnick l hertel and p smyth approximate inference for deep latent gaussian mixtures in nips workshop on bayesian deep learning vol 2 2016 p 131 n dilokthanakul p a m mediano m garnelo m c h lee h salimbeni k arulkumaran and m shanahan deep unsupervised clustering with gaussian mixture variational autoencoders arxiv161102648 cs stat 2017 j tomczak and m welling vae with a vampprior in international conference on artificial intelligence and statistics pmlr 2018 pp 12141223 c k snderby t raiko l maale s k snderby and o winther ladder variational autoencoder in 29th annual conference on neural information processing systems nips 2016 2016 a klushyn n chen r kurle and b cseke learning hierarchical priors in vaes advances in neural information processing systems p 10 2019 x chen d p kingma t salimans y duan p dhariwal j schulman i sutskever and p abbeel variational lossy autoencoder arxiv preprint arxiv161102731 2016 a razavi a v d oord and o vinyals generating diverse highfidelity images with vqvae2 advances in neural information processing systems 2020 b pang t han e nijkamp sc zhu and y n wu learning latent space energybased prior model advances in neural information processing systems vol 33 2020 j aneja a schwing j kautz and a vahdat ncpvae variational autoencoders with noise contrastive priors arxiv201002917 cs stat 2020 m bauer and a mnih resampled priors for variational autoencoders in the 22nd international conference on artificial intelligence and statistics pmlr 2019 pp 6675 t r davidson l falorsi n de cao t kipf and j m tomczak hyperspherical variational autoencoders in 34th conference on uncertainty in artificial intelligence 2018 uai 2018 association for uncertainty in artificial intelligence auai 2018 pp 856 865 e mathieu c le lan c j maddison r tomioka and y w teh continuous hierarchical representations with poincar variational autoencoders in advances in neural information processing systems 2019 pp 12 56512 576 ovinnikov poincar wasserstein autoencoder arxiv190101427 cs stat 20200316 l falorsi p de haan t r davidson n de cao m weiler p forr and t s cohen explorations in homeomorphic variational autoencoding arxiv180704689 cs stat 2018 n miolane and s holmes learning weighted submanifolds with variational autoencoders and riemannian variational autoencoders in proceedings of the ieeecvf conference on computer vision and pattern recognition 2020 pp 14 50314 511 g arvanitidis l k hansen and s hauberg a locally adaptive normal distribution advances in neural information processing systems pp 42584266 2016 n chen a klushyn r kurle x jiang j bayer and p smagt metrics for deep generative models in international conference on artificial intelligence and statistics pmlr 2018 pp 15401550 h shao a kumar and p t fletcher the riemannian geometry of deep generative models in 2018 ieeecvf conference on computer vision and pattern recognition workshops cvprw ieee 2018 pp 4284288 d kalatzis d eklund g arvanitidis and s hauberg variational autoencoders with riemannian brownian motion priors in international conference on machine learning pmlr 2020 pp 50535066 c chadebec c mantoux and s allassonnire geometryaware hamiltonian variational autoencoder arxiv201011518 cs math stat 2020 g arvanitidis b georgiev and b schlkopf a priorbased approximate latent riemannian metric arxiv210305290 cs stat 2021 g arvanitidis l k hansen and s hauberg latent space oddity on the curvature of deep generative models in 6th international conference on learning representations iclr 2018 2018 m f frenzel b teleaga and a ushio latent space cartography generalised metricinspired measures and measurebased transformations for generative models arxiv preprint arxiv190202113 2019 g arvanitidis s hauberg and b schlkopf geometrically enriched latent spaces arxiv200800565 cs stat 20200802 g lebanon metric learning for text documents ieee transactions on pattern analysis and machine intelligence vol 28 no 4 pp 497508 2006 m louis computational and statistical methods for trajectory analysis in a riemannian geometry setting phd thesis sorbonnes universits 2019 m girolami b calderhead and s a chin riemannian manifold hamiltonian monte carlo arxiv preprint arxiv09071100 2009 m girolami and b calderhead riemann manifold langevin and hamiltonian monte carlo methods journal of the royal statistical society series b statistical methodology vol 73 no 2 pp 123214 2011 a paszke s gross s chintala g chanan e yang z devito z lin a desmaison l antiga and a lerer automatic differentiation in pytorch 2017 r m neal hamiltonian importance sampling in talk presented at the banff international research station birs workshop on mathematical issues in molecular dynamics 2005 s duane a d kennedy b j pendleton and d roweth hybrid monte carlo physics letters b vol 195 no 2 pp 216222 1987 b leimkuhler and s reich simulating hamiltonian dynamics cambridge university press 2004 vol 14 j s liu monte carlo strategies in scientific computing springer science business media 2008 m arjovsky s chintala and l bottou wasserstein gan arxiv170107875 cs stat 20171206 h xiao k rasul and r vollgraf fashionmnist a novel image dataset for benchmarking machine learning algorithms arxiv preprint arxiv170807747 2017 g cohen s afshar j tapson and a van schaik emnist extending mnist to handwritten letters in 2017 international joint conference on neural networks ijcnn ieee 2017 pp 2921 2926 y lecun the mnist database of handwritten digits 1998 t salimans i goodfellow w zaremba v cheung a radford and x chen improved techniques for training gans in advances in neural information processing systems 2016 m heusel h ramsauer t unterthiner b nessler and s hochreiter gans trained by a two timescale update rule converge to a local nash equilibrium in advances in neural information processing systems 2017 t karras t aila s laine and j lehtinen progressive growing of gans for improved quality stability and variation in international conference on learning representations iclr 2017 m lucic k kurach m michalski s gelly and o bousquet are gans created equal a largescale study in advances in neural information processing systems 2018 p 10 100 101 102 103 104 105 106 107 108 109 110 111 112 k shmelkov c schmid and k alahari how good is my gan in proceedings of the european conference on computer vision eccv 2018 pp 213229 a borji pros and cons of gan evaluation measures computer vision and image understanding vol 179 pp 4165 2019 g huang z liu l van der maaten and k q weinberger densely connected convolutional networks in 2017 ieee conference on computer vision and pattern recognition cvpr ieee 2017 pp 22612269 b amos bamosdensenetpytorch 2020 originaldate 20170209t153323z online available httpsgithubcom bamosdensenetpytorch l breiman random forests machine learning vol 45 no 1 pp 532 2001 s b kotsiantis i zaharakis and p pintelas supervised machine learning a review of classification techniques emerging artificial intelligence applications in computer engineering vol 160 no 1 pp 324 2007 j wen e thibeausutre m diazmelo j sampergonzlez a routier s bottani d dormont s durrleman n burgos and o colliot convolutional neural networks for classification of alzheimers disease overview and reproducible evaluation medical image analysis vol 63 p 101694 2020 k aderghal m boissenin j benoispineau g catheline and k afdel classification of smri for ad diagnosis with convolutional neuronal networks a pilot 2d study on adni in lecture notes in computer science including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics vol 10132 lncs 2017 pp 690701 k aderghal a khvostikov a krylov j benoispineau k afdel and g catheline classification of alzheimer disease on imaging modalities with deep cnns using crossmodal transfer learning in 2018 ieee 31st international symposium on computerbased medical systems cbms 2018 pp 345350 issn 23729198 j islam and y zhang ganbased synthetic brain pet image generation brain informatics vol 7 no 1 2020 k oh yc chung k w kim ws kim and is oh classification and visualization of alzheimers disease using volumetric convolutional neural network and transfer learning scientific reports vol 9 no 1 p 18150 2019 m liu j zhang c lian and d shen weakly supervised deep learning for brain disease prognosis using mri and incomplete clinical scores ieee transactions on cybernetics vol 50 no 7 pp 33813392 2020 a valliani and a soni deep residual nets for improved alzheimers diagnosis in 8th acm international conference on bioinformatics computational biologyand health informatics acmbcb 17 boston massachusetts usa acm press 2017 pp 615615 k backstrom m nazari ih gu and a jakola an efficient 3d deep convolutional network for alzheimers disease diagnosis using mr images in 2018 ieee 15th international symposium on biomedical imaging isbi 2018 vol 2018april 2018 pp 149153 d cheng and m liu cnns based multimodality classification for ad diagnosis in 2017 10th international congress on image and signal processing biomedical engineering and informatics cispbmei 2017 pp 15 k a ellis a i bush d darby d de fazio j foster p hudson n t lautenschlager n lenzo r n martins p maruff c masters a milner k pike c rowe g savage c szoeke k taddei v villemagne m woodward d ames and aibl research group the australian imaging biomarkers and lifestyle aibl study of aging methodology and baseline characteristics of 1112 individuals recruited for a longitudinal study of alzheimers disease international psychogeriatrics vol 21 no 4 pp 672687 2009 k j gorgolewski t auer v d calhoun r c craddock s das e p duff g flandin s s ghosh t glatard y o halchenko d a handwerker m hanke d keator x li z michael c maumet b n nichols t e nichols j pellman jb poline a rokem g schaefer v sochat w triplett j a turner g varoquaux and r a poldrack the brain imaging data structure a format for organizing and describing outputs of neuroimaging experiments scientific data vol 3 no 1 p 160044 2016 n j tustison b b avants p a cook yuanjie zheng a egan p a yushkevich and j c gee n4itk improved n3 bias 113 114 115 116 117 118 119 correction ieee transactions on medical imaging vol 29 no 6 pp 13101320 2010 v fonov a evans r mckinstry c almli and d collins unbiased nonlinear average ageappropriate brain templates from birth to adulthood neuroimage vol 47 p s102 2009 v fonov a c evans k botteron c r almli r c mckinstry and d l collins unbiased average ageappropriate atlases for pediatric studies neuroimage vol 54 no 1 pp 313327 2011 b b avants n j tustison m stauffer g song b wu and j c gee the insight toolkit image registration framework frontiers in neuroinformatics vol 8 2014 v s fonov m dadar t pa r group and d l collins deep learning of quality control for stereotaxic registration of human brain mri biorxiv p 303487 2018 j bergstra and y bengio random search for hyperparameter optimization journal of machine learning research vol 13 no feb pp 281305 2012 k he x zhang s ren and j sun delving deep into rectifiers surpassing humanlevel performance on imagenet classification in 2015 ieee international conference on computer vision iccv santiago chile ieee 2015 pp 10261034 d p kingma and j ba adam a method for stochastic optimization arxiv preprint arxiv14126980 2014 clment chadebec is a phd student funded by p rairie at universit de paris and inria his research interests include machine learning riemannian geometry and computational statistics for medicine he received master degrees from ecole nationale des mines de paris and ecole normale suprieure parissaclay elina thibeausutre is a phd student at sorbonne universit and inria her research interest include deep learning application to neuroimaging data its interpretability and reproducibility she received master degrees from ecole nationale des mines de paris and ecole suprieure de physique et de chimie industrielles paris france ninon burgos cnrs researcher in the aramis lab a joint laboratory between sorbonne universit cnrs inserm and inria within the paris brain institute france she completed her phd at university college london uk in 2016 her research focuses on the development of computational imaging tools to improve the understanding and diagnosis of dementia stphanie allassonnire pr of applied mathematics in universit de paris p rairie fellow and deputy directeor she received her phd degree in applied mathematics 2007 studies one year as postdoctoral fellow in the cis jhu baltimore she then joined the applied mathematics department of ecole polytechnique in 2008 as assistant professor and moved to paris descartes school of medicine in 2016 as professor her researches focus on statistical analysis of medical databases in order to understanding the common features of populations designing classification early prediction and decision support systems a ppendix a p roof of p rop 1 a ppendix b d etailed e xperimental s etting proposition 2 the riemannian manifold rd g is geodesic ally complete we will show that given the manifold m rd endowed with the riemannian metric g whose local representation is given by g1 z  kz c k2 i 2  id li l exp or eq 4 in the paper any geodesic curve a b m is actually extensible to r that is the riemannian manifold rd g is geodesically complete the proof we derive is inspired from the one proposed in 79 proof let us suppose that there exists a geodesic curve such that it cannot be extended to r therefore there exist a b r such that i a b is the domain of definition of i m we show that such an assumption leads to an absurdity for this experiment we consider a vanilla vae a vae with vamp prior and a geometryaware vae for a fair comparison each model is trained with the same neural network architecture for the encoder and decoder along with the same latent space dimension the main parameters for the geometryaware vae are presented in table 8 we refer the reader to 73 for a more precise description of each of these parameters and their impact on the model for the vamp prior the number of pseudoinputs is set to 10 and we use the implementation provided by the authors each model is trained until the elbo does not improve for 20 epochs with an adam optimizer 119 and a learning rate of 103 since the data sets sizes are small the training is performed in a single batch table 7 neural net architectures the same architectures are used for the vanilla vae vamp vae and geometryaware vaes diag first since li are defined as lower triangular matrices with positive diagonal coefficients we have that li l is a symmetric positivedefinite matrix cholesky decomposition therefore x li l i x 0 x r 0 where the last equality comes for the constant speed of geodesic curves therefore kt t0 k2 kt0 kt0  t t0 this shows that for any t a b the geodesic curve remains within a compact set and so is bounded on i now consider the sequence tn b as geodesic curves have constant speed i tn tn nn is a compact set moreover by application of cauchylipschitz theorem one can find 0 such that for any n n can be defined on tn tn since tn can be as close to b as desired there exists n n such that n n we have tn b 2 this means that the domain of definition of the curve can be extended to a b 2 which concludes the proof d 400 relu d 400 relu 400 dd1 linear d input space dimension d latent space dimension table 8 geometryaware vae parameters ktk2t httit t gtt ktk22 ktk22 kt ci k22 t li li t exp  i1  ktk2t kt0 k2t0 400 d linear 400 d linear 400 d sigmoid 400 d linear d 400 relu llow then let t a b we recall that hence let t0 a b for any t a b parameters of sec 33 generation comparison data sets synthetic shapes reduced mnist bal reduced mnist unbal reduced emnist nlf parameters 102 102 102 102 103 103 103 103  latent space dimension same for vae and vampvae parameters of sec 4 data augmentation for this experiment the same parameters and neural networks architectures as presented in the former section are used except for reduced fashion where the dimension of the latent space is set to 5 as to training parameters for the vaes for each model we use an adam optimizer with a learning rate set to 103 since the data sets sizes are small the training is performed in a single batch as to the densenet 97 used as benchmark for data augmentation the implementation we use is the one in 98 with a growth rate equals to 10 depth of 20 and 05 reduction and is trained with a learning rate of 103 weight decay of 104 and a batch size of 200 the classifier is trained until the loss does not improve on the validation set for 50 epochs and tested on the original test sets eg 1000 samples for mnist for sec 423 the mlp has 400 hidden units with relu activation function it is trained with adam optimizer and a learning rate of 103 training is stopped if the loss does not improve on the validation set for 20 epochs parameters of sec 5 validation on medical imaging to generate new data on the adni database we amend the neural network architectures and use the one described in table 9 the parameters used in the geometryaware vae are provided in table 10 an adam optimizer with a learning rate of 105 and batch size of 25 are used the vae model is trained until the elbo does not improve for 50 epochs generating 50 adni images takes approx 30 s5 with the proposed method on intel core i7 cpu 6x11ghz and 16 gb ram llow 777504 d h1 rel d h3 relu d h3 relu h1 h2 relu h1 h2 relu h3 h2 relu h3 d lin 500 dd1 500 lin h2 h3 relu h2 h3 relu h2 h1 relu h3 d lin h3 d lin h1 d sig 400 table 10 geometryaware parameters settings for adni database data set adni nlf data sets synthetic shapes reduced mnist reduced emnist reduced fashion nlf parameters 102 102 102 102 103 103 103 103  latent space dimension same for vae and vampvae a ppendix d a dditional r esults s ec 423 table 9 neural net architecture diag table 11 geometryaware vae parameters parameters 103 15 102 a ppendix c a f ew m ore s ampling c omparisons s ec 33 in addition to the comparison performed in sec 331 we also compare qualitatively a vanilla vae a vae with vamp prior and a geometryaware vae on 4 reduced data sets and in higher dimensional latent spaces of dimension 10 the first one is created with 180 binary rings and circles with different diameters and thicknesses ensuring balanced classes the second one is composed of 120 samples of emnist letter m and referred to as reduced emnist another one is created with 120 samples from the classes 1 2 and 3 of mnist database ensuring balanced classes and is called reduced mnist the last one reduced fashion is again composed of 120 samples from 3 classes shoes trouser and bag from fashionmnist and ensuring balanced classes the models have the same architectures as described in table 7 and are trained with the parameters stated in table 11 10 pseudoinputs are again used to train the vae with vamp prior each model is trained until the elbo does not improve for 20 epochs with adam optimizer a learning rate of 103 and in a single batch in fig 10 are presented from top to bottom 1 an extract of the training samples for each data set 2 samples obtained with a vanilla vae with a gaussian prior 2 data generated from a vae with vamp prior 3 samples created by a geometryaware vae and using the prior or 4 samples from our method as discussed in the paper the proposed method is again able to visually outperform peers since for all data sets it is able to create sharper and more meaningful samples even if the number of training samples is quite small 5 depends on the length of the mcmc chain and hmc hyperparameter l we used 300 steps with l 15 further to the experiments presented in sec 423 we also provide the results of the 4 classifiers on reduced emnist and reduced fashion in fig 9 again for most classifiers the proposed method either equals or greatly outperform the baseline a ppendix e a few m ore s ample g eneration on adni in this section we first provide several slices of a 3d image generated by our model the model is trained on the class ad of train50 ie on 50 mri of patient having been diagnosed with alzheimer disease the generated image is presented in fig 11 we also present in fig 12 4 generated patients for a model trained on train50 the two left images show cognitively normal generated patients while the rightmost images represent ad generated patients a ppendix f t he i ntruders a nswers to f ig 7 in fig 7 of the paper the synthetic samples are the leftmost and rightmost images while the real patients are in the middle the model is trained on the class ad of trainfull ie 210 images a ppendix g c omplementary r esults on m edical i mages results on synthetic data only for the classification task on mris are added in tables 12 to 15 as observed on the toy examples the proposed model is again able to produce meaningful synthetic samples since each cnn outperforms greatly the baseline ie the real training data either on train50 or trainfull the fact that classification performances on aibl which is never used for training are better for a classifier trained on synthetic data than on the baseline shows again that the generative model does not overfit the training data coming from adni but rather produces samples that are also relevant for another database table 12 mean test performance of the 20 runs trained on train50 with the baseline hyperparameters adni image type real real highresolution synthetic synthetic synthetic synthetic synthetic synthetic synthetic real synthetic real synthetic real synthetic real synthetic real synthetic real synthetic images 500 1000 2000 3000 5000 10000 500 1000 2000 3000 5000 10000 sensitivity specificity 703 122 785 94 724 64 750 62 714 66 706 52 781 61 752 68 719 53 698 66 722 44 718 49 747 53 747 70 624 115 574 88 656 81 656 74 704 66 738 42 690 69 734 48 670 45 712 37 703 43 734 55 735 48 734 61 aibl balanced accuracy 663 24 679 23 690 19 703 20 709 30 722 14 735 20 743 19 694 16 705 21 712 16 726 16 741 22 740 27 sensitivity specificity 607 137 572 112 566 99 627 97 621 88 657 69 745 78 736 108 559 68 591 90 666 71 661 93 717 100 691 99 738 72 758 70 800 53 788 53 805 47 805 46 773 54 794 60 811 31 821 37 790 41 811 50 805 44 807 51 balanced accuracy 672 41 665 30 683 30 708 35 713 36 731 18 765 29 759 25 685 25 706 31 728 22 736 30 761 36 749 32 table 13 mean test performance of the 20 runs trained on trainfull with the baseline hyperparameters adni image type real real highresolution synthetic synthetic synthetic synthetic synthetic synthetic synthetic real synthetic real synthetic real synthetic real synthetic real synthetic real synthetic images 500 1000 2000 3000 5000 10000 500 1000 2000 3000 5000 10000 sensitivity specificity 791 62 845 38 816 68 829 45 819 45 849 35 840 35 842 54 825 34 846 44 854 40 847 36 846 42 842 28 763 42 767 40 795 58 820 58 877 34 874 35 884 33 886 39 819 54 843 51 864 59 868 45 869 36 885 29 aibl balanced accuracy 777 25 806 11 805 24 824 19 848 20 861 15 862 17 864 18 822 24 844 18 859 16 858 17 857 21 863 18 sensitivity specificity 706 67 716 64 747 93 772 74 747 63 774 58 768 42 775 74 760 63 770 70 772 69 772 48 769 52 791 47 863 36 892 27 873 48 888 52 921 19 909 30 922 18 910 32 897 33 904 34 904 38 917 29 914 30 910 26 balanced accuracy 784 24 804 26 810 32 830 20 834 27 842 18 845 18 842 24 829 25 837 23 838 22 844 18 842 22 851 19 table 14 mean test performance of the 20 runs trained on train50 with the optimized hyperparameters adni image type real real highresolution synthetic synthetic synthetic synthetic synthetic synthetic synthetic real synthetic real synthetic real synthetic real synthetic real synthetic real synthetic images 500 1000 2000 3000 5000 10000 500 1000 2000 3000 5000 10000 sensitivity specificity 754 50 736 62 758 30 767 46 739 36 744 61 771 45 775 53 732 42 761 53 752 38 765 38 771 37 778 46 755 53 706 59 776 53 785 49 798 40 798 49 774 52 773 47 780 33 795 29 786 44 792 42 767 41 782 49 aibl balanced accuracy 755 27 721 31 767 28 776 37 768 30 771 40 772 21 774 31 756 25 778 23 769 24 778 19 769 25 780 21 sensitivity specificity 686 85 578 123 732 90 787 75 782 69 764 101 811 59 817 54 692 94 793 58 778 88 809 79 807 61 817 49 826 42 846 42 836 40 832 48 824 37 824 43 820 39 797 41 827 41 825 42 822 45 814 42 812 37 819 46 balanced accuracy 756 41 712 51 784 40 809 43 803 35 794 47 815 26 807 29 760 42 809 32 800 36 812 37 809 27 819 22 table 15 mean test performance of the 20 runs trained on trainfull with the optimized hyperparameters adni image type real real highresolution synthetic synthetic synthetic synthetic synthetic synthetic synthetic real synthetic real synthetic real synthetic real synthetic real synthetic real synthetic images 500 1000 2000 3000 5000 10000 500 1000 2000 3000 5000 10000 sensitivity specificity 825 42 826 45 817 36 828 34 813 28 822 49 806 34 840 38 823 23 825 33 831 42 813 37 819 35 822 34 885 66 889 63 905 39 900 40 912 28 906 45 916 25 891 31 898 27 905 41 913 32 904 34 909 25 912 36 aibl balanced accuracy 855 24 857 25 861 14 864 21 862 17 864 20 861 19 865 17 860 18 865 19 872 17 858 26 864 13 867 18 100 sensitivity specificity 751 84 789 54 755 71 768 45 762 67 777 63 753 54 792 52 749 50 764 56 760 47 749 73 741 49 764 42 887 90 899 40 898 43 915 25 922 36 908 44 924 25 901 37 914 26 910 34 920 24 923 26 929 19 921 21 100 baseline augmented 200 baseline augmented 200 augmented 500 augmented 500 augmented 1000 balanced accuracy 819 32 844 17 826 29 842 17 842 26 843 20 838 20 847 23 832 24 837 20 840 20 836 32 835 22 843 18 augmented 1000 augmented 2000 augmented 2000 synthetic 200 synthetic 200 synthetic 500 synthetic 500 synthetic 1000 synthetic 1000 synthetic 2000 synthetic 2000 accuracy accuracy mlp svm knn a reduced emnist random forest mlp svm knn random forest b reduced fashionmnist fig 9 evolution of the accuracy of 4 benchmark classifiers on the reduced emnist data set left and the reduced fashion data set right stochastic classifiers are trained with 5 independent runs and we report the mean accuracy and standard deviation on the test set reduced emnist 120 reduced mnist 120 reduced fashion 120 synthetic 180 training samples vae n 0 id vae vamp prior rhvae n 0 id rhvae ours fig 10 comparison of 4 sampling methods on reduced emnist 120 letters m reduced mnist reduced fashionmnist and the synthetic data sets in higher dimensional latent spaces dimension 10 from top to bottom 1 samples extracted from the training set 2 samples generated with a vanilla vae and using the prior n 0 id 3 from the vamp prior vae 4 from a rhvae and the priorbased generation scheme and 5 from a rhvae and using the proposed method all the models are trained with the same encoder and decoder networks and identical latent space dimension an early stopping strategy is adopted and consists in stopping training if the elbo does not improve for 20 epochs the number of training samples is noted between parenthesis fig 11 several slices of a generated image the model is trained on the ad class of train50 ie 50 images of ad patients fig 12 images generated by our method when trained on train50 left cn generated patients right ad generated patients 