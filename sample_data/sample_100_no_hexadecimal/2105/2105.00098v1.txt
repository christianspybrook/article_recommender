Quantum Machine Learning with SQUID
Jakub Filipek1 , Shih-Chieh Hsu2 , Alessandro Roggero3,4 , and Nathan Wiebe5,6,2
1 Paul

G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA 98195, USA
of Physics, University of Washington, Seattle 98195, USA
3 Institute for Nuclear Theory, University of Washington, Seattle, WA 98195, USA
4 InQubator for Quantum Simulation (IQuS), Department of Physics, University of Washington, Seattle, WA 98195, USA
5 University of Toronto, Department of Computer Science, Toronto, ON M5G 1V7, Canada

arXiv:2105.00098v1 [quant-ph] 30 Apr 2021

2 Department

6 Pacific

Northwest National Laboratory, Richland, WA 99352, USA
April 30, 2021

to their classical counter-parts hard. As potential performance benefits are the main driver of the quantumenhanced machine learning, we believe that ease of comparison to classical machine learning should be one of
the priorities in the field.
Second approach is to classically pre-process data,
so it can fit in the limited space defining the quantum model (as in eg. [8]). While this approach allows
for direct comparison to classical performance on the
same data, it also requires to factor in what impact
pre-processing had on performance of both algorithms.
It requires scientists to carefully prepare experiments to
not give unfair advantages to either quantum or classical algorithms. Lastly, since no two studies will use the
same pre-processing there is additional overhead when
comparing two different quantum-enhanced approaches,
or performing meta-analysis of the field.
To combat these issues, we propose a standardized
approach of designing hybrid (quantum and classical)
models. Similarly to how TensorFlow [9] and PyTorch [10] changed classical machine learning field and
increased reproducibility of efforts, we propose Scaled
QUantum IDentifier (SQUID), which is an extensible
framework which can incorporate quantum models. As
it is based on top of PyTorch, it has most of the benefits
of a mature framework when it comes to purely classical architectures. For quantum models, we provide a
standardized model design where user has to implement
forward- and back-propagation functions.
By doing so, the pre-processing algorithms can be
standard across applications and approaches, making
them more directly comparable. It also reduces overhead on new researchers, as it significantly reduces the
amount of coding required for an experiment. Such mix
of both worlds also resembles quantum-inspired algorithms [1], which also benefit from above points.
The article is organized in following manner. In Section 2 we oultine the framework design and describe the
relevant internal details. In Section 3 we show an exam-

In this work we present the Scaled QUantum IDentifier (SQUID), an open-source framework for exploring hybrid Quantum-Classical algorithms for classification problems. The classical infrastructure is based on PyTorch and we
provide a standardized design to implement a
variety of quantum models with the capability
of back-propagation for efficient training. We
present the structure of our framework and provide examples of using SQUID in a standard
binary classification problem from the popular
MNIST dataset. In particular we highlight the
implications for scalability for gradient based optimization of quantum models on the choice of
output for variational quantum models.

1 Introduction
Quantum Machine Learning (QML) is a rapidly growing, emerging field, with a diverse set of ideas and applications. While there are significant differences in
applications as to where Machine Learning and where
Quantum Computing are applied, quantum-enhanced
machine learning has become one of the dominant subfields [1–3]. The main benefits of such algorithms are
potential quantum speed-ups [4, 5], and the potential
of recognizing statistical patterns hard to learn with
purely classical schemes [6].
However, machine learning algorithms on near-term
quantum devices face an issue of constrained resources.
While there exist encodings that efficiently use qubits,
they still do not allow to load datasets such as MNIST in
quantum memory, while additionally introducing overhead when encoding and decoding information between
classical and quantum devices. To counter that issue
researchers have used two approaches.
One was to use synthetic or very small datasets that
could be learned efficiently (as in eg. [7]). This however makes any benchmarks artificial, and comparison
1

ple application of the model using the MNIST dataset,
and study the impact of including information from single vs. all available output qubits. We describe the use
of the SQUID Helpers package and possible future extensions in Sec. 4. Finally, in Section. 5 we provide a
summary and perspective.

2 The squid framework
Our main goal when designing SQUID is to propose
a framework within which both classical and quantum
machine learning can work in concert to solve a classification problem. Properly utilizing classical computing,
when possible, is of great importance because quantum
and classical models for data will often have different
advantages and disadvantages. From an architectural
perspective, the key innovation that SQUID allows is
for classical neural networks to be globally trained in
conjunction with a quantum neural network to build optimal encoders and decoders for the classical inputs or
quantum outputs from the hybrid neural network. This
ability allows us to, in effect, learn a feature map that
not only allows us to represent large quantum datasets
in near term devices but also allows us to incorporate
classical information that may be known a priori into
the quantum model.
Before proceeding with the detailed description of the
model, it is important for work like this that hybridizes
between quantum and classical models to discuss the
correspondence between the quantum and classical machine learning models that we implicitly assume. In
all these cases we assume that in general our training dataset contains both classical vector and quantum
states and the following form
Strain = {(vclass [j], |vquant [j]i), j ∈ 0, . . . , Ntrain }

Figure 1: SQUID Model Architecture. A shows a current implementation, which is a simplified version of B, (|ψiin = 0).
In A the nodes can be thought of as neurons in classical machine learning, or quantum state vectors. The edges represent
transformations applied to these states. These typically involve trainable weights, but they might, as often in case of
Quantum Model perform a set of transformations defined by
incoming data. "Inputs to the Quantum Model" (~vclass ) in
particular can define both the input state to a quantum system
(WE ) along with the rotations that should be applied to that
input state (WV (θ)). Quantum Model should also perform a
measurement of the state (MD ) and return classical amplitude
estimates (~vout ). Part B contains also planned future extension
in which quantum features are also allowed to be passed in.

ally follows design principles set by PyTorch [10]. Currently there are many competing Quantum SDKs [11–
13], most of which include Python interfaces. Hence a
successful QML package should allow, a simple extensible solution, which can be adjusted to any specific SDK.
SQUID enables that by providing general classes, similar to nn.Module in PyTorch, one each for Quantum
and Classical Models. These satisfy minimal requirements of functions used by the backend MainModel to
properly propagate the gradient through combinations
of the models.

(1)

Here we assume a classical bit-encoding, meaning that
we assume that each vclass [j] ∈ RD . This is the typical
setting in machine learning, however, it is also possible
to envision that the actual training vectors are distributions over D symbols and the classical values vclass are
given by the probabilities of drawing each symbol from
the distribution. We further implicitly assume that the
quantum data |vquant [j]i is provided using an amplitude
encoding. By this we mean that the values of the training vectors are stored in the amplitudes of the |vquant [j]i
state. We make this choice because it is the most general setting that we can assume as it also subsumes the
case where the quantum training vectors correspond to
distinct quantum bit strings (otherwise known as a bitencoding).
The SQUID framework was designed with extensibility and simplicity as its core principles. It gener-

2.1 Framework Design
Main component of SQUID is the MainModel, which
itself accepts three smaller models. The first and last
models are currently enforced to be classical, while the
middle can be either quantum or classical. In case all
three models are classical, MainModel is equivalent to
PyTorch's nn.Sequential with three sub-components.
The complete framework is shown graphically on
Fig. 1, and the detailed relations between models within
the ensemble are described in the following subsections.
2

2.2 Propagating through the Main Model

Algorithm 1: Back-propagation through the
Main Model
Input: Main Model M, Loss L (Torch Tensor)
Let M1 , M2 , M3 refer respectively to Encoder,
Quantum Model, Decoder of M;
L.backward();
if M2 is Quantum then
Let be g23 the gradient of L with respect to
input to M3 (PyTorch-provided);
Let g12 be the gradient of L with respect to
input to M2 , this is achieved by passing g23
through user-provided backward function of
the M2 model;
Let o12 be the output of M1 and input of
M2 (Saved from forward iteration);
Let L0 be sum of all elements of g12 o12 ;
L0 .backward();
end

Calling the model, or calling the forward function is
exactly the same to PyTorch's forward pass. The only
difference, is when middle model is Quantum, and the
conversion between Tensors and numpy arrays [14] is
required. The reason for choosing numpy arrays to be
passed into Quantum model is due to the fact that many
QML packages accept them as the input and in fact
prefer them even over standard Pythonic lists.
The backward function offers the only major modification for the user in comparison to PyTorch, and it is
required to be called explicitly by the user. For classical
models standard PyTorch back-propagation (autograd)
is used, and exact gradients are calculated. In the case
there is a Quantum model in the middle, the automatic
gradient propagation stops at the end of the second
(quantum) model. This is because there was a conversion to/from numpy in the forward pass. SQUID uses
the backward function provided by implementation of
the Quantum Model. This both updates any parameters stored within the model, and provides the gradient
with respect to the output of the encoder.
The conversion from numpy array to PyTorch tensor
in backward call requires us to create a fake loss, which
we then use to propagate the gradients backward using
back-propagation. Given encoder forward output o12 ,
gradient of global loss with respect to that output g12
(provided by the Quantum Model as described above),
we define a fake loss L0 :
L0 (o12 , g12 ) =

XX
i

o12ij g12 ij

Algorithm 2: Quantum Model
Input: Number of qubits N , vector of M
~ desired number
quantum parameters θ,
N
of outputs Q1 ∈ [1, 2 ]
Construct unitary operation corresponding to
quantum circuit;
Measure Q1 output probabilities pk from circuit;
Use parameter
 shift rule and a total of
NC = min 2N M, 2Q1 (M + 1) circuits to
estimate the Q1 M -dimensional gradients gkw ;
Output: Returns result=~
p and grad=gkw

(2)

j

∂L0 (o12 , g12 )
= g12 ij
∂o12ij

(3)

complexity of the gradient calculation. This is especially relevant since the gradients propagated through
the quantum model require statistical sampling or a
quantum technique such as amplitude estimation to
evaluate [15–17]. Here we provide such a complexity analysis with the aim of bounding the scaling of
the number of quantum operations needed to ensure
that the gradients yielded by Algorithm 2 are accurate
within bounded error . Here in ensuring that the error
is  we mean that the gradient computation detailed in
Algorithm 2 outputs an estimate of the gradient ge such
that |g − ge|2 ≤ .
Let us consider a Monte-Carlo estimate of the gradient. The algorithm for generating such an estimate
involves measuring the expectation value of the gradient. This expectation value can be evaluated using
Hadamard tests to estimate each component of the gradient (see Appendix A). Using the empirical frequency
of measurements as an unbiased estimate of the probability, we have that if ge is the estimate that returns

After the above calculation a PyTorch backward call
is made on L0 , which propagates the gradient using
autograd. Hence, gradients with respect to all of the
Main Models parameters are calculated. For clarity the
process is shown in Algorithm 1.
It is worth mentioning that by using PyTorch built-in
autograd procedure any PyTorch loss can be used. For
example, in the Section 3 Cross Entropy loss is used.
Similarly any optimizer can be used. The main caveat
to using various optimizers is that if any parameters
are defined within the Quantum Model the user has
absolute control over updating them, and the overhead
of optimizer implementation falls onto the user.

2.3 Complexity of Gradient Evaluation
A crucial question that needs to be evaluated to assess
the practicality of any QML algorithm is the quantum
3

from our protocol
|g − E(e
g )|2 = 0.

Finally, an application of the Chernoff bound shows
that if we wish the error to be δ with probability at least
1 − η then we can repeat the experiment a logarithmic
number of times and use majority voting to estimate
the updated parameters. This results in
 



1
Q1 M 2 λ2
Q1 M 2 λ2
e
log
, (11)
∈O
N ∈O
δ2
η
δ2

(4)

As there are M different parameters and Q1 outputs
yielded by the quantum model, we further have that
E(|g − ge|22 ) =

Q
1M
X

E(gk2 − 2g gek + gek 2 ) =

k=1

Q
1M
X

V(e
gk ). (5)

e
where O(*)
denotes an asymptotic upper bound with
polylogarithmic multiplicative factors suppressed. On
a future fault-tolerant quantum device it would also be
possible to obtain a quadratic speedup in both  and M
at the cost of a longer circuit depth (see eg. [15, 18]).

k=1

Since the variance of the sum is the sum of the variances
and we are using the sample mean for our estimates of
the probability. This means that if N samples are used
per point then
Q
1M
X

V(e
gk ) ≤

k=1

Q
1M
X
k=1

2.4 Available classical models

V(gk )
N

(6)

There are two built-in classical models: Linear and Convolutional. Both accept arguments which specify numbers of neurons per layer, and activation functions between them.
However, since ClassicalModel is a sub-class of
nn.Module from PyTorch, it is straightforward for a
user to implement their own model. This is recommended, unless configuration files from SQUID helpers
are utilized (see Sec. 4.1).

Therefore we have that the mean square error of ge is at
most 2 if


Q1 M
Q1 M
N≥
max
V[g
]
∈
O
,
(7)
k
k
2
2
note that if the variances are small then the number of
samples required will be further reduced. Then from
Markov's inequality, the number of samples needed to
estimate the gradient within error  with probability
greater than 2/3 will be simply given by 3 times the
estimate in Eq. (7) above.
If a learning rate of λ is used for the gradient ascent
then the error in the quantum parameters (as quantified
by the Euclidean distance) is, with probability greater
than 2/3, at most λ. We denote by ∆ the error introduced by using the noisy estimator ge for the parameter
update
∆=

Y
j

M

e−iθj=1 Hj −

Y
j

M

θj=1 Hj
e−ie

,

2.5 Available quantum models
In this section we provide details on the implementation of quantum models within SQUID. The approach
we follow in this preliminary study is to construct the
most general models on a given set of qubits by expressing the quantum circuits as layers of structured operations acting in nearest-neighbors only. This allows for
both generality and a direct connection with real-world
implementations on near-term devices with limited connectivity. Despite this choice, the framework is general
and can be easily extended to accommodate quantum
models with a different structure.
The common construction for a variational quantum
classifier (see eg. [6, 7, 19]) is to start by considering
the encoding of an input D-dimensional feature vector
~v ∈ [0, 1]D into the quantum state of a register containing N ≥ dlog2 (D)e qubit by introducing an encoding
unitary operation WE as

(8)

2

with k * k2 the induced Euclidean norm. The generators
Hj used in SQUID (see Sec. 2.5) have unit norm, this
allows us to bound the error as
√

√
~e
~e
~
M
|
θ
−
θ|
∈
O
M
λ
. (9)
∆ ≤ |θ~ − θ|
≤
2
1

|Ψ (~v )i = WE (~v ) |0i ,

Therefore it follows from the fact that the error in the
~ − pk (e
~ ∈ O(∆),
output probabilities p~ satisfies |pk (θ)
θ)|
that the value of N needed to ensure that the maximum
error δ in pk after a single step of gradient ascent, with
probability at least 2/3, obeys


Q1 M 2 λ2
N ∈O
.
(10)
δ2

(12)

with |0i a reference state in the computational basis.
The encoded state |Ψi is then modified by acting with
a second unitary WV defined in terms of a set of Nv
variational parameters θ~ ∈ [0, π)Nv . The final state of
the quantum register right before measurement is then
 E
~ |Ψ (~v )i = WV (θ)W
~ E (~v ) |0i . (13)
Φ ~v , θ~ = WV (θ)
4

The output of the quantum models we consider here
are the probabilites of measuring each one of the computational basis states in the state |Φi, which can be
estimated by collecting statistics over a large number
of circuit executions. Given that the number of possible outcomes scales exponentially in the register size, a
small subset of probabilities is tipically selected in order
for the overall scheme to be scalable.
The decomposition of the total unitary operation
mapping |0i to |Φi as a function an the encoding unitary WE and a variational unitary WV is however artificial and does not necessarily lead to the most efficient
scheme. This is especially true in the SQUID framework where a classical network is devoted to optimally
determine an encoding of the classical data into a quantum state. The approach we take in SQUID is to consider instead the M -dimensional output from the classical encoder as the parameters describing a global unitary W in the quantum register, without artificially distinguishing between "encoding" parameters and "variational" parameters. This hibridization of the standard
approach described above is still completely general and
the global network can adjust to effectively reproduce
a factorized form W = WV WE if there is a measurable
advantage for the data under analysis.
We note that a generic unitary operation on N qubits
can depend on at most (4N − 1) parameters, which implies that we need to choose M < (4N − 1) for the
output of the classical encoder. In practice this is not a
limitation since unitary operations which can be decomposed efficiently into a polynomial number of one and
two qubit operations will depend at most on a polynomially large number of parameters.
In this first exploratory study we use a simple, but
general, parametrization of W in terms of a one qubit
2
unitary Uk1 (α, β, γ) and a two qubit unitary Ujk
(θ, φ, η)
both parametrized by 3 real parameters taking values
in [0, π). The unitary U1 can be written as

actly represented with the following circuit (see [20, 21])
U01 (α0 , β0 , γ0 )
U11 (α1 , β1 , γ1 )

U01 (α2 , β2 , γ2 )
U11 (α3 , β3 , γ3 )

2
requiring 1 application of U01
and 4 applications of Uk1
(on qubit 0 and 1) for a total of 15 parameters. This
construction can be readily extended to larger systems
2
applied
by interleaving layers of Uk1 with layers of Ujk
alternatively on even or odd partitions. For instance
with 4 qubits we consider circuits of the following form

U01
U11
U21
U31

2
U01

2
U23

U01
U11
U21

2
U12

U11
U21

U31

2
U01

2
U23

U01

.

U11
U21
U31

Note that in the costruction above we didn't include
the first and last single-qubit operations in the third U 1
layer. This allows to remove redundancy in the parameters since we replace the product of two U 1 operations
with a single U 1 , this simplification results in enhanced
stability in the training.

3 Example applications
As an initial application of our framework, we present
here results for binary classification on the MNIST
database [22] using digits 3 and 7. This is a standard benchmark for classification algorithms and analysis with a quantum model is made possible by the ability
of SQUID to compress the input features into data with
the appropriate dimensions.
The input feature vectors for this dataset are real vectors of dimension 784 representing a grayscale 28 × 28
pixel picture. In this section we will compare results
obtained with the architecture displayed in Fig. 2 composed by: a single layer encoder with M0 units, a blackbox classifier to be specified below and a single layer
decoder with M1 units. The classical black-box classifier used in this section consists of a simple single layer
network with 2 units (panel (B) of Fig. 2) and we take
M1 = 2 for it's output. In the following subsections we
will also consider different implementations of quantum
classifiers with the general structure displayed in panel
(C) of the same figure.
All of the calculations (classical and quantum) presented in this section were obtained using an Adam optimizer as implemented in PyTorch and using the hyperparameters reported in Tab 1. In all case we use

Uk1 (α, β, γ) = exp (iαYk ) exp (iβZk ) exp (iγYk ) , (14)
and we recognize the parameters (α, β, γ) to be the Euler angles in the Y ZY decomposition. In the expression
above Zk (Yk ) denote the Pauli Z matrix (Y matrix) for
qubit k, and we will denote Xk similarly in the follow2
ing. The two-qubit unitary Ujk
actin on the qubit pair
{j, k} is instead defined as
2
Ujk
(θ, φ, η) = eiθXj ⊗Xk +iφYj ⊗Yk +iηZj ⊗Zk .

2
U01
(θ, φ, η)

(15)

The usefulness of these choices comes from the possibility to represent a generic unitary by applying appropriately layers of Uk1 and Uk2 operations. For instance,
a general SU (4) transformation for 2 qubits can be ex5

(b)

25
20
15

Accuracy

0.995

10
5
0
25

(c)

0.99

20
15
Median accuracy
90% boundary - [VAL]
90% boundary - [TRAIN]

0.985
0

2×10

4

4×10

Number of parameters

Epochs
100

Learning rate
0.0001

Train size
9916

5
0.988

0.992

0.996

0
1

Accuracy

Figure 3: Results for the accuracy achieved on MNIST with the
classical model described in the text. Panel (a) shows the accuracy as a function of the number of parameters in the model
(green solid circles), the other two sets of points show the location of the 90% accuracy percentile for both the validation
set (red squares) and the training set (blue diamonds) . Panels
(b) and (c) show the histogram of achieved accuracies for the
smallest (M0 = 3) and largest (M0 = 60) models respectively.

Figure 2: Pictorial representation of the hybrid classifier model
used for MNIST classification. Panel (A) shows the complete
network including an encoder with M0 units and a decoder with
M1 units. Panel (B) shows the implemented classical classifier
composed by two units and panel (C) shows a schematic of the
quantum models: input parameters coming from the encoder
determine the unitary W while the output is obtained upon
measurement of the qubits.
Batch size
16

4

10

Bootstrap runs

(a)

Bootstrap runs

1

Ktot . In order to understand this point better we show
in the left panels the estimated histograms for accuracy
reached in our set of 48 bootstrap runs for the smallest
classical model (panel (b)) and the largest model (panel
(c)). As expected from the results in the main panel,
most of the density is in the same location but for the
larger models the tails are more important.
Note that the dispersion in the accuracy around the
median reported in the main panel of Fig. 3 is relatively
small, of the order of ≈ 0.002. This is caused in large
part by the simplicity of the classification problem, as
we can see in Fig. 4 (obtained for a medium sized model
with M0 = 40) the accuracy quickly converges to a narrow interval around the mean for both the training set
and the validation set. With more than 80 epochs the
accuracy for the training data set is able to reach 100%.
When the inner model is replaced by a quantum subroutine, as depicted in panel (C) of Fig. 2, the output dimension for a quantum circuit over N qubits is
bounded by M1 ≤ 2N . In the following sections we will
consider two limiting situation: the maximum possible
dimension M1 = 2N (indicated as "full" below) and the
minimum one M1 = 2 (indicated as "min" below) and
corresponding to the probability of measuring a single
⊗N
basis state (here we choose |0i ).

Val. size
2480

Table 1: Hyperparameters used for the results on MNIST.

48 independent optimization runs that were performed
in order to estimate the variance in the attained accuracy. In the following we will refer to this ensemble as
"Bootstrap runs".
We present the results obtained with the classical network in Fig. 3 (the full data is available in in Tab. 2 of
Appendix B). We can see from the evolution of the median accuracy (green circles) that the classical network
is able to achieve classification accuracies above 99%
but the increase in the number of hidden units at the
level of the input model doesn't seem to provide a statistically significant improvement on the final accuracy.
The displayed error bars are 68% confidence intervals
extracted from our finite population sample.
In the main panel, we also show the location of the
90% accuracy percentile, ie. the boundary value for
which 10% of bootstrap runs provide a higher accuracy,
for both the validation set (red squares) and the training set(blue diamonds). These results are consistent
with the expectation that, as the number of parameters Ktot in the model increases, the training set can
be described almost exactly by the network while at
the same time we see that the distribution of accuracies
for the validation set does not evolve significantly with

3.1 Separable Quantum Models
The first class of quantum models we consider here are
separable models with a single layer of U 1 unitaries and
are therefore fundamentally classical in that entanglement plays no role in shaping the output probabilities
6

(a)

Validation Set
0.995

0.995

0.99

0.99

0.985

0.985

6 qubit - full

Median [CL]
Median [QM - min]
Median [QM - full]

0.995

(c)

10
5

0.985

90% accuracy boundary

Accuracy

Accuracy

0.99

0.98

0.98

0.98
0.975

Training Set
20

40

60

Training Epochs

80

0

4

1×10

4

2×10

0

1

6 qubit - min

(d)

15
10

0.99
(b)

5
Number of parameters

0.975
0

15

100
0

20

40

60

80

0.975
100

0.97

Training Epochs

0

1×10

4

2×10

4

3×10

4

4×10

4

Number of parameters

Figure 4: Example of training of the classical model described in
the main text. The left panel shows the increase in classification
accuracy for the training set as a function of the number of
epochs. The right panel shows the same for the validation set.
All bands are 90% confidence intervals with averages indicated
by the solid lines.

5×10

Bootstrap runs

1

1

4

0.98 0.985 0.99 0.995

Bootstrap runs

1

0
1

Accuracy

Figure 5: Results for the accuracy achieved on MNIST with the
classical model (green points) and the separable quantum models described in the text (red and blue points). Panel (a) shows
the accuracy as a function of the number of parameters for the
classical model (green solid circles), the full quantum separable
models (blue solid circles) and the separable quantum models
with a single output variable M1 = 1 (red solid circles) indicated by "min". The inset panel (b) shows the location of the
90% accuracy percentile for the classical (green squares), full
separable quantum model (blue squares) and minimal separable quantum model (red squares). Panels (c) and (d) show the
histogram of achieved accuracies for the 6 qubit models with
either the full number of possible output variables (M1 = 64,
top panel) and the single output (bottom panel).

of the quantum model. The results are shown in Fig. 5
and the full data is available in Tab. 3 of Appendix B.
In this case, at least for the models with M1 = 2N , we
can see a mild increase of the final accuracy as a function of the number of parameters/qubits in the model,
the largest model is however outperformed by classical networks with a smaller size (see the blue circles
in Fig. 5(a)). The models with a latent space corresponding to the restricted output for the quantum layer
(shown as red circles in Fig. 5(a)) show instead a deterioration as we increase the number of qubits/parameters.
This effect is especially clear looking at the histograms
of achieved accuracy in the 48 bootstrap runs displayed
in the right panels of Fig. 5 for the largest model with
N = 6 qubits: employing a large output vector from
the quantum layer produces results with better than
99% for more than half of the runs while restricting the
output to a single probability prevents most runs from
reaching this threshold. Strikingly, this is true even
for the training set (not shown) where only a single
bootstrap run achieved an accuracy above 99%. This is
a first clear sign of the importance to supplement the
quantum classifier with a rich decoder at the possible
expense of a larger sample complexity.

and there doesn't seem to be any measurable advantage
in increasing the number of parameters. Interestingly,
for the models with restricted output size (red circles
denoted QM − min in panel (a) of Fig. 6), we see that
the optimization procedure is struggling to find a good
set of parameters for the larger models and the accuracy
decreases almost monotonically with size. It is possible
that better results could be obtained using directly the
accuracy as cost function instead of the cross-entropy.
In order to clarify that the effect we are seeing is not
coming from over-fitting of the training set, but really
from difficulties in exploring efficiently the energy surface, we show on the left panels the evolution of the 90%
accuracy percentile as a function of the number of parameters for both the validation and training set (panels
(b) and (c) respectively) for the 3 networks considered
here: the classical feed-forward network considered before (green squares) and the quantum models with entanglement either with the full output (red squares) or
the restricted output model (blue squares). As can be
clearly seen in panel (c) the optimizer is not able to find
a good parameter set for large models and the accuracy
in training decreases.

3.2 Quantum models with entanglement
We now turn to consider more general quantum models
that are capable of creating entanglement in the quantum register through the use of the two-qubit unitary
U 2 defined in Sec. 2.5. The resulting median accuracy
shown in panel (a) of Fig. 6 show a similar trend to
the simpler separable models above: the accuracy of
the quantum model never exceeds the classical accuracy

These results highlight the importance of supplementing the quantum classifier with a non trivial output
decoder in order to achieve a good efficiency, a possibil7

1

1
(b)

Median [CL]
Median [QM - full]
Median [QM - min]

0.995

0.995

0.99

0.99
0.985
1

0.985
0.995

0.98

0.975

0.99

90% percentile [CL]
90% percentile [QM - full]
90 % percentile [QM - min]

0

1×10

4

2×10

4

3×10

4

Training
(c)

4×10

Number of parameters

4

0

2×10

4

4×10

90% percentile

Accuracy

of data to ensure that dimensions of models inputs and
outputs match. This is done due to fail-first and fail-fast
principle. A single forward pass of data is faster than
a pass of a single batch, and in perspective of training
it is very cheap. The code then runs a typical training
for-loop, with an additional call to backward function
of Main Model, as explained in Section 2.2. In the end,
all of the results, as well as configuration is saved to a
single location. If bootstrap was used, along with the
results for each run, there is a folder with aggregated
results created for simpler analysis.
Helpers extension also provides very basic plotting
utilities for the results. However, those are meant as
a example of processing the output folders, since plots
are highly dependent on studies performed.

Validation

90% percentile

(a)

0.985

4

Number of parameters

Figure 6: Results for the accuracy achieved on MNIST with
the classical model (green points) and the quantum models
with entanglement described in the text (red and blue points).
Panel (a) shows the accuracy as a function of the number of
parameters for the classical model (green solid circles), the full
quantum models (blue solid circles) and the quantum models
with a single output variable M1 = 1 (red solid circles) indicated by "min". The left panels show the location of the 90%
accuracy percentile for the classical (green squares), full quantum (blue squares) and minimal quantum model (red squares)
in either the validation set (panel (b)) or the training set (panel
(c)).

4.2 Future Work
There is a vast amount of possible extensions to SQUID,
some of which can be included directly in a main
project, while others can be used as standalone packages. The main advantage of the SQUID is that it allows
for abstracting communication with a specific backend.
To do so however, custom QuantumModel subclasses interfacing with the backend API will be needed.
Additionally, as of right now SQUID allows only for
a classification and regression tasks. More advanced
scenarios would require changes both in SQUID as well
as, to a larger extent, SQUID helpers.
Another, and much sooner addition to SQUID will be
to start supporting various quantum computing frameworks. For example, Qiskit has created a great package
for optimization of quantum circuits [12]. This would
fit perfectly into SQUID ecosystem with a translation
layer. Such addition would allow the user to implement
hybrid models without the explicit definition of circuits
for training, provided the circuit gradient are correctly
passed by the backward function.
There are also others frameworks that have either already similar behavior or plans for optimization packages. Modularity of SQUID would allow code to be
backend agnostic, and work uniformly across multiple
types of quantum devices.

ity that is available only if we choose to measure more
than a single qubit from the quantum device.

4 SQUID Extensions and Future Work
As mentioned is Section 2 SQUID is designed with extensibility in mind. This means that it should be easy to
write additional packages on top of it, as well that additional features should involve changing at most few
modules. In the following two sections we describe
the SQUID Helpers package, designed to allow the use
of SQUID using user-provided configuration files, and
comment on possible extensions of the framework.

4.1 SQUID Helpers Package
To show how such extension could function, but also
to stream-line the workload for use cases, we provide
SQUID Helpers extension. It allows user to use yaml
configuration files to run SQUID code. As a result bootstrapping multiple runs of multiple test cases and aggregation of the results is much easier.
The conversion from configuration files to SQUID is
done when a file is first read, and if a bootstrap option
is provided, then random seed is changed during every
iteration. The change of the seed is deterministic, and
hence all of the results are exactly reproducible. At the
initialization step, there is a single batch forward pass

5 Conclusions
The great success of classical machine learning algorithms in tasks such as classification, together with the
expectation that quantum computers will allow us to explore algorithms in a larger complexity class than their
classical counterparts, makes the exploration of the connections between these ideas a fertile ground for the
discovery of novel approaches to automated inference.
8

In this work we have presented SQUID as a computational framework which allows to explore efficiently the
possible advantages of quantum computing for machine
learning purposes. This is achieved by embedding the
quantum algorithm part in a more general multi-layer
architecture that allows to interface classical and quantum networks while enjoying efficient optimization by
using the automatic differentiation engine provided by
PyTorch. While there are similar packages (notably
Xanadu's Strawberry Fields [23], IBM's Qiskit [12]),
they do not offer as much flexibility as SQUID. For example, Strawberry Fields offers much more complete
experience, with many examples and models, as well
as ability to run code on quantum computer, and not
a simulator. However, for the same reason, creating a
custom model is much easier in SQUID. Similar argument can be made about Qiskit, with an addition that
it does not contain bindings to PyTorch. This generalized frameworks provides several advantages over an
either purely classical or purely quantum approach: it
allows for a seamless dimensionality reduction of the
inference problem, a step that would be necessary to
explore high dimensional datasets on small quantum
devices, while at the same time allowing for automatic
tuning of the measurement settings needed to extract
information from the quantum state produced by the
algorithm. This latter feature, implemented in SQUID
by using a classical network as decoder after the central model as shown in Fig. 1, is extremely important
in order to reach high precisions. The use of an explicit decoder at the output of the quantum model allows for a more careful optimization of the trade-off
between measuring only vanishingly small fraction of
the possible output probabilities on one hand, with the
drawback that entanglement can start to be detrimental
due to information scrambling (see eg. [24, 25]), and a
full measurement of the probability distribution on the
computational basis states which will require an exponential number of repetitions. In this work we used a
simple classification problem from the MNIST database
to show the effect of this tradeoff for a concrete highdimensional problem.

been identified with a simplified approach as the one
currently implemented in SQUID, a successive study of
the sample complexity along the lines of the derivation
presented in Sec. 2.3 will be needed to assess the practical viability of the algorithm. Extensions to implement the effect of finite statistics, together with more
advanced effects such as models of decoherence for a
specific target device, can be added efficiently within
SQUID and we plan to explore their impact on classification problems in future work.

Acknowledgements
The work of A. Roggero was supported by the InQubator for Quantum Simulation under U.S. DOE grant No.
DE-SC0020970 and by the Institute for Nuclear Theory
under U.S. Department of Energy grant No. DE-FG0200ER41132. The work of J. Filipek was supported in
part by a Washington Research Foundation Fellowship
at the University of Washington. The work of S.-C. Hsu
is supported by the U.S. Department of Energy, Office
of Science, Office of Early Career Research Program under Award number DE-SC0015971. Support for Nathan
Wiebe was provided by the Laboratory Directed Research and Development Program at Pacific Northwest
National Laboratory, a multi-program national laboratory operated by Battelle for the U.S. Department of
Energy, Release No. PNNL-SA-157287 and the theoretical work on this project by NW was supported by the
U.S. Department of Energy, Office of Science, National
Quantum Information Science Research Centers, CoDesign Center for Quantum Advantage under contract
number DE-SC0012704. Additional support for Nathan
Wiebe was provided by Google Research Award.

References
[1] Vedran Dunjko and Peter Wittek. A non-review
of Quantum Machine Learning: trends and explorations. Quantum Views, 4:32, March 2020.
DOI: 10.22331/qv-2020-03-17-32. URL https://
doi.org/10.22331/qv-2020-03-17-32.
[2] Nathan Wiebe. Key questions for the quantum machine learner to ask themselves. New
Journal of Physics, 22(9):091001, sep 2020.
DOI: 10.1088/1367-2630/abac39. URL https://
doi.org/10.1088/1367-2630/abac39.
[3] Wen Guan, Gabriel Perdue, Arthur Pesah, Maria
Schuld, Koji Terashi, Sofia Vallecorsa, and JeanRoch Vlimant. Quantum machine learning in
high energy physics. Machine Learning: Science
and Technology, 2(1):011003, Mar 2021. ISSN

Thanks to the generality of the architecture developed in this work, future explorations of algorithms
with entanglement but with a classically efficient representation (such as tensor network states with polynomially large bond dimension, see eg. [26, 27]) could be
carried out within SQUID with only minimal modifications to the code. We expect the added flexibility in
interchanging classical and quantum components in a
global classifier to prove valuable in identifying promising datasets and inference problems where the presence
of entanglement and quantum correlations can provide
important accuracy gains. Once these problems have
9

[4]

[5]

[6]

[7]

[8]

[9]

[10]

2632-2153. DOI: 10.1088/2632-2153/abc17d. URL
http://dx.doi.org/10.1088/2632-2153/abc17d.
Patrick Rebentrost, Masoud Mohseni, and Seth
Lloyd. Quantum support vector machine for big
data classification. Phys. Rev. Lett., 113:130503,
Sep 2014. DOI: 10.1103/PhysRevLett.113.130503.
URL
https://link.aps.org/doi/10.1103/
PhysRevLett.113.130503.
Jacob Biamonte, Peter Wittek, Nicola Pancotti,
Patrick Rebentrost, Nathan Wiebe, and Seth
Lloyd. Quantum machine learning. Nature, 549:
195–202, 2017. DOI: 10.1038/nature23474. URL
https://doi.org/10.1038/nature23474.
Maria Schuld and Nathan Killoran.
Quantum machine learning in feature hilbert
spaces.
Phys. Rev. Lett., 122:040504, Feb
2019.
DOI: 10.1103/PhysRevLett.122.040504.
URL
https://link.aps.org/doi/10.1103/
PhysRevLett.122.040504.
Vojtěch Havlı́ček, Antonio D. Córcoles, Kristan
Temme, Aram W. Harrow, Abhinav Kandala,
Jerry M. Chow, and Jay M. Gambetta. Supervised learning with quantum-enhanced feature
spaces. Nature, 567(7747):209–212, Mar 2019.
ISSN 1476-4687. DOI: 10.1038/s41586-019-09802.
URL http://dx.doi.org/10.1038/s41586019-0980-2.
Evan Peters, João Caldeira, Alan Ho, Stefan Leichenauer, Masoud Mohseni, Hartmut
Neven, Panagiotis Spentzouris, Doug Strain, and
Gabriel N. Perdue. Machine learning of high dimensional data on a noisy quantum processor.
arXiv, (2101.09581), 2021.
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing
Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris
Olah, Mike Schuster, Jonathon Shlens, Benoit
Steiner, Ilya Sutskever, Kunal Talwar, Paul
Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin
Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine
learning on heterogeneous systems, 2015. URL
https://www.tensorflow.org/. Software available from tensorflow.org.
Adam Paszke, Sam Gross, Francisco Massa,
Adam Lerer, James Bradbury, Gregory Chanan,
et al.
Pytorch: An imperative style, highperformance deep learning library. In H. Wallach,

[11]

[12]
[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]

10

H. Larochelle, A. Beygelzimer, F. d'Alché-Buc,
E. Fox, and R. Garnett, editors, Advances in
Neural Information Processing Systems 32, pages
8026–8037. Curran Associates, Inc., 2019. URL
http://papers.nips.cc/paper/9015-pytorchan-imperative-style-high-performancedeep-learning-library.pdf.
Robert S. Smith, Michael J. Curtis, and William J.
Zeng. A practical quantum instruction set architecture, 2016.
Héctor Abraham et al. Qiskit: An open-source
framework for quantum computing, 2019.
Quantum AI team and collaborators. Cirq, October 2020.
URL https://doi.org/10.5281/
zenodo.4062499.
Charles R. Harris, K. Jarrod Millman, St'efan J.
van der Walt, Ralf Gommers, Pauli Virtanen,
David Cournapeau, et al. Array programming
with NumPy. Nature, 585(7825):357–362, September 2020. DOI: 10.1038/s41586-020-2649-2. URL
https://doi.org/10.1038/s41586-020-2649-2.
Gilles Brassard, Peter Høyer, Michele Mosca, and
Alain Tapp. Quantum amplitude amplification
and estimation. Quantum Computation and Information, page 53–74, 2002. ISSN 0271-4132.
DOI: 10.1090/conm/305/05215. URL http://
dx.doi.org/10.1090/conm/305/05215.
Yohichi Suzuki, Shumpei Uno, Rudy Raymond,
Tomoki Tanaka, Tamiya Onodera, and Naoki Yamamoto. Amplitude estimation without phase estimation. Quantum Information Processing, 19(2),
Jan 2020. ISSN 1573-1332. DOI: 10.1007/s11128019-2565-2. URL http://dx.doi.org/10.1007/
s11128-019-2565-2.
Dmitry Grinko, Julien Gacon, Christa Zoufal, and
Stefan Woerner. Iterative quantum amplitude estimation. npj Quantum Information, 7(1), Mar
2021. ISSN 2056-6387. DOI: 10.1038/s41534021-00379-1. URL http://dx.doi.org/10.1038/
s41534-021-00379-1.
András Gilyén, Srinivasan Arunachalam, and
Nathan Wiebe. Optimizing quantum optimization
algorithms via faster quantum gradient computation. In Proceedings of the Thirtieth Annual ACMSIAM Symposium on Discrete Algorithms, pages
1425–1444. SIAM, 2019.
Maria Schuld, Alex Bocharov, Krysta M. Svore,
and Nathan Wiebe.
Circuit-centric quantum
classifiers.
Phys. Rev. A, 101:032308, Mar
2020.
DOI: 10.1103/PhysRevA.101.032308.
URL
https://link.aps.org/doi/10.1103/
PhysRevA.101.032308.
G. Vidal and C. M. Dawson.
Universal
quantum circuit for two-qubit transformations

[21]

[22]

[23]

[24]

[25]

[26]

[27]

[28]

[29]

A Gradient evaluation for variational
quantum models

with three controlled-not gates.
Phys. Rev.
A, 69:010301, Jan 2004. DOI: 10.1103/PhysRevA.69.010301. URL https://link.aps.org/
doi/10.1103/PhysRevA.69.010301.
Farrokh Vatan and Colin Williams. Optimal quantum circuits for general two-qubit gates. Phys.
Rev. A, 69:032315, Mar 2004. DOI: 10.1103/PhysRevA.69.032315. URL https://link.aps.org/
doi/10.1103/PhysRevA.69.032315.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.
Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–
2324, 1998. DOI: 10.1109/5.726791.
Nathan Killoran, Josh Izaac, Nicolás Quesada,
Ville Bergholm, Matthew Amy, and Christian
Weedbrook. Strawberry fields: A software platform for photonic quantum computing. Quantum, 3:129, Mar 2019.
ISSN 2521-327X.
DOI: 10.22331/q-2019-03-11-129. URL http://
dx.doi.org/10.22331/q-2019-03-11-129.
Huitao Shen, Pengfei Zhang, Yi-Zhuang You, and
Hui Zhai. Information scrambling in quantum neural networks. Phys. Rev. Lett., 124:200504, May
2020.
DOI: 10.1103/PhysRevLett.124.200504.
URL
https://link.aps.org/doi/10.1103/
PhysRevLett.124.200504.
Carlos Ortiz Marrero, Mária Kieferová, and
Nathan Wiebe. Entanglement induced barren
plateaus, 2021.
Ding Liu, Shi-Ju Ran, Peter Wittek, Cheng
Peng, Raul Blázquez Garcı́a, Gang Su, and
Maciej Lewenstein. Machine learning by unitary tensor network of hierarchical tree structure.
New Journal of Physics, 21(7):073059, jul 2019.
DOI: 10.1088/1367-2630/ab31ef. URL https://
doi.org/10.1088/1367-2630/ab31ef.
Chase Roberts, Ashley Milsted, Martin Ganahl,
Adam Zalcman, Bruce Fontaine, Yijian Zou, Jack
Hidary, Guifre Vidal, and Stefan Leichenauer. Tensornetwork: A library for physics and machine
learning, 2019.
K. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii.
Quantum circuit learning.
Physical Review A, 98(3), Sep 2018.
ISSN 2469-9934.
DOI: 10.1103/physreva.98.032309. URL http://
dx.doi.org/10.1103/PhysRevA.98.032309.
Maria Schuld, Ville Bergholm, Christian Gogolin,
Josh Izaac, and Nathan Killoran. Evaluating analytic gradients on quantum hardware. Physical Review A, 99(3), Mar 2019. ISSN 2469-9934.
DOI: 10.1103/physreva.99.032331. URL http://
dx.doi.org/10.1103/PhysRevA.99.032331.

In this appendix we provide a derivation of the scheme
employed in the main text to evaluate gradients with
respect to the parameters of the quantum models. Following the discussion in Sec. 2.5 of the main text, a
generic variational quantum model is defined by a unitary transformation W (w)
~ dependent on M real parameters w
~ taking values in [0, π). The general construction employed in this work consists in decomposing the full unitary operation W (w)
~ into a combination
of one and two qubit unitaries denoted by Uk1 (α, β, γ)
2
and Ujk
(θ, φ, η) respectively. The subscript indices indicate the qubit (or pairs of qubits) the unitary acts
upon. Given the simple structure of a generic circuit as
the one depicted in Eq. (2.5), one can obtain a closed
form expression for every component of the gradient by
looking at the individual gradients of the basic unitaries
U 1 and U 2 directly. Here we will use as a concrete example the generic SU (4) unitary transformation from
Eq. (2.5) which we reproduce here for convenience
U01 (α0 , β0 , γ0 )
U11 (α1 , β1 , γ1 )

2
U01
(θ, φ, η)

U01 (α2 , β2 , γ2 )

.

U11 (α3 , β3 , γ3 )

Note that in this case M = 15. In the following we
will assume, without loss of generality, that this unitary
operation is applied to the initial state |00i of the two
qubit register and denote the resulting state by
|Φ (w)i
~ = W (w)
~ |0i .

(16)

The classical output generated by a measurement on
the qubit register can be completely characterized by
the 4 probabilities to find the system in each one of the
possible basis states:
pk = T r [Πk |Φ (w)ihΦ
~
(w)
~ |]

(17)

where we have introduced explicitly the projectors
Π0 =|00ih00| Π1 =|01ih01|
Π2 =|10ih10| Π3 =|11ih11| .

(18)

Computing the derivatives with respect to the 12 angles corresponding to the 4 one qubit SU (2) operations
is straightforward by recalling the definition Eq. (14) of
U 1 in terms of exponentials of Pauli operators. As an
example, the derivative with respect to γ0 of any of the
11

probabilities in Eq. (17) can be written explicitly as

model
cA
cB
cC
cD
cE
cF

∂
∂
pk =
h00|W † (w)Π
~ k W (w)|00i
~
∂γ0
∂γ0
=ih00|W † (w)Π
~ k W (w)Y
~ 0 |00i
− ih00|Y0 W † (w)Π
~ k W (w)|00i
~


†
= − 2I h00|W (w)Π
~ k W (w)Y
~ 0 |00i


∂W (w)
~
=2R h00|W † (w)Π
~ k
|00i ,
∂γ0

(19)

(20)

TR90
0.9966
0.9973
0.9980
0.9994
1
1

VR90
0.9951
0.9947
0.9967
0.9952
0.9950
0.9964

rekm = R [hk|W (w)
~ 0m |00i]

eikm = I [hk|W (w)
~ 0m |00i] ,
(23)
where we used the compact notation W (w)
~ 0m to indicate
the derivative with respect to the m-th parameter. This
requires a total of 2KM independent circuit executions
for a total of 2K(M + 1) observables. The gradient can
then be computed as

(21)


α2 , β2 , γ2 , α3 , β3 , γ3

Accuracy
0.9915+4
−28
0.9903+20
−18
0.9881+44
−2
0.9895+8
−4
0.9911+12
20
0.9891+16
20

with |ki the computational basis state associated to the
projector Πk . These expectation values can be estimated using an Hadamard test with one additional ancilla qubit and require the execution of 2K independent
circuits (one each for real and imaginary part).
For each one of the M parameters, we then use additional 2K Hadamard tests to estimate the expectation
values associated with the shifted unitaries

with a new set of parameters given by

π
w
~ 0 (γ0 ) = α0 , β0 , γ0 + , α1 , β1 , γ1 ,
2
θ, φ, η,

Ktot
2366
4727
9449
18893
31485
47225

Table 2: Results for the classical feed-forward models described
in the main text.

where I (R) indicating the imaginary (real) part. Note
that, for all of the 12 parameters characterizing the single qubit transformations, the derivative of the full variational circuit unitary W can be expressed in terms of
the same parametrized unitary with the appropriate angle angle shifted by π/2. For the case of γ0 considered
above we have for instance:
∂W (w)
~
= iW (w)Y
~ 0 = W (w
~ 0 (γ0 ))
∂γ0

M0
3
6
12
24
40
60

.



∂
pk =2R h00|W † (w)Π
~ k W (w)
~ 0m |00i
∂wm


=2 rk rekm − ikeikm .

Using the optimal implementation for the more general
SU (4) transformation derived in Ref. [21] (see Fig.6
there) one can show that we have the same property
2
for the 2 qubit unitary Ujk
. This property is usually
referred to as the parameter shift rule [28, 29].
In order to estimate the expectation values in the
last line of Eq. (19) we can employ two strategies: if
the required number of output probabilities K is the
maximum possible one with n qubits (ie. K = 2n ), it
is convenient to first decompose the projectors in the
computational basis states into a linear combination of
K = 2n diagonal operators obtained by considering all
the possible tensor products of identities and Pali Z, and
then to evaluate each one of the resulting expectation
values using a single Hadamard test each. The total
number of separate circuits required for this approach
is then KM , with M the total number of parameters.
In the more realistic situation where K fi 2n instead,
the strategy just described will still require an exponential number of measurement in the size of the qubit register. A more efficient alternative can be obtained by
evaluating explicitly the K pairs of expectation values




rk = R h00|W † (w)|ki
~
ik = I h00|W † (w)|ki
~
,
(22)

(24)

An alternative approach to reduce the number of independent circuits needed for gradient evaluation is to
use expectation values of unitary operators instead of
projectors. This extension can be easily implemented
within the SQUID framework.

B Additional information on the MNIST
benchmark
We report in Tab. 2 the parameters and results for the
classical models used in the MNIST classification discussed in Sec. 3 and corresponding to the results presented in Fig. 3 on the main text. The last two columns
in Tab. 2 denoted by TR90 and VR90 show the boundary value for the 90% accuracy percentile, the latter
refers to the validation data while the former to the
training set. The estimated errors correspond to a 68%
confidence interval.
The parameters of the separable quantum models
considered in the main text, together with the results
12

Accuracy
0.9837+30
−36
0.9847+32
−32
0.9854+16
−20
0.9859+26
−18
0.9839+26
−16
0.9869+24
−38
0.98125+38
−34

TR90
0.9898
0.9920
0.9921
0.9948
0.9905
0.9948
0.9881

Training Set

VR90
0.9872
0.9889
0.9887
0.9891
0.9868
0.9904
0.9858

M0
9
9
15
15
30
30
39
39
57
57
45
45

M1
4
1
4
1
16
1
16
1
16
1
64
1

Ktot
7070
7068
11780
11778
23567
23553
30632
30618
44762
44748
35390
35328

Accuracy
0.9879+12
−24
0.9865+22
−18
0.9875+18
−24
0.9859+24
−16
0.9889+28
−22
0.9847+24
−34
0.9885+24
−22
0.9833+18
−30
0.9883+22
−24
0.9810+24
−16
0.9895+18
−16
0.9823+32
−34

TR90
0.9940
0.9933
0.9960
0.9926
0.9987
0.9897
0.9984
0.9867
0.9993
0.9863
0.9995
0.9871

1

0.98

0.98

0.96

0.96

0.94

0.94

Full Output

0.92

0.92

1

Table 3: Results for the first set of separable quantum models
described in the text. The parenthesis in the label for the
quantum models indicates the number of qubits N employed.
model
qE(2)
qE1 (2)
qF (2)
qF1 (2)
qG(4)
qG1 (4)
qH(4)
qH1 (4)
qI(4)
qI1 (4)
qL(6)
qL1 (6)

Validation Set

1

1

0.98

0.98

0.96

0.96

Single Output

0.94
0

VR90
0.9899
0.9892
0.99
0.9893
0.9924
0.9879
0.9915
0.9854
0.9911
0.9840
0.9920
0.9867

20

40

0.94
80

60

0

20

Training Epochs

40

60

80

100

Training Epochs

Figure 7: Example of training of the largest quantum model
described in the main text. The left panels show the increase
in classification accuracy for the training set as a function of
the number of epochs. The right panels shows the same for
the validation set. The top panels are for the full model with
the maximum number of outputs M1 = 16 while the bottom
panels are for the minimal model with a single output. The
dashed black line indicates 99% accuracy. All bands are 90%
confidence intervals with means indicated by the solid lines.

25

1
(a)

Table 4: Results for the set of quantum models described in
the text. The parenthesis in the label for the quantum models
indicates the number of qubits N employed.

Median [CL]
Median [QM - full]
Median [QM - min]

0.995

4 qubit [MAX] - full

20
15

obtained from training on the MNIST classification
problem, are presented in Tab. 3. The models with a
latent space corresponding to the restricted output for
the quantum layer are indicated with a subscript 1 in
the table.
The same convention is used in Tab. 4 where we
present the parameters and results for the quantum
models with entanglement described in Sec. 3.2. We
also show in Fig. 7 the evolution of the accuracy for both
the training set (left panels) and validation set (right
panels). The top two panels are obtained with models
with maximal output size (M1 = 16 in this case) while
the bottom panels show the results using a restricted
output model with M1 = 1.
Finally, we present in Fig. 8 an extension of the results
presented in Fig. 6 where in the left panels we show the
accuracy histograms for the largest model considered in
this work for both the full output model (panel(c)) and
the restricted output model (panel(d)).

10

0.985
0.98
0.975
0.97
0.965

5

(c)
90% accuracy boundary

Accuracy

0.99

0

4

0

2×10

0
25

4

4×10

1

4 qubit [MAX] - min

20

0.995

15

0.99
(b)

10

0.985

Number of parameters

1×10

4

2×10

4

Bootstrap runs

Ktot
2358
4715
4713
9437
9423
14195
14133

(d)

3×10

4

4×10

Number of parameters

4

5

Bootstrap runs

M1
2
4
1
16
1
64
1

Accuracy

M0
3
6
6
12
12
18
18

Accuracy

model
qA(1)
qB(2)
qB1 (2)
qC(4)
qC1 (4)
qD(6)
qD1 (6)

0
0.975 0.98 0.985 0.99 0.995 1

Accuracy

Figure 8: Results for the accuracy achieved on MNIST with
the classical model (green points) and the quantum models
with entanglement described in the text (red and blue points).
Panel (a) shows the accuracy as a function of the number of
parameters for the classical model (green solid circles), the full
quantum models (blue solid circles) and the quantum models
with a single output variable M1 = 1 (red solid circles) indicated by "min". The inset panel (b) shows the location of
the 90% accuracy percentile for the classical (green squares),
full quantum (blue squares) and minimal quantum model (red
squares) with a single output. Panels (c) and (d) show the histogram of achieved accuracies for the largest quantum model
considered using N = 4 qubits with either the full number of
possible output variables (M1 = 16, top panel) and the single
output (bottom panel).

13

