Self-supervised Augmentation Consistency
for Adapting Semantic Segmentation

arXiv:2105.00097v1 [cs.CV] 30 Apr 2021

1

Nikita Araslanov1
Stefan Roth1,2
Department of Computer Science, TU Darmstadt

Abstract

mIoU

1. Introduction
Unsupervised domain adaptation (UDA) is a variant of
semi-supervised learning [6], where the available unlabelled data comes from a different distribution than the annotated dataset [4]. A case in point is to exploit synthetic
data, where annotation is more accessible compared to the
costly labelling of real-world images [59, 60]. Along with
some success in addressing UDA for semantic segmentation [67, 69, 80, 91], the developed methods are growing
increasingly sophisticated and often combine style transfer networks, adversarial training or network ensembles
[39, 46, 68, 77]. This increase in model complexity impedes
reproducibility, potentially slowing further progress.
In this work, we propose a UDA framework reaching
state-of-the-art segmentation accuracy (measured by the
Intersection-over-Union, IoU) without incurring substantial
training efforts. Toward this goal, we adopt a simple semisupervised approach, self-training [12, 42, 91], used in recent works only in conjunction with adversarial training or
Code is available at https://github.com/visinf/da-sac.

hessian.AI

Mean IoU on Cityscapes (val) after adaptation from GTA5 with VGG-16
49.9

50

We propose an approach to domain adaptation for semantic segmentation that is both practical and highly accurate. In contrast to previous work, we abandon the use
of computationally involved adversarial objectives, network
ensembles and style transfer. Instead, we employ standard
data augmentation techniques ‚Äì photometric noise, flipping
and scaling ‚Äì and ensure consistency of the semantic predictions across these image transformations. We develop
this principle in a lightweight self-supervised framework
trained on co-evolving pseudo labels without the need for
cumbersome extra training rounds. Simple in training from
a practitioner's standpoint, our approach is remarkably effective. We achieve significant improvements of the state-ofthe-art segmentation accuracy after adaptation, consistent
both across different choices of the backbone architecture
and adaptation scenarios.

2

Self-training
# number of rounds
Adversarial training

48

Ensemble

46

43.6

2019
44

1

42.3
3

42.4
2

PIT
[53]

FDA
[80]

TIR
[39]

42.2

42
39.0
40
38
36

37.2
1

4

1

Style transfer
46.5
6

Ours
44.9

43.8

SA-I2I
[55] CD-AM
[78]
LDR FADA
[77] [70]
1

LSE [65]
2020

2021

PyCDA [47]

Figure 1. Results preview. Unlike much recent work that combines multiple training paradigms, such as adversarial training and
style transfer, our approach retains the modest single-round training complexity of self-training, yet improves the state of the art for
adapting semantic segmentation by a significant margin.

network ensembles [17, 39, 54, 70, 80, 87, 86]. By contrast,
we use self-training standalone. Compared to previous selftraining methods [9, 43, 65, 91, 92], our approach also
sidesteps the inconvenience of multiple training rounds, as
they often require expert intervention between consecutive
rounds. We train our model using co-evolving pseudo labels
end-to-end without such need.
Our method leverages the ubiquitous data augmentation
techniques from fully supervised learning [11, 85]: photometric jitter, flipping and multi-scale cropping. We enforce
consistency of the semantic maps produced by the model
across these image perturbations. The following assumption formalises the key premise:
Assumption 1. Let f : I ‚Üí M represent a pixelwise
mapping from images I to semantic output M. Denote
œÅ : I ‚Üí I a photometric image transform and, similarly, œÑ0 : I ‚Üí I a spatial similarity transformation,
where , 0 ‚àº p(*) are control variables following some predefined density (e.g., p ‚â° N (0, 1)). Then, for any image
I ‚àà I, f is invariant under œÅ and equivariant under œÑ0 ,
i.e. f (œÅ (I)) = f (I) and f (œÑ0 (I)) = œÑ0 (f (I)).
Next, we introduce a training framework using a momentum
network ‚Äì a slowly advancing copy of the original model.

To appear in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), virtual, 2021.
¬© 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works.

The momentum network provides stable, yet recent targets
for model updates, as opposed to the fixed supervision in
model distillation [15, 87, 86]. We also re-visit the problem
of long-tail recognition in the context of generating pseudo
labels for self-supervision. In particular, we maintain an
exponentially moving class prior used to discount the confidence thresholds for those classes with few samples and
increase their relative contribution to the training loss. Our
framework is simple to train, adds moderate computational
overhead compared to a fully supervised setup, yet sets a
new state of the art on established benchmarks (cf. Fig. 1).

Features

PIT
[53]

Adversarial training
1-round training
SOTA-VGG
SOTA-ResNet

3

LDR
[77]

SA-I2I
[55]

IAST
[54]

RPT
[83]

3

3

3

3

3
3

(6)
3

(3)

(3)

3

3

Ours

3
3
3

Table 1. Relation to state of the art. Previous work reaches the
state of the art in terms of IoU either with VGG-16 (SOTA-VGG)
or ResNet-101 (SOTA-ResNet). Our framework uses neither adversarial training nor multiple training rounds (given in parentheses), yet outperforms the state of the art consistently in both cases.

2. Related Work
Self-training on pseudo labels. As a more computationally lightweight approach, self-training seeks high-quality
pseudo supervision coming in the form of class predictions
with high confidence. Our work belongs to this category.
Most of such previous methods pre-compute the labels 'offline', used subsequently to update the model, and repeat
this process for several rounds [43, 65, 91, 92]. More recent frameworks following this strategy have a composite
nature: they rely on adversarial (pre-)training [14, 20, 86],
style translation [17, 80] or both [54, 46, 39, 70, 73].
Training on co-evolving pseudo labels can be computationally unstable, hence requires additional regularisation.
Chen et al. [13] minimise the entropy with improved behaviour of the gradient near the saturation points. Using
fixed representations, be it from a 'frozen' network [15, 86],
a fixed set of global [53] or self-generated local labels
[47, 68, 81], further improves training robustness.
Overconfident predictions [28] have direct consequences
for the quality of pseudo labels. Zou et al. [92] attain some
degree of confidence calibration via regularising the loss
with prediction smoothing akin to temperature scaling [28].
Averaging the predictions of two classifiers [87], or using
Dropout-based sampling [7, 88], achieves the same goal.

Most of the work on scene adaptation for semantic segmentation has been influenced by a parallel stream of work
on domain adaptation (DA) and semi-supervised learning
for image classification [23, 24, 27, 45, 50]. The main idea
behind these methods is to formulate an upper bound on the
target risk using the so-called H‚àÜH-divergence [3]. In a
nutshell, it defines the discrepancy between the marginals
of the source and target data by means of a binary classifier.
In the following, we briefly review implementation variants
of this idea in the context of semantic segmentation.
Learning domain-invariant representations.
Adversarial feature alignment follows the GAN framework [24, 26] and minimises the gap between the source
and target feature representations in terms of some distance
(e.g., Wasserstein in [41]). The discriminator can be employed at multiple scales [15, 67, 77] and use local spatial priors [83]; it can be conditional [33] and class-specific
[22, 52], or align the features of 'hard' and 'easy' target
samples [56]. Often, self-supervised losses, such as entropy
minimisation [69], or a 'conservative loss' [90] assist in this
alignment.
The alternative to adversarial feature alignment are more
interpretable constraints, such as feature priors [51], bijective source-target association [37] or aligning the domains
directly in the image space with style transfer [89] used either alone [74] or, most commonly, jointly with adversarial feature alignment [8, 16, 25, 55, 78, 79, 82]. One issue with style translation is to ensure semantic consistency
despite the changes in appearance. To address this, Hoffman et al. [32] use semantic and cycle-consistency losses,
while Yang et al. [77] reconstruct the original image from
its label-space representation.
These methods tend to be computationally costly and
challenging to train, since they require concurrent training
of one or more independent networks, e.g. discriminators or
style transfer networks. Although Yang and Soatto [80] obviate the need for style networks by incorporating the phase
of a Fourier-transformed target image into a source sample,
multiple networks have to be trained, each with its own predefined phase band.

Spatial priors. Different from DA for classification, the
characteristic feature of adaptation methods for segmentation is the use of spatial priors. Local priors have been
enforced patch-wise [15, 47, 68] and in the form of precomputed super-pixels [81, 83]. Although global spatial
priors have also been used [91], their success hinges on the
similarity of the semantic layout in the current benchmarks.
Relation to our approach. As shown in Table 1, our work
streamlines the training process. First, we do not use adversarial training, as feature invariance alone does not guarantee label invariance [36, 84]. Second, we train our model
with co-evolving pseudo labels in one round. Our framework bears resemblance to the noisy mean teacher [76] and
combines consistency regularisation [2, 61, 64, 75] with
self-ensembling [40, 66]. Similar approaches have been explored in medical imaging [44, 58] and concurrent UDA
work [71], albeit limited in the scope of admissible aug2

masks
Momentum Net
Multi-scale
crops & flips

Multi-scale
fusion

Input Sample

Input Batch

Output Masks

Fused Prediction

pseudo labels

momentum updates

gradient
photometric
‚àº
noise

Segmentation Net
masks

Lt
target loss

(a) Framework overview

(b) Multi-scale crops and flips

(c) Multi-scale fusion

Figure 2. Overview. The segmentation network in our framework (a) maintains a slow copy of itself, the momentum network, which
provides stable targets for self-supervision. In addition to encouraging semantic invariance w.r.t. the photometric noise, we facilitate
consistent predictions across multiple scales and flips by first (b) feeding random multi-scale crops and flips to the momentum network and
then (c) fusing the predictions by simple averaging to produce the pseudo-supervision targets.

mentations. We leverage photometric invariance, scale and
flip equivariance [72] to extract high-fidelity pseudo supervision instead of more computationally expensive sampling
techniques [38]. Contrary to [65], we find that scale alone
is not predictive of the label quality, hence we average the
predictions produced at multiple scales and flips. This parallels uncertainty estimation using test-time augmentation
[1], but at training time [5].

put to the networks. Fig. 2b demonstrates this process. Following the noisy student model in image classification [76],
the input to the segmentation network additionally undergoes a photometric augmentation: we add random colour
jitter and smooth the images with a Gaussian filter at random. The momentum network, on the other hand, receives
a 'clean' input, i.e. without such augmentations. This is to
encourage model invariance to photometric perturbations.

3.3. Self-supervision

3. Self-Supervised Augmentation Consistency

Multi-scale fusion. We re-project the output masks from
the momentum network back to the original image canvas
of size h √ó w, as illustrated in Fig. 2c. For each pixel, the
overlapping areas average their predictions. Note that some
pixels may lie outside the crops, hence contain the result
of a single forward pass with the original image. We keep
these predictions intact. The merged maps are then used to
extract the pseudo masks for self-supervision.
A short long-tail interlude. Handling rare classes (i.e.
classes with only a few training samples) is notoriously difficult in recognition [29]. For semantic segmentation, we
here distinguish between the classes with low image-level
(e.g., "truck", "bus") and pixel-level (e.g., "traffic light",
"pole") frequency. While generating self-supervision, we
take special care of these cases and encourage (i) lower
thresholds for selecting their pseudo labels, (ii) increased
contributions to the gradient with a focal loss, and (iii) employ importance sampling. We describe these in detail next.
Sample-based moving threshold. Most previous work
with self-training employs multi-round training that requires interrupting the training process and re-generating
the pseudo labels [43, 46, 54, 65, 91]. One of the reasons is the need to re-compute the thresholds for filtering
the pseudo labels for supervision, which requires traversing the predictions for the complete target dataset with the
model parameters fixed. In pursuit of our goal of enabling
end-to-end training without expert intervention, we take a
different approach and compute the thresholds on-the-go.
As the main ingredient, we maintain an exponentially moving class prior. In detail, for each softmax prediction of the
momentum network, we first compute a prior estimate of

3.1. Framework overview
Shown in Fig. 2a, our framework comprises a segmentation network, which we intend to adapt to a target domain,
and its slowly changing copy updated with a momentum,
a momentum network. To perform self-supervised scene
adaptation, we first supply a batch of random crops and
horizontal flips from a sample image of the target domain
to both networks. For each pixel we average the predictions
(i.e. semantic masks) from the momentum network after the
appropriate inverse spatial transformation. We then create
a pseudo ground truth by selecting confident pixels from
the averaged map using thresholds based on running statistics, which are capable of adapting to individual samples.
Finally, the segmentation network uses stochastic gradient
descent to update its parameters w.r.t. these pseudo labels.
Our approach closely resembles the mean teacher framework [23, 66] and temporal ensembling [35, 40]. However,
as we will show empirically, the ensembling property itself plays only an auxiliary role. More importantly, akin
to the critic network in reinforcement learning [48] and the
momentum encoder in unsupervised learning [30], our momentum network provides stable targets for self-supervised
training of the segmentation network. This view allows us
to focus on the target-generating process, detailed next.

3.2. Batch construction
For each sampled target image, we generate N crops
with random scales, flips and locations, but preserving the
aspect ratio. We re-scale the crops as well as the original image to a fixed input resolution h√ów and pass them as the in3

Œ∏c,n

the probability that a pixel in sample n belongs to class c as
1 X
œác,n =
mc,n,i,j ,
(1)
hw i,j

0.8

0.2
0

(2)

0

0.02

0.03

0.04

œác

3.4. Training

(4)

Pre-training with source-only loss. Following [47, 83],
we use Adaptive Batch Normalisation (ABN) [45] to jumpstart our model on the segmentation task by minimising the
cross-entropy loss on the source data only. In our experiments, we found it unnecessary to re-compute the mean
and the standard deviation only at the end of the training.
Instead, in pre-training we alternate batches of source and
target images, but ignore the loss for the latter. For a target
batch, this implies updating the running mean and the standard deviation in the Batch Normalisation (BN) [34] layers
and leaving the remaining model parameters untouched.
Importance sampling. Our loss function in Eq. (6) accounts for long-tail classes with a high image frequency
(e.g., "traffic light", "pole"), and may not be effective for
the classes appearing in only few samples (e.g., "bus",
"train"). To alleviate this imbalance, we use importance
sampling [21] and increase the sample frequency of these
long-tail classes. We minimise the expected target loss by
re-sampling the target images using the density pt :


min En‚àºpt Ltn (œÜ) .
(7)

Fig. 3 plots Eq. (3) as a function of the moving class prior œác
for a selection of Œ≤. For predominant classes (e.g., "road"),
the exponential term has nearly no effect; the threshold is
static w.r.t. the peak class confidence, i.e. Œ∏c,n ‚âà Œ∂m‚àóc,n .
However, for long-tail classes such that œác ‚âà Œ≤, the threshold is lower than this upper bound, hence more pixels for
these classes are selected for supervision. To obtain the
pseudo labels, we apply the threshold Œ∏c,n to the peak predictions of the merged output from the momentum network:
(
c‚àó
mc‚àó ,n,i,j > Œ∏c,n
(5)
mÃÇn,i,j =
ignore otherwise,
where c‚àó = arg maxc mc,n,i,j is the dominant class for that
pixel. Note that the pixels with confidence values lower than
the threshold, as well as non-dominant predictions, will be
ignored in the self-supervised loss.
Focal loss with confidence regularisation. Our loss function incorporates a focal multiplier [49] to further increase
the contribution of the long-tail classes in the gradient signal. Unlike previous work [49, 65], however, our moving
class prior œác regulates the focal term:
Ltn (mÃÑ, m | œÜ) = ‚àímc‚àó,n (1 ‚àí œác‚àó )Œª log(mÃÑc‚àó,n ),

0.01

Figure 3. Sample-based moving threshold. Our thresholding
scheme has two hyperparameters, Œ∂ and Œ≤. In this example,
m‚àóc,n = 1 and Œ∂ = 0.75. Predominant classes (e.g., "road") have
œác fl 0, hence their threshold approximates Œ∂m‚àóc,n . Long-tail
classes (e.g., "traffic light") have œác ‚âà 0 and their thresholds are
further reduced with a steepness controlled by Œ≤ (see Eq. 3).

where Œ≤ and Œ∂ are hyperparameters and m‚àóc,n is the predicted peak confidence score for class c, i.e.
i,j

Œ≤ = 0.005
Œ≤ = 0.01
Œ≤ = 0.02

0.4

Our sample-based moving threshold Œ∏c,n takes lower values
when the moving prior œác ‚âà 0 (i.e. for long-tail classes), but
is bounded from above as œác ‚Üí 1. We define it as

Œ∏c,n = Œ∂ 1 ‚àí e‚àíœác /Œ≤ m‚àóc,n ,
(3)

m‚àóc,n = max mc,n,i,j .

Œ∂

0.6

where mc,n,:,: is the mask prediction for class c (with resolution h √ó w). We keep an exponentially moving average
after each training iteration t with a momentum Œ≥œá ‚àà [0, 1]:
œát+1
= Œ≥œá œátc + (1 ‚àí Œ≥œá )œác,n .
c

m‚àó
c,n

1

œÜ

To obtain pt , we use our pre-trained segmentation network
and pre-compute œác,n , the class prior estimate, for each image n using Eq. (1). At training time, we (i) sample a semantic class c uniformly, and then (ii) obtain a target sample
l with probability
œác,l
œáÃÇc,l = P
.
(8)
n œác,n

(6)

where mÃÑ is the prediction of the segmentation network with
parameters œÜ, the pseudo label c‚àó derives from mÃÇ in Eq. (5)
and Œª is a hyperparameter of the focal term. Recall that low
values of œác signify a long-tail category, hence should have
a higher weight. High values of Œª (i.e. > 1) increase the relative weighting on the long-tail classes, while setting Œª = 0
disables the focal term. Note that we also regularise our loss
with the confidence value of the momentum network, mc‚àó ,n
(Eq. 4). In case of an incorrect pseudo label, we expect this
confidence to be low and to regularise the training owing to
its calibration with the multi-scale fusion. We minimise the
loss in Eq. (6), applied for each pixel, w.r.t. œÜ.

This two-step sampling process ensures that all images have
non-zero sample probability owing to the prevalent classes
for which œáÃÇc,l > 0 for all l (e.g., "road" in urban scenes).
Joint target-source training. We train the segmentation
network with stochastic gradient descent using the crossentropy loss for the source and our focal loss for the target
4

(a) Input

(b) Segmentation net output

(c) Momentum net output

(d) Fused prediction

(e) Pseudo labels

Figure 4. Self-supervision example. In this image sample (a) and its crops, the segmentation network (b) tends to mistake the "motorcycle"
for a "bicycle". The momentum network (c) improves on this prediction, but may still produce an inconsistent labelling. Averaging the
predictions over multiple scales (d) corrects this inconsistency, allowing to produce high-precision pseudo labels (e) for self-supervision.

data sampled from pt , as defined by Eqs. (6) and (7). Fig. 4
illustrates the synthesis of pseudo labels. We periodically
update the parameters œà of the momentum network as
œàt+1 = Œ≥œà œàt + (1 ‚àí Œ≥œà )œÜ,

and report the results on the validation split. We measure
the segmentation accuracy with per-class Intersection-overUnion (IoU) and its average, the mean IoU (mIoU).

(9)

4.1. Implementation details

where œÜ are the parameters of the segmentation network. Œ≥œà
regulates the pace of the updates: low values result in faster,
but unstable training, while high Œ≥œà leads to a premature
and suboptimal convergence. We keep Œ≥œà moderate, but
update the momentum network only every T iterations.

We implement our framework in PyTorch [57]. We adopt
DeepLabv2 [10] as the segmentation architecture, and evaluate our method with two backbones, ResNet-101 [31] and
VGG16 [63], following recent work [39, 67, 68, 69, 73].
Both backbones initialise from the models pre-trained on
ImageNet [19]. We first train the models with ABN [45]
(cf. Sec. 3.4), implemented via SyncBN [57], on multiscale crops resized to 640 √ó 640 and a batch size of 16.
Next, training proceeds with the self-supervised target loss
(cf. Sec. 3.3) and the BatchNorm layers [34] frozen. The
batch size of 16 comprises 8 source images and 8 target images at resolution 1024 √ó 512, which is a common practice
[70, 80]. The target batch contains only two image samples
along with 3 random crops each (i.e. N = 3 in Sec. 3.2),
downscaled up to a factor of 0.5. As the photometric noise,
we use colour jitter, random blur and greyscaling (see Appendix B for details). The optimisation uses SGD with a
constant learning rate of 2.5 √ó 10‚àí4 , momentum 0.9 and
weight decay of 5√ó10‚àí4 . We accumulate the gradient in alternating source-target forward passes to keep the memory
footprint in check. Since the focal term in Eq. (6) reduces
the target loss magnitude w.r.t. the source loss, we scale it
up by a factor of 5 (2 for VGG-16). We train our VGGbased framework on two TITAN X GPUs (12GB), while
the ResNet-based variant requires four. This is a substantially reduced requirement compared to recent work (e.g.,

4. Experiments
Datasets. In our experiments we use three datasets. The
Cityscapes dataset [18] contains 2048 √ó 1024 images from
real-world traffic scenes, split into 2975 images for training and 500 for validation. The GTA5 dataset [59] contains 24 966 synthetic scenes with resolution 1914 √ó 1052
and pixelwise annotation aided by the GTA5 game engine.
We also use the SYNTHIA-RAND-CITYSCAPES subset
of the SYNTHIA dataset [60], which contains 9400 synthetic images with resolution 1280 √ó 760 and a semantic
annotation compatible with Cityscapes.
Setup. We adopt the established evaluation protocol from
previous work [47, 67, 69]. The synthetic traffic scenes
from GTA5 [59] and SYNTHIA [60] serve as the source
data, and the real images from the Cityscapes dataset as the
target (obviously ignoring the available semantic labels).
This results in two domain adaptation scenarios depending
on the choice of the source data: GTA5 ‚Üí Cityscapes and
SYNTHIA ‚Üí Cityscapes. As in previous work, at training
time we only use the training split of the Cityscapes dataset
5

Method

road sidew build wall fence pole light sign veg

terr

sky pers ride

car truck bus train moto bicy

mIoU

CyCADA [32]
ADVENT [69]
CBST [91]
PyCDA [47]
PIT [53]
FDA [80]
LDR [77]
FADA [70]
CD-AM [78]
SA-I2I [55]

85.2
86.9
90.4
86.7
86.2
86.1
90.1
92.3
90.1
91.1

31.3
26.4
25.1
28.9
31.8
30.3
37.5
32.6
41.4
40.9

60.7
70.2
70.8
58.8
81.9
73.6
81.4
85.3
78.9
82.3

76.9
81.5
76.9
80.4
80.4
81.7
83.0
83.5
83.1
81.2

0.0
1.6
28.6
6.2
1.2
24.0
26.9
15.2
27.8
33.7

35.4
36.1
36.1
37.2
41.8
42.2
43.6
43.8
44.9
46.5

Baseline (ours)
SAC (ours)

81.5 28.6
90.0 53.1

79.5 23.2 21.1 31.3 28.2 18.5 75.6 14.9 72.2 58.0 17.1 81.1 19.7 26.3 13.7 12.9 2.1
86.2 33.8 32.7 38.2 46.0 40.3 84.2 26.4 88.4 65.8 28.0 85.6 40.6 52.9 17.3 13.7 23.8

37.1
49.9

PyCDA‚Ä† [47]
CD-AM [78]
FADA [70]
LDR [77]
FDA [80]
SA-I2I [55]
PIT [53]
IAST [54]
RPT‚Ä† [83]

90.5
91.3
92.5
90.8
92.5
91.2
87.5
93.8
89.2

84.4
84.5
85.1
84.7
82.4
85.2
78.8
85.1
86.1

49.3
42.2
39.5
38.1
46.4
37.8
49.9
39.6
56.8

47.4
49.2
49.2
49.5
50.5
50.4
50.6
51.5
52.6

Baseline (ours)
SAC (ours)

80.2 29.3
90.4 53.9

76.8 23.8 21.9 37.7 35.4 21.1 79.8 21.3 75.0 59.5 17.5 83.5 22.4 33.4 13.0 30.7 12.3
86.6 42.4 27.3 45.1 48.5 42.7 87.4 40.1 86.1 67.5 29.7 88.5 49.1 54.6 9.8 26.6 45.3

40.8
53.8

Backbone: VGG-16

37.2
28.7
50.8
24.8
35.0
35.1
41.2
51.1
46.7
46.4

76.5
78.7
72.0
80.9
82.1
80.6
82.2
83.7
82.7
82.9

21.8
28.5
18.3
21.4
31.1
30.8
30.3
33.1
34.2
33.2

15.0
25.2
9.5
27.3
22.1
20.4
21.3
29.1
25.3
27.9

23.8
17.1
27.2
30.2
23.2
27.5
18.3
28.5
21.3
20.6

22.9
20.3
8.6
26.6
29.4
30.0
33.5
28.0
33.0
29.0

21.5
10.9
14.1
21.1
28.5
26.0
23.0
21.0
22.0
28.2

80.5
80.0
82.4
86.6
79.3
82.1
84.1
82.6
84.4
84.5

50.5
47.1
42.6
53.2
52.1
52.5
54.2
55.2
55.5
52.4

9.0
8.4
14.5
17.9
23.2
21.7
24.3
28.8
25.8
24.4

17.1
26.0
5.9
18.8
29.5
24.0
27.6
24.4
24.9
21.8

28.2
17.2
12.5
22.4
26.9
30.5
32.0
37.4
31.4
44.8

4.5
18.9
1.2
4.1
30.7
29.9
8.1
0.0
20.6
31.5

9.8
11.7
14.0
9.7
20.5
14.6
29.7
21.1
25.2
26.5

Backbone: ResNet-101

36.3
46.0
47.5
41.4
53.3
43.3
43.4
57.8
43.3

32.4
34.4
37.6
35.1
26.5
38.6
31.2
39.5
39.5

28.7
29.7
32.8
27.5
27.6
25.9
30.2
26.7
29.9

34.6
32.6
33.4
31.2
36.4
34.7
36.3
26.2
40.2

36.4
35.8
33.8
38.0
40.6
41.3
39.9
43.1
49.6

31.5
36.4
18.4
32.8
38.9
41.0
42.0
34.7
33.1

86.8
84.5
85.3
85.6
82.3
85.5
79.2
84.9
87.4

37.9
43.2
37.7
42.1
39.8
46.0
37.1
32.9
38.5

78.5
83.0
83.5
84.9
78.0
86.5
79.3
88.0
86.0

62.3
60.0
63.2
59.6
62.6
61.7
65.4
62.6
64.4

21.5
32.2
39.7
34.4
34.4
33.8
37.5
29.0
25.1

85.6
83.2
87.5
85.0
84.9
85.5
83.2
87.3
88.5

27.9
35.0
32.9
42.8
34.1
34.4
46.0
39.2
36.6

34.8
46.7
47.8
52.7
53.1
48.7
45.6
49.6
45.8

18.0
0.0
1.6
3.4
16.9
0.0
25.7
23.2
23.9

22.9
33.7
34.9
30.9
27.7
36.1
23.5
34.7
36.5

(‚Ä† ) denotes the use of PSPNet [85] instead of DeepLabv2 [10].

Table 2. Per-class IoU (%) comparison on GTA5 ‚Üí Cityscapes adaptation, evaluated on the Cityscapes validation set.

GTA5 ‚Üí Cityscapes (Table 2). Our method achieves a
clear improvement over the best published results [55, 83]
of +3.4% and +1.2% using the VGG-16 and ResNet-101
backbones, respectively. Note that RPT [83] and SA-I2I
[55] have a substantially higher model complexity. RPT
[83] uses PSPNet [85], which has a higher upper bound
than DeepLabv2 in a fully supervised setup (e.g., +5.7%
IoU on PASCAL VOC [85]); it requires extracting superpixels and training an encoder-decoder LSTM, thus increasing the model capacity and the computational overhead.
SA-I2I [55] initialises from a stronger baseline, BDL [46],
and relies on a style transfer network and adversarial training. While both RPT [83] and SA-I2I [55] require multiple
rounds of training, 3 and 6 (from BDL [46]), respectively,
we train with the target loss in a single pass. Notably, compared to the previous best approach for VGG with a ResNet
evaluation, SA-I2I [55], our improvement with ResNet-101
is substantial, +3.4%, and is comparable to the respective
margin on VGG-16.

FADA [70] requires 4 Tesla P40 GPUs with 24GB memory). Note that the momentum network is always in evaluation mode, has gradient tracking disabled, hence adds only
around 35% memory overhead. For the momentum network, we fix Œ≥œà = 0.99 and T = 100 in all our experiments. For the other hyperparameters, we use Œ≥œá = 0.99,
Œ∂ = 0.75, Œ≤ = 10‚àí3 and Œª = 3. Appendix C.2 provides
further detail on hyperparameter selection, as well as a sensitivity analysis of our framework w.r.t. Œ∂ and Œ≤. The inference follows the usual procedure of a single forward pass
through the segmentation network at the original image resolution without any post-processing.

4.2. Comparison to state of the art
We compare our approach to the current state of the
art on the two domain adaptation scenarios: GTA5 ‚Üí
Cityscapes in Table 2 and SYNTHIA ‚Üí Cityscapes in
Table 3. For a fair comparison, all numbers originate
from single-scale inference. In both cases, our approach,
denoted as SAC ("Self-supervised Augmentation Consistency"), substantially outperforms our baseline (i.e. the
source-only loss model with ABN, see Sec. 3.4), and, in
fact, sets a new state of the art in terms of mIoU. Importantly, while the ranking of previous works depends on the
backbone choice and the source data, we reach the top rank
consistently in all settings.

SYNTHIA ‚Üí Cityscapes (Table 3). Here, the result is
consistent with the previous scenario. Our approach attains
state-of-the-art accuracy for both backbones, improving by
7.6% and 1.4% with VGG-16 and ResNet-101 backbones
over the best results previously published [55, 83]. Again,
our method with ResNet-101 outperforms the previous best
method with full evaluation, PyCDA [47], by 5.9% IoU.
6

Method

mIoU13

mIoU

-
-
-
-
-
-
-

35.9
38.1
39.5
40.5
40.8
41.1
41.5

33.5 11.9 18.3 66.4 70.4 52.1 16.1 64.6 15.5 11.5 26.4
38.2 41.3 27.9 80.8 83.0 64.3 21.2 78.3 38.5 32.6 62.1

39.1
56.2

34.4
49.1

0.4
0.3
0.7
-
-
-
-
0.0
4.6
0.5

25.9
21.8
32.7
-
-
-
-
34.3
32.3
38.5

33.0
31.3
32.0
43.9
51.1
44.7
45.7
27.8
52.7
56.1

-
-
-
52.4
52.5
53.1
55.1
-
-
-

41.2
44.0
46.7
-
-
-
-
45.2
49.8
51.2

0.2
1.3

36.9 7.6 20.0 72.9 75.5 46.7 16.7 74.5 15.8 20.8 21.7
43.0 45.5 32.0 87.1 89.3 63.6 25.4 86.9 35.6 30.4 53.0

41.0
59.3

36.3
52.6

road sidew build wall fence pole light sign

veg

sky

pers ride

car

bus moto bicy

PyCDA [47]
PIT [53]
FADA [70]
FDA [80]
CD-AM [78]
LDR [77]
SA-I2I [55]

80.6
81.7
80.4
84.2
73.0
73.7
79.1

26.6
26.9
35.9
35.1
31.1
29.6
34.0

74.5
78.4
80.9
78.0
77.1
77.6
78.3

2.0
6.3
2.5
6.1
0.2
1.0
0.3

0.1
0.2
0.3
0.4
0.5
0.4
0.6

18.1
19.8
30.4
27.0
27.0
26.0
26.7

80.8
76.7
81.8
77.2
81.2
80.6
81.0

71.0
74.1
83.6
79.6
81.0
81.8
81.1

48.0
47.5
48.9
55.5
59.0
57.2
55.5

72.3
76.0
77.7
74.8
75.0
76.1
77.2

22.5
21.7
31.1
24.9
26.3
27.6
23.5

Baseline (ours)
SAC (ours)

60.7
77.9

26.9
38.6

67.1 8.3
83.5 15.8

0.0
1.5

ADVENT [69]
PIT [53]
PyCDA‚Ä† [47]
CD-AM [78]
FDA [80]
LDR [77]
SA-I2I [55]
FADA [70]
IAST [54]
RPT‚Ä† [83]

85.6
83.1
75.5
82.5
79.3
85.1
87.7
84.5
81.9
88.9

42.2
27.6
30.9
42.2
35.0
44.5
49.7
40.1
41.5
46.5

79.7 8.7
81.5 8.9
83.3 20.8
81.3 -
73.2 -
81.0 -
81.6 -
83.1 4.8
83.3 17.7
84.5 15.1

Baseline (ours)
SAC (ours)

63.9
89.3

25.9
47.2

71.0 11.0
85.5 26.5

Backbone: VGG-16

13.7
13.4
7.9
8.5
11.3
14.7
15.9

14.2
17.4
22.3
22.1
27.4
26.6
29.5

19.0
22.4
16.8
19.9
25.6
24.5
21.9

12.1
19.6
13.5
14.3
10.1
13.6
11.8

18.1
27.7
17.9
40.7
47.4
46.6
47.5

Backbone: ResNet-101

5.4
26.4
27.3
18.3
19.9
16.4
19.3
20.1
30.9
39.5

8.1
33.8
33.5
15.9
24.0
15.2
18.5
27.2
28.8
30.1

80.4
76.4
84.7
80.6
61.7
80.1
81.1
84.8
83.4
85.9

84.1
78.8
85.0
83.5
82.6
84.8
83.7
84.0
85.0
85.8

57.9
64.2
64.1
61.4
61.4
59.4
58.7
53.5
65.5
59.8

23.8
27.6
25.4
33.2
31.1
31.9
31.8
22.6
30.8
26.1

73.3
79.6
85.0
72.9
83.9
73.2
73.3
85.4
86.5
88.1

36.4
31.2
45.2
39.3
40.8
41.0
47.9
43.7
38.2
46.8

14.2
31.0
21.2
26.6
38.4
32.6
37.1
26.8
33.1
27.7

(‚Ä† ) denotes the use of PSPNet [85] instead of DeepLabv2 [10]. mIoU13 is the average IoU over 13 classes (i.e. excluding "wall", "fence" and "pole").

Table 3. Per-class IoU (%) comparison on SYNTHIA ‚Üí Cityscapes adaptation, evaluated on the Cityscapes validation set.
‚àÜ

mIoU

Configuration

-8.0
-6.4
-3.9
-2.6
-2.4
-1.9
-1.7
-1.6
-1.5
-0.6

41.9
43.5
46.0
47.3
47.5
48.0
48.2
48.3
48.4
49.3

No augmentation consistency
No momentum net (Œ≥œà = 0, T = 1)
No photometric noise
No multi-scale fusion
No focal loss (Œª = 0)
Min. entropy fusion (vs. averaging)
No class-based thresholding (Œ≤ ‚Üí 0)
No confidence regularisation
No importance sampling
No horizontal flipping

49.9

Full framework (VGG-16)

0.0

setting with the VGG-16 backbone. We independently
switch off each component and report the results in Table 4.
We find that two components, augmentation consistency
and the momentum network, play a crucial role. Disabling
the momentum network leads to a 6.4% IoU decrease,
while abolishing augmentation consistency leads to a drop
of 8.0% IoU. Recall that augmentation consistency comprises three augmentation techniques: photometric noise,
multi-scale fusion and random flipping. We further assess
their individual contributions. Training without the photometric jitter deteriorates the IoU more severely, by 3.9%,
compared to disabling the multi-scale fusion (‚àí2.6%) or
flipping (‚àí0.6%). We hypothesise that encouraging model
robustness to photometric noise additionally alleviates the
inductive bias inherited from the source domain to rely on
strong appearance cues (e.g., colour and texture), which can
be substantially different from the target domain.
Following the intuition that high-confidence predictions
should be preferred [65], we study an alternative implementation of the multi-scale fusion. For overlapping pixels, instead of averaging the predictions, we pool the prediction with the minimum entropy. The accuracy drop by
1.9% is somewhat expected. Averaging predictions via data
augmentation has previously been shown to produce wellcalibrated uncertainty estimates [1]. This is important for
our method, since it relies on the confidence values to select the predictions for use in self-supervision. Importance
sampling contributes 1.5% IoU to the total accuracy. This

Table 4. Ablation study. We use the GTA5 ‚Üí Cityscapes setting
with the VGG-based model to study the effect of the components
of our framework by individually removing each. We report the
mean IoU for the Cityscapes validation split.

Remarkably, in both settings our approach is more accurate or competitive with many recent works [65, 70, 77]
even when using a weaker backbone, i.e. VGG-16 instead
of ResNet-101. This is significant, as these improvements
are not due to increased training complexity or model capacity, in contrast to these previous works. Additional results, including the evaluation on Cityscapes test, are shown
in Appendices C and D.

4.3. Ablation study
To understand what makes our framework effective, we
conduct an ablation study using the GTA5 ‚Üí Cityscapes
7

Ground truth
Adapted ‚Üê VGG-16 ‚Üí Baseline
Adapted ‚Üê ResNet-101 ‚Üí Baseline

(a) GTA5 ‚Üí Cityscapes

(b) SYNTHIA ‚Üí Cityscapes

Figure 5. Qualitative examples. Our approach rectifies an appreciable amount of erroneous predictions from the baseline.

is surprisingly significant despite that our estimates œác,l are
only approximate (cf. Sec. 3.4), but the overall benefit is in
line with previous work [29]. Recall from Eq. (3) that our
confidence thresholds are computed per class to encourage
lower values for long-tail classes. Disabling this scheme
is equivalent to setting Œ≤ ‚Üí 0 in Eq. (3), which reduces
the mean IoU by 1.7%. This confirms our observation that
the model tends to predict lower confidences for the classes
occupying only few pixels. Similarly, the loss in Eq. (6)
without the focal term (Œª = 0) and confidence regularisation (mc‚àó,n = 1) are 2.4% and 1.6% IoU inferior. This is a
surprisingly significant contribution at a negligible computational cost.

align well with the object boundaries in the image, although
our framework has no explicit encoding of spatial priors,
which was previously deemed necessary [15, 68, 81, 83].
We believe that enforcing semantic consistency with data
augmentation makes our method less prone to the contextual bias [62], often blamed for coarse boundaries.

5. Conclusion
We presented a simple and accurate approach for domain
adaptation of semantic segmentation. With ordinary augmentation techniques and momentum updates, we achieve
state-of-the-art accuracy, yet make no sacrifice of the modest training or model complexity. No components of our
framework are strictly specialised; they build on a relatively
weak and broadly applicable assumption (cf. Sec. 1). Although this work focuses on semantic segmentation, we are
keen to explore the potential of the proposed techniques for
adaptation of other dense prediction tasks, such as optical
flow, monocular depth, panoptic and instance segmentation,
or even compositions of these multiple tasks.

4.4. Qualitative assessment
Fig. 5 presents a few qualitative examples, comparing our approach to the naive baseline (i.e. source-only
loss with ABN). Particularly prominent are the refinements
of the classes "road", "sidewalk" and "sky", but even
small-scale elements improve substantially (e.g., "person",
"fence" in the leftmost column). This is perhaps not surprising, owing to our multi-scale training and the thresholding technique, which initially ignores incorrectly predicted
pixels in self-supervision (as they initially tend to have low
confidence). Remarkably, the segment boundaries tend to

Acknowledgements.
This work has been co-funded by the
LOEWE initiative (Hesse, Germany) within the emergenCITY
center. Calculations for this research were partly conducted on
the Lichtenberg high performance computer of TU Darmstadt.

8

References

[16] Yun-Chun Chen, Yen-Yu Lin, Ming-Hsuan Yang, and JiaBin Huang. CrDoCo: Pixel-level domain transfer with crossdomain consistency. In CVPR, pp. 1791‚Äì1800, 2019. 2
[17] Jaehoon Choi, Taekyung Kim, and Changick Kim. Selfensembling with GAN-based data augmentation for domain
adaptation in semantic segmentation. In ICCV, pp. 6829‚Äì
6839, 2019. 1, 2
[18] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The Cityscapes
dataset for semantic urban scene understanding. In CVPR,
pp. 3213‚Äì3223, 2016. 5
[19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Fei-Fei Li. ImageNet: A large-scale hierarchical image
database. In CVPR, pp. 248‚Äì255, 2009. 5
[20] Jiahua Dong, Yang Cong, Gan Sun, Yuyang Liu, and Xiaowei Xu. CSCL: Critical semantic-consistent learning for
unsupervised domain adaptation. In ECCV, vol. VIII, pp.
745‚Äì762, 2020. 2
[21] Arnaud Doucet, Nando de Freitas, and Neil J. Gordon. An
introduction to sequential Monte Carlo methods. In Sequential Monte Carlo Methods in Practice, pp. 3‚Äì14. Springer,
2001. 4
[22] Liang Du, Jingang Tan, Hongye Yang, Jianfeng Feng, Xiangyang Xue, Qibao Zheng, Xiaoqing Ye, and Xiaolin
Zhang. SSF-DAN: Separated semantic feature based domain
adaptation network for semantic segmentation. In ICCV, pp.
982‚Äì991, 2019. 2
[23] Geoffrey French, Michal Mackiewicz, and Mark H. Fisher.
Self-ensembling for visual domain adaptation. In ICLR,
2018. 2, 3
[24] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran√ßois Laviolette, Mario
Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. J. Mach. Learn. Res., 17(1):2096‚Äì
2030, 2016. 2
[25] Rui Gong, Wen Li, Yuhua Chen, and Luc Van Gool. DLOW:
Domain flow for adaptation and generalization. In CVPR,
pp. 2477‚Äì2486, 2019. 2
[26] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In NIPS, pp.
2672‚Äì2680, 2014. 2
[27] Yves Grandvalet and Yoshua Bengio. Semi-supervised
learning by entropy minimization. In NIPS, pp. 529‚Äì536,
2004. 2
[28] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger.
On calibration of modern neural networks. In ICML, vol. 70,
pp. 1321‚Äì1330, 2017. 2
[29] Agrim Gupta, Piotr Doll√°r, and Ross B. Girshick. LVIS: A
dataset for large vocabulary instance segmentation. In CVPR,
pp. 5356‚Äì5364, 2019. 3, 8
[30] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and
Ross B. Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, pp. 9726‚Äì9735, 2020.
3

[1] Murat Seckin Ayhan and Philipp Berens. Test-time data augmentation for estimation of heteroscedastic aleatoric uncertainty in deep neural networks. In MIDL, 2018. 3, 7
[2] Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. In NIPS, pp. 3365‚Äì3373, 2014.
2
[3] Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan.
A theory of learning from different domains. Mach. Learn.,
79(1-2):151‚Äì175, 2010. 2
[4] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando
Pereira. Analysis of representations for domain adaptation.
In NIPS, pp. 137‚Äì144, 2006. 1
[5] David Berthelot, Nicholas Carlini, Ian J. Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel. MixMatch: A
holistic approach to semi-supervised learning. In NeurIPS,
pp. 5050‚Äì5060, 2019. 3
[6] Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In COLT, pp. 92‚Äì100, 1998.
1
[7] Minjie Cai, Feng Lu, and Yoichi Sato. Generalizing hand
segmentation in egocentric videos with uncertainty-guided
model adaptation. In CVPR, pp. 14380‚Äì14389, 2020. 2
[8] Wei-Lun Chang, Hui-Po Wang, Wen-Hsiao Peng, and WeiChen Chiu. All about structure: Adapting structural information across domains for boosting semantic segmentation.
In CVPR, pp. 1900‚Äì1909, 2019. 2
[9] Liang-Chieh Chen, Raphael Gontijo Lopes, Bowen Cheng,
Maxwell D. Collins, Ekin D. Cubuk, Barret Zoph, Hartwig
Adam, and Jonathon Shlens. Naive-student: Leveraging
semi-supervised learning in video sequences for urban scene
segmentation. In ECCV, vol. IX, pp. 695‚Äì714, 2020. 1
[10] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L. Yuille. DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. IEEE Trans. Pattern
Anal. Mach. Intell., 40(4):834‚Äì848, 2018. 5, 6, 7
[11] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian
Schroff, and Hartwig Adam. Encoder-decoder with atrous
separable convolution for semantic image segmentation. In
ECCV, vol. VII, pp. 833‚Äì851, 2018. 1
[12] Minmin Chen, Kilian Q. Weinberger, and John Blitzer. Cotraining for domain adaptation. In NIPS, pp. 2456‚Äì2464,
2011. 1
[13] Minghao Chen, Hongyang Xue, and Deng Cai. Domain adaptation for semantic segmentation with maximum
squares loss. In ICCV, pp. 2090‚Äì2099, 2019. 2
[14] Yi-Hsin Chen, Wei-Yu Chen, Yu-Ting Chen, Bo-Cheng Tsai,
Yu-Chiang Frank Wang, and Min Sun. No more discrimination: Cross city adaptation of road scene segmenters. In
ICCV, pp. 2011‚Äì2020, 2017. 2
[15] Yuhua Chen, Wen Li, and Luc Van Gool. ROAD: Reality oriented adaptation for semantic segmentation of urban scenes.
In CVPR, pp. 7892‚Äì7901, 2018. 2, 8

9

[31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR, pp.
770‚Äì778, 2016. 5
[32] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu,
Phillip Isola, Kate Saenko, Alexei A. Efros, and Trevor Darrell. CyCADA: Cycle-consistent adversarial domain adaptation. In ICML, vol. 80, pp. 1994‚Äì2003, 2018. 2, 6
[33] Weixiang Hong, Zhenzhen Wang, Ming Yang, and Junsong
Yuan. Conditional generative adversarial network for structured domain adaptation. In CVPR, pp. 1335‚Äì1344, 2018.
2
[34] Sergey Ioffe and Christian Szegedy. Batch Normalization:
Accelerating deep network training by reducing internal covariate shift. In ICML, vol. 37, pp. 448‚Äì456, 2015. 4, 5
[35] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov,
Dmitry P. Vetrov, and Andrew Gordon Wilson. Averaging
weights leads to wider optima and better generalization. In
UAI, pp. 876‚Äì885, 2018. 3
[36] Fredrik D. Johansson, David A. Sontag, and Rajesh Ranganath. Support and invertibility in domain-invariant representations. In AISTATS, vol. 89, pp. 527‚Äì536, 2019. 2
[37] Guoliang Kang, Yunchao Wei, Yi Yang, Yueting Zhuang,
and Alexander G. Hauptmann. Pixel-level cycle association:
A new perspective for domain adaptive semantic segmentation. In NeurIPS, pp. 3569‚Äì3580, 2020. 2
[38] Alex Kendall and Yarin Gal. What uncertainties do we need
in Bayesian deep learning for computer vision? In NIPS, pp.
5574‚Äì5584, 2017. 3
[39] Myeongjin Kim and Hyeran Byun. Learning texture invariant representation for domain adaptation of semantic segmentation. In CVPR, pp. 12972‚Äì12981, 2020. 1, 2, 5
[40] Samuli Laine and Timo Aila. Temporal ensembling for semisupervised learning. In ICLR, 2017. 2, 3
[41] Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, and
Daniel Ulbricht. Sliced Wasserstein discrepancy for unsupervised domain adaptation. In CVPR, pp. 10285‚Äì10295,
2019. 2
[42] Dong-Hyun Lee. Pseudo-label: The simple and efficient
semi-supervised learning method for deep neural networks.
In ICML Workshops, vol. 3, 2013. 1
[43] Guangrui Li, Guoliang Kang, Wu Liu, Yunchao Wei, and
Yi Yang. Content-consistent matching for domain adaptive
semantic segmentation. In ECCV, vol. XIV, pp. 440‚Äì456,
2020. 1, 2, 3
[44] Xiaomeng Li, Lequan Yu, Hao Chen, Chi-Wing Fu, and
Pheng-Ann Heng. Semi-supervised skin lesion segmentation via transformation consistent self-ensembling model. In
BMVC, 2018. 2
[45] Yanghao Li, Naiyan Wang, Jianping Shi, Xiaodi Hou, and
Jiaying Liu. Adaptive batch normalization for practical domain adaptation. Pattern Recognition, 80:109‚Äì117, 2018. 2,
4, 5
[46] Yunsheng Li, Lu Yuan, and Nuno Vasconcelos. Bidirectional
learning for domain adaptation of semantic segmentation. In
CVPR, pp. 6936‚Äì6945, 2019. 1, 2, 3, 6
[47] Qing Lian, Lixin Duan, Fengmao Lv, and Boqing Gong.
Constructing self-motivated pyramid curriculums for cross-

[48]

[49]

[50]

[51]

[52]

[53]

[54]

[55]

[56]

[57]

[58]

[59]

[60]

[61]

[62]

10

domain semantic segmentation: A non-adversarial approach.
In ICCV, pp. 6757‚Äì6766, 2019. 1, 2, 4, 5, 6, 7
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel,
Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and
Daan Wierstra. Continuous control with deep reinforcement
learning. In ICLR, 2016. 3
Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He,
and Piotr Doll√°r. Focal loss for dense object detection. IEEE
Trans. Pattern Anal. Mach. Intell., 42(2):318‚Äì327, 2020. 4
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and
Michael I. Jordan. Conditional adversarial domain adaptation. In NeurIPS, pp. 1647‚Äì1657, 2018. 2
Yawei Luo, Ping Liu, Tao Guan, Junqing Yu, and Yi Yang.
Significance-aware information bottleneck for domain adaptive semantic segmentation. In CVPR, pp. 6778‚Äì6787, 2019.
2
Yawei Luo, Liang Zheng, Tao Guan, Junqing Yu, and Yi
Yang. Taking a closer look at domain shift: Category-level
adversaries for semantics consistent domain adaptation. In
CVPR, pp. 2507‚Äì2516, 2019. 2
Fengmao Lv, Tao Liang, Xiang Chen, and Guosheng Lin.
Cross-domain semantic segmentation via domain-invariant
interactive relation transfer. In CVPR, pp. 4333‚Äì4342, 2020.
1, 2, 6, 7
Ke Mei, Chuang Zhu, Jiaqi Zou, and Shanghang Zhang. Instance adaptive self-training for unsupervised domain adaptation. In ECCV, vol. XXVI, pp. 415‚Äì430, 2020. 1, 2, 3, 6,
7
Luigi Musto and Andrea Zinelli. Semantically adaptive
image-to-image translation for domain adaptation of semantic segmentation. In BMVC, 2020. 1, 2, 6, 7
Fei Pan, Inkyu Shin, Fran√ßois Rameau, Seokju Lee, and
In So Kweon. Unsupervised intra-domain adaptation for semantic segmentation through self-supervision. In CVPR, pp.
3763‚Äì3772, 2020. 2
Adam Paszke, Sam Gross, and Francisco Massa et al. PyTorch: An imperative style, high-performance deep learning
library. In NeurIPS, pp. 8024‚Äì8035, 2019. 5
Christian S. Perone, Pedro L. Ballester, Rodrigo C. Barros, and Julien Cohen-Adad. Unsupervised domain adaptation for medical imaging segmentation with self-ensembling.
NeuroImage, 194:1‚Äì11, 2019. 2
Stephan R. Richter, Vibhav Vineet, Stefan Roth, and Vladlen
Koltun. Playing for data: Ground truth from computer
games. In ECCV, vol. II, pp. 102‚Äì118, 2016. 1, 5
Germ√°n Ros, Laura Sellart, Joanna Materzynska, David
V√°zquez, and Antonio M. L√≥pez. The SYNTHIA dataset:
A large collection of synthetic images for semantic segmentation of urban scenes. In CVPR, pp. 3234‚Äì3243, 2016. 1,
5
Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen.
Regularization with stochastic transformations and perturbations for deep semi-supervised learning. In NIPS, pp. 1163‚Äì
1171, 2016. 2
Rakshith Shetty, Bernt Schiele, and Mario Fritz. Not using the car to see the sidewalk ‚Äì quantifying and controlling
the effects of context in classification and segmentation. In
CVPR, pp. 8218‚Äì8226, 2019. 8

[77] Jinyu Yang, Weizhi An, Sheng Wang, Xinliang Zhu,
Chaochao Yan, and Junzhou Huang. Label-driven reconstruction for domain adaptation in semantic segmentation. In
ECCV, vol. XXVII, pp. 480‚Äì498, 2020. 1, 2, 6, 7
[78] Jinyu Yang, Weizhi An, Chaochao Yan, Peilin Zhao, and Junzhou Huang. Context-aware domain adaptation in semantic
segmentation. In WACV, pp. 514‚Äì524, 2021. 1, 2, 6, 7
[79] Yanchao Yang, Dong Lao, Ganesh Sundaramoorthi, and Stefano Soatto. Phase consistent ecological domain adaptation.
In CVPR, pp. 9008‚Äì9017, 2020. 2
[80] Yanchao Yang and Stefano Soatto. FDA: Fourier domain
adaptation for semantic segmentation. In CVPR, pp. 4084‚Äì
4094, 2020. 1, 2, 5, 6, 7
[81] Yang Zhang, Philip David, and Boqing Gong. Curriculum domain adaptation for semantic segmentation of urban
scenes. In ICCV, pp. 2039‚Äì2049, 2017. 2, 8
[82] Yiheng Zhang, Zhaofan Qiu, Ting Yao, Dong Liu, and Tao
Mei. Fully convolutional adaptation networks for semantic
segmentation. In CVPR, pp. 6810‚Äì6818, 2018. 2
[83] Yiheng Zhang, Zhaofan Qiu, Ting Yao, Chong-Wah Ngo,
Dong Liu, and Tao Mei. Transferring and regularizing prediction for semantic segmentation. In CVPR, pp. 9618‚Äì9627,
2020. 2, 4, 6, 7, 8
[84] Han Zhao, Remi Tachet des Combes, Kun Zhang, and Geoffrey J. Gordon. On learning invariant representations for
domain adaptation. In ICML, vol. 97, pp. 7523‚Äì7532, 2019.
2
[85] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia. Pyramid scene parsing network. In
CVPR, pp. 6230‚Äì6239, 2017. 1, 6, 7
[86] Zhedong Zheng and Yi Yang. Unsupervised scene adaptation
with memory regularization in vivo. In IJCAI, pp. 1076‚Äì
1082, 2020. 1, 2
[87] Zhedong Zheng and Yi Yang. Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic
segmentation. Int. J. Comput. Vis., 129(4):1106‚Äì1120, 2021.
1, 2
[88] Qianyu Zhou, Zhengyang Feng, Guangliang Cheng, Xin
Tan, Jianping Shi, and Lizhuang Ma. Uncertainty-aware consistency regularization for cross-domain semantic segmentation. arXiv:2004.08878 [cs.CV], 2020. 2
[89] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A.
Efros. Unpaired image-to-image translation using cycleconsistent adversarial networks. In ICCV, pp. 2242‚Äì2251,
2017. 2
[90] Xinge Zhu, Hui Zhou, Ceyuan Yang, Jianping Shi, and
Dahua Lin. Penalizing top performers: Conservative loss
for semantic segmentation adaptation. In ECCV, vol. VII,
pp. 568‚Äì583, 2018. 2
[91] Yang Zou, Zhiding Yu, B.V.K. Vijaya Kumar, and Jinsong
Wang. Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. In ECCV, vol.
III, pp. 297‚Äì313, 2018. 1, 2, 3, 6
[92] Yang Zou, Zhiding Yu, Xiaofeng Liu, B.V.K. Vijaya Kumar,
and Jinsong Wang. Confidence regularized self-training. In
ICCV, pp. 5981‚Äì5990, 2019. 1, 2

[63] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR,
2015. 5
[64] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao
Zhang, Han Zhang, Colin Raffel, Ekin Dogus Cubuk, Alexey
Kurakin, and Chun-Liang Li. FixMatch: Simplifying semisupervised learning with consistency and confidence. In
NeurIPS, pp. 596‚Äì608, 2020. 2
[65] Muhammad Subhani and Mohsen Ali. Learning from scaleinvariant examples for domain adaptation in semantic segmentation. In ECCV, vol. XXII, pp. 290‚Äì306, 2020. 1, 2, 3,
4, 7
[66] Antti Tarvainen and Harri Valpola. Mean teachers are better
role models: Weight-averaged consistency targets improve
semi-supervised deep learning results. In NIPS, pp. 1195‚Äì
1204, 2017. 2, 3
[67] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker.
Learning to adapt structured output space for semantic segmentation. In CVPR, pp. 7472‚Äì7481, 2018. 1, 2, 5
[68] Yi-Hsuan Tsai, Kihyuk Sohn, Samuel Schulter, and Manmohan Chandraker. Domain adaptation for structured output
via discriminative patch representations. In ICCV, pp. 1456‚Äì
1465, 2019. 1, 2, 5, 8
[69] Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu
Cord, and Patrick P√©rez. ADVENT: Adversarial entropy
minimization for domain adaptation in semantic segmentation. In CVPR, pp. 2517‚Äì2526, 2019. 1, 2, 5, 6, 7
[70] Haoran Wang, Tong Shen, Wei Zhang, Lingyu Duan, and
Tao Mei. Classes matter: A fine-grained adversarial approach to cross-domain semantic segmentation. In ECCV,
vol. XIV, pp. 642‚Äì659, 2020. 1, 2, 5, 6, 7
[71] Kaihong Wang, Chenhongyi Yang, and Margrit Betke.
Consistency regularization with high-dimensional nonadversarial source-guided perturbation for unsupervised domain adaptation in segmentation. AAAI, 2021. 2
[72] Yude Wang, Jie Zhang, Meina Kan, Shiguang Shan, and
Xilin Chen. Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation. In
CVPR, pp. 12272‚Äì12281, 2020. 3
[73] Zhonghao Wang, Mo Yu, Yunchao Wei, Rog√©rio Feris, Jinjun Xiong, Wen-Mei Hwu, Thomas S. Huang, and Honghui
Shi. Differential treatment for stuff and things: A simple
unsupervised domain adaptation method for semantic segmentation. In CVPR, pp. 12632‚Äì12641, 2020. 2, 5
[74] Zuxuan Wu, Xintong Han, Yen-Liang Lin, Mustafa G√∂khan
Uzunbas, Tom Goldstein, Ser-Nam Lim, and Larry S. Davis.
DCAN: Dual channel-wise alignment networks for unsupervised scene adaptation. In ECCV, vol. V, pp. 535‚Äì552, 2018.
2
[75] Qizhe Xie, Zihang Dai, Eduard H. Hovy, Thang Luong, and
Quoc Le. Unsupervised data augmentation for consistency
training. In NeurIPS, pp. 6256‚Äì6268, 2020. 2
[76] Qizhe Xie, Minh-Thang Luong, Eduard H. Hovy, and
Quoc V. Le. Self-training with noisy student improves imagenet classification. In CVPR, pp. 10684‚Äì10695, 2020. 2,
3

11

Self-supervised Augmentation Consistency
for Adapting Semantic Segmentation
‚Äì Supplemental Material ‚Äì

1

Nikita Araslanov1
Stefan Roth1,2
Department of Computer Science, TU Darmstadt

A. Overview
In this appendix, we first provide further training and
implementation details of our framework. We then take a
closer look at the accuracy of long-tail classes, before and
after adaptation. Next, we discuss our strategy for hyperparameter selection and perform a sensitivity analysis. We
also evaluate our framework using another segmentation architecture, FCN8s [98]. Finally, we discuss the limitations
of the current evaluation protocol and propose a revision
based on the best practices in the field at large.

1
2
3
4

import
import
import
import

2

hessian.AI

random
PIL
torchvision . transforms as tf
torchvision . transforms . functional as F

5
6
7

# Load the image
image = PIL . Image . open (...)

8
9
10
11
12
13

# Gaussian blur
# with a randomly sampled radius
radius = random . uniform (.1 ,2.)
gaussian = PIL . ImageFilter . GaussianBlur ( radius )
image = image . filter ( gaussian )

14

B. Further Technical Details

15

Photometric noise. Recall that our framework uses random Gaussian smoothing, greyscaling and colour jittering
to implement the photometric noise. We re-use the parameters for these operations from the MoCo-v2 framework [93].
In detail, the kernel radius for the Gaussian blur is sampled uniformly from the range [0.1, 2.0]. Note that this does
not correspond to the actual filter size.1 The colour jitter,
applied with probability 0.5, implements a perturbation of
the image brightness, contrast and saturation with a factor
sampled uniformly from [0.6, 1.4], while the hue factor is
sampled uniformly at random in the range of [0.9, 1.1]. We
convert a target image to its greyscale version with probability 0.2. Fig. 6 demonstrates an example implementation
of this procedure in Python.

17

16

18
19
20
21
22

# Colour jitter
# with probability 0.5
if 0.5 > random . random () :
jitter = tf . ColorJitter ( brightness =0.4 ,
contrast =0.4 ,
saturation =0.4 ,
hue =0.1)
image = jitter ( image )

23
24
25
26
27

# Convert to greyscale
# with probability 0.2
if 0.2 > random . random () :
image = F . to_grayscale ( image )

Figure 6. Python implementation of the photometric noise.

Training schedule. Our framework typically needs 150 ‚àí
200K iterations in total (i.e. including the source-only pretraining) until convergence, as determined on a random subset of 500 images from the training set (see our discussion in
Appendix D below). This varies slightly depending on the
backbone and the source data used. This schedule translates
to approximately 3 days of training with standard GPUs
(e.g., Titan X Pascal with 12 GB memory) for both VGG16 and ResNet-101 backbones. Recall that we used 4 GPUs
for our ResNet version of the framework, hence its training
time is comparable to the VGG variant, which uses only 2
GPUs. All our experiments use a constant learning rate for
simplicity, but more advanced schedules, such as cyclical
learning rates [35], the cosine schedule [93, 95] or ramp-ups
[40], may further improve the accuracy of our framework.

Constraint-free data augmentation. Similarly to the
multi-scale cropping of the target images, we scale the
source images randomly with a factor sampled uniformly
from [0.5, 1.0] prior to cropping. However, we do not enforce the semantic consistency for the source data, since the
ground truth of the source images is available. For both
the target and source images we also use random horizontal
flipping. We additionally experimented with moderate rotation (both with and without semantic consistency), but did
not observe a significant effect on the mean accuracy.
1 The Pillow Library [94] internally converts the radius r to the box
‚àö
length as L = 3 ‚àó r2 + 1.

i

CBT

IS

FL

3

3

3

sky pers ride

car

truck bus train moto bicy

mIoU

88.1

41.0

85.7 30.8 30.6 33.1 37.0 22.9 86.6 36.8 90.7 67.1 27.1 86.8 34.4 30.4

8.5

7.5

0.0

44.5

89.4
90.0
89.3

52.3
47.1
39.0

86.0 34.0 32.6 38.5 43.3 30.6 85.2 30.9 88.5 66.7 28.0 85.7 35.6 39.6 0.0
85.6 31.3 24.9 32.3 38.9 28.2 87.3 39.8 89.4 67.7 28.6 88.1 40.1 50.0 7.3
85.1 33.2 26.1 32.4 41.8 25.2 86.3 27.4 90.4 66.4 28.2 87.5 32.9 45.4 11.0

6.6
9.9
7.6

0.0
2.2
0.0

46.0
46.8
45.0

3
3

89.3
89.3
89.7

52.6
52.2
45.1

86.0 33.4 30.0 38.0 44.9 34.3 86.9 35.3 88.0 65.4 27.3 86.2 37.6 44.0 20.9 9.6 6.5
86.1 34.2 31.5 37.0 43.4 36.3 85.2 30.7 86.6 66.2 30.3 85.3 36.2 43.9 29.2 6.8 8.6
85.6 29.6 28.3 31.7 41.9 27.5 87.2 37.4 89.8 66.9 29.2 87.5 37.3 31.6 24.7 11.9 20.2

48.2
48.4
47.5

3

90.0

53.1

86.2 33.8 32.7 38.2 46.0 40.3 84.2 26.4 88.4 65.8 28.0 85.6 40.6 52.9 17.3 13.7 23.8

49.9

3
3

terr

3
3

3
3

road sidew build wall fence pole light sign veg

Table 5. Per-class IoU (%) on Cityscapes val using a VGG-16 backbone in the GTA5 ‚Üí Cityscapes setting. We study three components
in more detail: class-based thresholding (CBT), importance sampling (IS) and the focal loss (FL). The mIoU of the settings in the last
four rows are reproduced from the main text. Here, we elaborate on the per-class accuracy in a broader context of the supplementary
experiments in the first four rows.

C. Additional Experiments

on the IoU of the other classes. Therefore, the benefits of
individual framework components have to be understood in
the context of their aggregated effect on multiple classes,
e.g. using the mean IoU. For instance, consider the class
"train" for which IS appears to also decrease the IoU: while
CBT together with FL achieve 29.2% IoU, adding IS decreases the IoU to 17.3%. However, the IoU of other classes
increases (e.g., "motorcycle", "bicycle"), as does the mean
IoU. Furthermore, only few classes reach their maximum
accuracy when we enable all three long-tail components.
Yet, it is the setting with the best accuracy trade-off between the individual classes, i.e. with the highest mean IoU.
Overall, the long-tail components improve our framework
by 5.4% mean IoU combined, a substantial margin.

C.1. A closer look at long-tail adaptation
Recall that our framework features three components to
attune the adaptation process to the long-tail classes: classbased thresholding (CBT), importance sampling (IS) and
the focal loss (FL), which we summarily refer to as the longtail components in the following. Disabling the long-tail
components individually is equivalent to setting Œ≤ ‚Üí 0 for
CBT, using uniform sampling of the target images instead
of IS or assigning Œª to 0 for the FL. Here, we extend our ablation study of the GTA5 ‚Üí Cityscapes setup with VGG-16
(cf. Table 4 from the main text) and experiment with different combinations of the long-tail components. Table 5
details the per-class accuracy of the possible compositions.
We observe that the ubiquitous classes ‚Äì "road", "building", "vegetation", "sky", "person" and "car" ‚Äì are hardly
affected; it is primarily the long-tail categories that change
in accuracy. Furthermore, our long-tail components are mutually complementary. The mean IoU improves when one
of the components is active, from 44.5% to up to 46.8%.
It is boosted further with two of the components enabled
to 48.4%, and reaches its maximum for our model, 49.9%,
when all three components are in place.
We further identify the following tentative patterns. FL
tends to improve classes "wall", "fence" and "pole". CBT
increases the accuracy of the "traffic light" category (which
has high image frequency and occupies only a few pixels),
but also rare classes, such as "rider", "bus" and "train"
benefit from CBT, especially in conjunction with IS. IS
also enhances the mask quality of the classes "bicycle" and
"motorcycle". Nevertheless, we urge caution against interpreting the results for each class in isolation, despite such
widespread practice in the literature. Today's semantic segmentation models do not possess the notion of an 'ambiguous' class prediction and each pixel receives a meaningful
label. By the pigeon's hole principle, this implies that the
changes in the IoU of one class have an immediate effect

C.2. Hyperparameter search and sensitivity
To select Œ∂ and Œ≤, we first experimented with a few reasonable choices (Œ∂ ‚àà (0.7, 0.8), Œ≤ ‚àà (0.0001, 0.01))2 using
a more lightweight backbone (MobileNetV2 [97]). To measure performance, we use the mean IoU on the validation
set (500 images from Cityscapes train, as in the main text).
Here, we study our framework's sensitivity to the particular choice of Œ∂ and Œ≤. To make the results comparable to
our previous experiments, we use VGG-16 and report the
mean IoU on Cityscapes val in Table 7. We observe moderate deviation of the IoU w.r.t. Œ∂. A more tangible drop
in accuracy with Œ≤ = 0.01 is expected, as it leads to lowconfidence predictions, which are likely to be inaccurate, to
be included into the pseudo label. We note that while a suboptimal choice of these hyperparameters leads to inferior
results (with a standard deviation of ¬±1.4% mIoU), even
the weakest model with Œ∂ = 0.8 and Œ≤ = 0.01 did not fail
to considerably improve over the baseline (by 8.5% IoU, cf.
Table 2 in the main text).
2 While

Œ∂ may seem more interpretable (the maximum confidence
threshold), a reasonable range for Œ≤ can be derived from œác for the longtail classes, which is simply the fraction of pixels these classes tend to
occupy in the image (see Eq. 3).

ii

Method

road sidew build wall fence pole light sign veg

terr

sky pers ride

car truck bus train moto bicy

mIoU

GTA5 ‚Üí Cityscapes

Baseline (ours)
SAC-FCN (ours)

76.7 28.2
86.3 45.6

74.4 12.7 19.0 27.2 28.7 12.2 77.0 18.0 70.6 54.8 20.6 79.6 19.0 19.2 20.6 27.9 11.2
84.4 30.3 27.1 24.8 42.8 35.2 86.9 39.7 88.0 62.3 32.1 84.1 28.4 43.7 31.9 29.4 45.8

36.7 (37.1)
49.9 (49.9)

60.9 1.8
81.4 19.8

31.6 (34.4)
46.8 (49.1)

SYNTHIA ‚Üí Cityscapes

Baseline (ours)
SAC-FCN (ours)

50.7 23.8
74.7 34.2

0.1 27.7 10.5 15.7 60.1
1.9 27.2 34.8 27.2 80.0

-
-

72.4 50.1 16.0 66.5
86.3 61.5 20.8 82.5

-
-

13.7
31.2

-
-

8.5 26.8
32.0 53.9

Table 6. Per-class IoU (%) on Cityscapes val using VGG-16 with FCN8s. For reference, the numbers in parentheses in the last column
report the mean IoU of the DeepLabv2 architecture (cf. Tables 2 and 3 from the main text).

‚ÜìŒ∂
0.7
0.75
0.8

/

Œ≤ ‚Üí 0.0001
47.9
48.6
48.2

0.001

0.01

D. Towards Best-practice Evaluation

48.8
49.9
49.8

46.7
46.3
45.6

The current strategy to evaluate domain adaptation (DA)
methods for semantic segmentation is to use the ground
truth of 500 randomly selected images from the Cityscapes
train split for model selection and to report the final model
accuracy on the 500 Cityscapes val images [47]. In this
work, we adhered to this procedure to enable a fair comparison to previous work. However, this evaluation approach
is evidently in discord with the established best practice in
machine learning and with the benchmarking practice on
Cityscapes [18], in particular.
The test set is holdout data to be used only for an unbiased performance assessment (e.g., segmentation accuracy)
of the final model [96]. While it is conceivable to consult
the test set for verifying a number of model variants, such
access cannot be unrestrained. This is infeasible to ensure
when the test set annotation is public, as is the case with
Cityscapes val, however. Benchmark websites traditionally
enable a restricted access to the test annotation through impartial submission policies (e.g., limited number of submissions per time window and user), and Cityscapes officially
provides one.3
We, therefore, suggest a simple revision of the evaluation protocol for evaluating future DA methods. As before,
we use Cityscapes train as the training data for the target
domain, naturally without the ground truth. For model selection, however, we use Cityscapes val images with the
ground-truth labels. The holdout test set for reporting
the final segmentation accuracy after adaptation becomes
Cityscapes test, with the results obtained via submitting
the predicted segmentation masks to the official Cityscapes
benchmark server.
An additional advantage of this strategy is a clear interpretation of the final accuracy in the context of fully supervised methods that routinely use the same evaluation setup.
Also note that Cityscapes val contains images from different cities than Cityscapes train (which are also different
from Cityscapes test). Therefore, it is more suitable for detecting cases of model overfitting on particularities of the
city, since the validation set was previously a subset of the

Table 7. Mean IoU (%) on GTA5 ‚Üí Cityscapes (val) with varying Œ∂ and Œ≤. Our framework maintains strong accuracy under different settings of Œ∂ and Œ≤. Even with a poor choice (e.g., Œ∂ = 0.8,
Œ≤ = 0.01), it fares well w.r.t. the state of the art and outperforms
many previous works (cf. Table 2 from the main text).

C.3. VGG-16 with FCN8s
A number of previous works (e.g., [55, 77, 80]) used
the FCN8s [98] architecture with VGG-16, as opposed to
DeepLabv2 [10], adopted in other works (e.g., [39, 70])
and ours. Such architecture exchange appears to have been
dismissed in previous work as minor, which used only one
of the architectures in the experiments. However, the segmentation architecture alone may contribute to the observed
differences in accuracy of the methods and, more critically, to the improvements otherwise attributed to other aspects of the approach. To facilitate such transparency in
our work, we replace DeepLabv2 with its FCN8s counterpart in our framework (with the VGG-16 backbone) and
repeat the adaptation experiments from Sec. 4, i.e. using
two source domains, GTA5 and SYNTHIA, and Cityscapes
as the target domain. We keep the values of the hyperparameters the same, with an exception of the learning rate,
which we increase by a factor of 2 to 5 √ó 104 . Table 6 reports the results of the adaptation, which clearly show that
our framework generalises well to other segmentation architectures. Despite the FCN8s baseline model (source-only
loss with ABN) achieving a slightly inferior accuracy compared to DeepLabv2 (e.g., 31.6% vs. 34.4% IoU for SYNTHIA ‚Üí Cityscapes), our self-supervised training still attains a remarkably high accuracy, 46.8% IoU (vs. 49.1%
with DeepLabv2). This is substantially higher than the previous best method using FCN8s with the VGG-16 backbone, SA-I2I [55]: +3.4% on GTA5 ‚Üí Cityscapes and
+5.3% on SYNTHIA ‚Üí Cityscapes.

3 https://www.cityscapes-dataset.com

iii

Method

road sidew build wall fence pole light sign veg terr sky pers ride car truck bus train moto bicy

mIoU

GTA5 ‚Üí Cityscapes

SAC-FCN (ours)
SAC-VGG (ours)
SAC-ResNet (ours)

87.5 45.2
91.5 53.9
91.8 54.3

85.0 29.2 26.4 23.3 44.2 32.0 88.3 52.6 91.2 65.2 35.0 86.0 24.4 32.8 31.4 36.9 40.5
86.6 34.1 31.5 36.8 47.2 36.9 85.1 38.0 91.1 68.7 31.9 87.4 31.0 46.7 22.6 24.2 24.0
87.4 36.2 30.2 43.7 49.7 42.1 89.3 54.3 90.5 71.8 34.9 89.8 38.8 47.3 24.9 38.3 43.8

50.4 (49.9)
51.0 (49.9)
55.7 (53.8)

66.9 25.9
70.4 29.7
87.4 41.0

80.8 12.1
83.6 11.6
85.5 17.5

45.8 (46.8)
48.3 (49.1)
52.7 (52.6)

97.9 81.3
97.4 78.4

90.4 48.8 47.4 49.6 57.9 67.3 91.9 69.4 94.2 79.8 59.8 93.7 56.5 67.5 57.5 57.7 68.8
89.2 34.9 44.2 47.4 60.1 65.0 91.4 69.3 93.9 77.1 51.4 92.6 35.3 48.6 46.5 51.6 66.8

SYNTHIA ‚Üí Cityscapes

SAC-FCN (ours)
SAC-VGG (ours)
SAC-ResNet (ours)

2.0 24.4 37.1 27.5 78.8 - 88.9 63.9 25.0 84.7
1.8 34.2 41.2 29.2 81.0 - 87.1 67.9 25.4 75.9
2.6 40.5 44.7 34.4 87.9 - 91.2 68.0 31.0 89.3

-
-
-

27.4
34.3
33.2

-
-
-

36.9 50.2
42.5 57.5
38.6 49.9

Fully supervised (Cityscapes)

DeepLab-ResNet [10]
FCN-VGG [98]

70.4
65.3

Table 8. Per-class IoU (%) on Cityscapes test. In the last column, the numbers in parentheses report the mean IoU on Cityscapes val
from the previous evaluation scheme (cf. Tables 2 and 3 from the main text) for reference. SAC-FCN denotes our VGG-based model with
FCN8s [98] from Appendix C.3.

training images.
For future reference, we evaluate our framework (both
the DeepLabv2 and FCN8s variants) in the proposed setup
and report the results in Table 8. To ease the comparison,
we juxtapose our validation results reported in the main
text (from Table 6 for FCN8s).4 As we did not finetune
our method to Cityscapes val following the previous evaluation protocol, we expect the test accuracy on Cityscapes
test to be on a par with our previously reported accuracy
on Cityscapes val. The results in Table 8 clearly confirm
this expectation: the segmentation accuracy on Cityscapes
test is comparable to the accuracy on Cityscapes val (SYNTHIA ‚Üí Cityscapes) or even tangibly higher (GTA5 ‚Üí
Cityscapes). We remark that the remaining accuracy gap
to the fully supervised model is still considerable (70.4%
vs. 55.7% IoU achieved by our best DeepLabv2 model and
65.3% vs. 51.0% IoU compared to our best FCN8s variant),
which invites further effort from the research community.
We hope that future UDA methods for semantic segmentation will follow suit in reporting the results on Cityscapes

test. Owing to the regulated access to the test set, we believe this setting to offer more transparency and fairness to
the benchmarking process, and will successfully drive the
progress of UDA for semantic segmentation, as it has done
in the past for the fully supervised methods.

References
[93] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
Improved baselines with momentum contrastive learning.
arXiv:2003.04297 [cs.CV], 2020.
[94] Alex Clark. Pillow (PIL fork) documentation, 2015.
[95] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In ICLR, 2017.
[96] Brian D. Ripley. Pattern Recognition and Neural Networks.
Cambridge University Press, 1996.
[97] Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey
Zhmoginov, and Liang-Chieh Chen. MobileNetV2: Inverted
residuals and linear bottlenecks. In CVPR, pp. 4510‚Äì4520,
2018.
[98] Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. IEEE
Trans. Pattern Anal. Mach. Intell., 39(4):640‚Äì651, 2017.

4 To our best knowledge, no previous work published their results in
this evaluation setting before.

iv

