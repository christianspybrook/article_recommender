Entangling Quantum Generative Adversarial Networks
Murphy Yuezhen Niu,1, ∗ Alexander Zlokapa,2, 1, ∗ Michael Broughton,1
Sergio Boixo,1 Masoud Mohseni,1 Vadim Smelyanskyi,1 and Hartmut Neven1

arXiv:2105.00080v1 [quant-ph] 30 Apr 2021

2

1
Google AI Quantum, Venice, CA 90291, USA
Division of Physics, Mathematics and Astronomy, Caltech, Pasadena, CA 91125, USA

Generative adversarial networks (GANs) are one of the most widely adopted semisupervised and
unsupervised machine learning methods for high-definition image, video, and audio generation. In
this work, we propose a new type of architecture for quantum generative adversarial networks (entangling quantum GAN, EQ-GAN) that overcomes some limitations of previously proposed quantum
GANs. Leveraging the entangling power of quantum circuits, EQ-GAN guarantees the convergence
to a Nash equilibrium under minimax optimization of the discriminator and generator circuits by
performing entangling operations between both the generator output and true quantum data. We
show that EQ-GAN has additional robustness against coherent errors and demonstrate the effectiveness of EQ-GAN experimentally in a Google Sycamore superconducting quantum processor. By
adversarially learning efficient representations of quantum states, we prepare an approximate quantum random access memory (QRAM) and demonstrate its use in applications including the training
of quantum neural networks.

INTRODUCTION

Generative adversarial networks (GANs) [1] are one
of the most widely adopted generative machine learning methods, achieving state-of-the-art performance in a
variety of high-dimensional and complex tasks including
photorealistic image generation [2], super-resolution [3],
and molecular synthesis [4]. Given access only to a training dataset S = {xi } sampled from an underlying data
distribution pdata (x), a GAN can generate realistic examples outside S. Certain probability distributions generated by quantum computers are thought to be classically
hard to sample from under plausible conjectures [5–7],
and learning to generate these samples using a classical
GAN can also be formidably hard [8]. In this work, we
focus on developing a fully quantum mechanical GAN,
where the true data is given by a quantum state; the
task is then to learn a generator circuit that can reproduce the same quantum state. Following the framework
of a GAN, a discriminator circuit is presented either with
the true data or with fake data from the generator. The
generator and discriminator are then trained adversarially [9]: the generator attempts to fool the discriminator,
while the discriminator attempts to correctly distinguish
true and fake data. While we focus on quantum data,
we present viable applications of the resulting machine
learning architecture in the context of classical data.
Recent work on a quantum GAN (QuGAN) [10, 11]
has proposed a direct analogy of the classical GAN architecture in designing the generator and discriminator
circuits. We show that the proposed QuGAN does not
always converge but rather in certain cases oscillates between a finite set of states due to mode collapse, and in
general suffers from a non-unique Nash equilibrium. This
motivates a new entangling quantum GAN (EQ-GAN)
with a uniquely quantum twist: rather than providing
the discriminator with either true or fake data, we allow

the discriminator to entangle both true and fake data.
We prove the convergence of the EQ-GAN to the global
optimal Nash equilibrium. Numerical experiments confirm that the EQ-GAN converges on problem instances
that the QuGAN failed on.
While the EQ-GAN has favorable convergence properties, the task of learning a quantum circuit to generate an
unknown quantum state may also be solved in an entirely
supervised approach. Rather than adversarially training
the discriminator to distinguish between fake and real
data, one may freeze the discriminator to perform an exact swap test, measuring the state fidelity between the
true and fake data. While this would replicate the original state in the absence of noise, gate errors in the implementation of the discriminator will cause convergence to
the incorrect optimum. We show that the adversarial approach of the EQ-GAN is more robust to such errors than
the simpler supervised learning approach. Since training
quantum machine learning models can require extensive
time to compute gradients on current quantum hardware,
resilience to gate errors drifting during the training process is especially valuable in the noisy intermediate-scale
quantum (NISQ) era of quantum computing.
Finally, we provide applications of the EQ-GAN in
the broader context of quantum machine learning for
classical data. Many of the most attractive quantum
machine learning algorithms require a quantum random
access memory (QRAM) [12]. By learning a shallow
quantum circuit to generate a superposition of classical
data, an EQ-GAN can be used to create an approximate
QRAM. We demonstrate an application of such a QRAM
for quantum neural networks [13], improving the performance of a quantum neural network for a classification
task.

2
PRIOR ART

A GAN comprises of a parameterized generative network G(θg , z) and discriminator network D(θd , z). The
generator maps a vector sampled from an input distribution z ∼ p0 (z) to a data example G(θg , z), thus transforming p0 (z) to a new distribution pg (z) of fake data.
The discriminator takes an input sample x and gives the
probability D(θd , x) that the sample is real (from the
data) or fake (from the generative network). The training
corresponds to a minimax optimization problem, where
we alternate between improving the discriminator's ability to distinguish real/fake samples and improving the
generator's ability to fool the discriminator. Specifically,
we solve minθg maxθd V (θg , θd ) for a cost function V :
V (θg , θd ) = Ex∼pdata (x) [log D(θd , x)]
+ Ez∼p0 (z) [log (1 − D(θd , G(θg , z)))] .

(1)

If G and D have enough capacity, i.e. approach the
space of arbitrary functions, then it is proven in Ref. [1]
that the global optimum of this minimax game exists
and uniquely corresponds to pg (x) = pdata (x). While
a multilayer perceptron can be used to parameterize D
and G, the dimensionality of the functional space can
also be increased by replacing classical neural networks
with quantum neural networks. In the most general case,
the classical
data can be represented by a density matrix
P
σ = i pi |ψi ihψi | where pi ∈ [0, 1] are positive bounded
real numbers and |ψi i are orthogonal basis states. In the
first proposal of a quantum GAN (QuGAN) [10, 11], the
generative network is defined by a quantum circuit U that
outputs the quantum state ρ = U (θg )ρ0 U † (θg ) from the
initial state ρ0 . The discriminator takes either the real
data σ or the fake data ρ and performs a positive operator valued measurement (POVM) defined by T whose
outcome determines the probability of data being true,
or operator F whose outcome determines the probability
of data being fake, with ||T ||1 , ||F ||1 ≤ 1. Hence, the
discriminator predicts the probability that an unknown
input state ρin is true data by measuring the expectation
value of T :
D(θd , ρin ) = Tr[T ρin ].

θg

T

CONVERGENCE OF EQ-GAN

To ensure convergence to a unique Nash equilibrium,
we propose a new minimax optimization problem with a
discriminator that is not directly analogous to the discriminator of a classical GAN. Rather than evaluating
either fake or true data individually, the optimal discriminator is not only provided access to the true data σ and
an input state ρ(θg ) prepared by the generator circuit parameterized by θg , but also permited to perform a measurement on the joint system that in certain parameter
value gives fidelity measurement between the two inputs:
Dσfid (ρ(θg )) =



2
q
.
Tr σ 1/2 ρ(θg ) σ 1/2

(4)

Notice that in comparison Eq. 3 is a linear function of input states, which is not optimal in the state-certification
problem [14] of evaluating quantum generative models.
Let the discriminator Dσ (θd , ρ(θg )) represent the probability of measuring state |0i at the end of the discriminating circuit. If there exist parameters θdopt that realize a
perfect swap test, i.e. Dσ (θdopt , ρ(θg )) = 21 + 21 Dσfid (ρ(θg )),
then Dσ is sufficiently expressive to reach the optimal discriminator during optimization. Since a traditional swap
test across two n-qubit states requires two-qubit gates
that span over 2n qubits, implementation on a quantum
device with local connectivity incurs prohibitive overhead
in circuit depth. Hence, we implement the discriminator with a parameterized destructive ancilla-free swap
test [15]. The EQ-GAN architecture adversarially optimizes the generation of the state ρ(θg ) and the learning
of a fidelity measurement Dσ (Fig. 1).
We define a minimax cost function closer to that of the
classical GAN in Eq. 1:

(2)

Following Ref. [11], the QuGAN solves the minimax game
min max (Tr[T σ] − Tr[T ρ(θg )]) .

cost function in Eq. 3, perhaps overshooting σ. In the
subsequent generator update, T will again be opposite
to ρ. This leads to the oscillation of the generator and
discriminator, possibly preventing convergence; we show
a case of infinite oscillation in the supplementary material.

(3)

Unfortunately, minimax optimization might not converge to a good Nash equilibrium. When ρ is close to
σ, the optimal Hesltrom measurement operator T =
P + (σ − ρ) is close to orthogonal to the true quantum
data σ and opposite to ρ. The next step of training will
try align the generator state ρ with T to minimize the

min max V (θg , θd ) = min max[1 − Dσ (θd , ρ(θg ))],
θg

θd

θg

θd

(5)

where Dσ (θd , ρ(θg )) is the parameterization of the swaptest result. We now show that a Nash equilibrium exists at the desired location. Consider a swap test circuit
ansatz for the discriminator U (θd ) = exp[−iθd CSWAP],
which is the matrix exponentiation of a perfect controlled
swap gate with angle θd . Under such ansatz, the input
state ρin = |ψihψ| and σ = |ζihζ| will transform under
the discriminator circuit into:

3

HU (θd )H|0ia |ψi|ζi =

1
i sin θd
|1ia [|ζi|ψi − |ψi|ζi] + |0ia [(e−iθd + cos θd )|ψi|ζi − i sin θd |ζi|ψi].
2
2

Given the circuit ansatz defined above with the predefined range for the swap angle θ, the maximum value for
distinguishing between two arbitrary states is uniquely
achieved by perfect swap test angle θ = π/2. More particularly, the probability of measuring state |0i at the
end of the parameterized swap test depends on the swap
angle θ according to
Dσ (θd , ρ(θg )) =

1
[1 + cos2 θd + sin2 θd Dσfid (ρ(θg ))]. (7)
2

The discriminator aims to decrease the probablity of
measuring |0i, and thus minimize Eq. 7 by getting close
to θd = π/2 which corresponds to the perfect swap test
given Dσfid (ρ(θg ))) ≤ 1. The next step is for the generator to maximize Dσfid (ρ(θg ))) by moving closer to the true
data. Ultimately, the generator cannot improve when
ρ(θg ) = σ.

|0⟩
ρ(θg)

H

H

Dσ(θd,ρ)

U (θd)

σ
FIG. 1: EQ-GAN architecture. The generator varies θg to fool
the discriminator; the discriminator varies θd to distinguish
the state. Since an optimal discriminator performs a swap
test, the global optimum of the EQ-GAN occurs when ρ(θg ) =
σ. While we include an ancilla qubit in the figure for clarity,
we implement a destructive ancilla-free swap test [15].

The cost function defined in Eq. (5) does not assume
that the input states σ and ρ(θg ) have to be pure states.
For simplicity, the example we provided in Eq. (6) does
assume pure state input. In the supplementary material, we discuss one proposal for EQ-GAN to accommodate mixed state input by replacing the pure-state fidelity with a mixed state fidelity measurement. Moreover, other discriminator architectures may be chosen to
ensure the existence of a Nash equilibrium. In the experiments presented below, we use a hardware-efficient
ansatz designed to correct dominant coherent gate errors. Although a poorly chosen circuit parameterization
may yield a non-convex loss function landscape and thus
be difficult to optimize by gradient descent, this is an
issue shared with the QuGAN due to the difficulty of expressing arbitrary unitaries as shallow quantum circuits
as well as with classical GANs. However, the EQ-GAN
architecture successfully converges on problem instances

(6)

that are unreachable by a fully trained and properly parameterized a QuGAN (see supplementary material).

LEARNING TO SUPPRESS ERRORS

We now show the improved robustness of an EQ-GAN
against gate errors compared to a more straightforward
swap test approach to learning an unknown quantum
state. Rather than adversarially training the parameterized swap test used as a discriminator in EQ-GAN, a
perfect swap test could be applied every iteration by a
frozen discriminator. This may also cause the generator
circuit to converge to the true data, since the swap test
ensures a unique global optimum.
However, in the presence of gate errors in the swap test,
this unique global optimum will be offset from the true
data. Since EQ-GAN is agnostic to the precise parameterization of a perfect swap test, an appropriate ansatz
can learn to correct coherent errors observed on nearterm quantum hardware. In particular, the gate parameters such as conditional Z phase, single qubit Z phase
and swap angles in two-qubit entangling gate can drift
and oscillate over the time scale of O(10) minutes [16, 17].
Such unknown systematic and time-dependent coherent
errors provides significant challenges for applications in
quantum machine learning where gradient computation
and update requires many measurements.
The large deviations in single-qubit and two-qubit Z
rotation angles can largely be mitigated by including additional single-qubit Z phase compensations. The effectiveness and importance of such systematic error mitigation is recently demonstrated in the success of achieving the-state-of-art accuracy in energy estimation for
fermionic molecules [18]. In learning the discriminator
circuit that is closest to a true swap test, the adversarial learning of EQ-GAN provides a useful paradigm that
may be broadly applicable to improving the fidelity of
other near-term quantum algorithms.
Suppose the adversarial discriminator unitary is given
by U (θd ), where U (θdopt ) corresponds to a perfect swap
test in the absence of noise. Given a trace-preserving
completely positive noisy channel E, the discriminator is
replaced by a new unitary operation Ũ (θd ). While a supervised approach would apply an approximate swap test
given by Ũ (θdopt ), the adversarial swap test will generically perform better if there exist parameters θd∗ such that
||Ũ (θd∗ ) − U (θdopt )||2 < ||Ũ (θdopt ) − U (θdopt )||2 . Because the
discriminator defines the loss landscape optimized by the
generator, the ρ(θg ) produced by EQ-GAN may converge

4
to a state closer to σ than possible by a supervised approach if the parameterization of the noisy unitary Ũ is
general enough to mitigate errors.

|0⟩ Z θ X θ Z θ H
|0⟩ √
X √
Z
2

{

1

U (θd)

Extremal
discriminator loss

{

{

ρ(θg)

3

Zθ H
Zθ H
4

Perfect SWAP

5

σ

FIG. 2: EQ-GAN experiment for learning a single-qubit state.
The discriminator (U (θd ) is constructed with free Z rotation
angles to suppress CZ gate errors, allowing the generator ρ(θg )
to converge closer to the true data state σ by varying X and
Z rotation angles.

FIG. 3: Comparison of EQ-GAN and a perfect swap test on
a physical quantum device. We experimentally confirm that
the EQ-GAN converges to a higher state overlap by learning
to correct such errors with additional single-qubit rotations.
The "converged" EQ-GAN (dashed line) coincides with the
iteration where the discriminator loss is minimized.

APPLICATION TO QRAM

As an example, we consider the task of learning the superposition √12 (|0i + |1i) on a quantum device with noise
(Fig. 2). The discriminator is defined by a swap test with
CZ gate providing the necessary two-qubit operation. To
learn to correct gate errors, however, the discriminator
adversarially learns the angles of single-qubit Z rotations
insert directly after the CZ gate. Hence, the EQ-GAN
obtains a state overlap significantly better than that of
the perfect swap test (Fig. 3). Although both methods
do not stay at the optimal point, this is typical of noisy
gradient measurements and minimax optimization:after
convergence to the Nash equilibrium, discretization can
induce perturbations while non-zero higher-order gradients lead the training to deviate from the global optimum [19].
We report the average error after multiple runs of the
EQ-GAN and perfect swap test on an experimental device (Table I).

Many quantum machine learning applications require
a quantum random access memory (QRAM) to load classical data in superposition [12]. More particularly, a set
of classical data can be described by the empirical distribution {Pi } over all possible input data i. Most quantum
machine learning algorithmsPrequire
the conversion from
√
{Pi } into a quantum state i Pi |ψi i, i.e. a superposition of orthogonal basis states |ψi i representing each single classical data entry with an amplitude proportional
to the square root of the classical probability Pi . Preparing such a superposition of an arbitrary set of n states
takes O(n) operations at best, which ruins the exponential speedup. Given a suitable ansatz, we may use an
EQ-GAN to learn a state approximately equivalent to
the superposition of data.

|0⟩
|0⟩

QML model
Perfect swap
EQ-GAN

Minimum error in state fidelity
(2.4 ± 0.5) × 10−4
(0.6 ± 0.2) × 10−4

TABLE I: Comparison of EQ-GAN and a perfect swap test
on a Sycamore quantum device. The error of the EQ-GAN
(i.e. 1 − state fidelity) is significantly lower than that of the
perfect swap test, demonstrating the successful adversarial
training of an error-suppressed swap test. Uncertainties show
two standard deviations.

|0⟩
|0⟩

H
Yθ
Yθ
Yθ

|0⟩

1

|0⟩

2

|0⟩

3

|0⟩

(a) Class 0 ansatz

H
Yθ
Yθ

1

2

(b) Class 1 ansatz

FIG. 4: Variational QRAM ansatzes for generating peaks by
learning θi parameters [20]. Class 0 corresponds to a centered
peak, and Class 1 corresponds to an offset peak.

5
CONCLUSION

(a) Empirical PDF

(b) Variational QRAM

FIG. 5: Two-peak total dataset (sampled from normal distributions, N = 120) and variational QRAM of the training
dataset (N = 60). The variational QRAM is obtained by
training an EQ-GAN to generate a state ρ with the shallow
peak ansatz to approximate an exact superposition of states
σ. The training and test datasets (each N = 60) are both
balanced between the two classes.

To demonstrate a variational QRAM, we consider a
dataset of two peaks sampled from different Gaussian
distributions. Exactly encoding the empirical probability
density function requires a very deep circuit and multiplecontrol rotations; similarly, preparing a Gaussian distribution on a device with planar connectivity requires deep
circuits (see supplementary material). Hence, we select
shallow circuit ansatzes (Fig. 4) that generate concatenated exponential functions to approximate a symmetric
peak [20]. Once trained to approximate the empirical
data distribution, the variational QRAM closely reproduces the original dataset (Fig. 5).
Training data
Exact sampling
Variational QRAM

Accuracy
53% ± 6%
69% ± 2%

TABLE II: Test accuracy (N = 60) of a quantum neural network (QNN) either trained on the all samples of the training
dataset (N = 60) for a single epoch or trained on the variational QRAM for an equal number of circuit evaluations.
Although the QNN trained on the variational QRAM did not
have direct access to the original dataset, accuracy is evaluated on the raw dataset. Uncertainties show two standard
deviations.

As a proof of principle for using such QRAM in a quantum machine learning context, we train a quantum neural
network [13] and compute hinge loss either by considering
each data entry individually (encoded as a quantum circuit) or by considering each class individually (encoded
as a superposition in variational QRAM). Given the same
number of circuit evaluations to compute gradients, the
superposition converges to a better accuracy at the end of
training despite using an approximate distribution (Table II).

Motivated by limitations of preexisting quantum GAN
architectures in the literature, we propose the EQ-GAN
architecture to overcome issues of non-convexity and
mode collapse. We adopt a parameterization of HilbertSchmidt norm as the cost function as oppose to trace
distance based on the optimality of Hilbert-Scmidt norm
in state-certification problems. Similar advantages of
Hilbert-Schmidt norm has been shown in quantum embedding designs of quantum kernel learning [21]. Other
approaches to a quantum GAN may improve a quantum
GAN's convergence properties - notably, recent work
suggests that certain cost functions such as the Wasserstein metric may provide more robust convergence [22].
However, we find that the EQ-GAN's shallow discriminator is effective at suppressing device errors and ensures robust convergence in running laboratory quantum
computers, making the EQ-GAN particularly relevant for
near-term applications of quantum computing. Moreover, we demonstrate the first experimental applicaiton
of EQ-GAN using Google's cloud quantum computers
in improving the classification accuracy of classical data
using QRAM. This work opens up new directions in utilizing quantum generative models to achieve quantum
speedup in machine learning that necessitates the efficient and high-delity preparation of QRAM.
AZ acknowledges support from Caltech's Intelligent
Quantum Networks and Technologies (INQNET) research program and by the DOE/HEP QuantISED program grant, Quantum Machine Learning and Quantum
Computation Frameworks (QMLQCF) for HEP, award
number DE-SC0019227.

∗
These two authors contributed equally to this work.
[1] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, in Advances in neural information processing systems
(2014), pp. 2672–2680.
[2] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung,
A. Radford, X. Chen, and X. Chen, in Advances in Neural Information Processing Systems, edited by D. Lee,
M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett
(Curran Associates, Inc., 2016), vol. 29, pp. 2234–2242,
URL https://proceedings.neurips.cc/paper/2016/
file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf.
[3] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang,
et al., in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017).
[4] E. Putin, A. Asadulaev, Y. Ivanenkov, V. Aladinskiy, B. Sanchez-Lengeling, A. Aspuru-Guzik, and
A. Zhavoronkov, Journal of Chemical Information
and Modeling 58, 1194 (2018), pMID: 29762023,
https://doi.org/10.1021/acs.jcim.7b00690, URL https:

6
//doi.org/10.1021/acs.jcim.7b00690.
[5] S. Boixo, S. V. Isakov, V. N. Smelyanskiy, R. Babbush,
N. Ding, Z. Jiang, J. M. Martinis, and H. Neven, arXiv
preprint arXiv:1608.00263 (2016).
[6] S. Aaronson and A. Arkhipov, in Proceedings of
the Forty-Third Annual ACM Symposium on Theory
of Computing (Association for Computing Machinery,
New York, NY, USA, 2011), STOC '11, p. 333–342,
ISBN 9781450306911, URL https://doi.org/10.1145/
1993636.1993682.
[7] F. Arute, K. Arya, R. Babbush, D. Bacon, J. C. Bardin,
R. Barends, R. Biswas, S. Boixo, F. G. S. L. Brandao,
D. A. Buell, et al., Nature 574, 505 (2019), URL https:
//doi.org/10.1038/s41586-019-1666-5.
[8] M. Y. Niu, A. M. Dai, L. Li, A. Odena, Z. Zhao,
V. Smelyanskyi, H. Neven, and S. Boixo, arXiv preprint
arXiv:2010.11983 (2020).
[9] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, arXiv preprint
arXiv:1312.6199 (2013).
[10] P.-L. Dallaire-Demers and N. Killoran, Phys. Rev. A
98, 012324 (2018), URL https://link.aps.org/doi/
10.1103/PhysRevA.98.012324.
[11] S. Lloyd and C. Weedbrook, Phys. Rev. Lett. 121,
040502 (2018), URL https://link.aps.org/doi/10.
1103/PhysRevLett.121.040502.
[12] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost,
N. Wiebe, and S. Lloyd, arXiv:1611.09347 (2016).
[13] E. Farhi and H. Neven, arXiv preprint arXiv:1802.06002
(2018).
[14] C. Bădescu, R. O'Donnell, and J. Wright, in Proceedings
of the 51st Annual ACM SIGACT Symposium on Theory
of Computing (2019), pp. 503–514.
[15] J. C. Garcia-Escartin and P. Chamorro-Posada, Phys.
Rev. A 87, 052330 (2013), URL https://link.aps.org/
doi/10.1103/PhysRevA.87.052330.
[16] F. Arute, K. Arya, R. Babbush, D. Bacon, J. C. Bardin,
R. Barends, R. Biswas, S. Boixo, F. G. Brandao, D. A.
Buell, et al., arXiv:1910.11333 (2019), URL https://
arxiv.org/abs/1910.11333.
[17] F. Arute, K. Arya, R. Babbush, D. Bacon, J. C. Bardin,
R. Barends, A. Bengtsson, S. Boixo, M. Broughton, B. B.
Buckley, et al., arXiv preprint arXiv:2010.07965 (2020).
[18] F. Arute, K. Arya, R. Babbush, D. Bacon, J. C. Bardin,
R. Barends, S. Boixo, M. Broughton, B. B. Buckley, D. A.
Buell, et al., Science 369, 1084 (2020), ISSN 0036-8075,
URL https://science.sciencemag.org/content/369/
6507/1084.
[19] A. Brock, J. Donahue, and K. Simonyan, arXiv preprint
arXiv:1809.11096 (2018).
[20] N. Klco and M. J. Savage, Phys. Rev. A 102,
012612 (2020), URL https://link.aps.org/doi/10.
1103/PhysRevA.102.012612.
[21] S. Lloyd, M. Schuld, A. Ijaz, J. Izaac, and N. Killoran,
arXiv preprint arXiv:2001.03622 (2020).
[22] B. T. Kiani, G. D. Palma, M. Marvian, Z.-W. Liu, and
S. Lloyd, Quantum earth mover's distance: A new approach to learning quantum data (2021), 2101.03037.
[23] V. Bergholm, J. Izaac, M. Schuld, C. Gogolin, M. S.
Alam, S. Ahmed, J. M. Arrazola, C. Blank, A. Delgado,
S. Jahangiri, et al., Pennylane: Automatic differentiation of hybrid quantum-classical computations (2020),
1811.04968.
[24] L. Mescheder, A. Geiger, and S. Nowozin, in Pro-

ceedings of the 35th International Conference on Machine Learning, edited by J. Dy and A. Krause
(PMLR, Stockholmsmässan, Stockholm Sweden, 2018),
vol. 80 of Proceedings of Machine Learning Research, pp.
3481–3490, URL http://proceedings.mlr.press/v80/
mescheder18a.html.
[25] S. Aaronson, The complexity of quantum states and
transformations: From quantum money to black holes
(2016), 1607.05256.

7
Mode Collapse Example of QuGAN

We provide a concrete example of mode collapse in the
original QuGAN architecture [10, 11]. Consider a true
data state σ and a generator initialized in state ρ, where
each state is defined by
1 + cos(π/6)σ x + sin(π/6)σ y
,
2
x
y
1 + cos(π/6)σ − sin(π/6)σ
ρ=
.
2

(8)

σ=

(9)

Generator
Discriminator

0.2

Training EQ-GAN

While the original QuGAN architecture is shown to
oscillate indefinitely for an example constructed in the
main text (see Fig. 6), we provide numerical experiments
here to demonstrate the successful convergence of the
proposed EQ-GAN architecture.

1.0

State fidelity w.r.t. true data

Maximizing Eq. 3 with a Helstrom measurement by
y
decomposing σ − ρ = σ2 , the discriminator will take
y
T = P + (σ − ρ) = 1+σ
2 . Optimizing over the space of
density matrices, the generator will rotate ρ to be pary
allel to T , also giving ρ0 = 1+σ
2 . In the next iteration,
the discriminator attempts to perform a new Helstrom
measurement to distinguish σ from ρ0 , but this results in
T 0 = P + (σ − ρ0 ) = ρ. As the generator realigns to match
the new measurement operator, we find that ρ00 = ρ.
It is now straightforward to see that if the QuGAN is
trained to fully solve the minimax optimization problem
each iteration, it will never converge; instead, it will always oscillate between states ρ and ρ0 , neither of which
are the Nash equilibrium of the minimax game (Fig. 6)
for the QuGAN performance under such mode-collapse.

0.4

P + +
P+ =
i |φi ihφi | obtained from the positive part of
(k)
the spectral decomposition of σ − ρ. If Tσ is the kfold composition of T with itself, then the existence of
some k > 1 such that T (k) = ρ is sufficient to ensure
oscillation between k states. For a system of n qubits,
we may achieve this by preparing the target and initial
state separated by an angle of π/3 on the generalized
Bloch sphere.

0.9
0.8
0.7

QuGAN (partial training)
QuGAN (full training)
EQ-GAN (partial training)
EQ-GAN (full training)

0.6
0.5
0.4

0

20

40

60

80

100

120

Iteration

Loss

0.0
FIG. 7: Comparison of QuGAN [10, 11] and EQ-GAN learning the state given by Eq. 8. Full training denotes training the
generator for 50 epochs then the discriminator for 50 epochs
each iteration; partial training denotes only 1 epoch per iteration. The QuGAN remains more unstable than EQ-GAN
during training with either training configuration.

0.2
0.4
0.6
0.8
1.0
0

50

100

Training episodes

150

200

FIG. 6: Performance of QuGAN [10, 11] learning the state
defined in Eq. (8) with initialization given by Eq. 9. Mode
collapse manifests as an oscillation in the generator and discriminator loss without converging to a global optimum. The
implementation is based on the original architecture in Pennylane [23].

More generally, we can consider the oscillation between a finite set of states. Let the function Tσ (ρ) =
P + (σ − ρ) denote the optimal Helstrom measurement

In Fig. 7, we illustrate a subtlety in the oscillatory analysis presented in the main text. Within the GAN formalism, the generator and discriminator iteratively optimize
a given loss function. When the optimization is allowed
to converge to an extremum of the loss function in the
QuGAN architecture specifically, the result is determined
by a Helstrom measurement. It is for this case that indefinite oscillation is shown; in the case of learning the
state σ constructed in the main text, oscillation between
states ρ and ρ0 result in a constant state overlap of 3/4.
However, the iterative optimization procedure to move
towards the optimal Helstrom measurement may be only
partly completed, i.e. the generator and discriminator
are not allowed to extremize the loss function. With such

8
a selection of hyperparameters, we observe that oscillation between states continues (Fig. 7), leading to unstable
training for the QuGAN architecture. In comparison, the
same hyperparameters perform well for the EQ-GAN architecture, which steadily approaches the true data state.
Unstable training is difficult to overcome even in classical GAN architectures [24], and thus advances in understanding how to prevent such non-convergence are consequential for both quantum and classical machine learning.
To help ensure stable training of the EQ-GAN architecture, we introduce a training procedure that capitalizes on the fact that the discriminator must converge to
a swap test at the optimal Nash equilibrium. Rather
than training both the generator and discriminator from
the beginning, we pre-train the EQ-GAN in a supervised
setting. In the first phase, the discriminator is frozen
with the parameters of a perfect swap test, although the
unitary Ũ (θdopt ) may be an imperfect swap test; the generator is trained until the loss converges. In the second
phase of training, the discriminator is allowed to vary adversarially against the generator, seeking the parameters
θd∗ . In the context of gate errors, this second phase may
yield a unitary closer to a true swap test. The example
shown in Fig. 3 of the main text on a physical quantum devices is replicated in Fig. 8 here, showing the two
phases of training and the benefit of an adversarial swap
test in the presence of noise.

EQ-GAN Discriminator Architecture

We implement an ancilla-free swap test to perform
state discrimination (Fig. 9). To evaluate the swap
test on a Sycamore quantum device, we decompose each
CN OT gate into (I ⊗H)CZ(I ⊗H) operations to use the
native CZ gate. As discussed in the main text, the CZ
gate has unstable errors that can be effectively modeled
with Z rotations by an unknown angle on either qubit.
The EQ-GAN formalism can overcome the single-qubit
phase error by applying RZ(θ) gates directly after each
CZ operation. During adversarial training, the free angles θ are optimized with gradient descent to mitigate the
two-qubit gate error. Due to the convergence properties
provided by the generative adversarial framework, the
discriminator provably converges to the best state discriminator possible. This motivates early stopping (as
shown in Fig. 3) when the discriminator loss indicates
that the best state discriminator has been achieved.

FIG. 9: Ancilla-free swap test between two 3-qubit states. By
rewriting the controlled-swap operation as CNOT and Toffoli
gates and replacing computational basis operations with classical post processing, the swap test can be performed with an
ancillary classical bit.

QNN Architecture
Frozen
Adversarial
discriminator discriminator
Perfect SWAP

FIG. 8: Comparison of EQ-GAN and a perfect swap test
for a simulated noise model. Normally distributed noise on
single-qubit rotations are applied with a systematic bias away
from zero, causing the swap test to force convergence to the
incorrect state.

We seek to demonstrate an empirical difference in performance between training a quantum neural network
(QNN) with individual examples of a classical dataset
and with a superposition of data as obtained from a pretrained EQ-GAN. To use the native CZ two-qubit gate,
we implement a rank-4 entangling gate G given by


1 0
0 0


0 e−iθ 0 0
G(θ) = 
(10)
,
0 0 e−iθ 0
0 0
0 1
which is decomposed as shown in Fig. 10.
Due to the planar connectivity of a Sycamore quantum
device, we implement the QNN shown in Fig. 11 with a
four-qubit data state.

9

=

Rz(– π2 ) Rx( π2 ) Rz( π2 )

Z

Z
Z Rx(θ) Z

Rz( π2 ) Rx( π2 ) Rz(– π2 )

FIG. 10: Decomposition of the two-qubit entangling gate G(θ)
used in the QNN ansatz (Eq. 10).

|0⟩ X
|data⟩

{

H

H

FIG. 11: Quantum neural network architecture (left) and its
corresponding layout on the Sycamore device (right). A fourqubit data state is constructed with the circuits shown in
Fig. 4 and placed in the |datai state on the blue qubits. A
readout qubit (orange) performs parameterized two-qubit interactions shown in Fig. 10.

estimator. For each parameter query, the output of the
QNN is averaged over 10 trials to reduce any statistical
fluctuations. QNNs using the final learning rates (10−3.93
for sampling and 10−1.83 for superposition) are then evaluated over 50 trials to obtain the final performance reported in Table II with computed standard deviations.

Adversarial Learning Without Errors

While the main text discusses the benefit of adversarial
learning in the noisy case, i.e. the automatic suppression
of device errors, we provide additional motivation here
for using adversarial learning in the noiseless case. In
particular, we construct an example for which a perfect
swap test fails and adversarial learning successfully generates the true data state (Fig. 12).

The QNN is trained in two ways: it is either trained via
sampling (shown one training example each iteration, as
in Ref. [13]) or via superposition (shown a superposition
over an entire class each iteration). As discussed in the
main text, the superposition methodology does not use
an exact superposition of the training dataset. Instead,
it uses a shallow approximation obtained by pre-training
an EQ-GAN. We prepare a symmetric concatenation of
exponential functions to approximate a peak with minimal circuit depth. In comparison, preparing a Gaussian
distribution over n qubits requires (n−1)-controlled rotations, which must be decomposed into 2n−1 CZ gates to
use the native gate basis (see Fig. 10 of [20]); additional
swap operations are required to prepare the state on a
planar architecture. Given the empirical dataset, we may
also prepare an exact superposition of the data following
a state preparation procedure such as that proposed in
Ref. [25]. However, this also requires n-controlled rotations, leading to an exponential dependence in the number of qubits. All three versions of the QRAM are shown
in Fig. 14.
To ensure a fair comparison, we permit an equal number of queries to the quantum device. Consequently, for
N = 60 examples with 30 examples per class, training via
sampling is performed for 1 epoch with 60 corresponding
to 60 iterations performed on the quantum device. However, training via superposition evaluates the superposition of each class 30 times (since there are two classes),
also accessing the quantum device for 60 iterations.
Additionally, Bayesian optimization is used to tune different learning rates for the sampling and superposition
methodologies. In simulation, we optimize over Adam
learning rates from 10−4 to 10−1 with 10 random parameter tries and 40 evaluations of the Gaussian process

State fidelity w.r.t. true data

1.00
0.95
0.90
Perfect SWAP

0.85

EQ-GAN

0.80
0.75
0.70
0

50

100

150

200

250

300

Iteration
FIG. 12: Demonstration of a vanishing gradient for a perfect swap test and convergence for the EQ-GAN. While the
swap test cannot be trained by gradient descent, the EQ-GAN
achieves a state overlap of 0.97.

|0⟩
|0⟩

Z X
√

θ1

X √
Z
Zθ √
2

FIG. 13: Generator and data circuit with a vanishing gradient given data defined by X and Z rotations of π/2 and a
generator initialized with zero angles.

Given the generator ansatz shown in Fig. 13, define
the data state to have angles α0 = β0 = π/2 for corresponding rotations Rx (α0 ), Rz (β0 ). The generator then
optimizes angles α, β towards achieving full state overlap.
p In general, the gradient of the state overlap is
π
2 − 2 cos(2πα) cos(2πβ). By initializing the genera4
tor with α = β = 0, the gradient and all higher derivatives of the overlap vanish. Since a noiseless supervised
learning approach with a perfect swap test can only eval-

10

(a) Exponential peak ansatz

(b) Gaussian peak ansatz

|0⟩

|0⟩

H
|0⟩ Y θ
|0⟩ Y θ
|0⟩ Y θ

H

1

H
H

2

|0⟩

H
Ry

|0⟩

H

H Ry H

H Ry
H

|0⟩

H
H

3

X
√
H Ry √
Z √
X √
Z
–2

X
√

(cont.)

H

H

H Ry √
Z √
X √
Z
–2

Z √
X Z

X
√

Z
√

Z √
X √
Z
√

Z √
X √
Z
√

Z H
√

–2

X
√

–2

–2

Z √
X Z

X
√

Z
√

X
√
Z √
X √
Z
√

Z √
X √
Z
√

Z √
X √
Z
√

Z H
√

H Ry

–2

–2

–2

–2

H Ry √
Z √
X √
Z
–2

Z √
X Z
Z √
X √
Z
√
–2

Z √
X Z

X
√

Z
√

Z √
X √
Z
√

Z √
X √
Z
√

Z
√

–2

X
√
Z √
X √
Z
√

–2

Z H
√
Z
√
–2

–2

–2

H
H
H

H

H

(c) Exact superposition
|0⟩
|0⟩

H
X

H Ry H

H Ry H
X

H Ry H

|0⟩

H Ry √
X
Z √
X √
Z
√
–2

Z √
X Z
Z √
X √
Z
√
–2

X
√
Z √
X √
Z
√
–2

(cont.)

Z
X
√
√
Z
√
Z √
X √
Z
√
–2

–2

|0⟩

Z √
X Z

X
√

Z
√

Z √
X √
Z
√

Z √
X √
Z
√

Z
√

–2

–2

H

–2

H Ry H

H Ry H

H Ry H

X

H Ry H
X

H Ry H

H Ry H

H Ry H

X

H Ry H
X
X

H Ry H

H Ry H

H Ry H

H Ry √
X

Z √
X Z

X
√

Z H
√

Z √
X √
Z
√

Z √
X √
Z
√

Z √
X √
Z
√

Z X
√

H Ry H

H Ry H

H Ry H

–2

H Ry H

H Ry H

H Ry H

H Ry H

H Ry H

H Ry H

H Ry H

H Ry H

H Ry H

H Ry H

X

H Ry H

H Ry H
X
X
X

H Ry H

H Ry H

H Ry H

H Ry H

H Ry H

H Ry H

H Ry H

H Ry H

–2

–2

–2

H Ry H

H Ry H

H Ry H

H Ry H

X
X

H Ry H

H Ry H

H Ry H

H Ry H

H Ry
X

X

FIG. 14: QRAM ansatzes for n = 4 qubits in planar connectivity with (a) exponential peaks (3 two-qubit gates), (b) Gaussian
distribution (21 two-qubit gates), and (c) exact superposition (57 two-qubit gates). We adopt ansatz (a) because circuit depth
scales polynomially for a QRAM with n qubits, while (b) and (c) scale exponentially with n.

uate the gradient of a state overlap measurement, gradient descent will fail to converge to the correct values.
On the other hand, by allowing the discriminator to
change, the issue of a vanishing gradient is circumvented
and the generator learns the data state (Fig. 12). For
simplicity, we use the same discriminator architecture as
that used for suppressing errors. Parameters are optimized with vanilla stochastic gradient descent. The EQGAN learning rate schedule is manually tuned, and we
verify that no selection of learning rate allows the perfect
swap test9 to converge.

a probabilistic mixture of the quantum state by sampling from {Pi } and then prepare the associated state.
This obviates the possible double exponential overhead in
learning the full quantum channel that transform a fixed
initial state to the desired mixed state, as illustrated in
Fig. 15.

(a) EQ-GAN for pure state

(b) EQ-GAN for mixed state

Hybrid EQ-GAN for Mixed-State

While classical GANs use a random latent vector to
generate fake data, the quantum GAN proposed here and
in the existing literature does not require any such random input. This comes with a price, especially when
our goal is to learn quantum data in a mixed state. As
shown in Fig. 15, a factor of two overhead in the number
of qubits are needed for mixed-state learning based on
Choi's theorem.
A closer look at the mathematical nature of a mixed
state points us to a more efficient representation through
a hybrid classical-quantum network. A mixed state repP2n
resented in the most generic form ρ = i=1 Pi |ψi ihψi | is
specified by a classical probability distribution {Pi } over
2n discrete variables corresponding to the set of quantum
states {|ψi i} that diagonalize the density matrix. Naturally, one can efficiently represent the classical part of
this representation, {Pi }, with a classical neural network,
while a quantum circuit prepares the state |ψi i given parameter set θgi . In this way, we will be able to output

FIG. 15: Diagram for EQ-GAN architecture based on quantum swap tests. (a) EQ-GAN for learning how to generate
pure-state quantum data. (b) EQ-GAN for learning how to
generate mixed-state quantum data. The true input data is
represented by σ, and the fake input data ρ is prepared by a
unitary circuit Uθg . The discriminator QNN realizes a unitary transformation represented by Uθ~a jointly on the true
data, fake data and the ancillary qubit. Measurement on the
ancillary qubit is used for the cost function similarly to the
EQ-GAN defined in the main text.

