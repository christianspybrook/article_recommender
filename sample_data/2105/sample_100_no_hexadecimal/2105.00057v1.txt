SPEEDING UP PYTHON-BASED LAGRANGIAN FLUID-FLOW
PARTICLE SIMULATIONS VIA DYNAMIC COLLECTION DATA
STRUCTURES∗

arXiv:2105.00057v1 [physics.comp-ph] 30 Apr 2021

CHRISTIAN KEHL , ERIK VAN SEBILLE† , AND ANGUS GIBSON‡
Abstract. Array-like collection data structures are widely established in Python's scientific
computing-ecosystem for high-performance computations. The structure maps well to regular, gridded lattice structures that are common to computational problems in physics and geosciences. High
performance is, however, only guaranteed for static computations with a fixed computational domain. We show that for dynamic computations within an actively changing computational domain,
the array-like collections provided by NumPy and its derivatives are a bottleneck for large computations. In response, we describe the integration of naturally-dynamic collection data structures (e.g.
double-linked lists) into NumPy simulations and ctypes-based C-bindings. Our benchmarks verify
and quantify the performance increase attributed to the change of the collection data structure. Our
application scenario, a Lagrangian (oceanic) fluid-flow particle simulation within the Parcels framework, demonstrates the speed-up yield in a realistic setting and demonstrates the novel capabilities
that are facilitated by optimised collection data structures.
Key words. collection data structures, Python, just-in-time compilation, performance analysis,
performance optimisation, numerical methods, NumPy, scientific computing
AMS subject classifications. 68P05, 68Q25, 18B05, 70B05

1. Introduction. During the past decade, the rise of Python as a versatile programming platform for scientific research, particularly in physics and geoscience, has
led to the development of various domain-specific numerical simulation- and modelling platforms for subsurface modelling (e.g. GemPy [8]), plate tectonics (e.g.
GPlate [17]), Eulerian oceanic flow simulations (e.g VEROS [10] and others [16]) and
Lagrangian particle tracing within ocean currents (e.g. Parcels [9]). Simulation and
modelling with Python is advantageous for its coding simplicity for novice developers
and domain experts, its versatility due to its typeless, interpreter-based processing and
its computational efficiency via its links to establish high-performance linear algebra
libraries (e.g. BLAS [4] and LAPACK [2]) through packages such as NumPy [25, 11],
SciPy [27] and Scikits [26, 21]. Additionally, Python's embedding in modern DevOps
tools (e.g. Git) and its wide application in open-source and open-science projects are
additional reasons for its high acceptance in the scientific community.
The development of Parcels [14, 9], which is our application platform in this
paper, relies on the high-performance computation within Python. In this regard,
it already employs ctypes [20] as just-in-time (JIT) C-interface to execute highlyadaptive kernels for the advection and tracing of particles on ocean general circulation
model (OGCM) fields. Those kernels are provided as elementary Python functions by
the developers as well as the user community, and are compiled at runtime into C-code
for rapid field interpolation and advection evaluation. The available high-performance
packages, such as NumPy, handle dense-matrix data organisation with high efficiency.
On the other hand, problems that cannot be translated into a dense-matrix algebra
∗ Submitted

to the SIAM Journal on Scientific Computing editors 2021-01-25.
Funding: This work was funded by the European Research Council (ERC) under TOPIOS project (grant no. 715386). Simulations were carried out on the SURF's Dutch National e-Infrastructure
(project no. 16371 and 2019.034).
† Institute for Marine and Atmospheric Research, Utrecht University, the Netherlands
(c.kehl@uu.nl, e.vansebille@uu.nl).
‡ The Australian National University (angus.gibson@anu.edu.au).
1

2

C. KEHL, E. VAN SEBILLE AND A. GIBSON

problem, or data that are dynamically altered during the simulation, pose a challenge
to efficient calculations, because the re-organisation of the data container (i.e. arraylike collection) can in some cases consume more runtime as memory overhead than
is required for the particle advection itself. This issue also arises in other physics
simulations that concern large sets of dynamically-changing physics entities (e.g. the
common nBody simulation where bodies are 'on-the-fly' added and deleted).
After careful benchmarking (section 3), we identified this data collection reorganisation as the major barrier to increased performance for particle simulations
in fluid flows. In this paper, we propose the introduction of alternative data structures for the organisation of dynamic data entities in Python. Furthermore, those data
structures are expressed in a JIT-compliant format for increased performance. The
major contribution of this article is a detailed performance evaluation and comparison
of various data structures and data organisation patterns.
The starting point of our investigation is a common array-of-structure arrangement of the particles within a dense array. The performance benefit of a structure-ofarray arrangement, which allows for higher cache coherence and better data locality
in vectorized computations, is evident for static data arrays. On the other hand, a
list-of-array arrangement with sublists of defined maximum size, limiting the memory
reordering overhead to a fixed upper bound, only provides marginal improvements despite the theoretical upper bound on computational complexity (see subsection 4.1).
In scenarios with dynamic data arrays, where particle entities are inserted-into and
removed-from the particle collection arbitrarily during the simulation, the runtime
expense for data reordering, memory copies and poor data locality becomes evident.
In that context, double-linked list containers show a distinct performance advantage,
especially for large particle collections with more than 215 entities.
2. Related Work. In this section, we first review the current state of efficient
(collection) data structures in Python's scientific computing environment. As this is
a comprehensive topic with abundant literature, we concentrate mainly on non-arraylike collections and their access potential in Python – C wrappers.
Afterwards, we present the background knowledge on our physics application of
Lagrangian oceanic flow simulations via particles. That section mainly serves as an introduction to our workflow and provides background information to the performanceand runtime bottleneck concerns.
2.1. Efficient data structures in scientific computing. Native Python is
known for its hampered processing performance. In order to circumvent the compute
bottleneck, NumPy [25] was introduced as fast C-wrapper to BLAS and LAPACK,
which arranges its data efficiently as blockwise matrices or dense arrays (i.e. array collections). Those array-like collections, such as dense arrays and matrices,
are common high-performance data structures, especially in Python, for modern scientific computing. The data structure's main objective, which is random access of
items, is rarely required in modern computing. Frameworks such as SciPy [27], scikitimage [26], scikit-learn [21] and other machine-learning frameworks [3, 1, 6]) build on
top of NumPy's dense-array and matrix data structures. A common, fundamental
issue with array-like collections is that random insertions and deletions of items in
the collection requires array copies and merges (see [24]). This slows down the operation and requires an additional data buffer of the size of the collection, which can be
prohibitively expensive in modern big data-computing.
The Python-internal bisect module facilitates a more list-like collection behaviour
via binary searches of objects in array-like collections. That said, the binary search

SPEEDING UP PYTHON-BASED LAGRANGIAN PARTICLE SIMULATIONS

3

only simplifies object search and object access, but it provides no functionality to
speed-up native, internal item insertion or deletion. This functionality is provided by
the collections.deque module, which is a linked nodelist. In terms of item insertions
or deletions performance, it exhibits the expected constant runtime complexity (i.e.
logarithmic complexity, when including item localisation). That said, working with
the collections.deque module is difficult because the C-interface and data structure
access (e.g. via ctypes) is poorly documented. Moreover, the connection cannot
reliably be established because the C-interface is a double-linked list whereas the
Python interface is a uni-linked list.
The sortedcollections is another Python module that promises a C-comparable
performance behaviour for dynamic collections, such as the SortedList for 'std::list'
equivalence. The module internally wraps Python lists (i.e. array-like collections) in
an internally-managed item indexing scheme. The indexing scheme provides constantcomplexity insertions and deletions, while the use of the bisect module internally
facilitates fast object access. All together, the module provides all performancerelevant features and advantages of list-like collections. The drawback of the module
is the lack of Python – C interfaces for fast execution of simulations and analyses.
The gap we address in this paper is the development and benchmark of dynamic
collection data structures in Python with an integrated C-interface for rapid computations on dynamically-organised data.
2.2. Lagrangian Particle Tracing in Oceanic Flows with Parcels. Our
physics-motivated prime application scenario is the tracking of (virtual) particles in
oceanic flows. These virtual particles can for example represent plastic [15], plankton [18], nutrients [5] or fish [23]. The tracing is done by Lagrangian advection, where
particles follow a given background (Eulerian) advective flow field [9] and whose trajectories are numerically integrated over time as given in eq. (2.1), implemented in
e.g., an Euler-forward- or Runge-Kutta scheme [19].

(2.1)

X(t + ∆t) = X(t) +

Z

t+∆t

v(x(τ ), τ )dτ + dXb ,

t

with X(t) = x representing an individual particle's position at time t, advected
in ∆t timesteps and discretized over τ , v(x(t)) is the (gridded) fluid flow from a
numerical simulation, and dXb is an optional position change because of particle
'behaviour' (swimming, sinking, etc). The partial integral is required to match up the
temporal resolutions of the underlying flow field and the particle simulation.
Considering the computational complexity in this particle advection scheme, there
is a distinction between the theoretical and practical computational workload. Theoretically, the operations that majorly contribute to the compute load are (i) the
R t+∆t
velocity interpolation t
v(x(τ ), τ )dτ on the gridded Eulerian flow field, (ii) the
position integration X(t + ∆t) with a fixed- or adaptive timestepping and (iii) the
sum of partial integration results (i.e. fused multiply-add (fma) operations). Next to
those plain calculations for the particle trajectory, the Lagrangian simulation's main
purpose is the tracking of particle properties (be they of physical, biochemical or behavioural nature) over time. Those functions are executed in Parcels by the help of
user-defined kernels. Depending on the complexity of the particle model, those operations represent the majority of calculations in a practical setting. On the other hand,
as those kernels are case-specific and hardly generalizable, their compute workload is
not considered in this paper's experiments.

4

C. KEHL, E. VAN SEBILLE AND A. GIBSON

In terms of performance and benchmarking, the runtime of the whole simulations,
as well as each integration iteration individually, is the sum of the compute time (as
analysed above) and the input-/output (I/O) time. The latter is further split into
memory-I/O, file-I/O and plotting time (which is the special case for device output
happening not exclusively to files nor in-memory).
In realistic, practical Lagrangian fluid flow simulations, the compute time is by
far exceeded by the file-I/O, as the (oceanic) flow fields need to be dynamically loaded
from large file databases. Hence, from a practical perspective, Lagrangian fluid flow
simulations are typically strictly I/O-bound tasks. Parcels [9] already implements multiple I/O-enhancing techniques (e.g. deferred loading, dynamic and selective loading
of exclusively occupied cells), and thus we are hereby more concerned about improving
the particle integration itself for large particle sets (i.e. particle swarms). Therefore,
all experiments in this paper utilize synthetic, pre-loaded perlin-noise flow fields [22]
that remain resident in memory over the whole simulation, neglecting file-I/O time
of the flow fields.
An in-depth performance analysis is required for the case of large particle sets, because effects from hardware caching, memory layout and particle set data organisation
have a (potentially) measurable impact on the overall runtime. From the perspective
of Lagrangian fluid flow simulations as a particle system, a particle set can be statically allocated and remains fixed in its data entities over the the whole simulation.
Alternatively, in order to circumvent limited-precision floating-point errors and to
comply with case-specific simulation requirements, particles can be dynamically reemitted and removed from the particle set. Both cases (potentially) behave differently
in terms of performance and runtime due to data access- and alteration patterns for
different collection data structures that can be used to store a particle set.
The potential performance difference between static and dynamic particle sets,
its quantification, its rationale, its impact on different oceanic flow simulation cases,
and its exploitation for speed improvement are the main experimental subjects of this
article.
3. Status-Quo Benchmarks. The investigation into performance enhancing of
Lagrangian particle simulations starts with the following research questions: What are
the most costly functions and functionalities within the existing particle simulation?
In order to answer this question, we needed to (i) determine the most costly functions
within the simulation framework, (ii) quantify the controls (i.e. impact parameters)
on the computational runtime, and (iii) test the different computational settings of
the simulation, such as Python- and C-based kernel evaluation, the impact of task
parallelisation via message passing (MPI), and the cost of dynamic array operations.
For this quantification, the Parcels code1 in version 2.1.5 was profiled using the cProfile package. The outcome were line-by-line code timings, which were analysed with
snakevis[7], plotting a percentage runtime as in Figure 1.
Figure 1 shows the total runtime (blue bars) of a 365-day particle simulation
on a synthetic flow field generated by perlin noise (see Figure 2), advected with a
fixed temporal resolution of 1 hour (13,000 seconds for the removal-scenario, 265,000
seconds for the insertion-scenario). In both presented cases, the simulation targets
an average number of particles in the array-collection of 218 . At the third code
level, one can see a first significant difference: the active garbage collector (darker
green bar) consumes a significant amount of the runtime (5.6%) for dynamic removal,
1 OceanParcels

website - http://oceanparcels.org/

SPEEDING UP PYTHON-BASED LAGRANGIAN PARTICLE SIMULATIONS

(a) dynamic particle insertion

5

(b) dynamic particle removal

Figure 1. Plot of per-codeline runtime distributions for different dynamic array operations.
The average particle number target in both cases is 218 . Those plots show runs with no MPIparallelisation and C-evaluated kernels.

which is not the case for the dynamic-insertion scenario. Furthermore, in the third
and fifth level we see that both scenarios spend large amounts of time in localising
erroneous (30.46% for removal-scenario; 29.73% for insertion-scenario) and deleted
(31.15% for removal-scenario; 31.28% for insertion-scenario) particles. The removal
scenario, where particles are actually deleted, also spends approximately twice the
time in the removal function than the insertion scenario (4.48%, compared to 2.11%).
Those extra runtime expenses for item removal and garbage collection result in less
time being spent in the actual particle advection (only 25.46% for the removal scenario,
compared to the 35.89% for the insertion-scenario). The remaining percentages are
smaller helper functions, list-comprehensions and memory management. We note
here again that the synthetic flow field on which particles are advected is held entirely
in-memory and that particle positions are not written to external storage, so that
extensive file-I/O is not a limit in those cases.
The following top-5 most expensive execution functions emerge from an extensive
benchmark analysis of the different scenarios described above (i.e. static-vs.-dynamic
datasets, varying particle set sizes, etc.). Furthermore, these are the most-expensive
functions as their detrimental impact on performance scales with the size of the
particle set, and hence are over-pronounced even when running large particle sets
in parallel to reduce the common file-I/O performance malus in real-world ocean
simulations.
1. list comprehension in kernel execution that gathers all unadvected particles
(computational complexity: O(N ))
2. delete() function for removing individual particles from the particle set (computational complexity: O(N 2 ))
3. array-copy operation that flips the dense-array arrangement in memory from
column-major order (FORTRAN/Python-native orientation) to row-major
order (C/C++ -native orientation) for C-internal kernel evaluation (computational complexity: O(N ))
4. the actual particle advection computation function, which would ideally be
at the top of this list (34% of the runtime)
5. another array transpose-and-copy operation that performs the nD-densearray concatenation for periodically-released particles (computational com-

6

C. KEHL, E. VAN SEBILLE AND A. GIBSON

Figure 2. Probabilistic, synthetic fluid flow field, generated from perlin noise [22]. The background displays absolute flow speeds in range {0, ..., 1.0} m
in a metre-scale flow field, whereas the
s
green particle traces depict the particles' trajectory within the flow.

plexity: O(2N ))
Therefore, the clear target of this research is to reduce the impact of the abovelisted (super-)linearly runtime scaling of collection re-ordering- and alteration functions, to the point that the advection function becomes the major runtime expenditure
of the simulation. This would, as traditionally intended, transform the whole simulation process into a compute-bound application, which can subsequently be sped
up by common high-performance computing approaches, such as SIMD processing,
parallelisation and load distribution.
The reason for the runtime malus in the above-listed operations is that densearrays are random-access data structures that exhibit a high computational- and
memory complexity for random alterations. In other words, adding and removing
entities in an array-collection requires splitting the collection in multiple distinct
subsets that exclude removed- and include inserted elements, and concatenate them
together into a newly-allocated result collection (i.e. they are not in-place operations).
Thus, we need to exploit alternative data structures that exhibit a sub-linear scaling
on those operations.
4. High-performance collection data structures for Python-based numerical simulations. In this paper, we propose to supplement Python with dynamic collections, such as double-linked node-lists, bisect-search augmented arrays,
with explicit C-bindings to perform high-speed kernel executions in C. Those structures attempt to remedy the performance malus rooted in slow random-alteration

SPEEDING UP PYTHON-BASED LAGRANGIAN PARTICLE SIMULATIONS

7

methods on array-collections.
Our comparison baseline is a common Array-of-Structure (AoS) array-collection
data structure, which is a simple array housing complex multi-dimensional objects (see
Figure 3 and the supplement material for details). The simplicity is advantageous for
the conversion of the data structure into C, but it suffers from the severe speed malus
for dynamic alterations.

Figure 3. Array-of-Structure (AoS) collection memory layout - a collection data structure with
an item-linearized memory alignment. The linear item order is annotated by the black major arrow.

In order to address the trivial and most common request to do engineering optimisations, such as speedy list comprehensions, reduced memory copies and so forth,
we created a first comparison case of engineering-optimised AoS, to showcase the
potential as well as the limitations of simplistic and trivial improvements.
4.1. List-of-Arrays [-of-Structures] (LoA[oS]) collection. A first attempt
to provide an upper bound to the scaling complexity of random array alterations is
to split up the global array into a list of fixed-length arrays. We refer to this data
structure as List-of-Array (LoA) collection in the text remainder, though highlighting
that the underlying array structure is AoS by itself.
Such a splitting setup subsequently requires an address translation when randomly accessing and retrieving items of the collection, hence slowing down excessivelynumerous item look-ups. The advantage is that the complexity of item insertion- and
deletion is reduced from O(N ) = K ∗(N +(N ±1)+(N ±1)), with K being the number
of added or removed items and N being the total number of items (i.e. particles), to
N
O(N ) = K ∗ (D + M + (M ± 1) + (M ± 1) + (D + 2 M
2 )), with D = M and M being
the maximum size of the sub-arrays and a loose bound that N >> M . The latter
additional accounts for the effort of merging two sub-arrays when their occupancy is
below 50%. The layout is illustrated in Figure 4.
4.2. Structure-of-Arrays (SoA) collection. The SoA representation is a
transposition of the memory layout of AoS. Rather than using a single instance of the
container datatype at the top-level, whose elements are particle structures, there is
only one instance of the particle structure with an array of values in each field (e.g.
velocity, position). A particle is then denoted by an index, and can be reconstructed
by extracting the element at that index from all the constituent fields.

8

C. KEHL, E. VAN SEBILLE AND A. GIBSON

Figure 4. List-of-Array (LoA) collection memory layout - a node-list container with fixed-size,
pre-allocated, AoS-organised sub-arrays.

Due to its layout, SoA lends itself to a vectorized instruction flow, as common
to modern linear-algebra software frameworks (i.e. BLAS, LAPACK, NumPy) [12].
When accessed from Python, a loop over all particles may be turned into a loop of
NumPy-accelerated vectorized operations over the particle fields. This is particularly
apparent in the case of multiple particle initialization, where the allocation of many
independent objects can be replaced by the instantiation of just a few arrays. SoA
may also be more cache-friendly for the just-in-time advection calculation. A field
value for multiple particles may be loaded into a single cache line (see Figure 5), thus
an optimizing compiler may choose to use vectorized CPU instructions to operate on
multiple particles at once (see SM1 in the supplementary material).
The linearized in-memory layout of SoA is a trade-off of speed for dynamic flexibility. Cache coherence and high data locality in vectorized processing are prioritized
over efficient deletion of arbitrary particles. Accessing and manipulating individual
particles also becomes unwieldy, as all the fields are accessed separately.

Figure 5. Structure-of-Array (SoA) collection memory layout - a collection data structure with
an attribute-linearized memory alignment.

4.3. Nodal Double-Linked List collection. The proposed LoA-collection enforces an upper bound on the computational complexity of dynamic collection alterations, while the SoA-collection inherently only improved cache access during advection
execution, hence not underpinning tackling the dynamic collection alteration issue.
In order to reduce that computational complexity, the actual dynamic collection alteration process needs to be addressed.

SPEEDING UP PYTHON-BASED LAGRANGIAN PARTICLE SIMULATIONS

9

We here introduce a double-linked list implementation into the Parcels particle
framework. The novel aspect here is its straightforward implementation with ctypes,
which allows direct access of the data collection within the high-speed C-kernels.

Figure 6. Node-based double-linked list collection memory layout.

In this implementation, the Node objects are a complementary of Python classobjects and their C-representation as a structure. Each node stores a reference to
its predecessor, its successor and its data container. The data container in this case
is a particle-object reference (in Python) and its NumPy-object pointer reference (in
C). This NumPy-object reference of a particle is just a pointer to a NumPy-mapped
C-struct reference of one particle, not a pointer to a NumPy array.
During runtime, particles can dynamically be added and removed with the standard complexity of O(∞) for linked lists. Particles are bound to a Node, and insertion
and removal is performed by plain relinking of the predecessor- and successor references in the collection. In contrast to Python's internal deque implementation, the
list can commonly be traversed by begin-end-iterators or foreach-element-iterations
in Python and C equivalently. The traditional drawback remaining here too is the
lack of (fast) random-access, which usually requires an O(N ) list traversal compared
to the O(∞) random-access in arrays. Thus, it is expected that the nodes-based
double-linked list outperforms alternative implementations in scenarios with abundant dynamic particle operations.
For abbreviation purposes, the remainder of the paper refers to the double-linked
list as nodal list, as our benchmark logs- and plots refer to the collection as nodes. In
the context of this paper, it is hence implied that nodes in a nodal list are doublelinked.
5. Results. In order to assess, compare and quantify the effectiveness of the
individual data collection designs, we run benchmarks on the same setup as in section 3. In contrast to the initial benchmark, timings do not need to be resolved down
to individual atomic operations, which costs considerable runtime and memory in and
of itself. For evaluation purposes, the parameters of interest are listed as follows, and
measured at the highest distinct code level:
• Compute time: the bare time required to execute flow advection kernels and
field interpolations
• I/O time: the combined time require for file-I/O and memory I/O operations
• Plotting time: operation time required for console- and image output (i.e.
image rendering)
• Total time: the time between the start of the simulation and the end of

10

C. KEHL, E. VAN SEBILLE AND A. GIBSON

the simulation, excluding: pre-processing, setup of flow-fields and initial
collections, start-up phase, result collection and write-back, clean-up, postprocessing and shutdown. Approximately equal to the cumulative compute-,
I/O- and plotting time, with minor round-up deviations possible.
• File-I/O time: first component of I/O-time that measures all file-related operation times (i.e. data transfer between external storage and memory); the
demonstrated benchmarks also include memory-I/O of the gridded flow fields
in this measure, because the use of synthetic flow fields is a special case and
the interface commonly accesses files instead of pre-loaded memory sections.
• Memory-I/O: second component if I/O-time that measures memory management operation times (e.g. array- and variable copies; memory mapping
between C- and Python storage).
• Kernel-times: average compute-, I/O- and total processing time among all
iterations of the simulation.
• Per-particle time: average compute-, I/O- and total processing time for an
individual particle per simulation iteration.
• Memory consumption: the amount of memory acquired by the simulation
process for instruction-, data- and stack segment, as well as the size of shared
memory page blocks.
Furthermore, to enable appropriate comparisons, static and dynamic scenarios are
setup to cover in each simulation approximately the same average number of particles,
and hence the same amount of workload during the simulation time, while avoiding
irrelevant setups (e.g. simulations with ≤ 4 particles, single-particle additions and removals each iteration, etc.). In order to achieve that with dynamic scenarios, we start
2
N
N
items and insert 128 items with a periodicity tp = (tsim / 128
)/ 74 ,
with N = target
2
while removing items by
q defining a randomized, uniformly-distributed life expectancy
1
with b = 2 23 (tsim − t) and a = 0.
p(x) = b−a
In order to cover a wide range of scenarios, we benchmark each experiment with
N = 2x particles, with x ∈ {10...16}.

5.1. Static Datasets. A static setup with a fixed particle number within the
collections shows the runtime behaviour in Figure 7.

(a) Total time

(b) Average runtime per particle

(c) Memory consumption

Figure 7. Runtime behaviour of different collection data structures for a static simulation setup.

All adapted collection types consistently improve upon the runtime when compared to the AoS -case. However, we observe rather diverging runtime behaviours:
LoA and the engineering case show a linear time increase, which is just negatively
offset to the AoS -curve. This is rather unsurprising as both collection structures are

SPEEDING UP PYTHON-BASED LAGRANGIAN PARTICLE SIMULATIONS

11

conceptually or structurally similar to the AoS starting case. The SoA structure
shows a strong logarithmic time behaviour whereas the double-linked list shows a
weak logarithmic behaviour. Both of those behaviours agree with the approximations
of the computational complexity for data access (see subsection 4.3).
The overall consistently fastest collection implementation is the nodal list. This
statement is not only supported by the overall runtime (Figure 7(a)) but even more
so by the average runtime per individual particle (Figure 7(c)). The plot shows the
limited constant offset in time for setting up and managing the data structure, as
the per-particle runtime even for small datasets is significantly lower than the other
structures. This beneficial runtime behaviour comes, however, with a significantlyincreased memory demand, as each item record (i.e. each node with its particle data
package) needs to store the predecessor and successor reference.

(a) Compute vs. I/O ratio

(b) Speed-Up (relative to AoS )

Figure 8. Derivative performance metrics of different collection data structures for a static
simulation setup.

In Figure 8(b), we see that the speed-ups for minor data structure modifications
compared to AoS (e.g. LoA and engineered improvements) is limited, which is at
no point exceeding the 2x speed-up. Different collection structures, such as SoA and
nodal lists, yield higher speed-ups. Figure 8(b) also shows that SoA's starts with
less speed-up for small collections and a consistently increasing speed-up for larger
collections. In contrast, the speed-up of nodal lists is highest with smaller collections,
then depreciates for intermediate-sized collections while then increasing again for very
large datasets. The runtime (and hence also speed-up) is less reliable and stable for
nodal lists than for SoA collections.
Furthermore, Figure 8(a) shows that, for most collections, more time is spent in
memory management tasks the larger the dataset becomes, meaning that the simulation process is increasingly I/O-bound - more specifically: memory-I/O bound. The
single exception to this behaviour is the SoA data structure, which inherently requires
minimal memory management when static dataset sizes are concerned, leading to an
increasing compute-to-I/O ratio.
5.2. Fully-Dynamic Datasets. In the dynamic case, when combining the dynamic collection operations of insertion and removal, the performance figures significantly change, as seen in Figure 9.
Here, we first separate the analysis in two regimes: small collections (i.e. low
particle count) with N ≤ 25, 000 and large collections with N > 25, 000. In small
collections, we see two speed regimes, namely the array-like structures (i.e. AoS, engineering improvements, LoA and SoA) and the double-linked list. Array-like structures
experience a hyperbolic rise in runtime for small collections, because computational

12

C. KEHL, E. VAN SEBILLE AND A. GIBSON

(a) Total time

(b) Average runtime per particle

(c) Memory consumption

Figure 9. Runtime behaviour of different collection data structures for fully dynamic collection
operations during simulation.

effort still outweighs the additional memory-I/O workload (see also Figure 10(a)).
Furthermore, the fixed memory allocation requires more setup time than the dynamics of a nodal lists, which clearly outperforms the other collections.
In the second regime of large collections, we see further divergence of the data
structures into three categories: (Category A) row major order-aligned array-like collections (i.e. AoS and LoA), (Category B) column major order-aligned array-like
collections (i.e. engineering improvements and SoA) and (Category C) the nodal list.
For large collections, Cat. A collections scale linearly and with a steep slope relative
to N , due to their disadvantageous memory layout, which complicates memory operations via NumPy. Cat. B collections exhibit a more logarithmic runtime scale for
large collections. That is because, despite their fixed memory arrangements, NumPy
internally concatenates column major order arrays faster than the row major order
arrays. For array-like data structures in terms of runtime, both insertions and removals result in large number of array concatenation operations. As expected, the
double-linked list more favourably deals with dynamic collection operations. When
comparing the results in Figure 12 and subsection 5.2, double-linked lists are the only
collections structures that exhibit approximately equal runtimes for both cases. The
regime split as well as the performance argumentation is further supported by the
compute-to-I/O ratio in Figure 10(a), as well as an observation of the speed-up in
Figure 10(b).

(a) Compute vs. I/O ratio

(b) Speed-Up (relative to AoS )

Figure 10. Derivative performance metrics of different collection data structures for fully
dynamic collection operations during simulation.

SPEEDING UP PYTHON-BASED LAGRANGIAN PARTICLE SIMULATIONS

13

In comparison to previous experiments, the SoA structure seems particularly
impeded in performance from random operations, as it is consistently slower than the
engineering improvements. Furthermore, the specific advantages and disadvantages
balance the performance of AoS, LoA and SoA for intermediate dataset sizes. For
very large collections, the per-particle runtime is approximately equal for nodal lists,
engineering improvements and SoA collections.
The final point of note, and also the most interesting observation for extrapolating the results to massive dataset sizes with N >> 100, 000, are the observations
of memory consumption. The fully dynamic operations and their subsequent concatenation operations leave temporary data blocks as traces in memory, leading to an
exponential memory demand for row major order-aligned array-like collections (Cat.
A collections). The fixed-size buckets of LoA prevent such an excess in memory consumption. For SoA and nodal lists, their memory requirements remain linear in our
measurements.
Hence, combining the insights in runtime, per-particle runtime and memory behaviour, the only two viable collection data structures for massive, dynamically-altering
datasets (i.e. huge particle swarms in the fluid simulation) are double-linked (nodal)
lists and SoA. The double-linked list yields the highest performance in terms of speed,
but requires five times more memory than SoA due to the supplemental node information. If memory capacity is the major limiting factor, then the SoA data structure
delivers the maximum speed for the least memory demands.
6. Discussion. The runtime- and memory consumption results above demonstrate the diverse performance impacts that the choice of a specific collection data
structure has on large-scale simulations. Traditionally, the choice of the collection
data structure depended exclusively on
• the dataset size
• the available compute-related technologies (e.g. SIMD, parallelization, assembly compilers)
• the available memory
Our benchmark analysis demonstrates that the choice of the ideal collection data
structure depends on the scenario and the operations actually performed on the dataset.
In order to judge potential future improvements, we analyse the compute-to-I/O
ratio. Here, while the static- and dynamic case appear very different, their trends on
large data are similar: all new collection types show an increased computational load
for N ≤ 25, 000 items. Larger collections experience a drop-off in the ratio. On very
large collections with N ≥ 50, 000 items, only the SoA collection experiences another
increase in compute load. In conclusion, all collections with a bad data locality show
an I/O-bound processing behaviour, whereas the cache-optimised SoA collection has
a compute-bound processing behaviour. The data locality issue also explains the
large drop-off in the compute-to-I/O ratio for nodal lists: items of the collection are
sparsely distributed in the memory, in contrast to any array-like collection. This
impacts the ideal use of the collections, as nodal list perform well with dynamic
collection operations and simple compute kernels, such as the simple advection used
in our experiments. The SoA collection is expected to conversely outperform the
other collections in static- and dynamic experiments complex computational kernels
and an increasing amount of field interpolations. Furthermore, the high degree of
data locality is beneficial to shared-memory parallelization. In the future, we shall
combine the double-linked list collection with an improved data locality scheme.

14

C. KEHL, E. VAN SEBILLE AND A. GIBSON

Our findings directly impact studies in ocean physics, which can be split into the
following case-study categories:
1. Mass- / Momentum-conserving particle studies: Lagrangian particles are once
released and traced over stretched time periods. The set of particles stays
fixed (i.e. static).
2. End-of-state particle studies: particles are once released, their state is traced
over multiple iterations, and they are removed upon reaching a given end
state. The simulation either ends after removal of the last particle, or releases
particles anew when a particle converges on its end state. The simulation
either just includes dynamic data removal, or is fully dynamic. A special
case of such an end-of-state simulation is particle aging, where particles are
deleted after a defined maximum lifetime.
3. Density- and flux approximations: particles are continuously released and
their position and attribute modalities are traced. At regular intervals, a
regular-gridded map of those attributes is created by averaging the the particle
attributes over all particles covered within a gridcell (i.e. stencil buffer-like
operation). In order to facilitate a high particle density, particles are constantly created and inserted to the collection. In order to prevent accuracy
errors in the accumulation, long-traced particles are regularly removed from
the dataset. This is a traditional case of large dynamics in the collection data
structure that contains the particles.
According the our findings, each of those simulation cases shall employ its optimal collection structure. Qualitatively, the data structures enable developing new,
dynamic operations that are driven by the particle behaviour itself (e.g. split- or
merge of particles). Such particle-inherent dynamics are beyond the capabilities of
geophysical Lagrangian oceanic-flow simulations until now, as they require (i) a versatile definition of particle behaviour and (ii) an efficient, memory-conserving, fast
data structure that facilitates random insertions and removals. Specific example applications are the individual modelling of fractured particulate matter in the oceans,
or the aggregation of smaller particulates into larger clusters.
The performance results on the different data structures extend further to computational cases outside physics simulations: General data analysis requires iterative
attribute derivation and subsequent cost-function computation, as well as information condensation by removing or merging attributes. On large datasets, those data
operations map to the different experiments for static datasets, dynamic data insertion, dynamic data removal, and fully dynamic operations in section 5. Therefore,
(geophysical) data science applications and frameworks can benefit from improved
data organisation. This is particularly important where computation speed is crucial,
such as in emergency response systems and forecast systems that are connected to
real-time sensor networks.
Regarding the conceptual- and development approach, we acknowledge that performance optimisation and especially performance measurement are hardly possible
in Python. The challenge for measurement and improvement is that Python's native
core (e.g. typeless codes, class hierarchy, interpreter- and introspection support) is
too slow to measure runtimes and memory consumption below a certain resolution
(empirically ∆tm ≥ 20ms). Thus, temporary resource allocations that may happen
within NumPy or SciPy cannot be measured in Python because its measurement
function calls are too slow. This can be observed in the numbers (see [13]) and the
nearly-flat memory consumption graph of SoA. Furthermore, we acknowledge that
the manual provision of just-in-time C-compiler interfaces with ctypes itself consumes

SPEEDING UP PYTHON-BASED LAGRANGIAN PARTICLE SIMULATIONS

15

resources (i.e. runtime and memory) that may either be hidden, reduced or removed
with other C-binding implementations (e.g. Numba).
In order to check the results or apply the outlined structures to other modellingand simulation problems, the program codes for the individual structures are available
on github:
• AoS : https://github.com/OceanParcels/parcels/tree/benchmarking
• Engineering Optimizations: https://github.com/OceanParcels/parcels/tree/
engineering optim trials
• LoA: https://github.com/OceanParcels/parcels/tree/list of pset array trials
• Double-linked list: https://github.com/OceanParcels/parcels/tree/sorted pset
trials
• SoA: https://github.com/OceanParcels/parcels/tree/soa benchmark
7. Conclusion. This paper aimed at quantifying the detrimental performance
impact of using static, array-like collection data structures for dynamic collection
operations (e.g. item insertion and removal) in geophysics (and especially oceanic)
applications. Following the expected negative impact of detrimental data locality,
new high-performance collection data structures within Python were designed and
developed that also interfaces easily with C-kernels. That is because, in actual physics
calculations, the performance gain from optimised data structures could vanish if
calculations then require a slow, native Python processing. The whole process is
specifically analysed for large datasets cases (N > 215 ), for which those data structure
performance limits are increasingly impacting the overall simulation runtime.
We have demonstrated and quantified the negative performance impact in our
status-quo experiments (section 3), listing the five most runtime-costly code operations
that increasingly limit performance with growing datasets. Qualitatively, calls to the
insertion- (i.e. concatenate) and deletion (i.e. remove) functions, as well as related
array re-ordering functions, are prime performance barriers.
Consequently, the paper introduces new collection data structures to circumvent
the performance bottlenecks, of which two collections are array-like structures (i.e.
SoA, LoA) and one is a double-linked list structure. Our experiments showed that
using those newly proposed data structures yield significant runtime improvements.
The paper shows the two major experiment cases: a static dataset (a) and a fully
dynamic dataset with insertion and removal (b). In all cases, the double-linked list
is the fastest-computing data structure whereas SoA is the most efficient concerning
runtime improvement and memory consumption in combination.
The results and insights gained from the specific Lagrangian oceanic- and fluid
flow simulation are generalisable to other problems in (geo)physics as well as general
data science, as demonstrated in section 6. The developed data structures and the
gained insight on performance facilitate new application scenarios and modelling cases
in physical oceanography, which demonstrates the impact of our research.
Appendix A. Removal-only and insertion-only experiments.
Separate experiments were conducted for removal-only (Figure 11) and insertiononly(Figure 12) cases.
The removal-only experiment responds similar to the static case with two exceptions. Firstly, the LoA collections is partially slower than the AoS collection, which
can be attributed to the merge-operation of nodes with sublists of length N ≤ Nlim .
Secondly, the nodal list does not provide the expected speed-up compared to arraylike collections, especially SoA. This is because, with ctypes as JIT-interface for the
simulations, the node removal and re-linking requires additional operations to the

16

C. KEHL, E. VAN SEBILLE AND A. GIBSON

(a) Total time

(b) Memory
consumption

(c) Compute vs. I/O
ratio

(d) Speed-Up (relative
to AoS )

Figure 11. Runtime behaviour of different collection data structures for a dynamic removal of
elements during simulation.

traditional replacement of the next- and previous pointer.

(a) Total time

(b) Memory
consumption

(c) Compute vs. I/O
ratio

(d) Speed-Up (relative
to AoS )

Figure 12. Runtime behaviour of different collection data structures for a dynamic insertion
of elements during simulation.

The insertion-only case is similar to the fully-dynamic case, showing that particle
insertion represents a major runtime expense. The distinct fast-insertion complexity
of nodal lists is visible in its improved runtime. The SoA performance is comparable
to the nodal list because the particles are unordered, thus a particle insertion is
performed by simple array concatenation.
Acknowledgments. This work is part of the "Tracking Of Plastic In Our Seas"
(TOPIOS) project, supported through funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement no. 715386). Simulations were carried out on the Dutch
National e-Infrastructure with the support of SURF Cooperative (project no. 16371
and 2019.034).
REFERENCES
[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat,
G. Irving, M. Isard, et al., Tensorflow: A system for large-scale machine learning, in
12th {USENIX} symposium on operating systems design and implementation ({OSDI}
16), 2016, pp. 265–283.
[2] E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, and A. Greenbaum, Linear algebra package, 1999.
[3] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins,
J. Turian, D. Warde-Farley, and Y. Bengio, Theano: a cpu and gpu math expression compiler, in Proceedings of the Python for scientific computing conference (SciPy),
vol. 4, Austin, TX, 2010, pp. 1–7.

SPEEDING UP PYTHON-BASED LAGRANGIAN PARTICLE SIMULATIONS

17

[4] L. S. Blackford, A. Petitet, R. Pozo, K. Remington, R. C. Whaley, J. Demmel, J. Dongarra, I. Duff, S. Hammarling, G. Henry, et al., An updated set of basic linear algebra
subprograms (blas), ACM Transactions on Mathematical Software, 28 (2002), pp. 135–151.
[5] P. Cetina-Heredia, E. van Sebille, R. J. Matear, and M. Roughan, Nitrate
sources, supply, and phytoplankton growth in the great australian bight:
An
eulerian-lagrangian modeling approach, Journal of Geophysical Research: Oceans, 123
(2018), pp. 759–772, https://doi.org/https://doi.org/10.1002/2017JC013542, https://
agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2017JC013542, https://arxiv.org/abs/
https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1002/2017JC013542.
[6] F. Chollet et al., Keras: The python deep learning library, ascl, (2018), pp. ascl–1806.
[7] M. Davis, E. Bray, N. Schlömer, Y. Xiong, et al., Snakeviz, 2014, https://jiffyclub.github.
io/snakeviz/.
[8] M. de la Varga, A. Schaaf, and F. Wellmann, GemPy 1.0: open-source stochastic geological
modeling and inversion, Geoscientific Model Development, 12 (2019), pp. 1–32, https:
//doi.org/10.5194/gmd-12-1-2019, https://gmd.copernicus.org/articles/12/1/2019/.
[9] P. Delandmeter and E. van Sebille, The parcels v2.0 lagrangian framework: new field interpolation schemes, Geoscientific Model Development, 12 (2019), pp. 3571–3584, https://
doi.org/10.5194/gmd-12-3571-2019, https://gmd.copernicus.org/articles/12/3571/2019/.
[10] D. Häfner, R. L. Jacobsen, C. Eden, M. R. B. Kristensen, M. Jochum, R. Nuterman, and B. Vinter, Veros v0.1 – a fast and versatile ocean simulator in pure Python, Geoscientific Model Development, 11 (2018), pp. 3299–3312, https://doi.org/10.
5194/gmd-11-3299-2018, https://gmd.copernicus.org/articles/11/3299/2018/.
[11] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen,
D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N. J. Smith, R. Kern, M. Picus,
S. Hoyer, M. H. van Kerkwijk, M. Brett, A. Haldane, J. Fernández del Rı́o,
M. Wiebe, P. Peterson, P. Gérard-Marchant, K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant, Array programming with NumPy,
Nature, 585 (2020), p. 357–362, https://doi.org/10.1038/s41586-020-2649-2.
[12] C. Intel, Intel 64 and ia-32 architectures optimization reference manual, Intel Corporation,
Sept, (2014), pp. 212 – 215. Sec. 4.5 "Improving Memory Utilization".
[13] C. Kehl, Performance Measurement Data for Speeding Up Lagrangian Fluid-Flow
Particle Simulations in Python via Dynamic Collections, data repository 1, Utrecht
University, 2021, https://doi.org/10.24416/UU01-CV3OEH, https://doi.org/10.24416/
UU01-CV3OEH.
[14] M. Lange and E. van Sebille, Parcels v0.9: prototyping a lagrangian ocean analysis framework for the petascale age, Geoscientific Model Development, 10 (2017),
pp. 4175–4186, https://doi.org/10.5194/gmd-10-4175-2017, https://gmd.copernicus.org/
articles/10/4175/2017/.
[15] L.-M. Lebreton, S. Greer, and J. Borrero, Numerical modelling of floating debris in the
world's oceans, Marine Pollution Bulletin, 64 (2012), pp. 653 – 661, https://doi.org/
https://doi.org/10.1016/j.marpolbul.2011.10.027, http://www.sciencedirect.com/science/
article/pii/S0025326X11005674.
[16] P. Lemenkova, Processing oceanographic data by Python libraries NumPy, SciPy and Pandas, Aquatic Research, 2 (2019), pp. 73–91, https://doi.org/10.3153/AR19009, https:
//hal.archives-ouvertes.fr/hal-02093491.
[17] R. Müller, J. Cannon, X. Qin, R. Watson, M. Gurnis, and S. Williams, GPlates: Building
a virtual Earth through deep time, Geochemistry, Geophysics, Geosystems, 19 (2018), https:
//doi.org/10.1029/2018GC007584.
[18] P. D. Nooteboom, P. K. Bijl, E. van Sebille, A. S. von der Heydt, and H. A. Dijkstra,
Transport bias by ocean currents in sedimentary microplankton assemblages: Implications for paleoceanographic reconstructions, Paleoceanography and Paleoclimatology, 34
(2019), pp. 1178–1194, https://doi.org/https://doi.org/10.1029/2019PA003606, https://
agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019PA003606, https://arxiv.org/abs/
https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2019PA003606.
[19] T. Nordam and R. Duran, Numerical integrators for lagrangian oceanography,
Geoscientific Model Development, 13 (2020), pp. 5935–5957, https://doi.org/10.5194/
gmd-13-5935-2020, https://gmd.copernicus.org/articles/13/5935/2020/.
[20] T. E. Oliphant, Python for scientific computing, Computing in Science & Engineering, 9
(2007), pp. 10–20.
[21] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel,
M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al., Scikit-learn: Machine
learning in python, Journal of Machine Learning Research, 12 (2011), pp. 2825–2830.

18

C. KEHL, E. VAN SEBILLE AND A. GIBSON

[22] K. Perlin, An image synthesizer, SIGGRAPH Comput. Graph., 19 (1985), p. 287–296, https:
//doi.org/10.1145/325165.325247, https://doi.org/10.1145/325165.325247.
[23] J. Scutt Phillips, A. Sen Gupta, I. Senina, E. van Sebille, M. Lange, P. Lehodey,
J. Hampton, and S. Nicol, An individual-based model of skipjack tuna (katsuwonus
pelamis) movement in the tropical pacific ocean, Progress in Oceanography, 164 (2018),
pp. 63 – 74, https://doi.org/https://doi.org/10.1016/j.pocean.2018.04.007, http://www.
sciencedirect.com/science/article/pii/S0079661117302896.
[24] R. Sedgewick, Algorithms in C++, Parts 1-4: Fundamentals, Data Structure, Sorting,
Searching, Algorithms in C, Pearson Education, 1998, https://books.google.nl/books?id=
ZCchAeprwvYC.
[25] S. Van der Walt, S. C. Colbert, and G. Varoquaux, The NumPy array: a structure for
efficient numerical computation, Computing in science & engineering, 13 (2011), pp. 22–30.
[26] S. Van der Walt, J. L. Schönberger, J. Nunez-Iglesias, F. Boulogne, J. D. Warner,
N. Yager, E. Gouillart, and T. Yu, scikit-image: image processing in python, PeerJ, 2
(2014), p. e453.
[27] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau,
E. Burovski, P. Peterson, W. Weckesser, J. Bright, et al., Scipy 1.0: fundamental
algorithms for scientific computing in python, Nature methods, 17 (2020), pp. 261–272.

SUPPLEMENTARY MATERIALS: SPEEDING UP PYTHON-BASED
LAGRANGIAN FLUID-FLOW PARTICLE SIMULATIONS VIA
DYNAMIC COLLECTION DATA STRUCTURES∗

arXiv:2105.00057v1 [physics.comp-ph] 30 Apr 2021

CHRISTIAN KEHL , ERIK VAN SEBILLE† , AND ANGUS GIBSON‡

SM1. Memory Access Patterns for Contiguous Arrays. N xM arrays,
Arrays can be aligned in two patterns: in a usual and simple matter (1) the major
axis represents the structure attributes aligned contiguously in memory, while the
minor axis represents the multiple array items. Alternatively (2) the major axis can
represent the array items contiguously aligned in memory, whereas the attributes
follow the minor axis. Formally, with N = |I|, I = {x0 , x1 , ..., xn }, and M = |A|
for x(a)i ∀a ∈ A (with A representing or item structure attributes), the arrays can
structured as N xM (1) or as (2) M xN . This has performance implications for the
functional evaluation of the array, differing between a traditional sequential and a
modern vectorized evaluation. This is shown in Figure SM1 for N xM arrays (named
Array of Structures (AoS), and in Figure SM2 for M xN arrays (named Structure of
Arrays (SoA).
Comparing the memory access patterns in terms of performance, we see that
in traditional sequential processing the SoA pattern is faster, as it only requires 1
cache update for each iteration whereas it requires 4 updates with an SoA pattern.
In modern computing architectures, each iteration actually executes between 2 (for
multi-threaded CPUs) and 32 (for GPU warps) iterations simultaneously, thus a view
on per-iteration (i.e. per-item) computations is invalid. This simultaneous computation is referred to as Vectorization (on CPUs) or Simultaneous Multi-Processing
(SMP) (on general processors). Comparing the numbers for a 2-item evaluation,
AoS requires 2 cache updates and SoA requires 8 cache updates when assuming a
sequential processing. For vectorized processing though, evaluating 2 items requires
14 updates for AoS and just 4 updates for SoA. Furthermore, the number of cache
changes is approximately M × |PNU | (where |P U | is the number of processing units),
hence scaling favourably with the number of processing units and threads (in software). This naturally makes SoA superior in performance for modern processors for
M << N . Conversely, where N ≤ M one can just switch both array axes. Lastly,
in practice, the layout change can be easily achieved by changing from FORTRANcontiguous to C-contiguous order in Python arrays (or vice versa), which equally is
achieved by matrix transposition. Obviously, establishing the correct item order on
allocation and keeping the order static is vital, as a per-iteration matrix transposition
consumes vastly more processing cycles than is saved by the SoA evaluation.

∗ Submitted

to the SIAM Journal on Scientific Computing editors 2021-01-25.
Funding: This work was funded by the European Research Council (ERC) under TOPIOS project (grant no. 715386). Simulations were carried out on the SURF's Dutch National e-Infrastructure
(project no. 16371 and 2019.034).
† Institute for Marine and Atmospheric Research, Utrecht University, the Netherlands
(c.kehl@uu.nl, e.vansebille@uu.nl).
‡ The Australian National University (angus.gibson@anu.edu.au).
SM1

SM2

C. KEHL, E. VAN SEBILLE AND A. GIBSON

Figure SM1. Memory access patterns related to a simple particle advection function on an
N xM array of structures, for sequential and vectorized evaluation (listed as assembly mnemonics).

SUPPLEMENTARY MATERIALS: SPEEDING UP PYTHON-BASED LAGRANGIAN PARTICLE SIMULATIONS
SM3

Figure SM2. Memory access patterns related to a simple particle advection function on an
M xN structure of arrays, for sequential and vectorized evaluation (listed as assembly mnemonics).

