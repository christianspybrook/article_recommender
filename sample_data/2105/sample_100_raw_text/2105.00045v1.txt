Estimation and Selection Properties of the LAD
Fused Lasso Signal Approximator
arXiv:2105.00045v1 [stat.ME] 30 Apr 2021

Xiaoli Gao*
Department of Mathematics and Statistics
University of North Carolina at Greensboro

Abstract
The fused lasso is an important method for signal processing when the hidden signals are sparse and blocky. It is often used in combination with the
squared loss function. However, the squared loss is not suitable for heavy tail
error distributions nor is robust against outliers which arise often in practice.
The least absolute deviations (LAD) loss provides a robust alternative to the
squared loss. In this paper, we study the asymptotic properties of the fused
lasso estimator with the LAD loss for signal approximation. We refer to this
estimator as the LAD fused lasso signal approximator, or LAD-FLSA. We
investigate the estimation consistency properties of the LAD-FLSA and provide sufficient conditions under which the LAD-FLSA is sign consistent. We
also construct an unbiased estimator for the degrees of freedom of the LADFLSA for any given tuning parameters. Both simulation studies and real data
analysis are conducted to illustrate the performance of the LAD-FLSA and
the effect of the unbiased estimator of the degrees of freedom.
Keywords: Estimation consistency; Jump selection consistency; Block selection consistency;
Degrees of freedom; Fused lasso; Least absolute deviations; Sign consistency.

* Correspondence:

106 Petty Building, Greensboro, NC 27412. Email: x gao2@uncg.edu

1

1

Introduction

High-dimensional data arise in many fields including signal processing, image de-noising and genomic and genetic studies. When a model is sparse and has certain known structures, penalized
methods have been widely used since they can incorporate known structures into penalty functions in a natural way and can do estimation and variable selection simultaneously. A biological
example is the analysis of copy-number variations in a human genome. In this problem, we are
interested in detecting the changes in copy numbers based on data from comparative genomic
hybridization (CGH) arrays. For instance, Snijders et al. (2001) studied a CGH array consisting
of 2400 bacterial artificial chromosome (BAC) clones, where the log base 2 intensity ratios at all
clones are measured. In Figure 1, we plot a sample CGH copy number data on chromosomes 1–4
from cell line GM 13330. The data set has two characteristics: 1) there are only a small number of chromosomal locations where the copy numbers of genes change, that is, the underlying
signals are sparse; 2) the adjacent markers tend to have similar observations, i.e., the signals are
blocky.
In a signal approximation model with sample size n, the ith observation yi is considered to
be a realization of the true signal μ0i plus random noise εi ,
yi = μ0i + εi , i = 1, ⋯, n.

(1)

In many cases, the true signal vector μ0 = (μ01 , ⋯, μ0n )′ is both blocky and sparse, meaning that the
intensities of true signals within each block are the same and most blocks consist of zero signals.
The goal here is to find a solution not only to recover all the changes points, but also to identify
the nonzero blocks. We can use the lasso penalty to enforce a sparse solution by penalizing the
`1 norm of the signals ∥μ∥1 ≡ ∑ni=1 ∣μi ∣, and use the fusion (total variation) penalty to enforce a
blocky solution by penalizing ∥μ∥TV ≡ ∑ni=2 ∣μi − μi−1 ∣. The combination of these two penalties
results in the fused lasso (FL) penalty (Tibshirani et al., 2005).
For detecting changing points in copy number variations, Tibshirani and Wang (2008) proposed to use the fused lasso with a squares loss function. We refer to this approach as the least
squares fused lasso signal approximator (LS-FLSA). For λn = (λ1n , λ2n ), the LS-FLSA seeks to
̂`n2 (λn ) = (̂
̂`n2 (λn ))′ that minimizes
find μ
μ`12 (λn ), ⋯, μ
n

n

n

∑(yi − μi )2 + λ1n ∑ ∣μi ∣ + λ2n ∑ ∣μi − μi−1 ∣,
i=1

i=1

(2)

i=2

where λ1n and λ2n are two nonnegative penalty parameters.
Recently, Rinaldo (2009) studied the selection properties of the LS-FLSA and adaptive LSFLSA under the block partition assumption in the underlying signal. Several authors have also
studied the properties of related procedures. For example, Mammen and van de Geer (1997) studied the rate of convergence in bounded variation function classes of the parameter functions; Harchaoui and Lévy-Leduc (2010) investigated the estimation properties of change points using the
total variation penalty; Boysen et al. (2009) studied the asymptotic properties for jump-penalized
least-squares regression aiming at approximating a regression function by piecewise-constant
functions. These studies significantly advanced our understanding of LS-FLSA or fusion penalized LS methods in the context signal detection or nonparametric estimation. However, all these
2

results are obtained for methods with the least squares loss and/or require normality assumption on the errors. The LS-FLSA is easily affected by outliers which arise often in practice, for
example, in CGH copy number variation data.
A more robust fused lasso signal approximator can be constructed by using the LAD loss,
which we shall refer to as LAD-FLSA. For any given λn = (λ1n , λ2n ), the LAD-FLSA is defined
as
n

n

n

̂n (λn ) = arg min {∑ ∣yi − μi ∣ + λ1n ∑ ∣μi ∣ + λ2n ∑ ∣μi − μi−1 ∣} .
μ
μ∈Rn

i=1

i=1

(3)

i=2

̂n in (3) has been applied to detect copy number variation breakpoints in
The convex minimizer μ
Gao and Huang (2010a). However, its theoretic properties of have not been studied.
In this paper, we seek to answer the following questions about the LAD-FLSA: (1) how close
`1
̂n can be to the true model μ0 asymptotically? (2) how accurately μ
̂`n1 can recover the true
μ
nonzero blocks with a large probability when μ0 is both sparse and blocky? (3) what is the
degrees of freedom of LAD-FLSA? The contributions of this paper are as follows.
• We show that the LAD-FLSA is rate consistent if the number of blocks is relatively small.
• We provide sufficient conditions under which the LAD-FLSA is able to recover the block
patterns and distinguish nonzero blocks from zero ones correctly with high probability.
That is, the LAD-FLSA can determine all the jumps, identify all the nonzero blocks, and
also distinguish the positive signals from negative ones under some conditions.
• In terms of model complexity measures, we justify that the number of nonzero blocks
generated from a LAD-FLSA estimate is an unbiased estimator of the degrees of freedom
of the LAD-FLSA.
• Without the assumption of Gaussian or sub-Gaussian random error, our studies can be
widely applied for signal detection in signal processing when random noises do not follow
nice distributions or the signal-to-noise ratios are large.
The rest of the paper is organized as follows. We list some notations in Section 2. We study
the estimation consistency and sign consistency properties of the LAD-FLSA respectively in
Section 3 and 4. In Section 5 we derive an unbiased estimator of the degrees of freedom of the
LAD-FLSA. In Section 6 we conduct simulation studies and real data analysis to demonstrate the
performance of the LAD-FLSA. We also verify the effect of unbiased estimator of the degrees of
freedom numerically in this section. Section 7 concludes the paper with some discussions. All
the technical proofs are postponed to the Appendix.

2

Preliminaries

Suppose the true model μ0 = (μ01 , ⋯, μ0n )′ includes J0 blocks and there exists a unique vector
ν 0 = (ν10 , ⋯, νJ00 ) such that
J0

μ0i = ∑ νj0 I(i ∈ Bj0 ),
j=1

3

(4)

where {B10 , ⋯, BJ00 } is a mutually exclusive block partition of {1, ⋯, n} generated from μ0 . Based
on the block partition, we can rewrite {1, ⋯, n} as {i0 , ⋯, i1 − 1, i1 , ⋯, i2 − 1, ⋯, iJ0 −1 , ⋯, iJ0 − 1},
where 1 = i0 < i1 < ⋯ < iJ0 −1 ≤ iJ0 − 1 = n and {ij−1 , ⋯, ij − 1} gives the jth block set Bj0 . We
denote the jump set in the true model as J 0 . Then J 0 = {i1 , ⋯, iJ0 −1 } and J0 = ∣J 0 ∣ + 1, where
∣J 0 ∣ is the the cardinality of J 0 . Let K0 = K(μ0 ) = {1 ≤ j ≤ J0 ∶ νj0 ≠ 0} be the set of nonzero
blocks of μ0 and the number of nonzero blocks K0 = ∣K0 ∣. We now list the following notations
that will be used throughout the paper, some of which are adopted from Rinaldo (2009).
• For the true model μ0 defined in (4), we introduce the following notations (I–IV):
(I) b0j = ∣Bj0 ∣, the size of the block set Bj0 for 1 ≤ j ≤ J0 ;
(II) b0min = min1≤j≤J0 b0j , the smallest block size;
(III) an = mini∈J 0 ∣μ0i − μ0i−1 ∣, the smallest jump;
(IV) ρn = minj∈K0 ∣νj0 ∣, the smallest nonzero signal intensity.
̂n in (3) as follows:
• Corresponding notations are analogous to a LAD-FLSA estimate μ
(V) Ĵ = J (̂
μn ), Ĵ = J(̂
μn ), B̂j = Bj (̂
μn ) with ̂
bj = ∣B̂j ∣ and ν̂j = νj (̂
μn ) for 1 ≤ j ≤
̂
BĴ are the estimated jump set, number of blocks, block partitions of {1, ⋯, n} with
̂n ;
corresponding block size and the associated unique vector generated from μ
̂ = ∣K∣.
̂
̂ = K(̂
(VI) K
μn ) = {1 ≤ j ≤ Ĵ ∶ ν̂j ≠ 0} is the set of estimated nonzero blocks and K

3

Estimation consistency of LAD-FLSA estimators

̂n . We first consider the
In this section, we study the estimation consistency of the LAD-FLSA μ
following conditions:
(A1) Error assumption: random errors εi 's in model (1) are independent and identically distributed with median 0, and have a density f that is continuous and positive in a neighborhood of 0.
(A2) Block number assumption: the true block number J0 < M1 Λn for a constant M1 > 0, where
Λn = max{16n/(λ22n − 2n2 λ21n ), n/(λ2n − nλ1n )} + 1 with λ22n > 2nλ21n .
In (A1), we only require that the random errors have a density that is continuous and positive in
neighborhood of zero and have median zero. This is a much weaker condition than the Gaussian
random error assumption required in Harchaoui and Lévy-Leduc (2010) and Rinaldo (2009).
Indeed, (A1) allows all heavy-tail distributions of the errors, including the Cauchy distribution
whose moments do not exist. Condition (A2) requires that the number of blocks in the underlying
model can increase with n, but at a slower rate than O(Λn ). As a by-product, the tuning parameter
for jumps, λ2n , grows much faster that the tuning parameter for signals intensities, λ1n . It is a
reasonable assumption since the true model is block-wise, that is, the number of nonzero jumps
can be much smaller than the number of the nonzero signals. For example, if the number of
jumps is O((log(n))1/2 ), then we can let λ2n = n1/2 (log(n))−1/4 and λ1n = n−1 .
4

̂n in (3), we first
In order to study the asymptotic properties of the LAD-FLSA estimator μ
investigate its LS-FLSA approximation,
n

n

n

̃n (λn ) = arg min {∑(zi − (f (0))1/2 μi )2 + λ1n ∑ ∣μi ∣ + λ2n ∑ ∣μi − μi−1 ∣} ,
μ
μ

i=1

i=1

(5)

i=2

where zi = (f (0))1/2 μ0i + ηi with ηi = (4f (0))−1/2 sgn(εi ) for 1 ≤ i ≤ n consist of pseudo-signal
data. Thus, all estimates listed in (V–VI) can be analogous to the corresponding ones generated
̃n . We now provide some rate upper bounds for the number of blocks generated from μ
̃n
from μ
̂n in (3), respectively.
in (5) and μ
Lemma 1 Under (A1), we have (i) J̃ ≤ 16n/(λ22n − 2n2 λ21n ) + 1, provided λ22n > 2n2 λ21n and (ii)
Ĵ ≤ n/(λ2n − nλ1n ) + 1, provided λ2n > nλ1n . In addition, suppose (A2) holds, we also have (iii)
J̃ + Ĵ + J0 < (M1 + 2)Λn , where both M1 and Λn are defined in (A2).
The proof of Lemma 1 is given in the Appendix. Lemma 1 gives upper bounds for the number
̃n and μ
̂n . We can interpret bounds in (i) and (ii) as the maximal
of blocks associated with μ
̂n and μ
̃n may belong, respectively. Similarly, (iii) provides
dimension of any linear space where μ
̂n , μ
̃n and μ0 can
us an unified rate upper bound for the dimension of any linear space to which μ
̂n and μ
̃n . Furthermore,
belong. Lemma 1 is useful in obtaining the estimation consistencies of μ
it is important to notice that those upper bounds in Lemma 1 are mainly affected by the rate of
λ2n , which is reasonable since the number of jumps in an FLSA model is mainly determined by
λ2n .
Denote ∥μ∥2n = ∑ni=1 μ2i /n and ∥μ∥22 = ∑ni=1 μ2i . Below we present the estimation properties of
̃n in (5).
μ
Lemma 2 Suppose (A1-A2) hold. Then there exists a constant 0 < c < 1, such that
P (∥̃
μn − μ0 ∥n ≥ αn ) ≤ Λn exp{Λn log n − (1 − c)2 (f (0)/2)nαn2 },
√
where Λn is defined in (A2) and αn = 1/(c f (0))[λ1n +2λ2n +((M1 +1)Λn /n)1/2 ]. Furthermore,
√
if we let αn = (2M2 Λn (log n)/n)1/2 and choose λ1n and λ2n such that λ1n + 2λ2n = c f (0)αn −
((M1 + 1)Λn /n)1/2 for a constant M2 > 1/(f (0)(1 − c)2 , then
P (∥̃
μn − μ0 ∥n ≥ αn ) ≤ Λn n{1−M2 f (0)(1−c)

2 }Λ

n

.

The proof of Lemma 2 is given in the Appendix. Lemma 2 gives us the estimation consistency
̃n (using pseudo data zi 's and bounded noises ηi 's). It is worthresult for a pseudo LS-FLSA μ
while to point out that even though we only report the consistency result for a pseudo LS-FLSA
̃n with bounded noises ηi 's in Lemma 2, we can obtain a similar consistency result for
estimator μ
the regular LS-FLSA estimator (2) under the assumption of Gaussian noises without much extra
work. Thus, the estimation consistency properties of the LS signal approximator with the total
variation penalty in Harchaoui and Lévy-Leduc (2010) can also be obtained from Lemma 2 by
taking λ2n = 0 and Λn = Kmax .
̃n in Lemma 2 plays an important role in deriving the correspondThe consistency result of μ
̂n in the following Theorem 1.
ing estimation consistency result of μ
5

Theorem 1 Suppose (A1) and (A2) hold. Then there exists a constant 0 < c < 1 such that
P (∥̂
μn − μ0 ∥n ≥ γn ) ≤ Λn exp{Λn log n − (1 − c)2 (f (0)/8)nγn2 } + (8/f (0))(Λn /(nγn2 ))1/2 ,
√
where Λn is defined in (A2) and γn = 2/(c f (0))[λ1n + 2λ2n + ((M1 + 1)Λn /n)1/2 ].
1/2 for a constant M > 1/(f (0)(1 − c)2 ) and
Furthermore, if we let γn = (8M3 Λn (log n)/n)
3
√
choose λ1n and λ2n such that λ1n + 2λ2n = (c f (0)/2)γn − ((M1 + 1)Λn /n)1/2 , then
2 −1}Λ
n

P (∥̂
μn − μ0 ∥n ≥ γn ) ≤ Λn n−{M3 f (0)(1−c)

√
+ O (1/ log n) .

̂n can
The proof of Theorem 1 is given in the Appendix. Theorem 1 implies that the LAD-FLSA μ
0
1/2
be consistent for estimating μ at the rate of O (Λn (log n)/n) ). Furthermore, if the number of
blocks in true signals is bounded, the rate of convergence can be stated more explicitly as in the
following Corollary 1.
Corollary 1 Suppose (A1) holds and there exists Jmax > 0 such that J0 ≤ Jmax . Then
√
̂ J)
̃ < Jmax } ∩ {∥̂
P ({max(J,
μn − μ0 ∥n ≥ θn }) ≤ Jmax n−c2M Jmax + O (1/ log n)
for θn = (8M Jmax (log n)/n)1/2 and λ1n + 2λ2n = (c1M Jmax (log n)/n)1/2 − (Jmax /n)1/2 . Here
M > 1/((1 − c)2 (f (0)) is a constant, c1M = (2M c2 (f (0))1/2 and c2M = f (0)M (1 − c)2 − 1.
̂n is consistent for estimating μ0 at the rate O ((Jmax (log n)/n)1/2 )
Corollary 1 says that the μ
if the numbers of both true and estimated jumps are bounded above. This convergence rate can
be compared to n−1/2 , which is argued by Yao and Yu (1989) to be optimal for LS estimators
of the levels of a step function. Notice that if limn→∞ P(Ĵ = J 0 ) = 1, then ∑ni=1 (̂
μi − μ0i )2 =
J0 0
0
bj (̂
νj − νj0 )2 ≥ b0min ∑Jj=1
(̂
νj − νj0 )2 for large n almost surely. Thus Corollary 1 implies that,
∑j=1
for large n
2
P ({Ĵ = J 0 } ∩ {∥̂
νn − ν 0 ∥2 ≥ (8M Jmax (log n)/b0min )1/2 }) ≤ Jmax n−{f (0)M (1−c) −1}Jmax ,

(6)

̂n can converge to ν 0 in `2 norm at
where the convergence rate is affected by b0min . Therefore, ν
0
1/2
̂n can converge faster to the
rate O ((Jmax (log n)/bmin ) ). In other words, a block estimator ν
true model ν0 with larger block size.

4

Block sign consistency of LAD-FLSA

In this section, we study the sign consistency of the LAD-FLSA. The sign consistency has been
studied by Zhao and Yu (2006) and Gao and Huang (2010b) for both the LS-Lasso and LADLasso in high-dimensional linear regression settings. It is a stronger result than variable selection
consistency since it not only requires that variables to be selected correctly, but also their signs
are estimated correctly with high probability.
In light of the block structure in the hidden signals, we consider the selection consistency and
sign consistency for jumps and blocks separately.

6

̂n is jump selection consistent if
Definition 1 μ
lim P ({Ĵ = J0 } ⋂{∩1≤j≤J0 {B̂j = Bj0 }}) = 1.

n→∞

̂n is jump sign consistent if
Definition 2 μ
̂i−1 ) = sgn(μ0i − μ0i−1 ), ∀i ∈ J 0 }) = 1.
lim P ({Ĵ = J 0 } ⋂{sgn(̂
μi − μ

n→∞

̂n can partition the signals into blocks correctly with probability
Definition 1 requires that μ
̂n finds not only
converging to one. Definition 2 is a stronger requirement since it requires that μ
all the jumps, but also the jump directions (up/down) correctly. A jump selection consistent
estimator can recover the jumps set J 0 correctly with high probability, but does not tell us which
blocks have nonzero intensities. In other words, there may exist δ > 0 and 1 ≤ j ≤ J0 such that
̂n .
P ({̂
νj ≠ 0} ∩ {νj0 = 0}) > δ for a jump sign consistent μ
We now define the block selection consistency and the block sign consistency in Definitions
3 and 4. The latter is a stronger definition since it requires the signs of the signals to be recovered
correctly.
̂n is block selection consistent if
Definition 3 μ
̂ = K0 }) = 1.
lim P ({Ĵ = J 0 } ⋂{K

n→∞

̂n is block sign consistent if
Definition 4 μ
̂ = K0 } ⋂{sgn(̂
lim P ({Ĵ = J 0 } ⋂{K
νj ) = sgn(νj0 ), ∀j ∈ J0 }) = 1.

n→∞

4.1

Jump selection consistency

For λ1n = 0, a LAD-FLSA becomes a LAD signal approximator using only the total variation
penalty (LAD-FSA), defined as
n

n

̂Fn (λ2n ) = μ
̂FL
μ
n (0, λ2n ) = arg min {∑ ∣yi − μi ∣ + λ2n ∑ ∣μi − μi−1 ∣} .
i=1

(7)

i=2

̂Fn (λ2n ) can do the block partition correctly. Then we expect to sort out those nonzero
Suppose μ
blocks by increasing λ1n slowly from 0. So we first investigate the jump selection consistency of
̂Fn (λ2n ). Below we list some conditions on the smallest value of true jumps and smallest size
μ
of the true blocks in model (1) and (4) for the jump sign consistency. Recall that b0min and an are
defined in II and III in Section 2.
(B1) (a) λ2n → ∞; (b) there exists a δ > 0, such that λ2n (log(n − J0 ))−1/2 > (1 + δ)/2.
0
0
1/2
1/2
(B2) (a) (b
√min ) an → ∞; (b) there exists δ > 0, such that (bmin / log(J0 )) an > 3(1 +
δ)/( 2f (0)) for sufficiently large n.

(B3) λ2n < (f (0)/3)b0min an for sufficiently large n.
7

Here (B1) and (B3) indicate that λ2n increases with n faster than O (log(n − J0 )1/2 ) but slower
than O(b0min an ). (B2-a) requires that either the smallest jump or the smallest size of all blocks in
the true model should be large enough so that {1, ⋯, n} can be partitioned into different blocks
correctly. (B2-b) strengthes (B1-a) by providing a lower bound. Conditions (B1-B3) provide us
some helpful information in finding an optimal tuning parameter in model (7). When the above
̂Fn (λ2n ) can group all signals into different
conditions are satisfied, the LAD-FSA estimator μ
blocks correctly with a large probability.
Theorem 2 Consider the signal approximation model (1) with the true model (4). A LAD-FSA
̂Fn (λ2n ) is jump sign consistent under (A1) and (B1-B3).
estimator μ
The proof of Theorem 2 is postponed to the Appendix. Theorem 2 tells us that we can apply a
LAD-FSA approach to recover not only the true jumps, but also their signs correctly with high
probability if the true hidden signal vector is blocky and the tuning parameter λ2n is chosen
appropriately.

4.2

Block selection consistency

We have seen that a LAD-FSA solution can be jump selection consistent to the blocky hidden
signal vector under some conditions. In many cases, the true signal vector includes some zero
blocks, which cannot be separated from nonzero ones using the LAD-FSA approach since the total variation penalty only shrinks adjacent differences but not signals themselves. The additional
lasso penalty of FLSA can force the estimates of some block intensities to be exactly zero. We
are interested in finding a LAD-FLSA solution to not only recover the true jumps, but also find
the zero blocks and keep only the nonzero ones with a large probability. Eventually, we need to
study how to choose tuning parameters λ1n and λ2n appropriately, such that the LAD-FLSA is
block selection consistent.
When the true block model in (4) is also sparse, we need the following additional conditions
to separate nonzero blocks from zero ones.
(C1): (a) λ1n (b0min√
)1/2 → ∞ when n → ∞; (b) there exists δ > 0, such that λ1n (b0min / log(J0 −
K0 ))1/2 > 4 2(1 + δ).
(C2): λ2n /b0min < λ1n /8 for sufficiently large n.
0
0
1/2
1/2
(C3): (a)
√ ρn (bmin ) → ∞ when n → ∞; (b) there exists δ > 0 such that ρn (bmin / log(K0 )) >
2 2(1 + δ)/f (0).

(C4): λ2n /b0min < f (0)ρn /3 for sufficiently large n.
(C5): λ1n < f (0)ρn /2 for sufficiently large n.
Here Condition (C1) and (C2) indicate that either λ1n or the smallest block size b0min should grow
with n with a lower bound provided in (C1-b) since λ2n grows with n from (B1). Especially,
if λ1n is relatively small as seen in (C5), b0min must be large enough. (C4) and (C5) provide us
a lower bound for the smallest nonzero signal ρn when n is large. Above interpretations are
consistent with (C3-a), which requires either the block size or the true nonzero signal intensities
8

should be large enough such that the nonzero blocks can be separated from zero ones. In other
words, if ρn is relatively smaller, it becomes harder to separate nonzero ones from zero ones.
However, it is not impossible for us to distinguish those nonzero blocks if we have larger enough
block size since more observations can be used to estimate νj0 within jth block. (C3-b) provides
us an upper bound of the number of nonzero blocks. It is worthwhile to point out that even though
these conditions seem to be complicated, some of them can be redundant. For instance, (C2) and
(C5) can be used to derive a smaller upper bound than the one in (C4). Thus, if both (C2) and
(C5) are satisfied, (C4) can be redundant. One can see that (C3-a) can be also redundant if (C5)
and (C1-a) hold.
Theorem 3 Under (A1), (B1-B3) and (C1-C5), a LAD-FLSA solution is block sign consistent.
Theorem 3 tells us that the LAD-FLSA can first recover the block patterns of hidden signals by
detecting all the true jumps, and then rule out those nonzero blocks. Furthermore, with a very
large probability, those nonzero blocks are identified correctly to have either positive or negative
signals. Thus, the LAD-FLSA is justified to be a promising approach for signal processing when
the true hidden signal vector is both blocky and sparse and the observed data are contaminated
by outliers. The proof of Theorem 3 is provided in the Appendix.
Remark 1: The block assumption of the true model in (4) is crucial in our study. If the
model is grouped, but not blocky, fused lasso might be misleading since the fusion term is used
to generate the block-wise solution. Some other techniques such as group lasso (Yuan and Lin,
2006) or smooth lasso (Hebiri, 2008) can be more useful to generate the corresponding group
sparsity structure.
Remark 2: The relaxation of Gaussian or sub-Gaussian random error assumption is important since it is very common to see some contaminated data in signal processing, especially when
repeated measurements are not available. Some normalization methods such as Loess have been
used in preprocessing the real data in order to improve the robustness of LS-FLSA. However,
those techniques may over-smooth the data and then generate some false negatives.

4.3

Additional remarks on asymptotic properties

We will provide two additional comments on the asymptotic results obtained in Section 3 and 4.
Remark 3: An LAD-FLSA may not reach the estimation consistency and sign consistency simultaneously.
The rate estimation consistency in Theorem 3 holds for λ1n + 2λ2n = O(log(n)/n)1/2 . However, from (B1-b) and (C2), we know one of the sufficient conditions for the sign consistency in
Theorem 3 requiring λkn > O(log(n))1/2 for k = 1, 2. So an LAD-FLSA may not be able to reach
both the estimation consistency and sign consistency simultaneously. However, this claim is not
theoretically justified since all conditions assumed in both Theorem 3 and 3 are sufficient.
Remark 4: The weak irrepresentable condition is not necessary for the jump point detection
consistency in Theorem 2.
To understand Remark 4, we will transform the signal approximation model in (7) into a
Lasso representation. Consider a linear regression model
p

yi = ∑ xij βj + εi ,
j=1

9

1 ≤ i ≤ n,

(8)

where (yi , xi1 , ⋯, xip ) and β = (β1 , ⋯, βp )′ represent the observed data and coefficients vector. A
Lasso solution (Tibshirani, 1996) of β is
p

n

p

̂
β(λ)
= arg min {(1/2) ∑(yi − ∑ xij βj )2 + λ ∑ ∣βj ∣} .
i=1

j=1

j=1

If we further divide the coefficients vector β = (β1′ , β2′ )′ , where β1 include those nonzero coefficients and β2 includes zeros only, and correspondingly, we can write X = (X1 , X2 ) and
s1 = sgn(β1 ) consist of sign mappings of non-zero coefficients in the true model, then the weak
irrepresentable condition of the designed matrix X means
∣X′2 X1 (X′1 X1 )−1 s1 ∣ < 1,

(9)

where 1 is a vector with element being 1. Naturally, we can write the LAD-FSA in (7) into a
Lasso solution of ν̂ = (ν1 , ⋯, νn )′ ,
n

̂nF (λ2n ) = arg min {∥y − Zν∥2 + λ2n ∑ ∣νi ∣} ,
ν

(10)

i=2

where ν1 = μ1 , νi = μi − μi−1 for 2 ≤ i ≤ n and Z is the low triangular design matrix with nonzero
items being 1. Zhao and Yu (2006) proved that the weak irrepresentable condition is a necessary
condition for a Lasso solution in (8) to be sign consistent under two regularity conditions. We
list the result in the following Lemma 3.
Lemma 3 (Zhao and Yu, 2006) Suppose two regularity conditions are satisfied for the designed
matrix X: (1) there exists a positive definite matrix C such that the covariance matrix X′ X/n →
C as n → ∞, and (2) max1≤i≤n x′i xi /n → 0 as n → ∞. Then Lasso is general sign consistent,
̂
limn→∞ P (∃λ ≥ 0, sgn(β(λ))
= sgn(β0 )) = 1, only if there exists N so that X satisfies the weak
irrepresentable condition holds for n > N . Here β0 is the true coefficient vector.
Unfortunately, it is easy for us to verify that the design matrix Z in (10) does not satisfy the weak
irrepresentable condition. For example, if we consider a signal approximation data with only five
observations where μ1 ≠ μ2 ≠ μ3 = μ4 = μ5 , then the first row vector of Z′2 Z1 (Z′1 Z1 )−1 is (0, 0, 1)′
Thus (9) is violated. However, there is no contradiction between the sign consistency result in
Theorem 2. and Lemma 3 since both two regularity conditions in Lemma 3 are violated for
design matrix Z. Suppose ρ1 ≤ ⋯ρn are eigenvalues of Z′ Z/n. we know that (a) ρ1 < 1/(3n) → 0
and ρn > 4n1/2 → ∞ when n → ∞, and in addition, (b) max1≤i≤n z′i zi /n = 1.

5

Degrees of freedom of LAD-FLSA

It is crucial to seek appropriate λ1n and λ2n in (3). Large λ1n will generate all zero coefficients,
while large λ2n will generate all zero jumps. Conditions on λ1n and λ2n in Section 3 and 4 provide
us some guidance in choosing the rates of two tuning parameters to obtain a well-behaved LADFLSA estimate. This section helps us to choose two optimal tuning parameters from the model
selection point of view.
10

For given λ1 and λ2 , a LAD-FLSA approach is a modeling procedure including both model
selection and model fitting. The complexity of a modeling procedure is defined as the generalized
degrees of freedom (df) and measured by the sum of the sensitivity of the predicted values. See
Ye (1998) and Gao and Fang (2011) for the discussion on the df for a modeling procedure under
̂i (y; λ1 , λ2 ) be a LAD-FLSA
both the `2 and `1 loss functions, respectively. For 1 ≤ i ≤ n, let μ
fitted value of yi for any given λ1 and λ2 . The degrees of freedom of a LAD-FLSA approach,
n

df(λ1 , λ2 ) = ∑ ∂E[̂
μi (y; λ1 , λ2 )]/∂yi .

(11)

i=1

In Theorem 4, we provide an unbiased estimator of df(λ1 , λ2 ) in (11) for a LAD-FLSA modeling
procedure.
Theorem 4 Consider a LAD-FLSA modeling procedure defined for (1), (3) and (4). For any
fixed positive λ1 and λ2 , we have
̂ 1 , λ2 )∣] = df(λ1 , λ2 ).
E[∣K(λ

(12)

̂ 1 , λ2 )∣, is an unbiased estimaTheorem 4 indicates that the number of nonzero blocks, ∣K(λ
tor of the degrees of freedom of a LAD-FLSA modeling procedure with any given λ1 and λ2 .
We provide both the numerical demonstration and theoretical proof are provided in Section 6.3
and the Appendix, respectively. In fact, such an unbiased estimator in (12) can be also observed from Theorem 2 of Li and Zhu (2008). For example, if λ1 = 0, for any λ2 > 0, the
LAD-FLSA reduces to a LAD-LASSO solution of w (wi = μi − μi−1 ) for 2 ≤ i ≤ n. Then
n
yi (0, λ2 )/∂yi = ∣Ĵ(0, λ2 )∣. Suppose λ2 > 0 is fixed, the block partition is decided. Then
∑i=1 ∂̂
for λ1 > 0, the LAD-FLSA becomes a LASSO model of ν (ν is the block intensity vector).
̂ λ2 )∣.
Therefore, ∑ni=1 ∂̂
yi (λ1 , λ2 )/∂yi = ∣K(λ,
Results in Theorem 4 can be used to choose two optimal tuning parameters from the model
selection point of view. Let yi0 's denote new observations generated from the same mechanism
̂(y; λ1 , λ2 ) is defined as
generating yi 's. The prediction error of μ
n

E0 {∑ ∣̂
μi (λ1 , λ2 ) − yi0 ∣},

(13)

i=1

where E0 is taken over yi0 's. From Theorem 4, we can estimate the prediction error (13) by
n

̂ 1 , λ2 )∣.
̂i (λ1 , λ2 )∣ + ∣K(λ
∑ ∣yi − μ
i=1

Thus some existing model selection criteria can be modified to choose an optimal combination of
tuning parameters. For instance, we can extend AICR (Ronchetti, 1985), BIC (Schwarz, 1978)
and GCV (Wahba, 1990) to the LAD-FLSA as follows,
̂ 1 , λ2 )∣,
̂i (λ1 , λ2 )∣ + ∣K(λ
AICR ∶ ∑ni=1 ∣yi − μ
n
̂ 1 , λ2 )∣ log(n)/2,
̂i (λ1 , λ2 )∣ + ∣K(λ
BIC ∶ ∑i=1 ∣yi − μ
n
̂ 1 , λ2 )∣/n].
̂i (λ1 , λ2 )∣/[1 − ∣K(λ
GCV ∶ ∑i=1 ∣yi − μ
11

(14)

6

Numerical studies

In this section, we first use some simulation studies and real data analysis to demonstrate the
performances of the LAD-FLSA approach in recovering the true hidden signals. Then we verify
Theorem 4 numerically using a sample copy number data.

6.1

Recovery of hidden signals

We illustrate the performance of the LAD-FLSA by modifying the block example studied in both
Donoho and Johnstone (1995) and Harchaoui and Lévy-Leduc (2010), where the signal vector is
only blocky but not sparse. We choose t = (.1, .23, .65, .76, .9)′ and h = (1.5, −3, 4.3, −3.1, −2)′
and round ∑j hj (1 + sgn(i/n − tj ))/2 to the nearest integers to get μ0i for 1 ≤ i ≤ n. Then the
generated true hidden signal vector,
μ0 = (0′p1 2′p2 − 2′p3 3′p4 0′p5 2′n−q )′
is blocky and sparse with four nonzero blocks and two zero ones. Here q = p1 + ⋯ + p5 . The
observed data are generated from model (1) by simulating εi 's from 1) normal distribution with
mean 0 and standard deviation σ, 2) double exponential distribution with center 0 and standard
deviation σ, and 3) standard cauchy distribution with a multiplier 0.1σ. Similar to Harchaoui and
Lévy-Leduc (2010), we consider weak, mild and strong noises by setting σ = 0.1, 0.5 and 1 in all
three types of distributions. In Figure 4, we plot a sample data set generated from 2), where the
observed data, the true hidden signals and the LAD-FLSA estimates are plotted using gray, black
and red colors, respectively.
The data is standardized as yi /s(y) and analyzed using the LAD-FLSA approach in (3),
where s(y) is the standard deviation of (y1 , ⋯, yn )′ . We choose "optimal" λ1 and λ2 by minimizing BIC in (14) for 0 < λ1 < 0.5 with increments of 0.01 and (n/ log(n))1/2 < λ2n < n1/2
with increments of 0.1, respectively. To demonstrate the robust properties of the LAD-FLSA
approach, we also report the simulation results from the LS-FLSA approach in (2). For each
̂0
model, we illustrate the variable selection effect using CFR+6, the ratio of either recovering μ
correctly or over-fitting the model by including six additional noises over 1000 replicates. We
choose "six" here since we have six blocks in the true model. We also report JUMP, the average
number (with standard deviation) of jumps over 1000 replicates. A jump is counted only if the
adjacent differences is at least 0.1. We demonstrate the estimation effects by computing the least
absolute relative error (LARE) as follows:
LARE(̂
μn , μ0 ) =

n
μi − μ0i ∣
∑i=1 ∣̂
.
n
∑i=1 ∣μi ∣

(15)

The simulation results for sample size n = 1000 and 5000 are reported in Table 1, where we
can see that the LAD-FLSA approach has much better performance than the LS-FLSA for both
strong and mild signal noises. When the signal noises are weak, the LAD-FLSA still has some
advantages over the LS-FLSA especially when the data is contaminated by Cauchy distributed
noises. For example, for Cauchy error and σ = 0.1, the LAD-FLSA recovers the true signal vector
exactly at a ratio of 87% for n = 1000 and 92% for n = 5000, while the LS-FLSA only recovers
the true model exactly 49% and 78% of the time.
12

6.2

BAC array

In Section 1 we introduced a sample BAC CGH data, where the observation of each entry for
cell line GM 13330 is the log 2 fluorescence ratios from all 23 chromosomes resulted from the
BAC experiment sorted in the order of the clones locations on the genome. The purpose of the
study is to detect the locations where there are some significant deletions or amplifications. As a
demonstration of the effect of the LAD-FLSA applied to copy number analysis, we only analyze
the data from chromosome 1–4 with 129, 67, 83 and 167 markers, respectively. Since the log
2 ratios at many markers are observed to be around 0 and the data may also have some spatial
dependence properties, it is reasonable to assume the true hidden signals to be both sparse and
blocky. We analyze each chromosome independently by using both the LAD-FLSA in (3) and
LS-FLSA in (2). Tuning parameters are chosen the same as in Section 6.1. The final estimates
from both methods on all 4 chromosomes are plotted together in Figure 1. The LAD-FLSA
estimates (top panel) provides four blocks with one amplification region in chromosome 1 and
one deletion region in chromosome 4. Besides the two variation regions detected by the LADFLSA, the LS-FLSA estimates (bottom panel) also show an amplification at a single point in
chromosome 2, which is not confirmed by spectral karyotyping in Snijders et al. (2001).

6.3

Effect of unbiased estimator of the degrees of freedom

We now conduct some simulations based upon the sample BAC array studied in Section 6.2 to
examine Theorem 4 numerically. To illustrate the effect of the unbiased estimator of the degrees
of freedom, we only take chromosome 1 with 129 locations as an example. One can see the
sample data from Figure 1.
We generate 500 Monte Carlo simulations based on the same hypothetical model
yi0 = yi + ε0i , i = 1, ⋯, 129,
where yi s are observations at 129 locations and ε0i 's are independent normal with center 0 and
standard deviation 0.1σ ∗ , where σ ∗ is the standard deviation of y. For each combined (λ1 , λ2 )
̂ 1 , λ2 )∣, and compute the true df(λ1 , λ2 )
̂ 1 , λ2 ) from ∣K(λ
with 0 < λ1 , λ2 ≤ 1, we record df(λ
defined in (11) using the Monte Carlo simulation from Algorithm 1 in Ye (1998). In Figure 2,
̂ 1 , λ2 ) of the LAD-FLSA estimate for every combination of 0 < λ1 , λ2 ≤ 1, with the
we plot df(λ
̂ 1 , λ2 )∣ over 500 repetitions
increment of 0.05, respectively. The averages of df(λ1 , λ2 ) and ∣K(λ
are reported in Figure 3. Those simulation results show that the number of estimated nonzero
̂ 1 , λ2 )∣ is a promising estimate to the df(λ1 , λ2 ) numerically, especially when the
blocks ∣K(λ
number of estimated nonzero blocks is not deviated from the true one seriously.

7

Concluding remarks

In this paper, we study the asymptotic properties of the LAD signal-approximation approach
using the fused lasso penalty. By assuming the true model to be both blocky and sparse, we
investigate both the estimation consistency and sign consistency of the LAD-FLSA estimator. In
terms of estimation consistency, the consistency rate is optimal up to an logarithmic factor if the
13

dimension of any linear space where the true model and its estimates belong is bounded from
above. In terms of sign consistency, we justify that a LAD-FLSA approach can not only recover
the true block pattern but also distinguish those nonzero blocks from the zero ones correctly with
high probability under reasonable conditions. In fact, those jump selection and block selection
consistency results can be made stronger by matching the corresponding signs correctly with a
large probability. Thus, by choosing two tuning parameters λ1 and λ2 properly, we can reach a
well-behaved LAD-FLSA estimate to recover the true hidden signal vector under some random
noises. The consistency results in this paper extend the theoretical properties of the LS-FSA
in Harchaoui and Lévy-Leduc (2010) and the LS-FLSA in Rinaldo (2009) to the LAD signal
approximation, which amplify the study of signal approximation using linear regression when
the random error does not follow a Gaussian distribution. Furthermore, we demonstrate that the
number of estimated nonzero blocks is an unbiased estimator of the degrees of freedom of the
LAD-FLSA. Thus, the existing model selection criteria can be extended to the LAD-FLSA for
choosing the tuning parameters.
As in many recent studies, our results are proved for penalty parameters that satisfy the conditions as stated in the theorems. It is not clear whether the penalty parameters selected using
data-driven procedures satisfy those conditions. However, our numerical study shows a satisfactory finite-sample performance of the LAD-FLSA. Particularly, we note that the tuning parameters selected based on the BIC seem sufficient for our simulated data. This is an important and
challenging problem that requires further investigation, but is beyond the scope of the current
paper. Also, a basic assumption required in our results is that the random error terms εi in (1) are
independent. Since the observations y1 , . . . , yn are in a natural order in this model, for example,
copy number variation data based on genetic markers are ordered according to their chromosomal
locations, it would be interesting to study the behavior of the LAD-FLSA allowing for certain
dependence structure in the error terms.

Appendix
Proof of Lemma 1
̃i = w
̃i (λ1n , λ2n ) = μ
̃i − μ
̃i−1 and w
̂i = w
̂i (λ1n , λ2n ) = μ
̂i − μ
̂i−1 be the LS-FLSA and LADLet w
FLSA estimates of ith jump in (5) and (3). Using the Karush–Kuhn–Tucker (KKT) conditions
(5), we get
i
n
k
√
√
̃j ) + λ1n ∑ sgn(∑ w
̃j ) = −λ2n sgn(w
̃i ) if w
̃i ≠ 0.
−2 f (0)(zi − f (0) ∑ w
j=1

k=i

j=1

Then
n

λ22n J̃ ≤ 8f (0) ∑(zi −

√

i

̃j )2 + 2λ21n n2 J̃.
f (0) ∑ w
j=1

i=1

Thus
n

∣J̃∣ ≤ 8f (0)(λ22n − 2n2 λ21n )−1 ∑ zi2 ≤ 16f (0)n/(λ22n − 2n2 λ21n ).
i=1

14

(16)

Using the KKT equations of (3), we have
i

n

k

̂j ) + λ1n ∑ sgn(∑ w
̂j ) = −λ2n sgn(w
̂i ) if w
̂i ≠ 0.
−sgn(yi − ∑ w
j=1

k=i

j=1

Then
∣Ĵ∣ ≤ n/(λ2n − λ1n n).

(17)

Finally, combining with (16), (17) and (A2), (iii) holds, which completes the proof of Lemma 1.
◻
Proof of Lemma 2
̃n in (5), we have
From the definition of μ
√
√
n
n
∑i=1 (zi − f (0)μi )2 ≤ ∑i=1 (zi − f (0)μ0i )2
̃i−1 ∣.
+λ1n ∑ni=1 [∣μ0i ∣ − ∣̃
μi ∣] + λ2n ∑ni=2 [∣μ0i − μ0i−1 ∣ − ∣̃
μi − μ

(18)

From the triangle inequality, (18) becomes
n
f (0) ∑
μi − μ0i )2
√i=1 (̃
̃i−1 ∣]
≤ 2√f (0) ∑ni=1 ηi (̃
μi − μ0i ) + λ1n ∑ni=1 ∣̃
μi − μ0i ∣ + λ2n ∑ni=2 [∣μ0i − μ0i−1 ∣ − ∣̃
μi − μ
n
n
0
0
μi − μi ) + (λ1n + 2λ2n ) ∑i=1 ∣̃
μi − μi ∣.
≤ 2 f (0) ∑i=1 ηi (̃

(19)

The rest of the proof is similar to the proof of Proposition 2 in Harchaoui and Lévy-Leduc (2010).
For μ ∈ Rn , we define
n
√
G(μ) = 2 f (0) ∑ ηi (μi − μ0i )/∥μ − μ0 ∥2 .
i=1

Thus (19) becomes
n
√
μn − μ0 ∥2 + G(̃
μn )∥̃
μn − μ0 ∥2 .
f (0) ∑(̃
μi − μ0i )2 ≤ (λ1n + 2λ2n ) n∥̃
i=1

Then,
√

√
f (0)∥̃
μn − μ0 ∥2 ≤ (λ1n + 2λ2n ) n + G(̃
μn ).

(20)

̃n may belong. From
Let {SK } be a collection of any K-dimensional linear space to which μ
Lemma 1, 1 ≤ K ≤ Λn . From (20), for any δn > 0,
√
√
P(∥̃
μn − μ0 ∥2 ≥ δn ) ≤ P(G(̃
μn ) ≥ f (0)δn − (λ1n + 2λ2n ) n)
√
√
(21)
n
≤ ∑Λk=1
nk P (supμ∈SK G(μ) ≥ f (0)δn − (λ1n + 2λ2n ) n) .
Notice that E(G(μ)) = 0 and Var(G(μ)) = 1. As a consequence of Cirel'son, Ibragimov and
Sudakov's (1976) inequality,
P{ sup G(μ) ≥ E [ sup G(μ)] + z} ≤ exp{−z 2 /2} for some constant z > 0.
μ∈SK

μ∈SK

15

(22)

Consider the collection {SK }. Let Ω be the D-dimensional space to which μ − μ0 belongs and
ψ1 , ⋯, ψD be its orthogonal basis.
√
2 f (0) ∑ni=1 ηi ωi
√
supμ∈SK G(μ) ≤ supω∈Ω
√ n∥ω∥nn
2 f (0) ∑i=1 ηi (∑D
j=1 aj ψj,i )
= supa∈RD
√
D
n∥ ∑j=1 aj ψj ∥n
(23)
√
n
D
2 f (0) ∑j=1 aj (∑i=1 ηi ψj,i )
= supa∈RD
1/2
(∑D
j=1 aj )
√
1/2
n
2
),
≤ 2 f (0) (∑D
j=1 (∑i=1 ηi ψj,i ) )
where the last "≤" is obtained using the Cauchy-Schwarz inequality. From (A2) and (i) in Lemma
1, there exists M1 > 0 such that D < (M1 + 1)Λn . Then by taking expectations on both sides of
(23), we have
√
2 1/2
n
E[supμ∈SK G(μ)] ≤ 2 f (0)E [(∑D
]
j=1 (∑i=1 ηi ψj,i ) )
√
2 1/2
n
≤√
2 f (0) (∑D
j=1 E [(∑i=1 ηi ψj,i ) ])
≤ D ≤ ((M1 + 1)Λn )1/2 .

(24)

Combining (22) and (24), we get
P ( sup G(μ) ≥ ((M1 + 1)Λn )1/2 + z) ≤ exp{−z 2 /2}.

(25)

μ∈SK

Let 0 < c < 1 such that
√
√
c f (0)δn = (λ1n + 2λ2n ) n + ((M1 + 1)Λn )1/2 .
√
√
Then we can choose a positive z = f (0)δn − (λ1n + 2λ2n ) n − ((M1 + 1)Λn )1/2 in (25).
Combining (21) and (25),
√
√
P(∥̃
μn − μ0 ∥2 ≥ δn ) ≤ Λn exp{Λn log n − (1/2)[ f (0)δn − (λ1n + 2λ2n ) n − (M1 + 1)Λn ]2 }
(26)
≤ Λn exp{Λn log n − (1/2)(1 − c)2 f (0)δn2 }.
√
For αn = δn / n, we have
P(∥̃
μn − μ0 ∥n ≥ αn ) ≤ Λn exp{Λn log n − (1/2)(1 − c)2 f (0)nαn2 }.
Thus the first part of Lemma 2 holds. Furthermore, if we also have αn = {2M2 Λn (log n)/n}1/2 ,
then
√
2
P(∥̃
μn − μ0 ∥n ≥ 2M2 Λn (log n)/n) ≤ Λn n{1−M2 f (0)(1−c) }Λn ,
which completes the proof. ◻
Proof of Theorem 1
16

Define

n

n

n

n

Ln (μ) = n−1 [∑ ∣yi − μi ∣ − ∑ ∣yi − μ0i ∣ + λ1n ∑ ∣μi ∣ + λ2n ∑ ∣μi − μi−1 ∣]
i=1

i=1

i=1

i=2

and
n

Mn (μ) = n

−1

n

[f (0) ∑(μi − μ0i )2
i=1

n
n
0
− ∑ sgn(εi )(μi − μi ) + λ1n ∑ ∣μi ∣ + λ2n ∑ ∣μi
i=1
i=1
i=2

− μi−1 ∣] .

̂n = arg min{Ln (μ)} and μ
̃n = arg min{Mn (μ)}. Define Rni = Rni (μi , εi ) = ∣εi − (μi −
Then μ
μ0i )∣ − ∣εi ∣ + sgn(εi )(μi − μ0i ) and ξni = Rni − E[Rni ]. Following Gao and Huang (2010b), we can
verify
n

∣Ln (μ) − Mn (μ)∣ = ∑ξni /n + τn (μ, μ0 ),

(27)

i=1

̃n ∥2 ≤ δ}, Sδd = {μ ∶
where τn (μ, μ0 ) = o(∥μ − μ0 ∥2n ). For any δ > 0, we define Sδ = {μ ∶ ∥μ − μ
0
0
̃n ∥2 = δ} and Sδ = {μ ∶ ∥μ − μ ∥2 ≤ δ}. We define
∥μ − μ
∆n (δ) = sup ∣Ln (μ) − Mn (μ)∣
μ∈Sδ

and
hn (δ) = infd (Mn (μ) − Mn (̃
μn )).
μ∈Sδ

We have
̃n ∥2n + 2n−1 f (0)∑ni=1 (μi − μ
̃i )(̃
Mn (μ) − Mn (̃
μn ) = f (0)∥μ − μ
μi − μ0i ) + n−1 ∑ni=1 sgn(εi )(̃
μi − μi )
(28)
n
n
−1
−1
̃i−1 ∣].
+n λ1n ∑i=1 [∣μi ∣ − ∣̃
μi ∣] + n λ2n ∑i=2 [∣μi − μi−1 ∣ − ∣̃
μi − μ
Since ∂Mn (μ)/∂μi ∣μi =̃μi = 0,
̃i )] = 0.
2n−1 f (0)(̃
μi − μ0i ) − n−1 sgn(εi ) + n−1 λ2n [sgn(̃
μi − μi−1 ) − sgn(μi+1 − μ
̃i ) on both sides and take sums. Then (29) becomes
Multiply (μi − μ
̃n ∥2n + n−1 λ1n ∑ni=1 [∣̃
Mn (μ) − Mn (̃
μn ) = f (0)∥μ − μ
μi ∣ − μi sgn(̃
μi ) − (∣̃
μi ∣ − ∣μi ∣)]
n
−1
̃i )(μi − μ
̃i ) + sgn(̃
+n λ2n ∑i=2 [sgn(μi+1 − μ
μi − μi−1 )(̃
μi − μi )]
̃i−1 ∣]
+n−1 λ2n ∑ni=2 [∣μi − μi−1 ∣ − ∣̃
μi − μ
̃n ∥2n
> f (0)∥μ − μ
̃i )(μi − μ
̃i ) + sgn(̃
+n−1 λ2n ∑ni=2 [sgn(μi+1 − μ
μi − μi−1 )(̃
μi − μi )]
n
−1
̃i−1 ∣].
+n λ2n ∑i=2 [∣μi − μi−1 ∣ − ∣̃
μi − μ
Then for any κn > 0, we have
hn (κn ) > f (0)κ2n /n.
From the Convex Minimization Theorem in Hjort and Pollard (1993), we have
̃n ∥2 ≥ κn )
P(∥̂
μn − μ
≤ P(∆n (κn ) > hn (κn )/2)
(29)
n
−1
2
0
2
≤ P (supμ∈Sκn n ∣∑i=1 ξni ∣ ≥ f (0)κn /(2n)) + P (supμ∈Sκn ∣τn (μ, μ )∣ ≥ f (0)κn /(2n)) .
17

Suppose rn = o(1) and τn = τn (̃
μn , μ0 ) = rn ∥̃
μn − μ0 ∥2n . Then
limn→∞ P (supμ∈Sκn ∣τn (μ, μ0 )∣ ≥ f (0)κ2n /(2n))
μn − μ0 ∥2n ≥ f (0)κ2n /(4n))
≤ limn→∞ P (supμ∈Sκn 2∣rn ∣∥̃
̃n ∥2n ≥ f (0)κ2n /(4n))
+ limn→∞ P (supμ∈Sκn 2∣rn ∣∥μ − μ
μn − μ0 ∥2n ≥ f (0)κ2n /(8n)) .
= limn→∞ P (supμ∈Sκn ∣rn ∣∥̃

(30)

Combining (27) and (29),
̃∥2 ≥ κn ) ≤ P (supμ∈Sκn ∑ni=1 ξni /n > f (0)κ2n /(2n))
P(∥̂
μ−μ
μn − μ0 ∥2n ≥ f (0)κ2n /(8n)) + o(1)
+P (supμ∈Sκn ∣rn ∣∥̃
≤ P (supμ∈S 0′

κn

∣ ∑ni=1 ξni /n∣

>

μn
f (0)κ2n /(2n)) + P (∥̃

− μ0 ∥2

(31)
≥ κn ) ,

where κ′n = 2κn . Let the ui 's be a Rademacher sequence and ψ1 , ⋯, ψD be the orthogonal basis
of a D-dimensional space to which μ − μ0 belongs. Using the Contraction Theorem in Ledoux
and Talagrand (1991) and also the Cauchy-Schwarz inequality, we have
E [supμ∈S 0′ ∣ ∑ni=1 ξni /n∣] = (8/n)E [supμ∈S 0′ ∣ ∑ni=1 ui (μi − μ0i )∣]
κ
κ
n

n

n
≤ (8/n)E [supa∈RD supμ∈S 0′ ∣ ∑D
j=1 aj ∑i=1 ui (ψj,i )∣]
κn

n
2 1/2
2 1/2 ]
≤ (8/n)E [supa∈RD supμ∈S 0′ (∑D
(∑D
j=1 aj )
j=1 (∑i=1 ui ψj,i ) )
κn
√
2 1/2
≤ (8/n) supμ∈S 0′ D(∑D
j=1 aj )
κn
√
= (8/n) supμ∈S 0′ D∥μ − μ0 ∥2
κn
√
≤ 16 Λn κn /n.

P (supμ∈S 0′ ∣ ∑ni=1 ξni /n∣ > f (0)κ2n /(2n)) ≤ E [supμ∈S 0′ ∣ ∑ni=1 ξni /n∣] /(f (0)κ2n /(2n))
κn
κn
(32)
√
√
≤ 32 Λn /(f (0)κn ) ≤ 8 Λn /(f (0)κn ).
√
Let γn = κn / n. Combining (31) and (32),
̃n ∥2 ≥ κn /2) + P(∥̃
P(∥μ − μ0 ∥n ≥ γn ) ≤ P(∥̂
μn − μ
μn − μ0 ∥2 ≥ κn /2)
̃n ∥2 ≥ κn /2) + P(∥̃
= P(∥̂
μn − μ0 ∥2 ≥ κn /2)
√μn − μ
√
≤ 32√Λn /(f (0)γn n) + 2P (∥̃
μn − μ0 ∥n ≥ γn /2)
√
≤ 32 Λn /(f (0)γn n) + 2Λn exp{Λn log n − (1 − c)2 f (0)nγn2 /8}.
√
The last "≤" is from (32) and Lemma 2 by choosing γn = 2(c f (0))−1 [λ1n + 2λ2n + ((M1 +
1)Λn /n)1/2 ]. Thus the first part of Theorem 2 holds. Furthermore, if we let λ1n + 2λ2n =
1/2
2
1/2
[2c2 f√
(0)M3 Λn (log√
n)/n]1/2 −[(M
√ 1 +1)Λn ] for M3 > 1/((1−c) f (0)) and γn = (8M3 Λn (log n)/n) ,
then Λn /(f (0)γn n) = O(1/ log n). Thus,
√
P(∥̂
μn − μ0 ∥n ≥ γn ) ≤ O(1/ log n) + 2Λn exp{Λn log n(1 − M3 (1 − c)2 f (0))}.
18

◻
Proof of Corollary 1
̂n , μ
̃n or μ0
If we replace the upper bound of maximal dimension of any linear space where μ
belong by Jmax in the proof of Theorem 1, we can obtain the consistency result in Corollary 1.
We do not repeat the proof here. ◻
Proof of Theorem 2
Suppose vector μ has J blocks and {B1 , ⋯, BJ } is the corresponding unique block partition.
Let νj be the intensity at jth block for 1 ≤ j ≤ J. From Lemma A.1 in Rinaldo (2009), the
subdifferential of the total variation penalty
⎧
−λ2n sgn(νj+1 − νj ),
j=1
⎪
⎪
⎪
∂ (λ2n ∑ ∣νj − νj−1 ∣) = ⎨ λ2n (sgn(νj+1 − νj ) − sgn(νj − νj−1 )), 1 < j < J ,
⎪
⎪
j=2
⎪
j=J
⎩ λ2n sgn(νj − νj−1 ),
J

(33)

where sgn(x) = 1, 0, −1 when x > 0, = 0, < 0, respectively. We define c0j and ̂
cj as the subdif0
̂n scaled by the corresponding block sizes. In other words, we
ferentials (33) at both ν and ν
have
0
⎧
−λ2n sgn(νj+1
− νj0 )/b0j ,
j=1
⎪
⎪
⎪
0
0
0
0
0
0
cj = ⎨ λ2n (sgn(νj+1 − νj ) − sgn(νj − νj−1 ))/bj , 1 < j < J0
⎪
⎪ λ2n sgn(ν 0 − ν 0 )/b0 ,
⎪
j = J0
⎩
j
j−1
j

(34)

⎧
−λ2n sgn(̂
νj+1 − ν̂j )/̂
bj ,
j=1
⎪
⎪
⎪
̂
̂
cj = ⎨ λ2n (sgn(̂
νj+1 − ν̂j ) − sgn(̂
νj − ν̂j−1 ))/bj , 1 < j < Ĵ .
⎪
⎪
⎪
νj − ν̂j−1 )/̂
bj ,
j = Ĵ
⎩ λ2n sgn(̂

(35)

and

̂n , we let B̂j(i) be the block estimate where i stays, that is, μ
̂i are all the same
For an estimate μ
0
0
for i ∈ B̂j(i) . Let ̂
bj(i) = ∣B̂j(i) ∣ be the size of B̂j(i) . Then Bj(i) (bj(i) ) is the corresponding block
0
set (size). From notations (I) and (V) in Section 2, we have b0j(i) = ∣Bj(i)
∣ for 1 ≤ j ≤ J0 . From the
̂F is a LAD-FSA solution if and only if
KKT conditions, μ
⎧
⎪
̂i ) = ̂
bj(i)̂
cj(i)
⎪ ∑k∈B̂j(i) sgn(yk − μ
⎨
⎪
̂i )∣ < 2λ2n
∣
sgn(yk − μ
⎪
⎩ ∑k∈B̂j(i)

if i ∈ Ĵ
.
if i ∉ Ĵ

(36)

̂i and μ0i satisfy
Let μ
⎧
⎪
̂ = μ0i + (2f (0)b0j(i) )−1 (∑k∈B0 sgn(εk ) − b0j(i) c0j(i) + ̂
hi )
⎪ μ
j(i)
⎨ i
⎪
̂i = μ
̂i−1
⎪
⎩ μ

∀i ∈ J 0
.
∀i ∉ J 0 .

(37)

Here ̂
hi is the remainder term with the stochastically equicontinuity, more specifically,
∣(b0j(i) )−1/2̂
hi ∣ = Op (1), ∀1 ≤ i ≤ n.
19

(38)

0
0
μi −μ0i ))−sgn(εk )
ςki with rki = sgn(εk −(̂
In fact, ̂
hi = 2f (0)b0j(i) (̂
μi −μ0i )+ ∑k∈Bj(i)
E[rki ]+ ∑k∈Bj(i)
0
. Define the difference vector w = (w1 , ⋯, wn )′ , with w1 = μ1
and ςki = rki − Eε [rki ] for k ∈ Bj(i)
̂n in (37).
̂i ) = sgn(wi0 ), ∀i ∈ J 0 , then (36) holds for μ
and wi = μi − μi−1 for 2 ≤ i ≤ n. If sgn(w
̂n is a LAD-FSA solution. Define
Thus, μ

̂iF ) = sgn(wi0 ), ∀i ∈ J 0 }
Rλ2n ≡ {Ĵ = J 0 } ∩ {sgn(w
Then Rλ2n holds if
⎧
̂i ) = sgn(wi0 )
⎪
⎪sgn(w
⎨
̂i )R < 2λ2n
R ∑k∈B̂ sgn(yk − μ
⎪
⎪
j(i)
⎩

∀i ∈ J 0
if i ∉ J 0

(39a)
(39b)

̂i ) = sgn(wi0 ), ∀i ∈ J 0 holds if
It is easy to verify that sgn(w
̂i )(wi0 − w
̂i )∣ < ∣wi0 ∣, for i ∈ J 0
∣sgn(w

(40)

̂n (37) into (40) and (39b), and then use the triangle inequality, we know that Rλ2n holds if
Plug μ
0
0
maxi∈J 0 ∣(b0j(i) )−1 ∑k∈Bj(i)
sgn(εk ) − (b0j(i−1) )−1 ∑k∈Bj(i−1)
sgn(εk )∣/wi0 +
maxi∈J 0 ∣(b0 )−1̂
hi − (b0
)−1̂
hi−1 ∣/w0 + maxi∈J 0 ∣c0 − c0
∣/w0

j(i)

i

j(i−1)

j(i)

j(i−1)

i

(41)

< 2f (0).
and
max
∣sgn(εi ) − sgn(εi−1 ) + ̂
hi − ̂
hi−1 ∣ < 4λ2n .
0
i∉J

(42)

We have
E[sgn(εi ) − sgn(εi−1 )] = 0
and
Var[sgn(εi ) − sgn(εi−1 )] = 2 for 2 ≤ i ≤ n
and for 2 ≤ il , i2 ≤ n,
Cov(sgn(εi1 ) − sgn(εi1 −1 ), sgn(εi2 ) − sgn(εi2 −1 )) = −1, 0 for ∣i1 − i2 ∣ = 1, otherwise.
Suppose d∗i are independent copies of N (0, 2). Then we have
P(I4 ) ≡ P(maxi∉J 0 ∣sgn(εi ) − sgn(εi−1 )∣ > 2λ2n )
≤ P(maxi∉J 0 ∣d∗i ∣ > 2λ2n )
≤ 2 exp{−4λ22n + log ∣J0c ∣}
where we get the first "≤" using Slepian's inequality, the second "≤" using Chernoff's bound.
Then P(I4 ) = o(1) if conditions in (B1) hold. Define
Xi = (2f (0)b0j(i) )−1 ∑ sgn(εk ) − (2f (0)b0j(i−1) )−1
0
k∈Bj(i)

∑
0
k∈Bj(i−1)

20

sgn(εk )∣ ∀i ∈ J 0 .

Then E[Xi ] = 0 and maxi∈J 0 Var[Xi ] ≤ (2f (0)b0j(i) )−1 . Consider independent copies Xi∗ ∼
N (0, (2f (0)b0j(i) )−1 ), i ∈ J 0 . We have
0
0
P(I1 ) ≡ P(maxi∈J 0 ∣(b0j(i) )−1 ∑k∈Bj(i)
sgn(εk ) − (b0j(i−1) )−1 ∑k∈Bj(i−1)
sgn(εk )∣ > 2f (0)an /3)
≤ P(maxi∈J 0 ∣Xi∗ ∣ > an /3) ≤ 2 exp{−2b0min f 2 (0)a2n /9 + log ∣J 0 ∣}.

Thus P(I1 ) = o(1) if conditions in (B2) holds. Since maxi∈J 0 ∣c0j(i) − c0j(i−1) ∣ ≤ 2λ2n /b0min , from
(B3),
P(I2 ) ≡ P(max
∣c0j(i) − c0j(i−1) ∣ > 2f (0)an /3) = 0.
0
i∈J

Furthermore, we have
P(I5 ) ≡ P(max
∣̂
hi − ̂
hi−1 ∣ > 2λ2n ) = 0.
0
i∉J

From (38), we have
P(I3 ) ≡ P(maxi∈J 0 ∣(wi0 b0j(i) )−1̂
hi − (wi0 b0j(i−1) )−1̂
hi−1 ∣ > 2f (0)/3)
0
0
−1/2
−1/2
̂
̂
≤ P(maxi∈J 0 ∣(bj(i) ) hi − (bj(i−1) ) hi−1 ∣ > 2f (0)(b0min )1/2 an /3)
= o(1).
Then from (41) and (42), we get
P(Rcλ2n ) ≤ P(I1 ) + P(I2 ) + P(I3 ) + P(I4 ) + P(I5 ) → 0 when n → ∞.
◻
Proof of Theorem 3
From Theorem 2, we know that if (A1) and (B1-B3) hold, the LAD-FLSA can choose all jumps
with probability 1. Thus, we can prove the main results based on the true block partition. By the
̂n is a LAD-FLSA solution if and only if
KKT, ν
⎧
⎪
bj ̂
cj = λ1n̂
bj sgn(̂
νj ) if ν̂j ≠ 0
⎪ ∑k∈Bj0 sgn(yk − ν̂j ) + ̂
.
⎨
⎪
0 sgn(yk − ν
̂j )∣ + ̂
cj ∣ < λ1n̂
bj
if ν̂j = 0
bj ̂
∣
⎪
⎩ ∑k∈Bj

(43)

Let ν̂j and νj0 satisfy
⎧
⎪
⎪ ν̂j = νj0 + (2f (0)b0j )−1 (∑i∈Bj0 sgn(εi ) + b0j c0j − λ1n b0j sgn(νj0 ) + ̂
hj )
⎨
⎪
⎪
⎩ ν̂j = 0

∀ j ∈ K0
.
∀ j ∈ K0

(44)

Here, by abuse of notation, ̂
hj is the remainder term with the stochastically equicontinunity,
∣(b0j )−1/2̂
hj ∣ = Op (1), ∀1 ≤ i ≤ n.

(45)

In fact, ̂
hj = 2f (0)b0j (̂
νj − νj0 ) + ∑i∈Bj0 E[rij ] + ∑i∈Bj0 ςij , with rij = sgn(εi − (̂
νj − νj0 )) − sgn(εi ) and
ςij = rij − Eε [rij ] for i ∈ Bj0 and 1 ≤ j ≤ J0 + 1. If {sgn(̂
νj ) = sgn(νj0 ), ∀ j ∈ K0 }, then νn in (44)
satisfies kkt-flsa, and therefore is a LAD-FLSA solution. Define an event
̂ = K0 } ∩ {sgn(̂
Rn = R(λ1n , λ2n ) ≡ {K
νj ) = sgn(νj0 ), ∀ j ∈ K0 }.
21

Then Rn holds if
{

{sgn(̂
νj ) = sgn(νj0 ),
∀ j ∈ K0
.
∣ ∑k∈Bj0 sgn(yk − ν̂j )∣ + ̂
bj ̂
cj ∣ < λ1n̂
bj ∀ j ∉ K 0

(46)

We can verify that sgn(̂
νj ) = sgn(νj0 ), ∀j ∈ K0 holds if ∣sgn(̂
νj )(νj0 − ν̂j )∣ < ∣νj0 ∣, for j ∈ K0 .
Therefore, from (44) and (46), Rn holds if
{

∣ ∑i∈B0 sgn(εi ) + b0j c0j − λ1n b0j sgn(νj0 ) + ̂
hj ∣ < 2f (0)b0j ∣νj0 ∣ ∀ j ∈ K0
.
∣ ∑i∈B0 sgn(εi ) + b0j c0j + ̂
hj ∣ < λ1n b0j
∀ j ∉ K0

(47)

Thus we have
P(Rc ) ≤ P(maxj∈K0 ∣ ∑i∈Bj0 sgn(εi )∣ > 2f (0) minj∈K0 b0j minj∈K0 ∣νj0 ∣/4)
+P(maxj∈K0 ∣c0j ∣ > 2f (0) minj∈K0 ∣νj0 ∣/4)
+P(maxj∈K0 ∣λ1n sgn(νj0 )∣ > 2f (0) minj∈K0 ∣νj0 ∣/4)
+P(maxj∈K0 ∣̂
hj /(b0j νj0 )∣ > f (0)/2)
+P(maxj∉K0 ∣ ∑i∈Bj0 sgn(εi )∣ > λ1n minj∈K0 b0j /3)
+P(maxj∉K0 ∣c0j ∣ > λ1n /3)
+P(maxj∉K0 ∣̂
hj /b0j ∣ > λ1n /3)
≡ P(S1 ) + P(S2 ) + P(S3 ) + P(S4 ) + P(S5 ) + P(S6 ) + P(S7 ).

(48)

Let Zj = ∑i∈Bj0 sgn(εi )/b0j . Then E[Zj ] = 0 and Var(Zj ) = 1/b0j . Then Zj s are independent
sub-Gaussian. From (C3), we have
P (S1 ) ≤ 2K0 exp{−b0min f 2 (0)ρ2n /8} = o(1).
We can verify P (S2 ) = o(1) from (C4), P (S3 ) = o(1) from (C5) and P (S6 ) = o(1) from (C2)
From (C1),
P (S5 ) ≤ 2(J0 − K0 ) exp{−b0min λ21n /32} = o(1).
Furthermore, we have P (S7 ) = o(1) and P (S4 ) = o(1). From (48), we have P (R) → 1 when
n → ∞, which completes the proof. ◻
The rest of the Appendix are presented to prove Theorem 4.
Recall that w is the jump coefficients vector with wi = μi − μi−1 for 2 ≤ i ≤ n and ν =
(ν1 , ⋯, νJ )′ is the block coefficients factor. From Proposition 3 in Rosset and Zhu (2007), we
know that the following results of the LAD-FLSA solution.
Lemma 4

(i) For any λ1 = 0, there exists a set of values of λ2 ,
0 = λ2,0 < λ2,1 < ⋯ < λ2,m2 < λ2,m2 +1 = ∞

̂ λ2,k ) for 1 ≤ k ≤ m2 is not uniquely defined, the set of optimal solutions of
such that w(0,
̂ λ2 ) is
each λ2,k is a straight line in Rn , and for any λ2 ∈ (λ2,k , λ2,k+1 ), the solution w(0,
constant.
22

(ii) For above λ2,k , 1 ≤ k ≤ m2 , there exists a set of values of λ1 ,
0 = λ1,0 < λ1,1 < ⋯ < λ1,m2 < λ1,m2 +1 = ∞
̂(λ1,j , λ2,k ) for 1 ≤ j ≤ m1 is not uniquely defined, the set of optimal solutions
such that ν
̂(λ1 , λ2,k )
of each λ1,j is a straight line in Rn , and for any λ1 ∈ (λ1,j , λ1,j+1 ), the solution ν
is constant.
In Lemma 4, if we define λ2,k , 1 ≤ k ≤ m2 from (i) as the transition points for w and N0,λ2
as the set of y ∈ Rn such that λ2 is a transition point for w, then the jumps set J (0, λ2 ) only
changes at those λ2,k 's. Furthermore, Let λ2 = λ2,k for some 1 ≤ k ≤ m2 . If we can also define
λ1,j , 1 ≤ j ≤ m1 from (ii) as the transition points for ν and Nλ1 ,λ2 as the set of y ∈ Rn such that
λ1 is a transition point for ν, then the set of nonzero blocks, K(λ1 , λ2 ), only changes at λ1,j 's or
λ2,k 's, and Nλ1 ,λ2 , is in a finite collection of hyperplanes in Rn . From Lemma 4, we know that
̂(λ1 , λ2 ) is fixed and then {1, 2, ⋯, n} is divided into two sets,
for any given y ∈ Rn /Nλ1 ,λ2 , μ
c
Ey,λ1 ,λ2 and Ey,λ1 ,λ2 , where Ey,λ1 ,λ2 = {1 ≤ i ≤ n ∶ yi − ν̂j(i) = 0, ν̂j(i) ≠ 0} and j(i) specifies the
̂i stays. Thus, we have
block where μ
∣Ey,λ1 ,λ2 ∣ = ∣K(λ1 , λ2 )∣.

(49)

Lemma 5 For any λ1 > 0 and λ2 > 0, if y ∈ Rn /Nλ1 ,λ2 , ν̂(λ1 , λ2 , y) is a continuous function of
y, and then Ey,λ1 ,λ2 is locally constant.
Proof of Lemma 5
Let L(μ, y) denote the function
n

n

n

L(μ, y) = ∑ ∣yi − μi ∣ + λ1 ∑ ∣μi ∣ + λ2 ∑ ∣μi − μi−1 ∣.
i=1

i=1

i=2

̂ does not change from Lemma 4. For any y0 ∈ Rn /Nλ1 ,λ2 , and for any
Since y ∈ Rn /Nλ1 ,λ2 , ν
̂(λ1 , λ2 , ym ) → ν
̂(λ1 , λ2 , y0 ). It
sequence {ym } such that {ym } → y0 , we want to prove that ν
̂(λ1 , λ2 , ym ) → μ
̂(λ1 , λ2 , y0 ). Because ∥̂
is equivalent to prove μ
μ(λ1 , λ2 , y)∥1 ≤ ∥̂
μ(0, 0, y)∥1 =
̂(λ1 , λ2 , y) is bounded. Thus we only need to check that for every converging subsequence
∥y∥1 , μ
̂(λ1 , λ2 , ymk ) → μ
̂(λ1 , λ2 , y0 ). Suppose that μ
̂(λ1 , λ2 , ymk ) →
of {ym }, say {ymk }, we have μ
μ̌(λ1 , λ2 ) when mk → ∞. Let ∆(μ, y, y′ ) = L(μ, y) − L(μ, y′ ). On the one hand, we have
L(̂
μ(λ1 , λ2 , y0 ), y0 )
= L(̂
μ(λ1 , λ2 , y0 ), ymk ) + ∆(̂
μ(λ1 , λ2 , y0 ), y0 , ymk )
≥ L(̂
μ(λ1 , λ2 , ymk ), ymk ) + ∆(̂
μ(λ1 , λ2 , y0 ), y0 , ymk )
= L(̂
μ(λ1 , λ2 , ymk ), y0 ) + ∆(̂
μ(λ1 , λ2 , ymk ), ymk , y0 ) + ∆(̂
μ(λ1 , λ2 , y0 ), y0 , ymk ).
On the other hand, we have
∆(̂
μ(λ, ymk ), ymk , y0 ) + ∆(̂
μ(λ1 , λ2 , y0 ), y0 , ymk )
n

̂i (λ1 , λ2 , ymk )∣ − ∣yi,0 − μ
̂i (λ1 , λ2 , ymk )∣
= ∑[∣yi,mk − μ
i=1

̂i (λ1 , λ2 , y0 )∣ − ∣yi,mk − μ
̂i (λ1 , λ2 , y0 )∣]
+∣yi,0 − μ
n

≤ 2 ∑ ∣yi,mk − yi,0 ∣ → 0 when k → ∞
i=1

23

̂(λ1 , λ2 , y0 )
Thus L(̂
μ(λ1 , λ2 , y0 ), y0 ) ≥ limk→∞ L(̂
μ(λ1 , λ2 , ymk ), y0 ) = L(μ̌(λ1 , λ2 , y0 ), y0 ). Since μ
̂(λ1 , λ2 , y0 ). ◻
is the unique minimizer of L(μ, y0 ), we have μ̌(λ1 , λ2 , y0 ) = μ
Proof of Theorem 4
From (49) and Lemma 5, there exists  > 0 such that y ∈ Ball(y, ), Ey,λ1 ,λ2 stays the same
when neither λ1 and λ2 is a transitional point. Thus, ∂̂
νj(i) (λ1 , λ2 )/∂yi = 1 if i ∈ Ey,λ1 ,λ2 and
νj(i) (λ1 , λ2 )/∂yi = ∣Ey,λ1 ,λ2 ∣ =
∂̂
νj(i) (λ1 , λ2 )/∂yi = 0 if i ∉ Ey,λ1 ,λ2 . Overall, we have ∑ni=1 ∂̂
̂ 1 , λ2 )∣ for y ∈ Nλ ,λ . Since Nλ ,λ is in a collection of finite hyperplanes, we can obtain the
∣K(λ
1 2
1 2
conclusion by taking the expectation. ◻

References
Boysen, L., Kempe, A., Liebscher, V., Munk, A. ad Wittich, O.L. (2009). Consistencies and rates
of convergence of jump-penalized least squares estimators. Annals of Statistics, 37, 157–183.
Cirel'son, B. S., Ibragimov, I. A. and Sudakov, V. N. (1976). Norm of Gaussian sample function. In Proceedings of the Third Japan–U.S.S.R. Symposium on Probability Theory. Springer
Lecture Notes in Mathematics, 550, 20-41. Springer, Berlin.
Donoho, D.L., and Johnstone, I.M. (1995), Adapting to Unknown Smoothness via Wavelet
Shrinkage. Journal of the American Statistical Association, 90 1200–1224.
Gao, X.L. and Fang, Y.X. (2011). Generalized degrees of freedom under the L1 loss function.
Journal of Statistical Planning and Inference, 141, 677–686 .
Gao, X.L. and Huang, J. (2010a). A robust penalized method for the analysis of noisy DNA copy
number data, BMC Genomics, 11:517.
Gao, X.L. and Huang, J. (2010b). Asymptotic properities of LAD-Lasso in high-dimensional
settings. Statistic Sinica, 20, 1485–1506.
Harchaoui, Z. and Lévy-Leduc, C. (2010). Multiple change-point estimation with a total variation
penalty. Journal of the American Statistical Association, 105, 1480–1493.
Hebiri, M. (2008). Regularization with the smooth-lasso procedure. Preprint, Laboratoire de
Probabilités et Modèles Aléatoires.
Hjort, N. L. and Pollard, D. (1993). Asymptotics for minimisers of convex processes. Statistical
Research Report, University of Oslo.
Kato, K. (2009). On the degrees of freedom in shrinkage estimation. Journal of Multivariate
Analysis, 100, 1338–1352.
Ledoux, M. and Talagrand, M. (1991). Probability in Branch Spaces: isoperimetry and processes.
Springer Verlag, New York.
Li, Y. and Zhu, J.(2008). L1-Norm quantile regression. Journal of Computational & Graphical
Statistics, 17, 163–185.
Mammen, E. and Van De Geer, S. (1997). Locally adaptive regression splines. Annals of Statistics, 25, 387–413.
Rinaldo, A. (2009). Properties and refinements of the fused lasso. Annals of Statistics, 37, 2922–
2952.
24

Ronchetti, E. (1985) Robust model selection in regression. Statistics & Probability Letters 3,
21–23.
Snijders, A.M., Nowak, N., Segraves, R., Blackwood, S., Brown, N., Conroy, J., Hamilton,
G., Hindle, A.K., Huey, B., Kimura, K., Law, S., Myambo, K., Palmer, J., Ylstra, B., Yue,
J.P., Gray, J.W., Jain, A.N., Pinkel, D. and Albertson D. (2001). Assembly of microarrays for
genome-wide measurement of DNA copy number. Nature Genetics, 29, 263–264.
Schwarz, G.E. (1978). Estimating the dimension of a model. Annals of Statistics, 6, 461–464.
Tibshirani, R. (1996). Regression shrinkage and selection via the lasso, Journal of the Royal
Statistical Society: Series B, 58, 267–288.
Tibshirani, R., Saunders, M., Rosset, S., zhu, J., and Knight, K. (2005). Sparsity and smoothness
via the fused lasso. Journal of the Royal Statistical Society: Series B, 67, 91–108.
Tibshirani, R. and Wang, P. (2008). Spatial smoothing and hot spot detection for CGH data using
the fused lasso. Biostatistics, 9(1), 18–29.
Wahba, G. (1990). Spline models for observational data. Philadelphia: Society for Industrial and
Applied Mathematics.
Yao, Y., and Au, S. T. (1989). Least-squares estimation of a step function. Sankhya: The Indian
Journal of Statistics, Series A, 51, 370–381.
Ye, J. (1998). On measuring and correcting the effects of data mining and model selection. Journal of American Statistist Association, 93, 120–131.
Yuan, M. and Lin, Y. (2006). Model selection and estimation in regression with grouped variables.
Journal of Royal Statistical Scociety, Series B, 68, 49–67.
Zhao, P. and Yu, B. (2006). On model selection consistency of LASSO. Journal of Machine
Learning Research, 7, 2541–2563.
Zou, H., Hastie, T. and Tibshirani, R. (2007). On the "degrees of freedom" of the lasso. Annals
of Statistics 35, 2173–2192.

25

Figure 1: Copy Number data set from the GM 13330 BAC CGH array. The top and bottom
panels give outputs from the LAD-FLSA and LS-FLSA, respectively. Both observed data (gray
dots) and estimates (dark solid lines) from chromosome 1–4 are plotted . Data from different
chromosomes are separated by gray vertical lines.

26

Figure 2: The estimated degrees of freedom of LAD-FLSA for every combined λ1 and λ2 for the
chromosome 1 data from the GM 13330 BAC array.

Figure 3: Hypothetical model from 500 Monto Carlo simulations of 129 markers for chromosome
1 data from the GM 13330 BAC array. It shows that the estimated number of nonzero blocks
∣K(λ1 , λ2 )∣ is very close to the true D(Mλ1 ,λ2 ) using the 45 degree line.

27

Figure 4: Example of observed data (grey) with true hidden signals (black) and LAD-FLSA
estimates (red). There are 6 blocks with 4 nonzero ones. Random
noise εi 's are generated from
√
double exponential distributions with center 0 and scale 0.5/ 2.
Table 1: Simulation results for Section 6.1.

i

σ

Normal

1.0
0.5

Double Exp.

0.1
1.0
0.5
0.1

Cauchy

1.0
0.5
0.1

Model
LAD-FLSA
LS-FLSA
LAD-FLSA
LS-FLSA
LAD-FLSA
LS-FLSA
LAD-FLSA
LS-FLSA
LAD-FLSA
LS-FLSA
LAD-FLSA
LS-FLSA
LAD-FLSA
LS-FLSA
LAD-FLSA
LS-FLSA
LAD-FLSA
LS-FLSA

LARE1
0.197
0.035
0.098
0.016
0.019
0.013
0.154
0.031
0.077
0.016
0.015
0.013
0.048
0.239
0.028
0.120
0.007
0.029

n = 1000
CFR+62
JUMP3
89%(17%)
7.12(1.32)
18%(3%)
7.82(1.47)
97%(32%) 5.59 (0.75)
48%(13%)
5.68(0.74)
100%(93%) 5.00(0.00)
100%(93%) 5.00(0.00)
88% (22%) 7.42(1.54)
12%(0%)
7.42(1.42)
97%(34%)
5.95(0.90)
57%(12%)
5.73(0.78)
100%(97%) 5.00(0.00)
100%(97%) 5.00(0.00)
87%(56%)
6.12(1.07)
17%(4%)
16.37(5.38)
99%(70%)
5.56(0.86)
39%(17%) 10.67(3.62)
95%(92%)
5.18(0.46)
94%(78%)
6.30(1.34)

LARE
0.173
0.021
0.087
0.007
0.017
0.003
0.128
0.021
0.064
0.007
0.013
0.003
0.029
0.275
0.015
0.132
0.003
0.023

n = 5000
CFR+6
82%(15%)
5%(0%)
96%(22%)
57%(7%)
100%(94%)
100%(94%)
89%(25%)
3%(1%)
100%(41%)
62%(19%)
100%(90%)
100%(89%)
82%(45%)
2%(0%)
87%(66%)
15%(3%)
96%(87%)
87%(49%)

JUMP
7.24(1.33)
7.7(1.48)
5.54(0.72)
5.61(0.74)
5.00(0.00)
5.00(0.00)
7.18(1.35)
7.31(1.43)
5.74(0.86)
5.68(0.87)
5.00(0.00)
5.00(0.00)
6.14(0.95)
59.41(14.38)
5.59(0.78)
32.05(8.51)
5.17(0.40)
10.14(3.13)

NOTE 1: LARE is the least absolute relative ratio defined in (15).
̂0 correctly or plus at most six additional false posives
NOTE 2: CFR+6 is the ratio of recovering μ
(correctly fitted ratio).
NOTE 3: JUMP is the average number (standard deviation) of the number of jumps.

28

