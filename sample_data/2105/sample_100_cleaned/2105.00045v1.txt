estimation selection properties ladfused lasso signal approximatorarxiv210500045v1 statme 30 apr 2021xiaoli gaodepartment mathematics statisticsuniversity north carolina greensboroabstractfused lasso important method signal processing hidden signals sparse blocky combinationsquared loss function squared loss suitable heavy tailerror distributions robust outliers arise practiceabsolute deviations lad loss provides robust alternativesquared loss paper study asymptotic properties fusedlasso estimator lad loss signal approximation referestimator lad fused lasso signal approximator ladflsainvestigate estimation consistency properties ladflsa provide sufficient conditions ladflsa sign consistentconstruct unbiased estimator degrees freedom ladflsa given tuning parameters simulation studies real dataanalysis conducted illustrate performance ladflsaeffect unbiased estimator degrees freedomkeywords estimation consistency jump selection consistency block selection consistencydegrees freedom fused lasso absolute deviations sign consistencycorrespondence106 petty building greensboro nc 27412 email x gao2uncgeduintroductionhighdimensional data arise fields including signal processing image denoising genomic genetic studies model sparse certain known structures penalizedmethods widely incorporate known structures penalty functions natural way estimation variable selection simultaneously biologicalexample analysis copynumber variations human genome probleminterested detecting changes copy numbers based data comparative genomichybridization cgh arrays instance snijders et al 2001 studied cgh array consisting2400 bacterial artificial chromosome bac clones log base 2 intensity ratiosclones measured figure 1 plot sample cgh copy number data chromosomes 14cell line gm 13330 data set characteristics 1 small number chromosomal locations copy numbers genes change underlyingsignals sparse 2 adjacent markers tend similar observations signalsblockysignal approximation model sample size n ith observation yi consideredrealization true signal 0i plus random noiseyi 0i 1 ncases true signal vector 0 01 0n blocky sparse meaningintensities true signals block blocks consist zero signalsgoal solution recover changes points identifynonzero blocks use lasso penalty enforce sparse solution penalizing1 norm signals 1 ni1 use fusion total variation penalty enforceblocky solution penalizing tv ni2 i1 combination penaltiesresults fused lasso fl penalty tibshirani et al 2005detecting changing points copy number variations tibshirani wang 2008 proposed use fused lasso squares loss function refer approachsquares fused lasso signal approximator lsflsa n 1n 2n lsflsa seeksn2 nn2 n minimizes12 nyi 2 1n 2n i11n 2n nonnegative penalty parametersrecently rinaldo 2009 studied selection properties lsflsa adaptive lsflsa block partition assumption underlying signal authorsstudied properties related procedures example mammen van geer 1997 studied rate convergence bounded variation function classes parameter functions harchaoui lvyleduc 2010 investigated estimation properties change pointstotal variation penalty boysen et al 2009 studied asymptotic properties jumppenalizedleastsquares regression aiming approximating regression function piecewiseconstantfunctions studies significantly advanced understanding lsflsa fusion penalized ls methods context signal detection nonparametric estimationresults obtained methods squares loss andor require normality assumption errors lsflsa easily affected outliers arise practiceexample cgh copy number variation datarobust fused lasso signal approximator constructed lad lossshall refer ladflsa given n 1n 2n ladflsa definedn n arg min yi 1n 2n i1n 3 applied detect copy number variation breakpointsconvex minimizergao huang 2010a theoretic properties studiedpaper seek answer following questions ladflsa 1 closen true model 0 asymptotically 2 accuratelyn1 recover truenonzero blocks large probability 0 sparse blocky 3degrees freedom ladflsa contributions paper followsladflsa rate consistent number blocks relatively smallprovide sufficient conditions ladflsa able recover blockpatterns distinguish nonzero blocks zero ones correctly high probabilityladflsa determine jumps identify nonzero blocksdistinguish positive signals negative ones conditionsterms model complexity measures justify number nonzero blocksgenerated ladflsa estimate unbiased estimator degrees freedomladflsaassumption gaussian subgaussian random error studieswidely applied signal detection signal processing random noises follownice distributions signaltonoise ratios largerest paper organized follows list notations section 2 studyestimation consistency sign consistency properties ladflsa respectivelysection 3 4 section 5 derive unbiased estimator degrees freedomladflsa section 6 conduct simulation studies real data analysis demonstrateperformance ladflsa verify effect unbiased estimator degreesfreedom numerically section section 7 concludes paper discussionstechnical proofs postponed appendixpreliminariessuppose true model 0 01 0n includes j0 blocks exists unique vector0 10 j000i j0 ii bj0b10 bj00 mutually exclusive block partition 1 n generated 0 basedblock partition rewrite 1 n i0 i1 1 i1 i2 1 ij0 1 ij0 11 i0 i1 ij0 1 ij0 1 n ij1 ij 1 gives jth block set bj0denote jump set true model j 0 j 0 i1 ij0 1 j0 j 0 1j 0 cardinality j 0 let k0 k0 1 j j0 j0 0 set nonzeroblocks 0 number nonzero blocks k0 k0 list following notationspaper adopted rinaldo 2009true model 0 defined 4 introduce following notations iivb0j bj0 size block set bj0 1 j j0ii b0min min1jj0 b0j smallest block sizeiii minij 0 0i 0i1 smallest jumpiv n minjk0 j0 smallest nonzero signal intensityn 3 followscorresponding notations analogous ladflsa estimatev jn jn bj bjnbj bj j jn 1 jb estimated jump set number blocks block partitions 1 ncorresponding block size associated unique vector generatedkkvi kn 1 j j 0 set estimated nonzero blocks kestimation consistency ladflsa estimatorsn considersection study estimation consistency ladflsafollowing conditionsa1 error assumption random errors s model 1 independent identically distributed median 0 density f continuous positive neighborhood 0a2 block number assumption true block number j0 m1 n constant m1 0n max16n22n 2n2 21n n2n n1n 1 22n 2n21na1 require random errors density continuous positiveneighborhood zero median zero weaker condition gaussianrandom error assumption required harchaoui lvyleduc 2010 rinaldo 2009a1 allows heavytail distributions errors including cauchy distributionmoments exist condition a2 requires number blocks underlyingmodel increase n slower rate byproduct tuning parameterjumps 2n grows faster tuning parameter signals intensities 1nreasonable assumption true model blockwise number nonzero jumpssmaller number nonzero signals example numberjumps ologn12 let 2n n12 logn14 1n n1n 3order study asymptotic properties ladflsa estimatorinvestigate lsflsa approximationn n arg min zi f 012 2 1n 2n i1zi f 012 0i 4f 012 sgni 1 n consist pseudosignaldata estimates listed vvi analogous corresponding ones generatedn provide rate upper bounds number blocks generatedn 3 respectively5lemma 1 a1 j 16n22n 2n2 21n 1 provided 22n 2n2 21n iin2n n1n 1 provided 2n n1n addition suppose a2 holds iiij j0 m1 2n m1 n defined a2proof lemma 1 given appendix lemma 1 gives upper bounds numbernn interpret bounds ii maximalblocks associatednn belong respectively similarly iii providesdimension linear spacenn 0unified rate upper bound dimension linear spacenn furthermorebelong lemma 1 useful obtaining estimation consistenciesimportant notice upper bounds lemma 1 mainly affected rate2n reasonable number jumps flsa model mainly determined2ndenote 2n ni1 2i n 22 ni1 2i present estimation propertiesn 5lemma 2 suppose a1a2 hold exists constant 0 c 1n 0 n n n expn log n 1 c2 f 02nn2n defined a2 n 1c f 01n 22n m1 1n n12 furthermorelet n 2m2 n log nn12 choose 1n 2n 1n 22n c f 0nm1 1n n12 constant m2 1f 01 c2n 0 n n n n1m2 f 01cproof lemma 2 given appendix lemma 2 gives estimation consistencyn pseudo data zi s bounded noises s worthresult pseudo lsflsapoint report consistency result pseudo lsflsan bounded noises s lemma 2 obtain similar consistency resultestimatorregular lsflsa estimator 2 assumption gaussian noises extrawork estimation consistency properties ls signal approximator totalvariation penalty harchaoui lvyleduc 2010 obtained lemma 2taking 2n 0 n kmaxn lemma 2 plays important role deriving correspondthe consistency resultn following theorem 1ing estimation consistency resulttheorem 1 suppose a1 a2 hold exists constant 0 c 1n 0 n n n expn log n 1 c2 f 08nn2 8f 0n nn2 12n defined a2 n 2c f 01n 22n m1 1n n1212 constant m 1f 01 c2furthermore let n 8m3 n log nnchoose 1n 2n 1n 22n c f 02n m1 1n n122 1n 0 n n n nm3 f 01co 1 log nnproof theorem 1 given appendix theorem 1 implies ladflsaconsistent estimating rate o n log nn furthermore numberblocks true signals bounded rate convergence stated explicitlyfollowing corollary 1corollary 1 suppose a1 holds exists jmax 0 j0 jmaxjmaxp maxjn 0 n n jmax nc2m jmax o 1 log nn 8m jmax log nn12 1n 22n c1m jmax log nn12 jmax n12m 11 c2 f 0 constant c1m 2m c2 f 012 c2m f 0m 1 c2 1n consistent estimating 0 rate o jmax log nn12corollary 1 saysnumbers true estimated jumps bounded convergence ratecompared n12 argued yao yu 1989 optimal ls estimatorslevels step function notice limn p j 0 1 ni10i 2j0 0bjj j0 2 b0min jj1j j0 2 large n surely corollary 1 implieslarge np j 0n 0 2 8m jmax log nb0min 12 jmax nf 0m 1c 1jmaxn converge 0 2 normconvergence rate affected b0minn converge fasterrate o jmax log nbmin words block estimatortrue model 0 larger block sizeblock sign consistency ladflsasection study sign consistency ladflsa sign consistencystudied zhao yu 2006 gao huang 2010b lslasso ladlasso highdimensional linear regression settings stronger result variable selectionconsistency requires variables selected correctly signsestimated correctly high probabilitylight block structure hidden signals consider selection consistencysign consistency jumps blocks separatelyn jump selection consistentdefinition 1lim p j0 1jj0 bj bj0 1n jump sign consistentdefinition 2i1 sgn0i 0i1 j 0 1lim p j 0 sgnn partition signals blocks correctly probabilitydefinition 1 requiresn findsconverging definition 2 stronger requirement requiresjumps jump directions updown correctly jump selection consistentestimator recover jumps set j 0 correctly high probability tellblocks nonzero intensities words exist 0 1 j j0j 0 j0 0 jump sign consistentdefine block selection consistency block sign consistency definitions3 4 stronger definition requires signs signals recoveredcorrectlyn block selection consistentdefinition 3k0 1lim p j 0 kn block sign consistentdefinition 4k0 sgnlim p j 0 kj sgnj0 j j0 1jump selection consistency1n 0 ladflsa lad signal approximator total variationpenalty ladfsa definedfn 2nn 0 2n arg min yi 2n i1fn 2n block partition correctly expect sort nonzerosupposeblocks increasing 1n slowly 0 investigate jump selection consistencyfn 2n list conditions smallest value true jumps smallest sizetrue blocks model 1 4 jump sign consistency recall b0mindefined ii iii section 2b1 2n b exists 0 2n logn j0 12 1 2b2 bmin b exists 0 bmin logj0 312f 0 sufficiently large nb3 2n f 03b0min sufficiently large nb1 b3 indicate 2n increases n faster o logn j0 12 slowerob0min b2a requires smallest jump smallest size blockstrue model large 1 n partitioned different blockscorrectly b2b strengthes b1a providing lower bound conditions b1b3 providehelpful information finding optimal tuning parameter model 7fn 2n group signals differentconditions satisfied ladfsa estimatorblocks correctly large probabilitytheorem 2 consider signal approximation model 1 true model 4 ladfsafn 2n jump sign consistent a1 b1b3estimatorproof theorem 2 postponed appendix theorem 2 tells applyladfsa approach recover true jumps signs correctly highprobability true hidden signal vector blocky tuning parameter 2n chosenappropriatelyblock selection consistencyseen ladfsa solution jump selection consistent blocky hiddensignal vector conditions cases true signal vector includes zeroblocks separated nonzero ones ladfsa approach total variation penalty shrinks adjacent differences signals additionallasso penalty flsa force estimates block intensities exactly zerointerested finding ladflsa solution recover true jumpszero blocks nonzero ones large probability eventually needstudy choose tuning parameters 1n 2n appropriately ladflsablock selection consistenttrue block model 4 sparse need following additional conditionsseparate nonzero blocks zero onesc1 1n b0min12 n b exists 0 1n b0min logj0k0 12 4 21c2 2n b0min 1n 8 sufficiently large nc3n bmin n b exists 0 n bmin logk02 21 f 0c4 2n b0min f 0n 3 sufficiently large nc5 1n f 0n 2 sufficiently large ncondition c1 c2 indicate 1n smallest block size b0min grown lower bound provided c1b 2n grows n b1 especially1n relatively small seen c5 b0min large c4 c5 providelower bound smallest nonzero signal n n large interpretationsconsistent c3a requires block size true nonzero signal intensitieslarge nonzero blocks separated zero oneswords n relatively smaller harder separate nonzero ones zero onesimpossible distinguish nonzero blocks largerblock size observations estimate j0 jth block c3b providesupper bound number nonzero blocks worthwhile pointconditions complicated redundant instance c2c5 derive smaller upper bound c4 c2c5 satisfied c4 redundant c3a redundant c5c1a holdtheorem 3 a1 b1b3 c1c5 ladflsa solution block sign consistenttheorem 3 tells ladflsa recover block patterns hidden signalsdetecting true jumps rule nonzero blocks furthermorelarge probability nonzero blocks identified correctly positive negativesignals ladflsa justified promising approach signal processingtrue hidden signal vector blocky sparse observed data contaminatedoutliers proof theorem 3 provided appendixremark 1 block assumption true model 4 crucial studymodel grouped blocky fused lasso misleading fusion termgenerate blockwise solution techniques group lasso yuan lin2006 smooth lasso hebiri 2008 useful generate corresponding groupsparsity structureremark 2 relaxation gaussian subgaussian random error assumption important common contaminated data signal processing especiallyrepeated measurements available normalization methods loesspreprocessing real data order improve robustness lsflsatechniques oversmooth data generate false negativesadditional remarks asymptotic propertiesprovide additional comments asymptotic results obtained section 3 4remark 3 ladflsa reach estimation consistency sign consistency simultaneouslyrate estimation consistency theorem 3 holds 1n 22n olognn12 b1b c2 know sufficient conditions sign consistencytheorem 3 requiring kn ologn12 k 1 2 ladflsa able reachestimation consistency sign consistency simultaneously claimtheoretically justified conditions assumed theorem 3 3 sufficientremark 4 weak irrepresentable condition necessary jump point detectionconsistency theorem 2understand remark 4 transform signal approximation model 7lasso representation consider linear regression modelyi xij j1 nyi xi1 xip 1 p represent observed data coefficients vectorlasso solution tibshirani 1996arg min 12 yi xij j 2 jdivide coefficients vector 1 2 1 include nonzero coefficients 2 includes zeros correspondingly write x x1 x2s1 sgn1 consist sign mappings nonzero coefficients true model weakirrepresentable condition designed matrix x meansx2 x1 x1 x1 1 s1 11 vector element 1 naturally write ladfsa 7lasso solution 1 nnf 2n arg min y z2 2n1 1 i1 2 n z low triangular design matrix nonzeroitems 1 zhao yu 2006 proved weak irrepresentable condition necessarycondition lasso solution 8 sign consistent regularity conditionslist result following lemma 3lemma 3 zhao yu 2006 suppose regularity conditions satisfied designedmatrix x 1 exists positive definite matrix c covariance matrix x xnc n 2 max1in xi xi n 0 n lasso general sign consistentlimn p 0 sgnsgn0 1 exists n x satisfies weakirrepresentable condition holds n n 0 true coefficient vectorunfortunately easy verify design matrix z 10 satisfy weakirrepresentable condition example consider signal approximation dataobservations 1 2 3 4 5 row vector z2 z1 z1 z1 1 0 0 19 violated contradiction sign consistency resulttheorem 2 lemma 3 regularity conditions lemma 3 violateddesign matrix z suppose 1 n eigenvalues z zn know 1 13n 0n 4n12 n addition b max1in zi zi n 1degrees freedom ladflsacrucial seek appropriate 1n 2n 3 large 1n generate zero coefficientslarge 2n generate zero jumps conditions 1n 2n section 3 4 provideguidance choosing rates tuning parameters obtain wellbehaved ladflsa estimate section helps choose optimal tuning parameters modelselection point viewgiven 1 2 ladflsa approach modeling procedure including modelselection model fitting complexity modeling procedure defined generalizeddegrees freedom df measured sum sensitivity predicted valuesye 1998 gao fang 2011 discussion df modeling procedurey 1 2 ladflsa2 1 loss functions respectively 1 n letfitted value yi given 1 2 degrees freedom ladflsa approachdf1 2 ey 1 2 yitheorem 4 provide unbiased estimator df1 2 11 ladflsa modelingproceduretheorem 4 consider ladflsa modeling procedure defined 1 3 4fixed positive 1 21 2 df1 21 2 unbiased estimatheorem 4 indicates number nonzero blocks ktor degrees freedom ladflsa modeling procedure given 1 2provide numerical demonstration theoretical proof provided section 63appendix respectively fact unbiased estimator 12 observed theorem 2 li zhu 2008 example 1 0 2 0ladflsa reduces ladlasso solution w wi i1 2 nyi 0 2 yi 0 2 suppose 2 0 fixed block partition decidedi11 0 ladflsa lasso model block intensity vector2ni1yi 1 2 yi kresults theorem 4 choose optimal tuning parameters modelselection point view let yi0 s denote new observations generated mechanismy 1 2 definedgenerating yi s prediction errore01 2 yi0e0 taken yi0 s theorem 4 estimate prediction error 131 21 2 kyiexisting model selection criteria modified choose optimal combinationtuning parameters instance extend aicr ronchetti 1985 bic schwarz 1978gcv wahba 1990 ladflsa follows1 21 2 kaicr ni1 yi1 2 logn21 2 kbic i1 yi1 2 n1 2 1 kgcv i1 yinumerical studiessection use simulation studies real data analysis demonstrateperformances ladflsa approach recovering true hidden signals verifytheorem 4 numerically sample copy number datarecovery hidden signalsillustrate performance ladflsa modifying block example studieddonoho johnstone 1995 harchaoui lvyleduc 2010 signal vectorblocky sparse choose t 1 23 65 76 9 h 15 3 43 31 2round j hj 1 sgnin tj 2 nearest integers 0i 1 ngenerated true hidden signal vector0 0p1 2p2 2p3 3p4 0p5 2nqblocky sparse nonzero blocks zero ones q p1 p5observed data generated model 1 simulating s 1 normal distributionmean 0 standard deviation 2 double exponential distribution center 0 standarddeviation 3 standard cauchy distribution multiplier 01 similar harchaouilvyleduc 2010 consider weak mild strong noises setting 01 05 1types distributions figure 4 plot sample data set generated 2observed data true hidden signals ladflsa estimates plotted gray blackred colors respectivelydata standardized yi sy analyzed ladflsa approach 3sy standard deviation y1 yn choose optimal 1 2 minimizing bic 14 0 1 05 increments 001 n logn12 2n n12increments 01 respectively demonstrate robust properties ladflsaapproach report simulation results lsflsa approach 2model illustrate variable selection effect cfr6 ratio recoveringcorrectly overfitting model including additional noises 1000 replicateschoose blocks true model report jump averagenumber standard deviation jumps 1000 replicates jump countedadjacent differences 01 demonstrate estimation effects computingabsolute relative error lare followslaren 00ii1i1simulation results sample size n 1000 5000 reported table 1ladflsa approach better performance lsflsastrong mild signal noises signal noises weak ladflsaadvantages lsflsa especially data contaminated cauchy distributednoises example cauchy error 01 ladflsa recovers true signal vectorexactly ratio 87 n 1000 92 n 5000 lsflsa recoverstrue model exactly 49 78 timebac arraysection 1 introduced sample bac cgh data observation entrycell line gm 13330 log 2 fluorescence ratios 23 chromosomes resultedbac experiment sorted order clones locations genome purposestudy detect locations significant deletions amplificationsdemonstration effect ladflsa applied copy number analysis analyzedata chromosome 14 129 67 83 167 markers respectively log2 ratios markers observed 0 data spatialdependence properties reasonable assume true hidden signals sparseblocky analyze chromosome independently ladflsa 3lsflsa 2 tuning parameters chosen section 61 final estimatesmethods 4 chromosomes plotted figure 1 ladflsaestimates panel provides blocks amplification region chromosome 1deletion region chromosome 4 variation regions detected ladflsa lsflsa estimates panel amplification single pointchromosome 2 confirmed spectral karyotyping snijders et al 2001effect unbiased estimator degrees freedomconduct simulations based sample bac array studied section 62examine theorem 4 numerically illustrate effect unbiased estimator degreesfreedom chromosome 1 129 locations examplesample data figure 1generate 500 monte carlo simulations based hypothetical modelyi0 yi 0i 1 129yi s observations 129 locations 0i s independent normal center 0standard deviation 01 standard deviation y combined 1 21 2 compute true df1 21 2 k0 1 2 1 record dfdefined 11 monte carlo simulation algorithm 1 ye 1998 figure 21 2 ladflsa estimate combination 0 1 2 1plot df1 2 500 repetitionsincrement 005 respectively averages df1 2 kreported figure 3 simulation results number estimated nonzero1 2 promising estimate df1 2 numerically especiallyblocks knumber estimated nonzero blocks deviated true seriouslyconcluding remarkspaper study asymptotic properties lad signalapproximation approachfused lasso penalty assuming true model blocky sparseinvestigate estimation consistency sign consistency ladflsa estimatorterms estimation consistency consistency rate optimal logarithmic factordimension linear space true model estimates belong boundedterms sign consistency justify ladflsa approach recovertrue block pattern distinguish nonzero blocks zero ones correctlyhigh probability reasonable conditions fact jump selection block selectionconsistency results stronger matching corresponding signs correctlylarge probability choosing tuning parameters 1 2 properly reachwellbehaved ladflsa estimate recover true hidden signal vector randomnoises consistency results paper extend theoretical properties lsfsaharchaoui lvyleduc 2010 lsflsa rinaldo 2009 lad signalapproximation amplify study signal approximation linear regressionrandom error follow gaussian distribution furthermore demonstratenumber estimated nonzero blocks unbiased estimator degrees freedomladflsa existing model selection criteria extended ladflsachoosing tuning parametersrecent studies results proved penalty parameters satisfy conditions stated theorems clear penalty parameters selecteddatadriven procedures satisfy conditions numerical study shows satisfactory finitesample performance ladflsa particularly note tuning parameters selected based bic sufficient simulated data importantchallenging problem requires investigation scope currentpaper basic assumption required results random error terms 1independent observations y1 yn natural order model examplecopy number variation data based genetic markers ordered according chromosomallocations interesting study behavior ladflsa allowing certaindependence structure error termsappendixproof lemma 1w1n 2ni1 ww1n 2ni1 lsflsa ladlet wflsa estimates ith jump 5 3 karushkuhntucker kkt conditions5j 1n sgn wj 2n sgnww02 f 0zi f 0 w22n j 8f 0 zij 2 221n n2 jf 0 wj 8f 022n 2n2 21n 1 zi2 16f 0n22n 2n2 21nkkt equations 3j 1n sgn wj 2n sgnww0sgnyi wn2n 1n nfinally combining 16 17 a2 iii holds completes proof lemma 1proof lemma 2n 5definitioni1 zi f 0i 2 i1 zi f 00i 2i11n ni1 0i2n ni2 0i 0i1triangle inequality 18f 00i 2i1i12f 0 ni10i 1n ni10i 2n ni2 0i 0i11n 22n i12 f 0 i1rest proof similar proof proposition 2 harchaoui lvyleduc 2010rn defineg 2 f 0 0i 0 219n 0 2 gn 0 2f 00i 2 1n 22n nf 0n 0 2 1n 22n n gn belonglet sk collection kdimensional linear spacelemma 1 1 k n 20 n 0n 0 2 n pgn f 0n 1n 22n nk1nk p supsk g f 0n 1n 22n nnotice 0 varg 1 consequence cirelson ibragimovsudakovs 1976 inequalityp sup g e sup g z expz 2 2 constant z 0consider collection sk let ddimensional space 0 belongs1 d orthogonal basis2 f 0 ni1supsk g supnnn2 f 0 i1 dj1 aj jisupardn j1 aj j n2 f 0 j1 aj i1 jisupardj1 aj2 f 0 dj1 i1 jiobtained cauchyschwarz inequality a2 lemma1 exists m1 0 d m1 1n taking expectations sides232 12esupsk g 2 f 0e dj1 i1 ji2 122 f 0 dj1 e i1 jid m1 1n 12combining 22 24p sup g m1 1n 12 z expz 2 2let 0 c 1c f 0n 1n 22n n m1 1n 12choose positive z f 0n 1n 22n n m1 1n 12 25combining 21 25n 0 2 n n expn log n 12 f 0n 1n 22n n m1 1n 2n expn log n 121 c2 f 0n2n n nn 0 n n n expn log n 121 c2 f 0nn2lemma 2 holds furthermore n 2m2 n log nn12n 0 n 2m2 n log nn n n1m2 f 01c ncompletes proofproof theorem 1defineln n1 yi yi 0i 1n 2n i1mn nf 0 0i 2sgni 1n 2ni1n arg minlnn arg minmn define rni rni0i sgni 0i ni rni erni following gao huang 2010bverifyln mn ni n n 0n 2 sdn 0 o 0 2n 0 define sn 2 s 2 definen sup ln mnhn infd mn mnn 2n 2n1 f 0ni1mn mnn f 00i n1 ni1 sgnii1n 1n i1n 2n i2 i1mn 002n1 f 00i n1 sgni n1 2n sgni1 sgni1sides sums 29multiplyn 2n n1 1n ni1mn mnn f 0sgnsgnn 2n i2 sgni1i1i1n1 2n ni2 i1n 2nf 0sgnn1 2n ni2 sgni1i1i1n 2n i2 i1n 0hn n f 02n nconvex minimization theorem hjort pollard 1993n 2 nnpn n hn n 2p supsn n i1 ni f 0n 2n p supsn n f 0n 2nsuppose rn o1 n nn 0 rnn 0 2nlimn p supsn n 0 f 02n 2nn 0 2n f 02n 4nlimn p supsn 2rnn 2n f 02n 4nlimn p supsn 2rnn 0 2n f 02n 8nlimn p supsn rncombining 27 292 n p supsn ni1 ni n f 02n 2nn 0 2n f 02n 8n o1p supsn rnp sups 0ni1 ni nf 02n 2n p0 2nn 2n let ui s rademacher sequence 1 d orthogonal basisddimensional space 0 belongs contraction theorem ledouxtalagrand 1991 cauchyschwarz inequalitye sups 0 ni1 ni n 8ne sups 0 ni1 ui 0i8ne supard sups 0 dj1 aj i1 ui ji2 122 128ne supard sups 0 dj1 ajj1 i1 ui ji2 128n sups 0 ddj1 aj8n sups 0 d 0 216 n n np sups 0 ni1 ni n f 02n 2n e sups 0 ni1 ni n f 02n 2n32 n f 0n 8 n f 0nlet n n n combining 31 32n 2 n 2 pp 0 n n pnn 0 2 n 2n 2 n 2 pn 0 2 n 2n32n f 0n n 2pn 0 n n 232 n f 0n n 2n expn log n 1 c2 f 0nn2 832 lemma 2 choosing n 2c f 01 1n 22n m11n n12 theorem 2 holds furthermore let 1n 22n2c2 f0m3 n lognn12 m1 1n m3 11c f 0 n 8m3 n log nnn f 0n n o1 log nn 0 n n o1 log n 2n expn log n1 m3 1 c2 f 0proof corollary 1nn 0replace upper bound maximal dimension linear spacebelong jmax proof theorem 1 obtain consistency result corollary 1repeat proofproof theorem 2suppose vector j blocks b1 bj corresponding unique block partitionlet j intensity jth block 1 j j lemma a1 rinaldo 2009subdifferential total variation penalty2n sgnj1 j2n j j1 2n sgnj1 j sgnj j1 1 j j2n sgnj j1sgnx 1 0 1 x 0 0 0 respectively define c0jcj subdif0n scaled corresponding block sizes wordsferentials 332n sgnj1j0 b0jcj 2n sgnj1 j sgnj j1 bj 1 j j02n sgn 0 0 b0j j02n sgnj1 jbjcj 2n sgnj1 j sgnj j1 bj 1 jj j1bjj2n sgnn let bji block estimate staysestimatebji letbji bji size bji bji bji corresponding blockset size notations v section 2 b0ji bji1 j j0f ladfsa solutionkkt conditionsbjicjikbji sgnyk22nsgnykkbji0i satisfylet0i 2f 0b0ji 1 kb0 sgnk b0ji c0jihij 0j 0hi remainder term stochastically equicontinuity specificallyb0ji 12hi op 1 1 n0i sgnkki rki sgnkfacthi 2f 0b0ji0i kbjierki kbjidefine difference vector w w1 wn w1 1ki rki e rki k bjin 37sgnwi0 j 0 36 holdswi i1 2 n sgnwn ladfsa solution definesgnwi0 j 0r2n j 0 sgnwr2n holdssgnwi0sgnwr 22nr kb sgnykj 0j 039a39bsgnwi0 j 0 holdseasy verify sgnwwi0 wwi0 j 0sgnwn 37 40 39b use triangle inequality know r2n holdsplugmaxij 0 b0ji 1 kbjisgnk b0ji1 1 kbji1sgnk wi0maxij 0 b0 1hi b0hi1 w0 maxij 0 c0 c0ji1ji12f 0maxsgni sgni1hihi1 42nesgni sgni1 0varsgni sgni1 2 2 n2 il i2 ncovsgni1 sgni1 1 sgni2 sgni2 1 1 0 i1 i2 1suppose di independent copies n 0 2pi4 pmaxij 0 sgni sgni1 22npmaxij 0 di 22n2 exp422n log j0cslepians inequality second chernoffs boundpi4 o1 conditions b1 hold definexi 2f 0b0ji 1 sgnk 2f 0b0ji1 1kbjikbji1sgnk j 0exi 0 maxij 0 varxi 2f 0b0ji 1 consider independent copies xin 0 2f 0b0ji 1 j 0pi1 pmaxij 0 b0ji 1 kbjisgnk b0ji1 1 kbji1sgnk 2f 0an 3pmaxij 0 xi 3 2 exp2b0min f 2 0a2n 9 log j 0pi1 o1 conditions b2 holds maxij 0 c0ji c0ji1 22n b0minpi2 pmaxc0ji c0ji1 2f 0an 3 0furthermorepi5 pmaxhihi1 22n 038pi3 pmaxij 0 wi0 b0ji 1hi wi0 b0ji1 1hi1 2f 03pmaxij 0 bji hi bji1 hi1 2f 0b0min 12 3o141 42prc2n pi1 pi2 pi3 pi4 pi5 0 nproof theorem 3theorem 2 know a1 b1b3 hold ladflsa choose jumpsprobability 1 prove main results based true block partitionn ladflsa solutionkktbjcj 1nbj sgnj j 0kbj0 sgnyk j0 sgnykjcj 1nj 0bjkbjlet j j0 satisfyj j0 2f 0b0j 1 ibj0 sgni b0j c0j 1n b0j sgnj0hjj 0j k0j k0abuse notationhj remainder term stochastically equicontinunityb0j 12hj op 1 1 nfacthj 2f 0b0jj j0 ibj0 erij ibj0 ij rij sgnij j0 sgniij rij e rij bj0 1 j j0 1 sgnj sgnj0 j k0 n 44satisfies kktflsa ladflsa solution define eventk0 sgnrn r1n 2n kj sgnj0 j k0rn holdssgnj sgnj0j k0kbj0 sgnyk jbjcj 1nbj j k 0verify sgnj sgnj0 j k0 holds sgnj j0 j j0 j k044 46 rn holdsib0 sgni b0j c0j 1n b0j sgnj0hj 2f 0b0j j0 j k0ib0 sgni b0j c0jhj 1n b0jj k0prc pmaxjk0 ibj0 sgni 2f 0 minjk0 b0j minjk0 j0 4pmaxjk0 c0j 2f 0 minjk0 j0 4pmaxjk0 1n sgnj0 2f 0 minjk0 j0 4pmaxjk0hj b0j j0 f 02pmaxjk0 ibj0 sgni 1n minjk0 b0j 3pmaxjk0 c0j 1n 3pmaxjk0hj b0j 1n 3ps1 ps2 ps3 ps4 ps5 ps6 ps7let zj ibj0 sgni b0j ezj 0 varzj 1b0j zj s independentsubgaussian c3p s1 2k0 expb0min f 2 02n 8 o1verify p s2 o1 c4 p s3 o1 c5 p s6 o1 c2c1p s5 2j0 k0 expb0min 21n 32 o1furthermore p s7 o1 p s4 o1 48 p r 1n completes proofrest appendix presented prove theorem 4recall w jump coefficients vector wi i1 2 n1 j block coefficients factor proposition 3 rosset zhu 2007know following results ladflsa solutionlemma 41 0 exists set values 20 20 21 2m2 2m2 12k 1 k m2 uniquely defined set optimal solutionsw022k straight line rn 2 2k 2k1 solution w0constantii 2k 1 k m2 exists set values 10 10 11 1m2 1m2 11j 2k 1 j m1 uniquely defined set optimal solutions1 2k1j straight line rn 1 1j 1j1 solutionconstantlemma 4 define 2k 1 k m2 transition points w n02set y rn 2 transition point w jumps set j 0 2changes 2k s furthermore let 2 2k 1 k m2 define1j 1 j m1 ii transition points n1 2 set y rn1 transition point set nonzero blocks k1 2 changes 1j s2k s n1 2 finite collection hyperplanes rn lemma 4 know1 2 fixed 1 2 n divided setsgiven y rn n1 2ey1 2 ey1 2 ey1 2 1 n yi ji 0 ji 0 ji specifiesstaysblockey1 2 k1 2lemma 5 1 0 2 0 y rn n1 2 1 2 y continuous functiony ey1 2 locally constantproof lemma 5let l y denote functionl y yi 1 2 i1change lemma 4 y0 rn n1 2y rn n1 21 2 ym1 2 y0sequence ym ym y0 want prove1 2 ym1 2 y0equivalent prove1 2 y10 0 y11 2 y bounded need check converging subsequencey11 2 ymk1 2 y0 suppose1 2 ymkym ymk1 2 mk let y y l y l y hand1 2 y0 y01 2 y0 ymk1 2 y0 y0 ymk1 2 ymk ymk1 2 y0 y0 ymk1 2 ymk y01 2 ymk ymk y01 2 y0 y0 ymkhandymk ymk y01 2 y0 y0 ymk1 2 ymk yi01 2 ymkyimk1 2 y0 yimk1 2 y0yi02 yimk yi0 0 k1 2 y0l1 2 y0 y0 limk l1 2 ymk y0 l1 2 y0 y01 2 y0unique minimizer l y0 1 2 y0proof theorem 449 lemma 5 exists 0 y bally ey1 2 stays1 2 transitional pointji 1 2 yi 1 ey1 2ji 1 2 yi ey1 2ji 1 2 yi 0 ey1 2 overall ni11 2 y n n collection finite hyperplanes obtain1 21 2conclusion taking expectationreferencesboysen l kempe liebscher v munk ad wittich ol 2009 consistencies ratesconvergence jumppenalized squares estimators annals statistics 37 157183cirelson b s ibragimov sudakov v n 1976 norm gaussian sample function proceedings japanussr symposium probability theory springerlecture notes mathematics 550 2041 springer berlindonoho dl johnstone im 1995 adapting unknown smoothness waveletshrinkage journal american statistical association 90 12001224gao xl fang yx 2011 generalized degrees freedom l1 loss functionjournal statistical planning inference 141 677686gao xl huang j 2010a robust penalized method analysis noisy dna copynumber data bmc genomics 11517gao xl huang j 2010b asymptotic properities ladlasso highdimensionalsettings statistic sinica 20 14851506harchaoui z lvyleduc c 2010 multiple changepoint estimation total variationpenalty journal american statistical association 105 14801493hebiri m 2008 regularization smoothlasso procedure preprint laboratoireprobabilits et modles alatoireshjort n l pollard d 1993 asymptotics minimisers convex processes statisticalresearch report university oslokato k 2009 degrees freedom shrinkage estimation journal multivariateanalysis 100 13381352ledoux m talagrand m 1991 probability branch spaces isoperimetry processesspringer verlag new yorkli y zhu j2008 l1norm quantile regression journal computational graphicalstatistics 17 163185mammen e van geer s 1997 locally adaptive regression splines annals statistics 25 387413rinaldo 2009 properties refinements fused lasso annals statistics 37 29222952ronchetti e 1985 robust model selection regression statistics probability letters 32123snijders nowak n segraves r blackwood s brown n conroy j hamiltong hindle ak huey b kimura k law s myambo k palmer j ylstra b yuejp gray jw jain pinkel d albertson d 2001 assembly microarraysgenomewide measurement dna copy number nature genetics 29 263264schwarz ge 1978 estimating dimension model annals statistics 6 461464tibshirani r 1996 regression shrinkage selection lasso journal royalstatistical society series b 58 267288tibshirani r saunders m rosset s zhu j knight k 2005 sparsity smoothnessfused lasso journal royal statistical society series b 67 91108tibshirani r wang p 2008 spatial smoothing hot spot detection cgh datafused lasso biostatistics 91 1829wahba g 1990 spline models observational data philadelphia society industrialapplied mathematicsyao y au s t 1989 leastsquares estimation step function sankhya indianjournal statistics series 51 370381ye j 1998 measuring correcting effects data mining model selection journal american statistist association 93 120131yuan m lin y 2006 model selection estimation regression grouped variablesjournal royal statistical scociety series b 68 4967zhao p yu b 2006 model selection consistency lasso journal machinelearning research 7 25412563zou h hastie t tibshirani r 2007 degrees freedom lasso annalsstatistics 35 21732192figure 1 copy number data set gm 13330 bac cgh arraypanels outputs ladflsa lsflsa respectively observed data graydots estimates dark solid lines chromosome 14 plotted data differentchromosomes separated gray vertical linesfigure 2 estimated degrees freedom ladflsa combined 1 2chromosome 1 data gm 13330 bac arrayfigure 3 hypothetical model 500 monto carlo simulations 129 markers chromosome1 data gm 13330 bac array shows estimated number nonzero blocksk1 2 close true dm1 2 45 degree linefigure 4 example observed data grey true hidden signals black ladflsaestimates red 6 blocks 4 nonzero ones randomnoise s generateddouble exponential distributions center 0 scale 05 2table 1 simulation results section 61normaldouble expcauchymodelladflsalsflsaladflsalsflsaladflsalsflsaladflsalsflsaladflsalsflsaladflsalsflsaladflsalsflsaladflsalsflsaladflsalsflsalare1019700350098001600190013015400310077001600150013004802390028012000070029n 1000cfr62jump389177121321837821479732 559 075481356807410093 50000010093 50000088 22 7421541207421429734595090571257307810097 50000010097 5000008756612107174163753899705560863917 106736295925180469478630134lare017300210087000700170003012800210064000700130003002902750015013200030023n 5000cfr6821596225771009410094892510041621910090100898245876615396878749jump724133771485540725610745000005000007181357311435740865680875000005000006140955941143855907832058515170401014313note 1 lare absolute relative ratio defined 150 correctly plus additional false posivesnote 2 cfr6 ratio recoveringcorrectly fitted rationote 3 jump average number standard deviation number jumps