memory reduction ring abstraction gpurdma distributed quantum monte carlo solverpreprintarxiv210500027v2 csdc 13 2021weile weilouisiana state universitywwei9lsueduarghya chatterjeenersc berkeley labronnielblgoveduardo dazevedooak ridge national laboratorydazevedoefornlgovoscar hernandezoak ridge national laboratoryoscarornlgovkevin huckuniversity oregonkhuckcsuoregoneduhartmut kaiserlouisiana state universityhkaisercctlsuedu14 2021abstractscientific applications run leadership computing facilities face challengeunable fit leading science cases accelerator devices memory constraints memorybound applications work authors studied department energy missioncritical condensed matter physics application dynamical cluster approximation dcapaper discusses device memorybound challenges successfully reduced proposingeffective alltoall communication methoda ring communication algorithm implementationtakes advantage acceleration gpus remote direct memory access rdma fast dataexchange gpusadditionally ring algorithm optimized subring communicators multithreadedsupport reduce communication overhead expose concurrency respectivelycomputation communication analyzed autonomic performanceenvironment exascale apex profiling tool paper discusses performancetradeoff ring algorithm implementation memory analysis ring algorithm showsallocation size authors memoryintensive data structure gpureduced 1p original size p number gpus ring communicatorcommunication analysis suggests distributed quantum monte carlo execution time growslinearly subring size increases cost messages passing network interfaceconnector limiting factorkeywords dca quantum monte carlo gpu remote direct memory access memorybound issue exascalemachinesintroductiondynamical cluster approximation dca2 highperformance research software application 1 2 3 4provides modern c implementation solve quantum manybody problems dca implements quantumcluster method quantum monte carlo qmc kernel modeling strongly correlated electron systemsdca software currently uses different programming modelsmessage passing interface mpi computeunified device architecture cuda high performance parallex hpxc threadingtogetherarghya chatterjee contributed work previous appointment oak ridge national laboratory oakridge tn2 dca available httpsgithubcomcompfusedcapreprint 14 2021numerical librariesbasic linear algebra subprograms blas linear algebra package lapack matrixalgebra gpu magmato expose parallel computationqmc kernel 5 twoparticle greens function gt needed computing important fundamentalquantities critical temperature tc superconductivity words larger gt allows condensedmatter physicists explore larger complex higher fidelity physics cases dca currently stores gtsingle gpu device limits largest gt processed gpu new approachpartitioning large gt multiple gpus significantly increase scientists capabilities explore higherfidelity simulations paper focuses memorybound issue dca successfully addressedproposing effective alltoall communication methoda ring algorithmto update distributed gt devicearraycontributionsprimary contributions work outlined follows1 memory consumption qmc solver application reduced store larger gt arraymultigpus significant contribution enables physicists evaluate larger scientific problem sizescompute gt array single computation significantly increases accuracyfidelitysimulation certain material2 ring abstraction layer designed updates large distributed gt array ring algorithmimproved adding subring communicator multithreaded communication reducecommunication overhead expose concurrency respectively3 ring abstraction layer implemented nvidia gpudirect remote direct memory accessrdma fast device memory transfer4 autonomic performance environment exascale apex performance measurement libraryextended support use case driving tool development researchbackgroundqmc solver applications widely missioncritical department energysapplication landscape purpose paper authors chose use primary qmc applicationsdca code productionscale scientific problem runs fastest supercomputer summit oakridge leadership computing facility 4600 nodes node contains nvidia volta v100 gpus attainingpeak performance 735 pflops mixed precision implementation 5monte carlo simulations embarrassingly parallel authors exploited distributed systemstwolevel mpi threading parallelization scheme figure 1 dca highly optimizedscalable existing hardware effort focus solving memorybound issue described section1 advantage summits gpu rdma capabilityfigure 1 shows parallelism hierarchy iteration qmc solver mpi distribution onnode threadingparallelism example rank r0 rn assigned markov chain initial input particlegreens function gti t means twoparticle rank index rank spawns multiple independentworker threads walkers accumulators work computation performed gpu walkerthread generates measurement result gi array thread id performing nonuniform fourier transformimplemented matrixmatrix multiplication walker passes gi corresponding accumulator threadwords thread gi array rank k different gi arrays knumber walker threads rank accumulator thread updates gti formula eq 1 compute0 updated partial g 0 fed coarsegraining stepupdate ranklocal gti gtimeasurement end measurements mpireduce operation performed gt ranksproduce final complete gt root rank gt allocated measurements start lifespans end dca programmemorybound issue dcaresults balduzzi et al 5 storing gt accelerator device allows condensedmatter scientists explore larger complex higher fidelity physics cases problem size limiteddevice memory size updating device array gt timeconsuming memoryintensive processpreprint 14 2021gt0walker 0gt1walker 0gt2gt3accu 1walker2walker 1gt1accu 2gt2accu 1gt3accu 2mpi distributionaccu 0walker 2gt0accu 0walker 1onnode threadingmpi distributionfigure 1 workflow qmc dca solverdca computation distributed gt approach needed reduce memory allocation operationdeviceoriginal dca algorithm gt updated product smaller matrices singleparticle greens functiong computation update particleparticle channel accumulated according eq 1gt k1 k2 k3g k3 k2 k3 k1 g k2 k1ki combined index represents particular point momentum frequency space1 1 specifies electron spin value g singleparticle greens function describes configurationsingle electronsability handle larger gt allows simulations complex materials significantly increase details accuracyfidelity previous design kept gt gpu subslice gt computed singlecomputation simple singleorbital coarsegrained hubbard model physics insights prior knowledgedecide subslices gt contain important physics avoid generation gtsimple model allows generic behavior comes electronic corrections materials studieddistinguish different specific materials materialspecific modeling requires complex modelsinclude orbitaland otherdegrees freedom requires larger gt new distributedring implementation enables large gt array computed single computation complexmultiorbital models ensure important physics cases overlookedgpu rdma technologygpu rdma allows direct peer access multigpu memory highspeed network nvidia gpusgpudirect technology allows direct transfer data gpu device memory gpusnode nvlink2 interconnect andor gpus different nodes rdma supportbypass buffers host memorycudaaware mpi3 implementation directly pass gpu buffer pointer mpi calls acceleration supportgpudirect mpi library allows buffers sent kernel memory network3 httpsdevelopernvidiacomblogintroductioncudaawarempipreprint 14 2021staging host memory cudaaware mpi implementations openmpimvapich2 ibm spectrum mpi4implementation ring abstractiondistributed gt qmc solverintroducing communication phase ring abstraction layer important understandauthors distributed large device array gt mpi ranks original gt compared gtd versionsdistributed figure 2original gt implementation measurementswhich computed matrixmatrix multiplicationaredistributed statically independently mpi ranks avoid internode communications mpi rankkeeps partial copy gti accumulate measurements rank rank indexmeasurements finished reduction step taken accumulate gti mpi ranks final completegt root mpi rank size gti rank size final complete gtdistributed gtd implementation large device array gt evenly partitioned mpi ranksportion local mpi rank instead keeping partial copy gt rank keeps instanced accumulate measurements portion subslice final complete g notation dgtid size rank reduced 1pgt refers distributed version means ith rank gtisize final complete gt comparing configuration original gt implementation pdnumber mpi ranks example figure 2b ranks rank keeps gtionefourth size original gt array sized distributed g d implementation rank gcompute final complete gtiranks words rank broadcast locally generated gi remainder ranksmeasurement step efficiently perform alltoall broadcast ring abstraction layer built section32 circulates gi rankspipeline ring algorithmpipeline ring algorithm implemented broadcasts g array circularly measurementalgorithm algorithm 1 visualized figure 3algorithm 1 pipeline ring algorithmgenerategsigmagsigmabufupdateg4gsigmabuf0myrank worldrankringsize mpiw orldsizele f trank myrank 1 ringsize ringsizerightrank myrank 1 ringsize ringsizesendbufswapgsigmabufringsizempiirecvrecvbuf sourceleftrank tag recvtag recvrequestmpiisendsendbuf sourcerightrank tag sendtag sendrequestmpiwaitrecvrequestupdateg4recvbufmpiwaitsendrequestsendbufswaprecvbufend4 ibmspectrum mpi supported summit supercomputer cudaaware mpi implementationauthors paperpreprint 14 2021rank 1rank 0gt0rank 2gt1rank 3gt2gt3mpireducerank 0gt gtioriginal gt implementationgpu rdmarank 1rank 0gt0rank 2gt1gt2rank 3gt3mpireduce optionalrank 0gt0gt1gt2gt3b distributed gt implementationfigure 2 comparison original gt vs distributed gtd implementation rank contains gpu resourcepreprint 14 2021rank 1rank 0rank 2threadsendbuffsendrecvrecvbuffrank 3waitrecvwaitsendupdategt0sendbuffswaprecvbufffigure 3 workflow ring algorithm iterationstart new measurement singleparticle greens function g line 1 generatedd line 2 formula eq 1 lines 3 8 algorithm initializes indices leftupdate gtiright neighbors prepares sending message buffer previously generated g buffer processesorganized ring rank considered neighbors swap operationavoid unnecessary memory copies sendbuf preparation walkeraccumulator thread allocatesadditional recvbuf buffer size gsigmabuf hold incoming gsigmabuf buffer leftrankloop core pipeline ring algorithm iteration thread rank receivesg buffer left neighbor rank sends g buffer right neighbor rank synchronization stepd line 13line 12 performed afterward ensure rank receives new buffer update local gtisynchronization step follows ensure send requests finalized line 14 lastly swapoperation exchange content pointers sendbuf recvbuf avoid unnecessary memory copyprepare iteration communication multithreaded version section 322 thread indexcommunicates threads index neighbor ranks thread allocates buffers sendbuffrecvbuffloop terminated ringsize 1 steps time locally generated gi traveledd ranks eventually gmpi ranks updated gtireaches left neighbor birth rankexample g0 generated rank 0 end rank ring communicatord endadditionally gt large stored node optional accumulate gtimeasurements instead parallel write file taken321subring optimizationsubring optimization strategy proposed reduce message communication times large device arraygt fit fewer devices subring algorithm visualized figure 4ring algorithm section 32 size ring communicator mpiworldsize set sizeglobal mpicommworld size gt equally distributed mpi ranksd measurement gcomplete update gtitravel mpiworldsize ranks totalmpiworldsize numbers gi sent received concurrently measurement globald relatively small rank cause high commumpicommworld communicator size gtinication overheadgt distributed fitted fewer devices shorter travel distance required gi reducingcommunication overhead reduction step performed end measurements accumulate gtssi means ith rank sth subringpreprint 14 2021rank 0rank 1rank 2subring communicator 0rank 3rank 4rank 5subring communicator 1threadsendbuffsendrecvrecvbuffrank 6rank 7rank 8subring communicator 2figure 4 workflow subring algorithm iteration consecutive s rank forms subring communicatorcommunication occurs subring communicators measurements finished s number rankssubringbeginning mpi initialization global mpicommworld partitioned new subringcommunicators mpicommsplit new communicator information passed dcaconcurrency class substituting original global mpicommworld new communicatorminor modifications needed transform ring algorithm algorithm 1 subring algorithm 2line 4 myrank initialized subringrank instead worldrank subringrank rank index localsubring communicator line 5 ringsize initialized subringsize instead mpiworldsize subringsizesize new communicator general ring algorithm special case subring algorithmsubringsize general ring algorithm equal mpiworldsize subring groupmpi ranksalgorithm 2 modified ring algorithm support subring communicationmyrank subringrankringsize subringsize322multithreaded ring communicationadvantage multithreaded qmc model dca multithreaded ring communication supportimplemented ring algorithm figure 1 shows original dca method walkeraccumulator thread rank independent threads rank synchronizeranklocal measurements finished measurement walkeraccumulator threadgenerates threadprivate gi update gtmultithreaded ring algorithm allows concurrent message exchange threads ranklocalthread index exchange threadprivate gi conceptually k parallel independent rings knumber threads rank threads local thread id form closed ring example threadindex 0 rank 0 send g thread index 0 rank 1 receive g thread index 0rank ring algorithmchanges ring algorithm offsetting tag values recvtag sendtag thread indexvalue example lines 10 11 algorithm 1 modified algorithm 3algorithm 3 modified ring algorithm support multithreaded ringmpiirecvrecvbuf sourceleftrank tag recvtag threadid recvrequestmpiisendsendbuf sourcerightrank tag sendtag threadid sendrequestpreprint 14 2021125 gbs125 gbsgpugpunic16gbscpugpu16gbs64 gbscpugpugpugpunvlink50 gbspciegen4edr ibxbussmpfigure 5 architectural layout single node summitefficiently send receive g thread allocate additional recvbuff hold incoming gsigmabufbuffer leftrank perform sendreceive efficiently original dca method k numbersbuffers g size rank multithreaded ring method 2k numbers buffers g sizerank k number threads rankresultssection evaluates work perspectivesincluding correctness memory analysis scalingfunction activitieswith help apex profiling tool experiments run summitsummit node configurationsummit 4600 node 200 pflops ibm ac922 node consists ibm power9 cpus 512 gbddr4 ram nvidia v100 gpus total 96 gb highbandwidth memory summit node figure 5divided sockets socket ibm power9 cpu nvidia v100 gpus connectednvidias highspeed nvlink2 nvlink2 capable 25 gbs transfer rate directionibm power9 cpus summit node connected peripheral component interconnect express bus64 gbs bidirectional mellanox infiniband edr network interface connector nic attachedsummit node ports nic 125 gbs portapexapex 6 performance measurement library distributed asynchronous multitasking systems provideslightweight measurements perturbing high concurrency synchronous asynchronous interfacessupport performance measurement systems employ operating userlevel threading apex usesdependency chain addition stack produce traces task dependency graphs synchronousapex instrumentation application programming interface api add instrumentation given runtime includes support timers counters support c threads linux systems underlying posixthreads automatically instrumented preloaded shared object library intercepts wraps pthreadcalls application nvidia cuda profiling tools interface 7 provides cuda host callback deviceactivity measurements additionally hardware operating monitored asynchronouspreprint 14 2021table 1 comparison function differences original gt accumulated gtd runs5e7errorrealimaginary371e09174e18310e10419e18461e09216e18337e10389e18truetruemeasurement involves periodic ondemand interrogation operating hardware statesrun time states cpu use resident set size memory high water mark nvidia management libraryinterface 8 provides periodic cuda device monitoring apex work apex extended captureadditional timers counters related cuda devicetodevice memory transfers support key mpi callsprovided minimal implementation mpi profiling interface 9accuracy analysisverify implementation generates correct results input configuration run originalring algorithm methods differences original gt accumulated gtd arrays comparednormalized l1 loss function absolute deviations eq 2 normalized l2 loss function square errorseq 3 compute normalized error original gt accumulated gtd arraysentrywise norm used5 baseline l1error l2error arrays smaller5e7 dca testing protocoll1errorkvecgt gtd k 1kvecgt k 1l2errorkvecgt gtd k 2kvecgt k 2input configuration singleband hubbard model chosen standard model correlatedelectron systems studies cuprate hightemperature superconductorscluster size configured 36site 6x6 cluster stateoftheart simulations size 100000 montecarlo measurements chosen observe runtime performance ring algorithms runtime scaleslinearly number measurements constant number ranks cluster size configured66 fourpointfermionicfrequencies set 64 leads 212336640 entries gt gt entrydoubleprecision complex number gt memory size 34 gb configuration produce large gthit memorybound issues summit gpusin gpu 16 gbfor regular gt versionconfigurations run summit node times ranks node seven walkeraccumulatorthreads rank distributed gtd version ring size set subringrun results implementation generates correct results table 1 l1error l2erroraccumulated gtd acceptable rangememory analysismemory analysis results device memory required gtd decreases linearly size subringnumber mpi ranks subcommunicator fits ring algorithm apex profiling toolcollect memory allocation information time performance results reflect correctly ringalgorithm method gt evenly distributed mpi ranksin rank uses 1 gpuwithinsubring communicatorexample requested size cudamalloc api compared original gt figure 6a distributedgtd subring size figure 6b methods shows distributed gtd method produced timesmemory allocation original gt device array 7 s cases distributed gtd method allocatedd original g method allocated 340 gb g113 gb gti5 entrywisenorm defined httpsenwikipediaorgwikimatrixnormpreprint 14 202140037535032530027525022520017515012510007505002500010 s20 s30 s40 s50 s60 soriginal gt implementation40037535032530027525022520017515012510007505002500020 s40 s60 sb distributed gtd implementation subring sizefigure 6 cudamalloc requested size gb time visualized vampirscaling resultspipeline subring algorithm rank sends s 1 receives s 1 messages s size subring total number messages scales quadratically os 2 number messages crossingcommunication link increases linearly os figure 7 shows elapsed computation time 1400 measurementsrank subring algorithm running ranks summit node message 170 mbdata approximated linear leastsquare line indicates elapsed computation time increaseslinearly subring size increases suggests subring algorithm constrained total volumemessages restricted slowest communication link effective bandwidth subring algorithmestimatedeffective bandwidth 170 106 s 1 400 elapsed time6 gbs data s 60 10 nodes figure 7 effective bandwidth 50theoretical peak bandwidth nic 125 gbs port summit nodeauthors acknowledge enabling ring algorithm solve existing small problemsize single band hubbardmodel lower cluster size overkill communication overhead drastically increaseruntime authors propose ring algorithm gt fit single gpumemory original dca executed large problem size gt fit singlegpu program simply crashes failing allocate memory device scalability issuering algorithm core focus authors implementation optimization design strategiessubring algorithm subring optimization originally proposed ring algorithm potentiallyundesirably long period time finish run dca especially requesting thousands computenodes subring optimization scientists able run large science cases maintaining acceptablecommunication overheadcurrent subring size configured manually authors plan design runtime adaptivityoptimization automatically adapt optimal subring size optimization distribute gtminimal number devices preserves optimal runtime performance runtime adaptivitypreprint 14 2021data pointlinear fitexecution time seconds2500200015001000500number mpi ranks subring communicatorfigure 7 time 1400 iterations rank subring algorithm ranks summit node message size170 mb489 scpu thread 00cpu thread 50cpu thread 40cpu thread 70cpu thread 30cpu thread 20cpu thread 80cpu thread 60490 s491 s492 s493 smpiwaitmpiwaitmpiwaitmpiwait494 s495 s496 smpiwaitcumemcpyasyncmpiwait497 s498 smpiwaitmpiwaitmpiwaitcumemcpyasyncmpiwaitfigure 8 vampir timeline graph shows processes activities time rank 0 dca multithreaded ring algorithmhelpful dca iterative convergence algorithm gt size dynamically changedmultiple dca runs production science runs leadership computing facilitiesdiscussionconcurrency overlappingmultithreaded ring implementation provides sufficient concurrency overlaps communication computation apex profiling tool collect data process activities time visualize datavampirdca run multithreaded ring support obtained timeline activities rank 0 49 s figure 8concurrency overlap observed multithreaded ring algorithm threadsblocked mpiwait threads rank perform useful computation tasks example shortblocks labeled mpiwait related kernel activitiescurrent ring algorithm observed lockstep algorithm computation update gtstart previous communication step g message exchange finished expose currencyhpx 10a taskbased programming modelcould overlap communication computationexample dca kernel function wrapped hpx future represents ongoing computationasynchronous task communication tasks attached chained futurized kernel task wei etpreprint 14 202110 s20 s30 s40 s50 s60 soriginal gt method20 s40 s60 sb distributed gtd method subring sizefigure 9 device memory gb time seven walkeraccumulator thread visualized vampiral 4 reported dca hpx userlevel 11 threading support achieves 20 speedup originalc threading kernellevel faster context switching hpx threadingtradeoff concurrency memorywalkeraccumulator threads increase multithreaded ring algorithm gpu memory usage increaseddevice memory needed store extra threadprivate gi buffers cause new memorybound challenge concurrent threads possible solution reduce concurrent threadsachieve usable device memoryconfiguration run original gt distributed gtd versions seven threadsthread respectively figure 9comparison seven threads figures 9a 9b spike memory usage increase gtallocation second significant wave thread allocating gioriginal algorithm needs 34gb gt 96gb total new algorithm needs 13gb gtd 112 gbtotal nongt allocation original algorithm 62 gb distributed gtd method 99gb leadsoverhead 37 gb gtd version gi composed samesize matrices spin spin matrixmatrix sized 017 gb original algorithm total g allocation 01727 238 gb 2matrices gi 7 seven threads distributed gtd method total g allocation017237 714gb 3 allocations gi sendbuf recvbuf thread overhead overallg allocation ring algorithm 714 238 476 gb 1 gb nongt allocation37gb figure 9a significant reduction allocated memory 42nd second 1gb memorydeallocation g observe similar drop wave pattern figure 9b sendbufrecvbuf matrices dynamically allocated dip allocations hidden explains1gb differencepreprint 14 202110 s20 s30 s40 s50 soriginal gt method10 s20 s30 s40 s50 s60 s70 sb distributed gtd method subring sizefigure 10 device memory gb time walkeraccumulator thread visualized vampirthread figures 10a 10b maximum device usage distributed gtdversion 33 gb 19 gb original gt version 52 gb usable device memorygained concurrent walkeraccumulator threads reduced example saved device memoryreduced threads fit larger gt furthermore comparison experiment run summit noderanks node input configuration subring size measurement 4200 totalthreading numbers rank distributed gtd seven threads 87 s 13 times speedupthread 116 s result suggests insufficient device memory code usefewer threads loss 30 run time performance authors considering quantifyingmodeling tradeoff future research developmentsolve nic bottleneck issue new memorybound challenge caused multithreaded communicationstoring additional g authors considering plan g cpu host cpu hostmemory summit node contains 512 gb ddr4 memory use ibm power9 processors 66 16 gb 96 gb device memory summit nics connected cpudirectly connected gpu nvlink2 connection cpu gpu peak 50 gbs fastercompared nics peak bandwidth 125 gbs bottleneck possible future extensionconsider keeping gt cpu instead gpu device memory smaller subringsubring kept single nodeadditionally authors explored bidirectional ring implementation alternates ring communicationdirections threads extensive testing authors concluded bidirectional ring improvedperformance 13x acrossrack rack 18 compute nodes summit current unidirectionalring potential performance benefits bidirectional ring approach currentunidirectional ring reserving rack authors continue investigate coordination hardwarevendors address performance bidirectional ring implementationconclusionspaper presents authors successfully solved memorybound challenge dca allowphysicists explore significantly large science cases increase accuracy fidelity simulations certainmaterials effective alltoall communication methoda ring abstraction layerwas designed purpose6 summituser guide httpsdocsolcfornlgovsystemssummituserguidehtmlpreprint 14 2021distributed device array gt updated multigpus gt device array distributedmultigpus allocation size memoryintensive data structure gpu reduced 1poriginal size p number gpus ring communicator significant memory reductionlarger gt capability primary contribution work condensed matter scientists ableexplore larger science casescalculating 4point correlation function gt storage gt grows o l 3 f 3 l numbercluster sites f number frequencies new capability enable largescale simulations 36site6x6 cluster 64 frequencies 1 obtain accurate information 2 enable resolution longerwavelength correlations longer periodicity real space resolved smaller clusterssize grow fairly large depends memory leadership computing facilitiesprovide relevant science problems domain specialists like study range orders 10sof100sgigabits gt potentially opening research use host memory losingperformancering algorithm implementation takes advantage gpudirect rdma technology directly efficientlyexchange device memory optimization techniques improve ring algorithm performancesubring communicators multithreaded supports optimizations reduced communication overheadexpose concurrency respectively performance profiling tools improved apexallows kernel communication information captured indepth ring algorithm demonstratedeffectively reduce memory allocation needed gt device array gpu paper discussestradeoffs concurrency memory usage multithreaded ring algorithm nic bottleneck issuefuture authors plan explore hpx run time overlap computation communicationdca expose concurrency asynchronicityacknowledgementauthors like thank thomas maier ornl giovanni balduzzi eth zurich insightsoptimization phase dcawork supported scientific discovery advanced computing scidac program fundeddepartment energy office science advanced scientific computing research ascr basic energy sciencesbes division materials sciences engineering rapids scidac institute sciencedata subcontract 4000159855 ornl research resources oak ridge leadershipcomputing facility doe office science user facility supported contract deac0500or22725center computation technology louisiana state universityreferences1 m h hettler n tahvildarzadeh m jarrell t pruschke h r krishnamurthy nonlocal dynamicalcorrelations strongly interacting electron systems phys rev b 58r7475r7479 sep 19982 m h hettler m mukherjee m jarrell h r krishnamurthy dynamical cluster approximation nonlocaldynamics correlated electron systems phys rev b 611273912756 20003 thomas maier mark jarrell thomas pruschke matthias h hettler quantum cluster theories rev modphys 7710271080 oct 20054 weile wei arghya chatterjee kevin huck oscar hernandez hartmut kaiser performance analysisquantum monte carlo application multiple hardware architectures hpx runtime 20205 giovanni balduzzi arghya chatterjee ying wai li peter w doak urs haehner ed f dazevedo thomasmaier thomas schulthess accelerating dca dynamical cluster approximation scientific applicationsummit supercomputer 2019 28th international conference parallel architectures compilationtechniques pact pages 433444 seattle wa usa 2019 ieee6 kevin huck allan porterfield nick chaimov hartmut kaiser allen d malony thomas sterling robfowler autonomic performance environment exascale supercomputing frontiers innovations234966 20157 nvidia cuda profiling tools interface 2020 httpsdocsnvidiacomcudacuptiindexhtmlpreprint 14 20218 nvidianvidia management library nvml 2020httpsdevelopernvidiacomnvidiamanagementlibrarynvml9 marc snir steve otto steven husslederman david walker jack dongarra mpi complete referencemit press cambridge ma usa 199810 hartmut kaiser patrick diehl adrian s lemoine bryce adelstein lelbach parsa amini agustn berge johnbiddiscombe steven r brandt nikunj gupta thomas heller kevin huck zahra khatami alireza kheirkhahanauriane reverdell shahrzad shirzad mikael simberg bibek wagle weile wei tianyi zhang hpxc standard library parallelism concurrency journal open source software 5532352 202011 tianyi zhang shahrzad shirzad patrick diehl r tohid weile wei hartmut kaiser introductionhpxmp modern openmp implementation leveraging hpx asynchronous manytask proceedingsinternational workshop opencl pages 110 new york ny united states 2019 associationcomputing machinery