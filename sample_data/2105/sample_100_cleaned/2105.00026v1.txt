data augmentation high dimensional lowsample size setting geometrybasedvariational autoencoderarxiv210500026v1 statml 30 apr 2021clment chadebec elina thibeausutre ninon burgos stphanie allassonnire alzheimersdisease neuroimaging initiative australian imaging biomarkers lifestyle flagship studyageingabstractin paper propose new method perform data augmentation reliable way high dimensional lowsample size hdlss setting geometrybased variational autoencoder approach combines proper latent spacemodeling vae seen riemannian manifold new generation scheme produces meaningful samplesespecially context small data sets proposed method tested wide experimental study robustnessdata sets classifiers training samples size stressed validated medical imaging classification taskchallenging adni database small number 3d brain mris considered augmented proposed vaeframework case proposed method allows significant reliable gain classification metrics instancebalanced accuracy jumps 663 743 stateoftheart cnn classifier trained 50 mris cognitively normal cn50 alzheimer disease ad patients 777 863 trained 243 cn 210 ad improving greatlysensitivity specificity metricsntroductionven larger data sets availablelack labeled data remains tremendous issuefields application good examplehealthcare practitioners dealtime low sample sizes think small patientcohorts high dimensional data thinkneuroimaging data 3d volumes millionsvoxels unfortunately leads poor representation given population makes classical statisticalanalyses unreliable 1 2 remarkable performance algorithms heavily relying deep learningframework 3 extremely attractivepopular results strongly conditionednumber training samples models usuallyneed trained huge data sets prevent overfittingdata preparation article obtained alzheimersdisease neuroimaging initiative adni database httpadniloniusceduinvestigators adni contributed designimplementation adni andor provided data participateanalysis writing report complete listing adni investigatorshttpadniloniusceduwpcontentuploadshow applyadni acknowledgement listpdfdata preparation article obtained australianimaging biomarkers lifestyle flagship study ageing aibl fundedcommonwealth scientific industrial research organisation csiroavailable adni database httpadniloniusceduaibl researchers contributed data participate analysiswriting report aibl researchers listed wwwaiblcsiroauclment chadebec stphanie allassonnire universitparis inria centre recherche des cordeliers inserm sorbonneuniversit paris franceelina thibeausutre ninon burgos sorbonne universitinstitut du cerveau paris brain institute icm inserm u 1127 cnrsumr 7225 aphp hpital la piti salptrire inria aramisprojectteam paris francestatistically meaningful results 4way address issues perform data augmentation da 5 nutshell da art increasingsize given data set creating synthetic labeleddata instance easiest way imagesapply simple transformations additiongaussian noise cropping padding assignlabel initial image created onesaugmentation techniques revealed usefulremain strongly data dependent limited transformations uninformative inducebias instance think digit representing 6gives 9 rotated assessing relevance augmented data straightforward simple datasets reveals challenging complex datarequire intervention expert assessing degreerelevance proposed transformations additionlack data imbalanced data sets severely limitgeneralizability tend bias algorithmrepresented classes oversampling methodaims balancing number samples classupsampling minority classes synthetic minorityoversampling technique smote introduced6 consists interpolating data points belongingminority classes feature space approachextended works authorsproposed oversample close decision boundaryk nearest neighbor k nn algorithm 7support vector machine svm 8 insist samples potentially misclassified oversamplingmethods aiming increasing number samplesminority classes taking account difficultylearned proposed 9 10methods hardly scale highdimensional data 11 12recent rise performance generative modelsgenerative adversarial networks gan 13variational autoencoders vae 14 15attractive models perform da gans seen wide use fields application 1617 18 19 20 including medicine 21 instancegans magnetic resonance images mri 2223 computed tomography ct 24 25 xray 26 2728 positron emission tomography pet 29 mass spectroscopy data 30 dermoscopy 31 mammography 3233 demonstrated promising results nonethelessstudies involved large training set1000 training samples small dimensionaldata everyday medical applications remainschallenging gather large cohorts labeled patients consequence today case high dimensional data combined low sample size remainspoorly explored compared gans vaesseen marginal perform daspeech applications 34 35 36attempts use generative models medical dataclassification 37 38 segmentation tasks 3940 41 nonetheless noted main limitationwider use models timeproduce blurry fuzzy samples undesirable effectemphasized trained smallnumber samples makes hard usepractice perform da high dimensional lowsample size hdlss settingpaper argue vaes actuallydata augmentation reliable way contexthdlss data provided bring modelinglatent space amend way generate datapaper propose following contributionspropose combine proper modelinglatent space vae seen riemannianmanifold new geometryaware nonpriorbasedgeneration procedure consists samplinginverse riemannian metric volume element choice framework discussedmotivated compared vae modelspropose use framework perform dataaugmentation challenging context hdlssdata robustness augmentation methoddata sets classifiers changesreliance number training samplestested series experimentsvalidate proposed method reallifeclassification tasks complex 3d mri adniaibl databases augmentation methodallows significant gain classification metrics50 samples class consideredvariational autoencodersection quickly recall idea vaesproposed improvements relevant papermodel settinglet x x set data vae aims maximizinglikelihood given parametric model passumed exist latent variables z living lowerdimensional space z referred latent spacemarginal distribution data writtenp x p xzqzdzq prior distribution latent variablesacting regulation factor p xz timetaken simple parametrized distribution gaussianbernoulli distribution referreddecoder parameters usually given neuralnetworks integral eq 1 timeintractable posterior distributionp zx rp xzqzp xzqzdzmakes direct application bayesian inference impossible recourse approximation techniquesvariational inference 42 needed variational distribution q zx introduced aims approximatingtrue posterior distribution p zx 14 variationaldistribution referred encoder initialversion vae q taken multivariate gaussianparameters given neural networks importance sampling applied deriveunbiased estimate marginal distribution p xwant maximize eq 1p xp xzqzezq p p xq zxjensens inequality allows finding lower boundobjective function eq 1log p x log ezq pezq log pezq log p x z log q zx elboevidence lower bound elbo tractablep x z q zx known optimized respect encoder decoder parametersimproving model literature reviewrecent years attempts improve vae modelbriefly discuss main areasimprovement relevant paper section221 enhancing variational approximate distributionlooking eq 2 noticednonetheless trying optimize lower boundtrue objective function effortsfocused making lower bound tighter tighter 4344 45 46 47 48 way enhanceexpressiveness approximate posterior distributionq elbo expressionwritten followselbo log p x klq zxp zxexpression makes terms appearfunction want maximize secondkullbackleibler kl divergenceapproximate posterior distribution q zx trueposterior p zx term nonnegativeequals 0 q ptrying tweak approximate posterior distributioncloser true posteriorelbo tighter enhance modelmethod proposed 49 consisted adding k markovchain monte carlo mcmc sampling stepsapproximate posterior distribution targetingtrue posterior precisely idea startz0 q zx use parametrized forward resp reversekernels rzk1 zk x resp rzk zk1 x create newestimate true marginal distribution p xobjective parametrized invertible mappings fx callednormalizing flows instead proposed 50 sample zstarting random variable z0 drawn initial distribution q zx k normalizing flows appliedz0 resulting random variable zk fxk fx1 z0density writesq zk x q z0 xdet jfxk 1jfxk jacobian k th normalizing flowideally like access normalizing flowstargeting true posterior allowing enrichingdistribution improve lower boundparticular respect model inspired hamiltonianmonte carlo sampler 51 relying hamiltonian dynamics proposed 49 52 strengthmodel relies choice normalizing flowsguided gradient true posterior distribution222improving prior distributionenhancing approximate posterior distributionresulted major improvements modelargued prior distribution latent variablesplays crucial role 53 vanilla vaeuses standard gaussian distribution prior naturalimprovement consisted mixture gaussiansinstead 54 55 enhancedproposal variational mixture posterior vamp 56addition models trying change prior distribution relying hierarchical latent variablesproposed 43 57 58 prior learning promisingidea emerged 59 recently 6061 62 allows accessing complex prior distributionsapproach relying acceptreject sampling improve expressiveness prior distribution 63cited proposals improvedvae model choice prior distribution remainstricky strongly conditioned training datatractability elbo223adding geometrical consideration modelmean time papers arguinggeometrical aspects taken accountinstance ground vanilla vae fails apprehend data having latent space specific geometrylatent space modelings proposed hypershere 64 vonmises distributions consideredinstead gaussians poincare disk 65 66works trying introduce riemannian geometryvae framework proposed model input dataspace 67 68 latent space 69 70 7172 riemannian manifolds authors 73 wentbridged gap sec 221 combiningmcmc sampling riemannian metric learningmodel proposed latent spaceriemannian manifold instead learn parametrizedriemannian metric space idea riemannian metric learning attractive allows modelinglatent space desired recently reusedcombined prior learning 74t p roposed m ethodsection present proposed methodconsists combining proper latent space modelingnew nonprior based generation scheme arguevast majority works dealing vae generatenew data prior distribution standardprocedure suboptimal particularcontext small data sets believe choiceprior distribution strongly data set dependentconstrained simple elboeq 2 remains tractable view adoptedconsider vae dimensionality reduction toolable extract latent structure datalatent space modeled riemannian manifoldrd g d dimension manifold gassociated riemannian metric latent structure apriori far trivial propose paper relysetting introduced 73 riemannianmetric directly learned data goingrecall elements riemannian geometryelements riemannian geometryframework differential geometry defineriemannian manifold m smooth manifold endowedriemannian metric g smooth inner product g p hip tangent space tp m definedpoint manifold p m chartcoordinate chart u homeomorphism mappingopen set u manifold open set veuclidean space manifold called ddimensionmanifold chart atlasv rd exists neighborhood upoint p manifold u homeomorphicrd givenp u chart x1 xd inducesbasis x1 xtangent space tp mlocal representation metric riemannian manifoldchart u written positive definitematrix gp gij p0ijd h xxj ip 0ijdpoint p u v w tp m p uhuwip u gpw propose workambientlike manifold rd g exists global chartencodermetric networkdecoderfig 1 geometryaware vae framework neural networks highlighted colored arrows hriemannian normalizingflows riemannian hamiltonian equations represents parameters decoder gaussian bernoulligiven id following assumework coordinate g refermetrics matrix representation chartways apprehend manifolds extrinsic view assumes manifold embeddedhigher dimensional euclidean space think 2dimensional sphere s 2 embedded r3 intrinsic view adopted paperassumption manifold studiedunderlying structure example curves lengthinterpreted distance defined euclideanspace requires use metric definedmanifold length curve pointsmanifold z1 z2 m parametrized t 0 10 z1 1 z2 givenz1 qktkt dthttit dtcurves minimizing length called geodesicsdistance dist elements connected manifoldintroduced followsdistz1 z2 inf l0 z1 1 z2manifold m said geodesically completegeodesic curves extended r wordspoint p manifold draw straight linerespect defined distance indefinitelydirectionsettinglatent space seen riemannianmanifold rd g particular characterisedriemannian metric g choice importantattempts try riemannianstructure latent space vaes 70 71 72 7576 77 proposed metrics involved jacobiangenerator function hard use practiceconstrained generator network architectureconsequence instead decide rely ideariemannian metric learning 78321 metricdiscussed riemannian metric plays crucialrole modeling latent space paperdecide use parametric metric inspired 79 havingfollowing matrix representationg1 zkz c k22idli lexpn number observations li lower triangular matrices positive diagonal coefficients learneddata parametrized neural networks cireferred centroids correspond meanxi encoded distributions latent variableszi zi q zi xi n xi xi t temperaturescaling metric close centroids regularization factor scales metric tensor farlatent codes shape metric powerfulaccess closedform expression inversemetric tensor usually useful compute shortestpaths exponential map metricsmooth differentiablep allows scaling riemannian volume element det gz fardata easily regularization factorsimilar metric proposed 69input data space x learned dataable refer geodesics entire learned manifoldneed following proposition proved appendixsupplementary materialsproposition 1 riemannian manifold rd g geodesically complete322 modelmetric learned way proposed 73rely riemannian hamiltonian dynamics 8081 main idea encode input data pointsxi means xi posterior distributions associated encoded latent variables zi0n xi xi means updatemetric centroids ci mean time input data pointsxi fed neural network outputsmatrices li update metric updated metricsample zik zi0 waynormalizing flows 50 riemannian hamiltonianequations employed instead zik feddecoder network outputs parametersconditional distribution p xz reparametrization tricksample zi0 common riemannianhamiltonian equations deterministic backpropagationperformed update parameters schemegeometryaware vae model frameworkfig 1 following refer proposedmodel geometryaware vae rhvae shortimplementation pytorch 82 availablesupplementary materials323 sampling latent spacepaper propose amend standard samplingprocedure classic vaes better exploit riemannianstructure latent space geometryaware vaeseen tool able capture intrinsic latent structuredata propose exploit propertydirectly generation procedure differs greatlystandard fully probabilistic view priordistribution generate new data believeapproach remains far optimalconsiders small data sets depending choiceprior poorly prospect latent spacesample locations usable informationdiscussed illustrated sec 324 sec 33instead propose sample following distributions z det g1 zpz rs z det g1 zdz100circlesringsu z log ptarget zkv v venergies hamiltonian 84 85shoes100100 7510025 0075 100affinegeodesic100 7525 00affinegeodesic75 100100hz v u z kvs compact set integraldefined fortunately use parametrized metricgiven eq 4 inverse closed formpretty straightforward evaluate numeratoreq 5 classic mcmc sampling methodsemployed sample p rd paper proposeuse hamiltonian monte carlo hmc sampler 83gradient logdensity computable giventarget density ptarget want sample ideahmc sampler introduce random variablev n 0 id independent z rely hamiltoniandynamics analogous physical systems z seenposition v velocity particle potentialenergy u z kinetic energy kv givenaffinegeodesicevolution time particle governedhamiltons equations followsequations integrated discretizationscheme known stormerverlet leapfrog integratorrun l timesvt 2 vt z u ztzt zt vt 2vt vt 2 z u ztintegrator step size hmc samplerproduces markov chain z n aforementionedintegrator precisely given z0n current statechain initial velocity sampled v0 n 0 ideq 6 run l times z0n v0zln vlproposal nzl isthen accepted probabilityexphzl vlmin 1 exphzn v0 shown chainz n timereversible converges stationary distribution ptarget 51 84 86method ptarget given eq 4 eq 5fortunately hmc sampler allows samplingdensities known normalizing constant thanksacceptance ratio computation denominatorptarget needed hamiltonian followshz v u z kv log det g1 z v veasy compute difficulty leftcomputation gradient z u z needed leapfrogintegrator actually pretty straightforward1 instance z z kzk 2 maxi kci kaffinegeodesicfig 2 geodesic interpolations learned metric differentlatent spaces latent spaces log metric volume elementpresented gray scale second row resulting interpolationseuclidean metric riemannian metric row learnedmanifolds corresponding decoded samples decoded samples interpolation curveschain rule paper typical choice lsamplers parameters 001 005 l 10 15like mention recent workp77authors distribution qz 1 det gz1sample wasserstein gan 87 nonethelessframework metric remain different324 discussion sampling distributionwonder rationale usedistribution p defined eq 5 pdesignmetric metric volume element det gzscaled factor far encoded data pointschoosing relatively small imposes shortestpaths travel populated area latentspace latent codes metricvolume element seen way quantifyinformation contained specific location latentspace smaller volume element information access fig 2 illustrates aspectsrow presented learned latent spaceslog metric volume element displayedgray scale different data setscomposed 180 binary circles rings different diameters thicknesses second composed160 samples extracted fashionmnist data set 88geometryaware vae n 0 idvae vamp priorvanilla vaecirclesringssamplescirclesringssamplescirclesringssamples100circlesringssamplesgeometryaware vae10010 8 6 4 2100fig 3 vae sampling comparison learned latent space means xi latent code distributions colored dotscrosses 100 latent space samples blue dots prior distribution proposed scheme geometryaware vaeslog metric volume element presented gray scale background 100 corresponding decoded samples data spacemeans xi distributions associatedlatent variables presented crosses dotsclass expected metric volume elementsmaller close latent variables small sconsidered 103 resp 101 common way studylearned riemannian manifold consists finding geodesiccurves shortest paths respect learnedriemannian metric second row fig 2compare types interpolation latent spaceexperiment pick points latent spaceperform linear geodesic interpolationriemannian metric row illustratesdecoded samples interpolation curvedisplays decoded samples accordinglatent space location corresponding codesoutcome experiment expected geodesiccurves travel codes explore areaslatent space information linear interpolations decoding geodesic curvesproduces far better meaningful interpolationsinput data space cases clearlystarting sample progressively distortedpath reaches ending point allows instanceinterpolating shoes intrinsictopology data path decodedsample interpolation curve looks like shoeimpossible euclidean metric shortestpaths straight lines travel areasinstance affine interpolation travelsareas latent data produces decodedsamples mainly superposition samplesred lines corresponding decoded samples framedred crosses areas codes belongingclass blue line corresponding blue framespoint supported plotsrow fig 2 clearly locationshighest metric volume element relevantstudy demonstrates informationlatent space contained codeswant generate new samples looklike input dataneed sample electeddistribution eq 5generation comparisonsection propose compare new generationprocedure priorbased methods contextlow sample size data sets331qualitative comparisonvalidate proposed generation methodhandmade synthetic data set composed 180 binary circles rings different diameters thicknessesappendix c train 1 vanilla vae 2 vaevamp prior 56 3 geometryaware vaeprior generate 4 geometryaware vaeproposed generation scheme compare generatedsamples model trained elboimprove 20 epochs relevant parameter settingavailable appendix b fig 3 comparesampling obtained vanilla vae left columnvae vamp prior 2nd column geometryaware vaestandard normal distribution prior 3rd columngeometryaware vae proposed samplingmethod sampling inverse metric volumeelement row presents learned latent spacesmeans encoded training data pointsclass crosses dots 100 samples issuedgeneration methods blue dotsrhvae modelslog metric volume element det g displayedgray scale background row showsresulting 100 decoded samples data spacetable 1gantrain higher better gantest closer baseline better scores benchmark densenet model trainedindependent runs generated data sg resp real train set strain tested real test set stest resp sg computegantrain resp gantest score 1000 synthetic samples class considered sg matches size stestreduced mnistbalancedmetricgantrain gantestbaseline906 12vae n 0 id834 24 671 49vamp841 30 749 43rhvae n 0 id 820 29 631 41901 14 881 27reduced mnistunbalancedgantrain gantest828 07747 32 528 106285 89 614 70693 18 469 84862 18 838 40outcome experiment samplingprior distribution leads poor latentspace prospecting drawback illustratedstandard gaussian distribution samplelatent space 1st 3rd column 1strow prior distribution having higher mass closezero insist latent samples close originunfortunately case latent codes close originbelong single class ringsnumber training samples roughlycircles rings end model overgeneratingsamples belonging certain class ringsspecific type data class undesirableeffect tenfolded considering geometrybased vae model adding mcmc steps trainingprocess explained fig 1 tends stretch latentspace nonetheless noted multimodalprior vamp prior mitigates allowsbetter prospecting model remainshard fit trained small data sets 56limitation priorbased generation methods reliesinability assess given sample qualitysample areas latent space containing littleinformation conduct generated samplesmeaningless appears striking smalldata sets considered interesting observationnoted 75 neural networks tendinterpolate poorly unseen locations fartraining data points looking decoded latentsamples row fig 3 eventually endconclusion actually appears networksinterpolate linearly training data pointscase illustrated instance reddots latent spaces fig 3 correspondingdecoded sample framed red sample locatedclasses decoded producesimage mainly corresponding superposition samplesbelonging different classes aspect supportedobservations discussing relevancegeodesic interpolations fig 2 sec 324drawbacks conduct poor representation actual data set diversity presentingirrelevant samples obviously notion irrelevancedisputable objective represent givenset data expect generated samples closetraining data having specificities enrichimpressively sampling inverse metric volreduced emnistgantrain845 13753 14432 44736 41826 13gantest545 65581 77556 50760 40ume element proposed sec 323 allows farmeaningful sample generation furthermore new sampling scheme avoids regions latent codecontain poor information focuses areasdecoded sample visually satisfyingsimilar effects observed reduced emnist 89 reducedmnist 90 reduced fashionmnist data sets higherdimensional latent spaces dimension 10 samplestime degraded classic generationemployed new allows generationdiverse sharper samples appendix c finallyproposed method overfit training datasamples located centroidsquantitative metrics following section supportpoint332 quantitative comparisonorder compare quantitatively diversity relevance samples generated generative modelmeasures proposed 91 92 93 94metrics suffer drawbacks 95 96decide use gantrain gantest measure discussed 95 appears suited measureability generative model perform data augmentationmetrics consist comparing accuracybenchmark classifier trained set generated data sgtested set real images stest gantrain trainedoriginal train set strain real images traingenerative model tested sg gantestaccuracies compared baseline accuracy givenclassifier trained strain tested stestmetrics interesting applicationgantrain measures qualitydiversity generated samples higher bettersecond gantest accounts generativemodels tendency overfit score significantly higherbaseline accuracy means overfitting ideallycloser baseline gantest score betterstick low sample size setting computescores data sets created downsampling wellknown databases data set created extracting500 samples 10 classes mnist ensuring balancedclasses second 500 samples mnistdatabase considered random split appliedclasses underrepresentedconsists selecting 500 samples 10 classesemnist data set having lowercase uppercasedata augmentation e valuation r obustnesssection relevance proposedimprovements perform data augmentation hdlsssetting series experimentsdatavaemodelsyntheticcnn modelcnn modeldatatrainingtrainedvalidationtestfig 4 overview data augmentation procedure input dataset divided train set baseline validation set testset train set augmented vae framework generateddata added baseline train benchmark classifierrobustness method data sets testedstandard benchmark classifier methods reliability common classifiers stressed finallyscalability larger data sets discussed421 materialsdata set created selecting 500 samplesclasses mnist data set ensuring balanced classesrefer reduced mnist second consistsselecting 500 samples mnist databaseapplying random split classes overrepresented reduced unbalanced mnist dataset create fashionmnistdata set classes hard distinguishtshirt dress shirt data set composed 300samples ensuring balanced classes referredreduced fashion finally select 500 samplesclasses emnist classes selectedcomposed lowercase uppercasecharacters end small databasestrong variability classes balance matchesinitial data set merge summary builtdata sets having different class numbers class splitssample sizes data sets divided80 allocated training referred baseline20 validation original data sets hugedecide use test set provided original databases1000 samples class mnist fashionprovides statistically meaningful results allowing reliable assessment models generalizationpower unseen datasettingsetting employ data augmentation consistsselecting data set splitting train setbaseline validation set test set baselineaugmented proposed vae framework generation procedure generated samples finally addedoriginal train set baseline fed classifierdata augmentation procedure illustratedfig 4 convolutional neural network cnn modelclassifiertraininputletters data sets divided baselinetrain set strain 80 validation set sval 20classifier training initial databases hugeuse original test set stest providesstatistically meaningful results generative modelssec 331 trained class straingenerate 1000 samples class sg createdvae gathering generated samples benchmarkclassifier chosen densenet 97 1 trainedstrain tested stest baseline 2 trained sgtested stest gantrain 3 trained straintested sg gantest loss improve50 epochs sval experiment modeltrained times report mean scoreassociated standard deviation table 1 expectedproposed method allows producing samples farmeaningful relevant particular perform daillustrated gantrain scoresclose accuracy obtained baselinehigher mnist unbalanced table 1 factable enhance classifiers accuracytrained synthetic data encouragingfirstly proves created samples closereal ones able capture truedistribution data secondly showsoverfit initial training data able addrelevant information synthetic samplesobservation supported gantest scoresproposed method close accuraciesachieved baseline case overfitting gantestscore expected significantly higher baselineclassifier tested generated samplestrained real data traingenerative model having score close baselineillustrates generative model able capturedistribution data memorize 95toy data setsproposed vae framework perform dadownsampled wellknown databasestens real training samples class consideredstick low sample size setting422 robustness data setsexperiment conduct consists assessingmethods robustness aforementioned datasets study propose consider densenet 97model2 benchmark classifier hand training data baseline augmented factor 5 1015 classic data augmentation methods random noiserandom crop rotation proposed methodcompared classic simple augmentationtechniques hand protocol describedfig 4 employed vanilla vae geometryawarevae generative models trained individuallyclass baseline elbo improve20 epochs vaes produce 200 5002 pytorch implementation provided 98table 2data augmentation densenet model benchmark meanaccuracy standard deviation independent runsreported rows aug correspond basictransformations noise crop gray cellsaccuracy higher synthetic data baseline rawdata test set proposed entire original data set1000 samples class mnist providesstatistically meaningful results allows good assessmentmodels generalization powermnistmnistunbalemnistunbal899 06815 07826 14baseline syntheticaug x5928 04865 09856 13aug x10882 22820 24857 03aug x15928 07858 34866 08vae200885 09840 20817 30vae500904 13873 12834 16vae1k912 10860 25843 16vae2k922 16880 22860 02rhvae200899 05823 09830 13rhvae500909 11840 32844 12rhvae1k917 08847 18847 24rhvae2k927 14868 10849 21ours200910 10841 20851 11923 11877 09851 11ours500ours1k932 08897 08870 10ours2k943 08891 19876 08syntheticvae200699 15646 18657 26vae500723 42694 41673 24vae1k834 24747 32753 14vae2k865 22796 38788 30rhvae200760 18615 29598 26rhvae500800 22668 33669 40rhvae1k820 29693 18736 41rhvae2k852 39773 32686 23ours200872 11795 16770 16ours500891 13804 21802 20ours1k901 14862 18826 13ours2k926 11875 13860 10standard normal prior generatebaselinefashion760 15775 20792 06800 05786 04787 03776 21793 11776 13780 13793 16790 14770 08785 09802 08781 18739 30714 85714 61767 16728 36743 26760 41743 31770 08785 08793 06783 091000 2000 new synthetic samples classclassic generation scheme sampling priorn 0 id proposed generation procedure referredfinally benchmark densenet model trainedindependent runs 1 baseline 2augmented data classic augmentation methods 3augmented data vaes 4 synthetic datacreated generative models experimentmean accuracy associated standard deviationruns reported table 2 early stoppingstrategy employed cnn training stoppedloss improve validation set 50 epochsoutcome study expectedgenerating synthetic samples proposed methodenhance relevance comparedmodels particular data augmentation tasksinstance illustrated second section table 2synthetic samples added baselineadding samples generated vae rhvaeprior distribution improve classifieraccuracy compared baseline gain remainslimited struggles exceed gain reachedclassic augmentation methods instance vaerhvae allows classifier achieve better scorereduced mnist reduced emnist data setscontrary proposed generation method able produce useful samples cnn model addinggenerated data baseline allows great gainmodel accuracy exceeds achievedmethod keeping relatively low standarddeviation data set highlighted boldsecondly relevance samples producedproposed scheme supported sectiontable 2 classifier trainedsynthetic samples generated vaessmall number generated samples 200 classclassifier able reach accuracy achievedbaseline instance cnn trained reducedmnist 200 synthetic samples class generatedmethod able achieve accuracy 872vs 899 baseline comparison vaerhvae fail produce meaningful samplesprior loss 15 20 points accuracyobserved combined potentially strong lossconfidence making samples unreliable factclassifier performs synthetic databaseline good news shows proposedframework able produce samples accountingoriginal data set diversity small numbergenerated samples interesting numbersynthetic data increases classifier able performbetter synthetic data baselinegain 3 6 points accuracy observedstrengthens observations sec 331sec 332 noted proposed methodable enrich initial data set relevant realisticsamplesfinally seen experiment geometric data augmentation methods questionableremain data set dependent example augmentingbaseline factor 10 add flips rotationsoriginal data significant effectreduced mnist data sets improves resultsreduced emnist fashionmnistexpert knowledge comes play assess relevancetransformations applied data fortunatelymethod propose require knowledgeappears robust data set changes423robustness classifiersaddition assessing robustness methoddata sets changes propose evaluate reliability classifiers consider differentcommon supervised classifiers multi layer perceptronmlp 3 random forest 99 k nn algorithmsvm 100 aforementioned classifierstrained 1 original training data set baseline 2 augmented data proposed method3 synthetic data generated methodindependent runs data setspresented sec 421 finally report mean accuracystandard deviation runs classifierdata set results balanced resp unbalancedreduced mnist data set fig 5a respfig 5b metrics obtained data setsavailable appendix d reflect tendency100100baselineaugmented 200augmented 1000synthetic 500augmented 2000synthetic 1000augmented 500synthetic 200synthetic 2000accuracyaccuracybaselineaugmented 200augmented 500augmented 1000augmented 2000synthetic 200synthetic 500synthetic 1000synthetic 2000mlpsvmknnrandom forestmlpsvmreduced mnist balancedknnrandom forestb reduced mnist unbalancedfig 5 evolution accuracy benchmark classifiers reduced balanced mnist left reduced unbalanced mnist data sets rightstochastic classifiers trained independent runs report mean accuracy standard deviation test set424 note method scalabilityfinally quickly discuss method scalabilitylarger data sets consider mnist dataset benchmark classifier taken densenetperforms data downsampleoriginal mnist database order progressively decreasenumber samples class start creatingdata set having 1000 samples class finally reach 20samples class created data set allocate 80training baseline reserve 20 validationset geometryaware vae trained classbaseline elbo improve 50 epochsgenerate synthetic samples 125 baselinebenchmark cnn trained independent runs1 baseline 2 augmented data 3synthetic data generated model evolution mean accuracy original test set 1000samples class according number samplesclass presented fig 6 outcomeexperiment fewer samples training setuseful method appears proposedaugmentation framework allows gain90 points cnn accuracy 20 samplesclass considered words numbersamples increases marginal gain decreasereduction perspectivecommonly acknowledged resultsbaseline increase closer perfect scorechallenging improve scoreaugmented data experiment nonethelessable improve model accuracyachieves high score instance 500 samplesclass augmentation method allows increasing100958949931928975967988984977989982985956931accuracyillustrated fig 5 method appears robust classifier changes allows improvingmodels accuracy significantly classifiersaccuracy achieved baseline presentedleftmost bar fig 5 classifier methodsstrength striking unbalanced data setsconsidered method able produce meaningful samples small number trainingdata able oversample minority classesreliable way observed previoussections synthetic samples helpful enhanceclassifiers generalization power perform bettertrained synthetic data baselinecases875876892baselineaugmented x125synthetic784100200number samples class5001000fig 6 evolution accuracy benchmark cnn classifier according number samples class train setbaseline mnist vae trained class baselineaugment size factor 125 cnn trained 5 times1 baseline blue 2 augmented baseline orange 3synthetic data green curves mean accuracyassociated standard deviation original test setmodel accuracy 977 988 finally datasets fewer 500 samples class classifierable outperform baseline trainedsynthetic data shows stronggeneralization power proposed method allowscreating new relevant data classifiervalidation m edical magingseries experiments assess validitydata augmentation framework binary classificationtask consisting differentiating alzheimers disease adpatients cognitively normal cn subjects basedt1weighted t1w mr images human brainstask performed cnn trained1 real images 2 synthetic samples 3section label definition preprocessing quality check datasplit cnn training evaluation clinica3 clinicadl4 opensource software packagesneuroimaging processingdata augmentation literature ad vs cn taskstudies use cnns differentiate adcn subjects anatomical mri 1013 httpsgithubcomaramislabclinica4 httpsgithubcomaramislabaddlfig 7 example true patients compared generated method intruders answers appendix fmetaanalysis use data augmentationtask results involving da nonethelesscited presented table 4 assessingreal impact data augmentation performancemodel remains challenging instanceillustrated works 102 103examples da led significantlydifferent results similar frameworkstudies interestingly shown table 4 studiesda task relied simple affinepixellevel transformations reveal data dependent note complex da actually performedad vs cn classification tasks pet images petfrequent mri neuroimaging data sets 104noted previous sections method applypretty straightforwardly modality mritechniques transfer learning 105 weaksupervision 106 preferred handle smallsamples data sets coupled daimprove network performancetable 3summary participant demographics minimental state examinationmmse global clinical dementia rating cdr scores baselinematerialsdata section obtainedalzheimers disease neuroimaging initiative adnidatabase adniloniuscedu australian imagingbiomarkers lifestyle aibl study aiblcsiroauadni launched 2003 publicprivate partnership led principal investigator michael w weinermd primary goal adni testserial mri pet biological markers clinicalneuropsychological assessment combined measureprogression mild cognitive impairment earlyad uptodate information wwwadniinfoorgadni data set composed cohorts adni1 adnigo adni2 adni3 data collection adni3ended data set contains imagesmetadata available 6 2019similarly adni aibl data set seeks discoverbiomarkers cognitive characteristics healthlifestyle factors determine development adcohort longitudinal diagnosis given according series clinical tests 110 data collectioncohortdiagnoses considered classification taskdata set label obs agesex mf mmsecdr403 733 60 185218 291 11 0 403adni05 169 1 192362 749 79 202160 231 212 10 406 05 22429 730 62 183246 288 121 1aibl05 31 1 36744 80 3343206 552 7 3 2cn baseline session participants diagnosed cognitively normal baseline stayedstable followupad baseline session participants diagnosed demented baseline stayed stablefollowuptable 3 summarizes demographics minimentalstate examination mmse global clinical dementiarating cdr scores baseline participants includeddata set mmse cdr scores classicaltable 4accuracy obtained studies performing ad vs cn classification cnns applied t1w mri data augmentationaccuracystudymethodsparticipants images baseline augmentedvalliani soni 2017 107rotation flip shift417417788813backstrom et al 2018 108flip3401198901cheng liu 2017 109 shift sampling rotation193193855aderghal et al 2017 102shift blur flip720720828837aderghal et al 2018 103shift blur720720900clinical scores assess dementia mmse scoremaximal value 30 cognitively normal personsdecreases symptoms detected cdr scoreminimal value 0 cognitively normal personsincreases symptoms detected551 hyperparameter choicesvae architecture cnn dependssize input architecture inputsize downsampled images highresolution imagesfig 8 different paradigmschoose architecture reuse architecture101 architecture obtained optimizingmanually networks adni data settask ad vs cn slight adaption downsampled images consists resizing numbernodes fullyconnected layers ratioinput output feature maps layersdenote architectures baseline secondlylaunch random search 117 allows exploring differenthyperperameter values hyperparameters exploredarchitecture number convolutional blocksfilters layer convolutional layersblock number fullyconnected layers dropoutrate hyperparameters learning rateweight decay search 100 different random architectures trained 5fold crossvalidation trainfull input choosearchitecture obtained best mean balanced accuracyvalidation sets crossvalidation denotearchitectures optimizedpreprocessing t1weighted mristeps performed section correspond procedure followed 101 listedraw data converted bids standard 111bias field correction applied n4itk 112t1w images linearly registered mnistandard space 113 114 ants 115cropped produced images size169208179 1 mm3 isotropic voxelsautomatic quality check performedopensource pretrained network 116 imagespassed quality checknifti files converted tensor formatoptional images downsampled trilinear interpolation leading image size8410489intensity rescaling minimum maximum values image performedsteps lead 1 downsampled images 84104892 highresolution images 169208179evaluation procedureadni data set split sets training validationtest test set created 100 randomlychosen participants diagnostic label 100 cn100 ad rest data set split training80 validation 20 sets ensure agesex site distributions setssignificantly differentsmaller training set denoted train50 extractedobtained training set denoted trainfullset comprises 50 images diagnostic label instead243 cn 210 ad trainfull ensure agesex distributions train50 trainfull significantly different site distribution50 sites adni data setrepresented smaller training setaibl data training hyperparametertuning independent test setcnn classifierscnn takes input image outputs vectorsize c corresponding number labels existingdata set prediction cnn given imagecorresponds class highest probabilityoutput vector552 network trainingweights convolutional fullyconnected layersinitialized described 118 correspondsdefault initialization method pytorch networkstrained 100 epochs baseline 50 epochsoptimized training validation losses computedcrossentropy loss experiment finalmodel obtained highest validation balanced accuracy training balanced accuracymodel evaluated end epochexperimental protocolprevious sections perform typesexperiments train model 1 real images2 synthetic data 3 synthetic real images current implementation augmentationfig 8 diagrams network architectures classification baseline architecture a1 101 seconda2 similar adapted process smaller inputs optimized architectures b1 b2 obtained independentlydifferent random searches convolution layers specify number channels kernel size fullyconnected layers specifynumber input nodes number output nodes fullyconnected layer followed leakyrelu activationdropout layer dropout rate specifiedhighresolution images possible imagesassess baseline performance cnnmaximum information available seriesexperiments training set train50trainfull cnn vae share trainingset vae use validation settraining training set vaes trainedad label cn labelexamples real generated ad images shownfig 7 experiment 20 runs cnn traininglaunched use smaller training set train50 allowsmimicking behavior framework smaller datasets frequent medical domainresultsresults presented table 5 resp table 6 obtainedbaseline resp optimized hyperparameterstrainfull train50 data set scores syntheticimages given appendix g experimentsdownsampled images highresolution specifiedvae augmentation performeddownsampled images classification performancegood best baseline performancegreatly exceedtrain50 baseline hyperparameters increase balanced accuracy 62 points adni89 points aibltrainfull baseline hyperparameters increase balanced accuracy 57 points adni47 aibltrain50 optimized hyperparameters increase balanced accuracy 25 points adni63 points aibltrainfull optimized hyperparametersincrease balanced accuracy 15 point adni01 point aiblperformance increase thanks da higherbaseline hyperparameters optimized ones possible explanationoptimized network close maximum performance reached setupimproved da hyperparametersvae subject similar searchplaces disadvantage hyperparametersperformance gain higher train50 trainfullsupports results obtained previous sectionfig 6baseline balanced accuracy baseline hyperparameters trainfull 806 adni 804 aiblsimilar results 101 da improvebalanced accuracy 863 adni 851 aiblperformance similar result autoencoderpretraining long compute longitudinal data 1830 cn 1106 ad images insteadbaseline data 243 cn 210 ad imagestable rows display baselineperformance obtained real images expectedtraining highresolution images leads better performance training downsampled imagescase optimized network train50obtained balanced accuracy 721 adni 712aibl highresolution images versus 755 adni756 aibl downsampled imagesexplained fact hyperparameters choicetrainfull guaranteelead similar results fewer data samplesd iscussioncontrary techniques specific field application method produced relevant data diversedata sets including 2d natural images mnist emnistfashion 3d medical images adni aiblnote networks learning medicaltable 5mean test performance series 20 runs trained baseline hyperparametersadnitrainingsettrain50trainfulldata setsensitivityspecificityrealreal highresolution500 synthetic real1000 synthetic real2000 synthetic real3000 synthetic real5000 synthetic real10000 synthetic realrealreal highresolution500 synthetic real1000 synthetic real2000 synthetic real3000 synthetic real5000 synthetic real10000 synthetic real703 122785 94719 53698 66722 44718 49747 53747 70791 62845 38825 34846 44854 40847 36846 42842 28624 115574 88670 45712 37703 43734 55735 48734 61763 42767 40819 54843 51864 59868 45869 36885 29aiblbalancedaccuracy663 24679 23694 16705 21712 16726 16741 22740 27777 25806 11822 24844 18859 16858 17857 21863 18sensitivityspecificity607 137572 112559 68591 90666 71661 93717 100691 99706 67716 64760 63770 70772 69772 48769 52791 47738 72758 70811 31821 37790 41811 50805 44807 51863 36892 27897 33904 34904 38917 29914 30910 26balancedaccuracy672 41665 30685 25706 31728 22736 30761 36749 32784 24804 26829 25837 23838 22844 18842 22851 19table 6mean test performance series 20 runs trained optimized hyperparametersadnitrainingsettrain50trainfullaiblimage typesensitivityspecificityrealreal highresolution500 synthetic real1000 synthetic real2000 synthetic real3000 synthetic real5000 synthetic real10000 synthetic realrealreal highresolution500 synthetic real1000 synthetic real2000 synthetic real3000 synthetic real5000 synthetic real10000 synthetic real754 50736 62732 42761 53752 38765 38771 37778 46825 42826 45823 23825 33831 42813 37819 35822 34755 53706 59780 33795 29786 44792 42767 41782 49885 66889 63898 27905 41913 32904 34909 25912 36images adni gave similar balanced accuraciesadni test subset aibl shows syntheticdata learned adni benefit way aibloverfit characteristics adniaddition robustness data sets usability synthetic data diverse classifiers assessedtoy data sets classifiers mlp random forestknn algorithm svm medical image data setsdifferent cnn studied baselineslightly optimized previous studyoptimized extensive search randomsearch classifiers performed best augmenteddata real data note dataaugmentation beneficial baseline networkoptimized networks obtainedsimilar performance data augmentation largesttraining set means data augmentation avoidspending time andor resources optimizing classifierbalancedaccuracy755 27721 31756 25778 23769 24778 19769 25780 21855 24857 25860 18865 19872 17858 26864 13867 18sensitivityspecificity686 85578 123692 94793 58778 88809 79807 61817 49751 84789 54749 50764 56760 47749 73741 49764 42826 42846 42827 41825 42822 45814 42812 37819 46887 90899 40914 26910 34920 24923 26929 19921 21balancedaccuracy756 41712 51760 42809 32800 36812 37809 27819 22819 32844 17832 24837 20840 20836 32835 22843 18ability model generate relevant dataenrich original training data supportedfact classifiers achieve betterclassification performance trained syntheticdata real train setgeneration framework appears suitedperform data augmentation hdlss settingbinary classification ad cn subjects t1w mricases classification performancegood maximum performance obtained real databetter instance methodallowed balanced accuracy baseline cnn jump663 743 trained 50 imagesclass 777 863 trained 243cn 210 ad improving greatly sensitivityspecificity metrics witnessed greater performanceimprovement studies cnn t1wmri differentiate ad cn subjects 102 103 107108 109 studies simple transformsaffine pixelwise bring variabilityimprove cnn performance complexmethods exist perform data augmentationwidely adopted field medical imagingsuspect mainly lack reproducibilityframeworks provide source codescripts easily reproduce experimentspaper adni aibl data set downloadfinal evaluation cnn performancenonetheless performance classificationsynthetic data improved wayschose study spend time optimizinghyperparameters vae sec 5 chosework downsampled images deal memoryissues easily look architecturetrain vae directly highresolution images leadingbetter performance witnessed experiments realimages couple advantagestechniques autoencoder pretraining weaksupervision data augmentation frameworkadvantages stack observeddata augmentation optimized hyperparameters finallychose train networks imageparticipant framework benefituse followup patientsimprove performance long followupexception context medical imagingassessed relevance data augmentationframework context small data sets mainissue field nonetheless training set 50 imagesclass seen large case rare diseasesinteresting evaluate reliabilitymethod smaller training sets 20 10 imagesclassc onclusionpaper proposed new vaebased data augmentation framework performance robustnessvalidated classification tasks toy reallifedata sets method relies combination properlatent space modeling vae seen riemannianmanifold new generation procedure exploitinggeometrical aspects particular generation methoduse prior standard showeddepending choice data set consideredlead poor latent space prospectingdegraded sampling proposed methodsuffer drawbacks proposed amendmentsmotivated discussed compared vaemodels demonstrated promising results modelappeared able generate new data faithfullydemonstrated strong generalization powermakes suited perform data augmentationchallenging context hdlss dataaugmentation experiment able enrich initialdata set classifier performs better augmenteddata real ones future work consistbuilding framework able handle longitudinal dataable generate observationpatient trajectoryacknowledgmentresearch leading results received funding french government managementagence nationale la recherche investissements davenir program reference anr19p3ia0001prairie 3ia institute reference anr10iaihu06agence nationale la recherche10ia institut hospitalouniversitaire6 work granted access hpcresources idris allocation 101637genci grand quipement national calcul intensifdata collection sharing project fundedalzheimers disease neuroimaging initiative adninational institutes health grant u01 ag024904dod adni department defense award numberw81xwh1220012 adni funded national institute aging national institute biomedical imaging bioengineering generous contributions following abbvie alzheimers associationalzheimers drug discovery foundation araclon biotechbioclinica biogen bristolmyers squibb companycerespir cogstate eisai elan pharmaceuticalseli lilly company euroimmun f hoffmannla roche affiliated company genentechfujirebio ge healthcare ixico janssen alzheimerimmunotherapy research development llc johnsonjohnson pharmaceutical research development llclumosity lundbeck merck meso scale diagnostics llc neurorx research neurotrack technologiesnovartis pharmaceuticals corporation pfizer piramalimaging servier takeda pharmaceutical companytransition therapeutics canadian institutes healthresearch providing funds support adni clinicalsites canada private sector contributions facilitatedfoundation national institutes healthwwwfnihorg grantee organization northerncalifornia institute research educationstudy coordinated alzheimers therapeutic research institute university southern californiaadni data disseminated laboratory neuroimaging university southern californiar eferencesk s button j p ioannidis c mokrysz b nosek j flint e srobinson m r munaf power failure small samplesize undermines reliability neuroscience nature reviewsneuroscience vol 14 5 pp 365376 2013b o turner e j paul m b miller k barbey smallsample sizes reduce replicability taskbased fmri studiescommunications biology vol 1 1 pp 110 2018goodfellow y bengio courville y bengio deeplearning mit press cambridge 2016 vol 1 issue 2c shorten t m khoshgoftaar survey image dataaugmentation deep learning journal big data vol 61 p 60 2019m tanner w h wong calculation posteriordistributions data augmentation journal americanstatistical association vol 82 398 pp 528540 1987n v chawla k w bowyer l o hall w p kegelmeyersmote synthetic minority oversampling technique journalartificial intelligence research vol 16 pp 321357 2002h han wy wang bh mao borderlinesmotenew oversampling method imbalanced data sets learningadvances intelligent computing ds huang xp zhanggb huang eds springer berlin heidelberg 2005 vol3644 pp 878887 series title lncsh m nguyen e w cooper k kamei borderline oversampling imbalanced data classification international journalknowledge engineering soft data paradigms vol 3 1 pp421 2011haibo yang bai e garcia shutao li adasynadaptive synthetic sampling approach imbalanced learning2008 ieee international joint conference neural networksieee world congress computational intelligence ieee 2008pp 13221328s barua m m islam x yao k murase mwmotemajority weighted minority oversampling technique imbalanced data set learning ieee transactions knowledge dataengineering vol 26 2 pp 405425 2012r blagus l lusa smote highdimensional classimbalanced data bmc bioinformatics vol 14 1 p 106 2013fernndez s garcia f herrera n v chawla smotelearning imbalanced data progress challengesmarking 15year anniversary journal artificial intelligenceresearch vol 61 pp 863905 2018goodfellow j pougetabadie m mirza b xu d wardefarley s ozair courville y bengio generative adversarial nets advances neural information processing systems2014 pp 26722680d p kingma m welling autoencoding variational bayesarxiv13126114 cs stat 2014d j rezende s mohamed d wierstra stochastic backpropagation approximate inference deep generative models international conference machine learning pmlr 2014pp 12781286x zhu y liu j li t wan z qin emotion classificationdata augmentation generative adversarial networkspacificasia conference knowledge discovery data miningspringer 2018 pp 349360g mariani f scheidegger r istrate c bekas c malossi bagan data augmentation balancing ganarxiv180309655 2018antoniou storkey h edwards data augmentation generative adversarial networks arxiv171104340 cs stat20180321s k lim y loo nt tran nm cheung g roigy elovici doping generative data augmentation unsupervised anomaly detection gan 2018 ieee internationalconference data mining icdm ieee 2018 pp 11221127y zhu m aoun m krijn j vanschoren h t campusdata augmentation conditional generative adversarialnetworks leaf counting arabidopsis plants bmvc2018 p 324x yi e walia p babyn generative adversarial networkmedical imaging review medical image analysis vol 58 p101552 2019hc shin n tenenholtz j k rogers c g schwarz m lsenjem j l gunter k p andriole m michalski medicalimage synthesis data augmentation anonymizationgenerative adversarial networks international workshopsimulation synthesis medical imaging ser lncs springer2018 pp 111f calimeri marzullo c stamile g terracina biomedical data augmentation generative adversarial neural networks international conference artificial neural networksspringer 2017 pp 626634m fridadar diamant e klang m amitai j goldbergerh greenspan ganbased synthetic medical image augmentation increased cnn performance liver lesion classification neurocomputing vol 321 pp 321331 2018v sandfort k yan p j pickhardt r m summers dataaugmentation generative adversarial networks cyclegan improve generalizability ct segmentation tasksscientific reports vol 9 1 p 16884 2019madani m moradi karargyris t syedamahmoodchest xray generation data augmentation cardiovascular abnormality classification medical imaging 2018 imageprocessing vol 10574international society opticsphotonics 2018 p 105741mh salehinejad s valaee t dowdell e colak j barfettgeneralization deep neural networks chest pathologyclassification xrays generative adversarial networks2018 ieee international conference acoustics speech signalprocessing icassp ieee 2018 pp 990994waheed m goyal d gupta khanna f alturjmanp r pinheiro covidgan data augmentation auxiliaryclassifier gan improved covid19 detection ieee access vol 8pp 91 91691 923 2020l bi j kim kumar d feng m fulham synthesispositron emission tomography pet images multichannelgenerative adversarial networks gans molecular imagingreconstruction analysis moving body organs strokeimaging treatment ser lncs springer 2017 pp 4351y liu y zhou x liu f dong c wang z wang wasserstein ganbased smallsample augmentation newgenerationartificial intelligence case study cancerstaging data biology engineering vol 5 1 pp 156163 2019c baur s albarqouni n navab generating highly realistic images skin lesions gans 20 contextawareoperating theaters assisted robotic endoscopy clinicalimagebased procedures skin image analysis springer 2018pp 260267d korkinof t rijken m oneill j yearsley h harveyb glocker highresolution mammogram synthesis progressive generative adversarial networks arxiv preprintarxiv180703401 2018e wu k wu d cox w lotter conditional infilling gansdata augmentation mammogram classification imageanalysis moving organ breast thoracic images springer2018 pp 98106wn hsu y zhang j glass unsupervised domain adaptation robust speech recognition variational autoencoderbased data augmentation 2017 ieee automatic speech recognition understanding workshop asru ieee 2017 pp 1623h nishizaki data augmentation feature extractionvariational autoencoder acoustic modeling 2017 asiapacific signal information processing association annual summitconference apsipa asc ieee 2017 pp 12221227z wu s wang y qian k yu data augmentationvariational autoencoder embedding based speaker verification interspeech 2019 isca 2019 pp 11631167p zhuang g schwing o koyejo fmri data augmentation synthesis 2019 ieee 16th international symposiumbiomedical imaging isbi 2019 ieee 2019 pp 17831787x liu y zou l kong z diao j yan j wang s li p jiaj data augmentation latent space interpolationimage classification 2018 24th international conferencepattern recognition icpr ieee 2018 pp 728733n painchaud y skandarani t judge o bernard lalande pm jodoin cardiac mri segmentation stronganatomical guarantees international conference medicalimage computing computerassisted intervention springer2019 pp 632640r selvan e b dam n s detlefsen s rischel k shengm nielsen pai lung segmentation chest xraysvariational data imputation arxiv200510052 cs eessstat 2020myronenko 3d mri brain tumor segmentation autoencoder regularization international miccai brainlesionworkshop springer 2018 pp 311320m jordan z ghahramani t s jaakkola l k saulintroduction variational methods graphical modelsmachine learning vol 37 2 pp 183233 1999y burda r grosse r salakhutdinov importance weightedautoencoders arxiv150900519 cs stat 20161107alemi fischer j v dillon k murphy deep variational information bottleneck arxiv preprint arxiv1612004102016higgins l matthey pal c burgess x glorot m botvinicks mohamed lerchner betavae learning basic visualconcepts constrained variational framework iclr vol 25 p 6 2017c cremer x li d duvenaud inference suboptimalityvariational autoencoders international conference machinelearning pmlr 2018 pp 10781086c zhang j btepage h kjellstrm s mandt advancesvariational inference ieee transactions pattern analysismachine intelligence vol 41 8 pp 20082026 2018f ruiz m titsias contrastive divergence combiningvariational inference mcmc international conferencemachine learning pmlr 2019 pp 55375545t salimans d kingma m welling markov chain montecarlo variational inference bridging gap internationalconference machine learning 2015 pp 12181226d rezende s mohamed variational inference normalizing flows international conference machine learningpmlr 2015 pp 15301538r m neal mcmc hamiltonian dynamicshandbook markov chain monte carlo vol 2 11 p 2 2011l caterini doucet d sejdinovic hamiltonian variational autoencoder advances neural information processingsystems 2018 pp 81678177m d hoffman m j johnson elbo surgery waycarve variational evidence lower bound workshopadvances approximate bayesian inference nips vol 1 2016p 2e nalisnick l hertel p smyth approximate inferencedeep latent gaussian mixtures nips workshop bayesiandeep learning vol 2 2016 p 131n dilokthanakul p m mediano m garnelo m c hlee h salimbeni k arulkumaran m shanahan deepunsupervised clustering gaussian mixture variational autoencoders arxiv161102648 cs stat 2017j tomczak m welling vae vampprior international conference artificial intelligence statistics pmlr2018 pp 12141223c k snderby t raiko l maale s k snderbyo winther ladder variational autoencoder 29th annualconference neural information processing systems nips 20162016klushyn n chen r kurle b cseke learning hierarchical priors vaes advances neural information processingsystems p 10 2019x chen d p kingma t salimans y duan p dhariwal j schulman sutskever p abbeel variational lossy autoencoderarxiv preprint arxiv161102731 2016razavi v d oord o vinyals generating diversehighfidelity images vqvae2 advances neural information processing systems 2020b pang t han e nijkamp sc zhu y n wu learning latent space energybased prior model advances neuralinformation processing systems vol 33 2020j aneja schwing j kautz vahdat ncpvae variational autoencoders noise contrastive priorsarxiv201002917 cs stat 2020m bauer mnih resampled priors variational autoencoders 22nd international conference artificial intelligence statistics pmlr 2019 pp 6675t r davidson l falorsi n cao t kipf j m tomczakhyperspherical variational autoencoders 34th conferenceuncertainty artificial intelligence 2018 uai 2018 associationuncertainty artificial intelligence auai 2018 pp 856865e mathieu c le lan c j maddison r tomioka y w tehcontinuous hierarchical representations poincar variational autoencoders advances neural information processingsystems 2019 pp 12 56512 576ovinnikovpoincarwassersteinautoencoderarxiv190101427 cs stat 20200316l falorsi p haan t r davidson n cao m weilerp forr t s cohen explorations homeomorphic variational autoencoding arxiv180704689 cs stat 2018n miolane s holmes learning weighted submanifoldsvariational autoencoders riemannian variational autoencoders proceedings ieeecvf conference vision pattern recognition 2020 pp 14 50314 511g arvanitidis l k hansen s hauberg locally adaptivenormal distribution advances neural information processingsystems pp 42584266 2016n chen klushyn r kurle x jiang j bayer p smagtmetrics deep generative models international conferenceartificial intelligence statistics pmlr 2018 pp 15401550h shao kumar p t fletcher riemannian geometrydeep generative models 2018 ieeecvf conference vision pattern recognition workshops cvprw ieee2018 pp 4284288d kalatzis d eklund g arvanitidis s hauberg variational autoencoders riemannian brownian motion priorsinternational conference machine learning pmlr 2020 pp50535066c chadebec c mantoux s allassonnire geometryaware hamiltonian variational autoencoder arxiv201011518cs math stat 2020g arvanitidis b georgiev b schlkopf priorbased approximate latent riemannian metric arxiv210305290 cs stat2021g arvanitidis l k hansen s hauberg latent spaceoddity curvature deep generative models 6thinternational conference learning representations iclr 20182018m f frenzel b teleaga ushio latent space cartography generalised metricinspired measures measurebased transformations generative models arxiv preprintarxiv190202113 2019g arvanitidis s hauberg b schlkopf geometricallyenriched latent spaces arxiv200800565 cs stat 20200802g lebanon metric learning text documents ieee transactions pattern analysis machine intelligence vol 28 4 pp497508 2006m louis computational statistical methods trajectoryanalysis riemannian geometry setting phd thesis sorbonnes universits 2019m girolami b calderhead s chin riemannian manifold hamiltonian monte carlo arxiv preprint arxiv090711002009m girolami b calderhead riemann manifold langevinhamiltonian monte carlo methods journal royalstatistical society series b statistical methodology vol 73 2pp 123214 2011paszke s gross s chintala g chanan e yang z devitoz lin desmaison l antiga lerer automaticdifferentiation pytorch 2017r m neal hamiltonian importance sampling talk presentedbanff international research station birs workshop mathematical issues molecular dynamics 2005s duane d kennedy b j pendleton d roweth hybrid monte carlo physics letters b vol 195 2 pp 2162221987b leimkuhler s reich simulating hamiltonian dynamicscambridge university press 2004 vol 14j s liu monte carlo strategies scientific computing springerscience business media 2008m arjovsky s chintala l bottou wasserstein ganarxiv170107875 cs stat 20171206h xiao k rasul r vollgraf fashionmnist novel imagedataset benchmarking machine learning algorithms arxivpreprint arxiv170807747 2017g cohen s afshar j tapson van schaik emnistextending mnist handwritten letters 2017 international jointconference neural networks ijcnn ieee 2017 pp 29212926y lecun mnist database handwritten digits 1998t salimans goodfellow w zaremba v cheung radfordx chen improved techniques training gans advances neural information processing systems 2016m heusel h ramsauer t unterthiner b nesslers hochreiter gans trained timescale update ruleconverge local nash equilibrium advances neuralinformation processing systems 2017t karras t aila s laine j lehtinen progressive growinggans improved quality stability variation international conference learning representations iclr 2017m lucic k kurach m michalski s gelly o bousquetgans created equal largescale study advancesneural information processing systems 2018 p 10100101102103104105106107108109110111112k shmelkov c schmid k alahari goodgan proceedings european conference visioneccv 2018 pp 213229borji pros cons gan evaluation measuresvision image understanding vol 179 pp 4165 2019g huang z liu l van der maaten k q weinbergerdensely connected convolutional networks 2017 ieee conference vision pattern recognition cvpr ieee2017 pp 22612269b amos bamosdensenetpytorch 2020 originaldate20170209t153323z online available httpsgithubcombamosdensenetpytorchl breiman random forests machine learning vol 45 1pp 532 2001s b kotsiantis zaharakis p pintelas supervised machine learning review classification techniques emergingartificial intelligence applications engineering vol 1601 pp 324 2007j wen e thibeausutre m diazmelo j sampergonzlezroutier s bottani d dormont s durrleman n burgoso colliot convolutional neural networks classificationalzheimers disease overview reproducible evaluationmedical image analysis vol 63 p 101694 2020k aderghal m boissenin j benoispineau g cathelinek afdel classification smri ad diagnosis convolutional neuronal networks pilot 2d study adnilecture notes science including subseries lecture notesartificial intelligence lecture notes bioinformatics vol10132 lncs 2017 pp 690701k aderghal khvostikov krylov j benoispineau k afdelg catheline classification alzheimer disease imaging modalities deep cnns crossmodal transferlearning 2018 ieee 31st international symposium computerbased medical systems cbms 2018 pp 345350 issn 23729198j islam y zhang ganbased synthetic brain pet imagegeneration brain informatics vol 7 1 2020k oh yc chung k w kim ws kim oh classification visualization alzheimers disease volumetricconvolutional neural network transfer learning scientificreports vol 9 1 p 18150 2019m liu j zhang c lian d shen weakly supervised deeplearning brain disease prognosis mri incompleteclinical scores ieee transactions cybernetics vol 50 7pp 33813392 2020valliani soni deep residual nets improvedalzheimers diagnosis 8th acm international conferencebioinformatics computational biologyand health informatics acmbcb 17 boston massachusetts usa acm press 2017 pp615615k backstrom m nazari ih gu jakola efficient 3ddeep convolutional network alzheimers disease diagnosismr images 2018 ieee 15th international symposiumbiomedical imaging isbi 2018 vol 2018april 2018 pp 149153d cheng m liu cnns based multimodality classificationad diagnosis 2017 10th international congress imagesignal processing biomedical engineering informatics cispbmei 2017 pp 15k ellis bush d darby d fazio j foster p hudson n t lautenschlager n lenzo r n martins p maruffc masters milner k pike c rowe g savage c szoekek taddei v villemagne m woodward d ames aiblresearch group australian imaging biomarkerslifestyle aibl study aging methodology baseline characteristics 1112 individuals recruited longitudinal studyalzheimers disease international psychogeriatrics vol 214 pp 672687 2009k j gorgolewski t auer v d calhoun r c craddocks das e p duff g flandin s s ghosh t glatard y ohalchenko d handwerker m hanke d keator x liz michael c maumet b n nichols t e nichols j pellmanjb poline rokem g schaefer v sochat w triplett jturner g varoquaux r poldrack brain imagingdata structure format organizing describing outputsneuroimaging experiments scientific data vol 3 1 p160044 2016n j tustison b b avants p cook yuanjie zheng eganp yushkevich j c gee n4itk improved n3 bias113114115116117118119correction ieee transactions medical imaging vol 29 6pp 13101320 2010v fonov evans r mckinstry c almli d collinsunbiased nonlinear average ageappropriate brain templatesbirth adulthood neuroimage vol 47 p s102 2009v fonov c evans k botteron c r almli r c mckinstryd l collins unbiased average ageappropriate atlasespediatric studies neuroimage vol 54 1 pp 313327 2011b b avants n j tustison m stauffer g song b wuj c gee insight toolkit image registration frameworkfrontiers neuroinformatics vol 8 2014v s fonov m dadar t pa r group d l collins deeplearning quality control stereotaxic registration humanbrain mri biorxiv p 303487 2018j bergstra y bengio random search hyperparameteroptimization journal machine learning research vol 13feb pp 281305 2012k x zhang s ren j sun delving deep rectifierssurpassing humanlevel performance imagenet classification 2015 ieee international conference visioniccv santiago chile ieee 2015 pp 10261034d p kingma j ba adam method stochastic optimization arxiv preprint arxiv14126980 2014clment chadebec phd student funded p rairie universit paris inria research interests include machine learningriemannian geometry computational statistics medicinereceived master degrees ecole nationale des mines parisecole normale suprieure parissaclayelina thibeausutre phd student sorbonne universit inriaresearch include deep learning application neuroimagingdata interpretability reproducibility received master degreesecole nationale des mines paris ecole suprieurephysique et chimie industrielles paris franceninon burgos cnrs researcher aramis lab joint laboratorysorbonne universit cnrs inserm inria parisbrain institute france completed phd university collegelondon uk 2016 research focuses development computational imaging tools improve understanding diagnosisdementiastphanie allassonnire pr applied mathematics universitparis p rairie fellow deputy directeor received phddegree applied mathematics 2007 studies year postdoctoral fellow cis jhu baltimore joined appliedmathematics department ecole polytechnique 2008 assistantprofessor moved paris descartes school medicine 2016professor researches focus statistical analysis medicaldatabases order understanding common features populations designing classification early prediction decision supportsystemsppendixp roof p rop 1ppendix bd etailed e xperimental s ettingproposition 2 riemannian manifold rd g geodesically completegiven manifold m rd endowedriemannian metric g local representationgiveng1 zkz c k22idli lexpeq 4 paper geodesic curve b mactually extensible r riemannian manifoldrd g geodesically complete proof deriveinspired proposed 79proof let suppose exists geodesiccurve extended rexist b r b domaindefinition m assumptionleads absurdityexperiment consider vanilla vae vaevamp prior geometryaware vae faircomparison model trained neuralnetwork architecture encoder decoderlatent space dimension main parametersgeometryaware vae presented table 8refer reader 73 precise descriptionparameters impact modelvamp prior number pseudoinputs set 10use implementation provided authorsmodel trained elbo improve 20epochs adam optimizer 119 learning rate103 data sets sizes small trainingperformed single batchtable 7neural net architectures architecturesvanilla vae vamp vae geometryaware vaesdiagli defined lower triangularmatrices positive diagonal coefficientsli lsymmetric positivedefinitematrixcholeskydecompositionx li lx 0 x r 0equality comes constant speedgeodesic curveskt t0 k2kt0 kt0t t0shows t b geodesic curveremains compact set boundedconsider sequence tn b geodesic curvesconstant speed tn tn nn compact setapplication cauchylipschitz theorem0 n n definedtn tn tn close b desiredexists n n n n tn b 2means domain definition curveextended b 2 concludes proofd 400 relud 400 relu400dd1lineard input space dimensiond latent space dimensiontable 8geometryaware vae parametersktk2t httit t gttktk22 ktk22kt ci k22t li li t expi1ktk2t kt0 k2t0400 d linear400 d linear400 d sigmoid400 d lineard 400 relullowlet t b recalllet t0 b t bparameters sec 33 generation comparisondata setssynthetic shapesreduced mnist balreduced mnist unbalreduced emnistnlfparameters102102102102103103103103latent space dimension vae vampvaeparameters sec 4 data augmentationexperiment parameters neural networks architectures presented sectionreduced fashion dimensionlatent space set 5 training parametersvaes model use adam optimizerlearning rate set 103 data sets sizessmall training performed single batchdensenet 97 benchmark data augmentationimplementation use 98 growthrate equals 10 depth 20 05 reduction trainedlearning rate 103 weight decay 104batch size 200 classifier trained lossimprove validation set 50 epochs testedoriginal test sets 1000 samples mnistsec 423 mlp 400 hidden units relu activationfunction trained adam optimizer learningrate 103 training stopped loss improvevalidation set 20 epochsparameters sec 5 validation medical imaginggenerate new data adni database amendneural network architectures use describedtable 9 parameters geometryaware vaeprovided table 10 adam optimizer learningrate 105 batch size 25 vae modeltrained elbo improve 50 epochsgenerating 50 adni images takes approx 30 s5proposed method intel core i7 cpu 6x11ghz 16gb ramllow777504d h1 reld h3 relud h3 reluh1 h2 reluh1 h2 reluh3 h2 reluh3 d lin500dd1500linh2 h3 reluh2 h3 reluh2 h1 reluh3 d linh3 d linh1 d sig400table 10geometryaware parameters settings adni databasedata setadninlfdata setssynthetic shapesreduced mnistreduced emnistreduced fashionnlfparameters102102102102103103103103latent space dimension vae vampvaeppendix ddditional r esults s ec 423table 9neural net architecturediagtable 11geometryaware vae parametersparameters10315 102ppendix cf ew m ore s ampling c omparisons s ec 33addition comparison performed sec 331compare qualitatively vanilla vae vae vampprior geometryaware vae 4 reduced data setshigher dimensional latent spaces dimension 10created 180 binary rings circlesdifferent diameters thicknesses ensuring balancedclasses second composed 120 samplesemnist letter m referred reduced emnistcreated 120 samples classes1 2 3 mnist database ensuring balanced classescalled reduced mnist reduced fashioncomposed 120 samples 3 classes shoestrouser bag fashionmnist ensuring balancedclasses models architectures describedtable 7 trained parameters statedtable 11 10 pseudoinputs train vaevamp prior model trained elboimprove 20 epochs adam optimizer learningrate 103 single batch fig 10 presented1 extract training samplesdata set 2 samples obtained vanilla vaegaussian prior 2 data generated vae vampprior 3 samples created geometryaware vaeprior 4 samples method discussedpaper proposed method able visuallyoutperform peers data sets able createsharper meaningful samples numbertraining samples small5 depends length mcmc chain hmc hyperparameter l 300 steps l 15experiments presented sec 423provide results 4 classifiers reduced emnistreduced fashion fig 9 classifiersproposed method equals greatly outperformbaselineppendix em ore s ample g eneration adnisection provide slices 3dimage generated model model trainedclass ad train50 50 mri patient havingdiagnosed alzheimer disease generatedimage presented fig 11 present fig 12 4generated patients model trained train50left images cognitively normal generated patientsrightmost images represent ad generated patientsppendix ft ntruders nswers f ig 7fig 7 paper synthetic samples leftmostrightmost images real patientsmiddle model trained class ad trainfull210 imagesppendix gc omplementary r esults m edical magesresults synthetic data classification taskmris added tables 12 15 observedtoy examples proposed model able producemeaningful synthetic samples cnn outperformsgreatly baseline real training data train50 trainfull fact classification performancesaibl training betterclassifier trained synthetic data baseline showsgenerative model overfit trainingdata coming adni produces samplesrelevant databasetable 12mean test performance 20 runs trained train50 baseline hyperparametersadniimage typerealreal highresolutionsyntheticsyntheticsyntheticsyntheticsyntheticsyntheticsynthetic realsynthetic realsynthetic realsynthetic realsynthetic realsynthetic realsyntheticimages500100020003000500010000500100020003000500010000sensitivityspecificity703 122785 94724 64750 62714 66706 52781 61752 68719 53698 66722 44718 49747 53747 70624 115574 88656 81656 74704 66738 42690 69734 48670 45712 37703 43734 55735 48734 61aiblbalancedaccuracy663 24679 23690 19703 20709 30722 14735 20743 19694 16705 21712 16726 16741 22740 27sensitivityspecificity607 137572 112566 99627 97621 88657 69745 78736 108559 68591 90666 71661 93717 100691 99738 72758 70800 53788 53805 47805 46773 54794 60811 31821 37790 41811 50805 44807 51balancedaccuracy672 41665 30683 30708 35713 36731 18765 29759 25685 25706 31728 22736 30761 36749 32table 13mean test performance 20 runs trained trainfull baseline hyperparametersadniimage typerealreal highresolutionsyntheticsyntheticsyntheticsyntheticsyntheticsyntheticsynthetic realsynthetic realsynthetic realsynthetic realsynthetic realsynthetic realsyntheticimages500100020003000500010000500100020003000500010000sensitivityspecificity791 62845 38816 68829 45819 45849 35840 35842 54825 34846 44854 40847 36846 42842 28763 42767 40795 58820 58877 34874 35884 33886 39819 54843 51864 59868 45869 36885 29aiblbalancedaccuracy777 25806 11805 24824 19848 20861 15862 17864 18822 24844 18859 16858 17857 21863 18sensitivityspecificity706 67716 64747 93772 74747 63774 58768 42775 74760 63770 70772 69772 48769 52791 47863 36892 27873 48888 52921 19909 30922 18910 32897 33904 34904 38917 29914 30910 26balancedaccuracy784 24804 26810 32830 20834 27842 18845 18842 24829 25837 23838 22844 18842 22851 19table 14mean test performance 20 runs trained train50 optimized hyperparametersadniimage typerealreal highresolutionsyntheticsyntheticsyntheticsyntheticsyntheticsyntheticsynthetic realsynthetic realsynthetic realsynthetic realsynthetic realsynthetic realsyntheticimages500100020003000500010000500100020003000500010000sensitivityspecificity754 50736 62758 30767 46739 36744 61771 45775 53732 42761 53752 38765 38771 37778 46755 53706 59776 53785 49798 40798 49774 52773 47780 33795 29786 44792 42767 41782 49aiblbalancedaccuracy755 27721 31767 28776 37768 30771 40772 21774 31756 25778 23769 24778 19769 25780 21sensitivityspecificity686 85578 123732 90787 75782 69764 101811 59817 54692 94793 58778 88809 79807 61817 49826 42846 42836 40832 48824 37824 43820 39797 41827 41825 42822 45814 42812 37819 46balancedaccuracy756 41712 51784 40809 43803 35794 47815 26807 29760 42809 32800 36812 37809 27819 22table 15mean test performance 20 runs trained trainfull optimized hyperparametersadniimage typerealreal highresolutionsyntheticsyntheticsyntheticsyntheticsyntheticsyntheticsynthetic realsynthetic realsynthetic realsynthetic realsynthetic realsynthetic realsyntheticimages500100020003000500010000500100020003000500010000sensitivityspecificity825 42826 45817 36828 34813 28822 49806 34840 38823 23825 33831 42813 37819 35822 34885 66889 63905 39900 40912 28906 45916 25891 31898 27905 41913 32904 34909 25912 36aiblbalancedaccuracy855 24857 25861 14864 21862 17864 20861 19865 17860 18865 19872 17858 26864 13867 18100sensitivityspecificity751 84789 54755 71768 45762 67777 63753 54792 52749 50764 56760 47749 73741 49764 42887 90899 40898 43915 25922 36908 44924 25901 37914 26910 34920 24923 26929 19921 21100baselineaugmented 200baselineaugmented 200augmented 500augmented 500augmented 1000balancedaccuracy819 32844 17826 29842 17842 26843 20838 20847 23832 24837 20840 20836 32835 22843 18augmented 1000augmented 2000augmented 2000synthetic 200synthetic 200synthetic 500synthetic 500synthetic 1000synthetic 1000synthetic 2000synthetic 2000accuracyaccuracymlpsvmknnreduced emnistrandom forestmlpsvmknnrandom forestb reduced fashionmnistfig 9 evolution accuracy 4 benchmark classifiers reduced emnist data set left reduced fashion data set rightstochastic classifiers trained 5 independent runs report mean accuracy standard deviation test setreduced emnist 120reduced mnist 120reduced fashion 120synthetic 180trainingsamplesvaen 0 idvaevamp priorrhvaen 0 idrhvaefig 10 comparison 4 sampling methods reduced emnist 120 letters m reduced mnist reduced fashionmnist synthetic datasets higher dimensional latent spaces dimension 10 1 samples extracted training set 2 samples generatedvanilla vae prior n 0 id 3 vamp prior vae 4 rhvae priorbased generation scheme 5rhvae proposed method models trained encoder decoder networks identical latent spacedimension early stopping strategy adopted consists stopping training elbo improve 20 epochs numbertraining samples noted parenthesisfig 11 slices generated image model trained ad class train50 50 images ad patientsfig 12 images generated method trained train50 left cn generated patients right ad generated patients