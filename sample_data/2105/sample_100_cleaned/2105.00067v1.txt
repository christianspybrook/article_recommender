unsupervised discriminative embedding subaction learning complexactivitiesarxiv210500067v1 cscv 30 apr 2021sirnam swetha hilde kuehne yogesh s rawat mubarak shahcenter research vision university central florida orlando flmitibm watson lab cambridge maabstractaction recognition detection context longuntrimmed video sequences seen increased attentionresearch community annotation complex activities usually time consuming challengingpractice recent works started tackleproblem unsupervised learning subactions complex activities paper proposes novel approachunsupervised subaction learning complex activitiesproposed method maps visual temporal representations latent space subactions learntdiscriminatively endtoend fashion endpropose learn subactions latent concepts noveldiscriminative latent concept learning dlcl module aidslearning subactions proposed dlcl module lendsidea latent concepts learn compact representations latent embedding space unsupervisedway result set latent vectors interpreted cluster centers embedding spacelatent space formed joint visual temporalembedding capturing visual similarity temporal ordering data joint learning discriminativelatent concept module novel eliminates needexplicit clustering validate approachbenchmark datasets proposed combination visualtemporal embedding discriminative latentconcepts allow learn robust action representationsunsupervised settingfigure 1 overview proposed approach given videoscomplex activity proposed model learns subactions latentconcepts endtoend manner latent concept assignmentinput video segment feature forms subaction predictionshown initial predictions refined viterbigenerate final predictions sample results activitychocolate milk seen latent concepts ablegroup subactions subaction pourmilk includes liftingbottle pouring milk jitter associated confusion chocolatemilk bottle liftedtemporal action segmentation 9 10 17 18 19 20frame video associated respective subaction class requires identify subactionstemporally localizeexisting works temporal action segmentation mainlyexplore supervised approaches framelevel annotations required subactions 21 8 12 922 11 23 complex activities usually longranged obtaining framelevel annotation arduousexpensive new line research focuses learningsubactions videos complex activity unsupervised setting 4 5 19 10 24 unsupervised setting problem challenging requiresbreaking complex activity video semanticallymeaningful subactions ii capturing temporal relationship subactions approaches tackleproblem stages stage1 introductionrecent years seen great progress video activity analysis research focusedclassification short video clips atomic shortrange actions 1 2 3 relatively easier taskcompared analysis untrimmed complex videosequences 4 5 6 7 8 9 10 11 12 untrimmed videoanalysis focus problem temporal action localization 13 14 15 16 set keyactions considered untrimmed videos taskembedding based visual andor temporal informationlearned second stage clustering appliedembedding space limits learning ability preventing embedding actually learn clusteringtime performing explicit clustering independent embedding learning makes model efficient utilize endtoend learningaddress problem propose endtoend approach subactions learned combining embedding latent concepts embedding spacetrained jointly latent concepts leading effective subaction discovery shown figure 1 allowjoint training propose novel discriminativelatent concept learning dlcl module combines latent concept learning contrastive loss ensuresubactions learnt latent embedding distantresulting latent concept vectors interpretedcluster centers removing need explicit clusteringlater stagesubactions softly bound temporal position activity incorporating temporal orderingcrucial recent works 10 19 incorporated temporal embedding predicting discrete temporal entitieslearning continuous temporal embedding shallowmlp architectures cases temporal informationgiven discrete continuous scalar valuejoint embedding space constructed predictingvalue input learn better spatiotemporal representations propose use temporal position encoding 25 instead scalar values learn respectiveembedding space jointly reconstructing visualtemporal representations embedding trainedjointly constrastive loss latent concept moduleembedding guided contributesoverall clusteringevaluate method benchmark datasetsbreakfast 8 50salads 26 youtube instructions 5evaluation test time follow protocol19 employ viterbi algorithm decode initialsubaction predictions coherent segments basedordered clustering subaction latent concepts detailed analysis shows impact proposed elementsreconstruction latent concept learningsummary propose novel endtoend unsupervised approach subaction learning complex activities following contributions workpropose unsupervised endtoend approachsubaction learning complex activities jointlylearning embedding simultaneously incorporates visual temporal informationmethod improves stateoftheartbenchmark datasets2 related workrecently lot learningsupervision essential action 3 1 2complex activity understanding 12 27 11 supervised approaches require large number framesannotated videos expensive tediousscaled large datasets weakly supervised approachesuse video readily available information like accompanying text narration audio works 28 29 use associated text narrations scripts learning actionsvideo line work weaksupervision includeworks assumed order subactionsknown 17 30 31 32 perframe annotations video subactions knowntraining authors 33 propose use combination audio text video identify steps instructional videoskitchen setting performance methodshighly dependent availability qualitytextaudio alignment video guaranteedheavily limit applicationworks assumption weak supervision removed learningaction classes works supervision addressed problem human motion segmentation 34 based sensorymotor data proposedapplication parallel synchronous grammar learnsimple action representations similar words languagelater bayesian nonparametric approach concurrentlymodel multiple sets time series data proposed35 work focuses motion capture data 36 37 benefit temporal structurevideos finetune networks labels additionally 38 39 40 41 leverage temporal structurevideos learn feature representation learn actionsrecently unsupervised approaches proposedlearn subactions complex activity 10 19 24 propose unsupervised approaches temporal segmentationcomplex activity subactions 4 proposessolve variant problem goal detect event boundaries event boundary segmentationcomplex activities focus identifying subactions instead learns identify boundaries multiple subactions long videos selfsupervised predictivelearning framework proposed solve utilizing difference observed predicted frame featuresdetermine event boundaries complex activitieswork focus solving temporal segmentation complex activity subactions shown10 19 24 10 iterative approach proposedalternates discriminative learning generlearn discriminative latent concepts contrastive loss integrating clustering latentembedding learningfigure 2 overview proposed model given videos complex activity extract visual features xnm compute positionalencoding vectors nm fed encoder map joint latent embedding learning subaction clusters learnsubaction clusters latent concepts yb attention block d takes randomly initialized vectors ynm learns latent concepts use contrastive loss learn yb discriminatively b znm ybk represent attentionlatent vector input xnm kth latent concept respectivelypendent embedding learning work presentendtoend model clustering incorporated embedding learning constrastive loss incorporatetemporal ordering propose use positional encodingspropose effective way unify visualtemporal representations learn visualtemporal embedding jointly reconstructing visual temporal representations proposed latent embedding bettercapturing visual temporal representations clustering friendly demonstrate later paper usefulness proposed model qualitatively quantitativelyative modeling discriminative learning mapvisual features latent space linear transformation compute weight matrix minimizespairwise ranking loss temporal ordering use generalized mallows model models distributionspermutations formulate complex activity sequence permutable subactions 19 model incorporates continuous temporal ordering framesjoint embedding space achieved trainingregressor predict timestamp frames videohidden layer representations embeddingclustering clusters ordered respecttime stamp refer model cte continuous temporal embedding 24 twostage embeddingpipeline proposed frame prediction unetmodel stage combined temporal discriminator stage followed clustering temporalembedding model employed similar 19latent embedding learning crucial unsupervisedlearning recently 7 formulated learning graph based latent embedding latent concepts supervised classification complex activities intuition modellong range videos latent concepts graphical nodescomplex activity recognition inspired ideologylatent concept learning model latent space proposedlcl unsupervised latent learning module jointembedding learning model subactionsworks unsupervised learning involve stage process utilize endtoendlearning making efficient clustering inde3 proposed model31 overviewgiven set n videos vn nn1 complex activity divide video segmentssegment extract i3d features 1 compute positional encoding vectors 25 described section 32video represented mn features xnm represent mth feature nth video corresponding positional encoding represented nm tasklearn subactions ordering activity predicting sequence subaction labelslnm 1 2 k feature xnm videonumber subactions labels k activitymaximum number possible subactions occuractivityoverview proposed model shown figure 2learn encoded representation xnm nmshown nm passed input featureattention block shown d figure 2 learnlatent conceptsclusters representative subactions attention block learns latent conceptsyb yb1 yb2 ybk discriminatively inputfeature nm assigned latent conceptuse combination reconstruction loss constrastiveloss learn embeddingshown b figure 2believe combination visual temporalinformation conjunction latent concept learninglearn latent embedding shown block b fig 2 makesmodel robust evaluate performancemodel based latent concept assignments inputfeature forms initial predictions modelsubaction transitions perform viterbi decodingestimate optimal subaction orderingnote unlike previous works 19 perform explicit clustering instead model learns clusterfeatures latent space discussed section 32 33eliminating need data availablemodel learn latent concepts incrementallyresulting subaction latent concepts temporally ordered video decoded wrt ordering given initial subaction probability assignmentsclip latent concept described section 35reconstructed positional encoding hyperparameter l loss function penalizing x0nm 0nmdissimilar xnm nm respectivelymean squared error combination latent visual featurerepresentation positional encodings inputattention block order ensure learntclusters representative subactions clustersdistant latent space describedsection33 discriminative latent concept learningidea having module learn subaction clusters discriminatively latent space endtoend fashion eliminating need explicit clusteringattention block inspired 7 takes inputfeature nm randomly initialized latent vectors yanalogous cluster center initialization shownfigure 2 latent concepts yb learntmlp weight w bias b transformsrandom latent vector initializations y latent conceptsyb yb w y b latent vector initialization y fixed w b learnable parameters makinglatent concepts yb learnable latent spacelatent concepts represent cluster centers learnedminimizing contrastive loss moving featureslatent space closer latent concepts similarity input feature nm latent concepts ybmeasured dot product activation function applied similarities compute activationvalues nm yb t finally attended latent vector representation computed znm ybcaptures latent concept relatedgiven input feature latent concepts tendlearn similaroverlapping conceptsintend learn objective learning latent concepts cluster latent representations discriminativelyachieve contrastive loss similarity latent vectors subactionmaximum confident latent concept maximizedsimilarity wrt latent concepts minimized showneq 2esimznm yklossd znm yb log pbksimznm yk6k eybk represents latent concept associated k thsubaction sim denotes cosine similarity k representslatent concept maximum confidence probabilityznm shownk argmaxp kznm32 joint visual temporal latent embeddingunsupervised learning approach learn clusterslatent space plays critical role learning semanticallymeaningful clusters employ encoderdecoder modelobtain latent representation skipconnections included encoder decoder shown c figure 2 help preserve commonality actionreduce redundant information like background latentrepresentationincorporating temporal ordering modelemploy positional encodings inspired 25 divide video segment sequence g equal groupsuse ordering index compute positional encoding vectors quantizing temporal index video clippositional encoding captures relativepositioning makes easy generalize highlyvarying video lengths idea learning mappingfeatures joint visual temporal embeddingencoderdecoder aids grouping clips subactionslatent space reconstruction loss autoencoder composed visual features positional encoding shownlossr lxnm x0nm lnm 0nmp kznm simznm ybk represents confidence probability latent vector znm latent conceptybk softmax activationxnm x0nm respectively represent input reconstructed visual feature nm 0nm respectively represent in4figure 3 qualitative comparison initial predictions woviterbi viterbi predictions approach activitytea seen model init able groupsubactions learn ordering subactions activity jitter subaction predictions occurs transitionsubaction expected transitionfinally transition modeling viterbi decoding smoothnessjitter subaction transitionsfigure 4 brief overview transition modeling viterbi decoding latent concept color coded best viewed colorlatent concepts ordered wrt mean time shown tvideo decoded coherent segments viterbialgorithm based ordered subaction latent concepts34 overall lossconsistent latent set assignments video maximizinglmn argmaxp lm lm1 p lm znmtotal loss learning proposed embedding composed losses section 32 33 losslossr lossdl1 lmn m135 temporal segmentationl1 lm 1 2 k represent set label sequence nth video p lm kznm probabilityznm belongs k th latent set described section 33 l1mn set label sequence maximumlikelihood nth videoinitial predictions test time assign featurevideo respective closest latent concept vectoreq 3 gives initial predictions directly basedembedding shown predictions figure 1 easeunderstanding refer latent sets analogousclusterstransition modeling viterbi decodingfigure 4 represents brief outline transition modelingviterbi decoding allow temporal decodingglobal ordering latent sets needs estimatedfollow protocol proposed 19 computemean timestamp set shown t figure 4sort ascending order set sortedordering terminal state orderingsubaction state transition probabilities subactionj defined p ji given05 j immediately follows05 jp ji10 j j terminal state4 experimentsexperiments define segment sequence8 frames video segment sequence divided128 equal groups ordering indexcompute positional encoding 25 segment extract 3 d features layer mixed 5c fed encoder embedding dimension 1024 use 3layerencoderdecoder adam optimizer learning rateset 1 104 evaluate approach 3 datasetsbreakfast dataset comprises 10 complex activitieshumans performing common kitchen activitiestotal 48 subactivities 1 712 videos varyinglengths based activity preparation style variations subaction orderings50salads dataset contains videos duration 45 hourssingle complex activity making mixed salad multimodal dataset includes rgb frames depth mapsaccelerometer data use rgb framesvideos dataset longer average framelength 10k frames provides annotations multiplegranularity levelsyoutube instructions dataset 5 activities 150videos 47 subactions videos takendecoding finally use ordering transitionprobabilities compute best path set orderinggiven input features xnm nm eq 3 compute probability embedded input feature znmbelonging latent set k maximize probabilityinput sequence following order defined eq 4methodctectecereals19 f v19 3 d3 dmocmocwo viterbiw viterbi209248401368375469table 2 comparison moc mean class activitiesbreakfast dataset applying viterbi f v represents fisher vectorsb chocolate milkfigure 5 illustrative comparison stateoftheart commetrics model predicts sequence cluster labels1 2 k video correspondencek groundtruth class labels map groundtruthprediction label correspondences inline 5 10 19activity use hungarian method onetoone mapping cluster exactly subactionreport performance mapping workuse mean frames mof 10 19f1score 5 addition report meanclass moc accuracy averages accuracyactivity class giving equal weights classesirrespective underlying data distribution mofpercentage correct predictions computed activity classes affected underlyingactivity classes distribution biased dominantactivity class f1score similar previous methodsreport mean score activities stateoftheart comparisons evaluate methodtask event boundary segmentation following protocol 4 compare method 4 indicatedvideobased hungarian matchingparing cte init init approachlearns model subactions intermittent subactiontransitions leading effective grouping subactionsviterbi decoding helps smoothen intermittent jitters predictions method provides coherent subactionpredictions able capture orderings subactionsmethodf1scoremofweakly supervisedrnnfc 42tcfpn 17nnvit 32d3tw 43cdfl 44333384457502unsupervisedmallow 10cte 19jvt 24264299346418481319474429746lstm al41 comparison stateoftheartcompare proposed method stateoftheart approaches present accuracy comparisonrecent works breakfast dataset table 1 presentperformance new metric moc table 2 approachachieves 474 mof 319 f1score 2 gainstateoftheart shown table 1 qualitative evaluation proposed approach figure 3 5figure 5 approach models subactionscoherently intermittent subaction transitionslearning ordering subactions complex activity example figure 3 model predicts stirteaintermittent transitions pourwater occursperson dips tea bag water closelyresembles subaction stirtea shown image figure 3 correctly predicts backgrounddip action ends annotation dipping teabag ground truth indicating goodnessproposed subaction learning intermittent transitionsindicate model confuses assign latent concepttable 1 comparison proposed method stateoftheartbreakfast dataset denotes results videobasedhungarian matching task event boundary segmentationyoutube directly background segmentssubaction frequency spread background varies based activity personperforming task background segments similar appearance temporal orderingbackground segments assignedlatent concepts confidence probabilityfollowing protocol 19 consider percent clipsconfidence background foregroundlabeled segments latent concepts assignmentsform initial predictions report results background ratio 60methodctejvtlstm al 4f1scoremofmethodf1scoremof355306frankwolfe 5mallow 10cte 19jvt 24244270283299346278390282344422606702296438397454lstm al 4table 3 comparison proposed method stateoftheartunsupervised approaches 50salads dataset granularity evaldenotes results videobased hungarian matchingtask event boundary segmentationtable 4 comparison proposed method stateoftheartunsupervised methods youtube instructions datasetdenotes results videobased hungarian matching taskevent boundary segmentationbased single feature viterbi aids generatingcoherent subaction segments sequence shownadditionally evaluate method task eventboundary segmentation compare stateoftheart approaches approach outperforms stateoftheart mof margin 31 breakfast dataset indicatingeffectiveness proposed method temporally segment meaningful subactions50salads dataset perform evaluation granularity level eval provide stateoftheart comparisontable 3 method outperforms 19 667 24116 f1score 3437 evaluate method task event boundary segmentation perform stateoftheart comparison table 310 gain stateoftheart 4 mof indicatingmethod effective subaction learning complexeventsyoutube instructions dataset follow protocol5 10 19 report performance approach considering background frames achieve 42moc 438 mof shown table 4 48gain mof stateoftheart method comparablef1score note 4 reported f1score backgroundframes included youtube instructions dataset follow procedure compare method 4table 4 indicated seen methodoutperforms stateoftheart event boundary segmentation task showing subaction learning capabilityidentify better event boundariesperformance effectiveness approach3 d features train 19 3 d features keeping embedding dimensioncompare performance shown table 2 mocwo viterbi improves 4 3 d features ctemoc viterbi drops 3 1 increasef1score approach outperformsbaseline embedding dimension huge marginindicating approach effectivenessdataset level comparisons activitylevel comparison cte 19 figure 6 showsjoint embedding outperforms cte activities indicating significance joint embedding dropperformance activity making cereals viterbidecoding figure 6b attributedordering subactions takebowl pourcerealssamples making cereals subaction takebowl occur impacting ordering subactions leading drop performance43 ablation experimentsperform ablation studies breakfastdataseteffect loss components begin examine influence lossr lossd modelperformances presented table 5 seenhaving loss components leads best performanceeffect discriminative learning use constrastiveloss lossd helps clusters apart latentspace helps obtaining discrete boundarieslatent space shown table 5 accuracy drastically reduces 358 11 indicating importancediscriminative learningeffect positional encoding positional encoding playscrucial role model helps temporally groupvideo clips latent space subactions softlybound temporal position activity removing42 evaluation embeddingdemonstrate impact proposed embeddingcompare joint embedding continuous temporal embedding 19 table 2 table 2 moc woviterbi seen proposed joint embeddingoutperforms continuous temporal embedding hugemargin 166 seen moc wo viterbicloser cte moc w viterbi suggestingembedding effective emphasize gainmof wo viterbifigure 7 mof vs subactions activities breakfastdataset k represents number subactions groundtruthvary subactions activity report mofport performance table 6 seen woskipconnections accuracy drops considerably indicating skipconnections help learning better representationsb final moffigure 6 activity level mof comparison breakfast datasetmocf1scorecte 19 column represents average mocactivities represents mof activity viterbimof computed based learnt cluster assignmentsmethod outperforms baseline activities b representsmof activity applying viterbilossrlossdwo pewo sc409203357287469319table 6 ablations experiments evaluate effect pesc breakfast dataset wo pe positional encodingsc skipconnectionsmoc257336358402401469effect subactions cluster size evaluations subaction cluster size k defined mentioned section 31 analyze impact subactioncluster size vary number subactions k 2k2 k number subactions groundtruth evaluate performance figure 7 shows mof vsnumber subactions activity breakfast dataset6 10 activities having k subactionsleads best performancetable 5 ablation experiments loss components performed breakfast dataset lossr lossd representsreconstruction loss contrastive loss respectively lf lpdenote reconstruction loss feature positional encodingrespectively5 conclusionwork proposed endtoend approachunsupervised learning subactions complex activitiesmain motivation approach design latent space incorporate visual positional encoding latent space learned jointly training embedding space conjunction contrastivelearning clustering allows robust learning results reasonable clustering subactions predict optimal subaction sequence employing viterbi algorithmoutperforms methods evaluation showsimpact proposed ideas ableimprove performance task compared existingmethodsreconstruction loss positional encoding expecteddeteriorate model performance observe similar trend table 5 additionally perform ablationremoving pe component branch train modelendtoend expected significant reductionaccuracy f1score shown table 6 indicatingsignificance positional encodingeffect skipconnections assess effectivenessskipconnections report performance removingskipconnections train model endtoend re8references17 li ding chenliang xu weaklysupervised action segmentation iterative soft boundary assignment cvpr20181 joao carreira andrew zisserman quo vadis actionrecognition new model kinetics dataset cvpr201718 alexander richard hilde kuehne juergen gall actionsets weakly supervised action segmentation ordering constraints cvpr 20182 christoph feichtenhofer haoqi fan jitendra malikkaiming slowfast networks video recognitioniccv 201919 anna kukleva hilde kuehne fadime sener jurgengall unsupervised learning action classes continuous temporal embedding cvpr 20193 karen simonyan andrew zisserman twostream convolutional networks action recognition videosneurips 201420 yazan abu farha jurgen gall mstcn multistagetemporal convolutional network action segmentationcvpr 20194 sathyanarayanan n aakur sudeep sarkar perceptual prediction framework self supervised event segmentation cvpr 201921 yazan abu farha jurgen gall mstcn multistagetemporal convolutional network action segmentationcvpr 20195 jeanbaptiste alayrac piotr bojanowski nishant agrawalivan laptev josef sivic simon lacostejulien unsupervised learning narrated instruction videos cvpr201622 hilde kuehne alexander richard juergen gall hybrid rnnhmm approach weakly supervised temporal action segmentation ieee transactions pattern analysismachine intelligence 20186 noureldien hussein efstratios gavves arnold wmsmeulders timeception complex action recognitioncvpr 201923 colin lea michael d flynn rene vidal austin reitergregory d hager temporal convolutional networks action segmentation detection cvpr 20177 noureldien hussein efstratios gavves arnold wmsmeulders videograph recognizing minuteslong humanactivities videos arxiv 201924 rosaura g vidalmata walter j scheirer anna kuklevadavid cox hilde kuehne joint visualtemporal embedding unsupervised learning actions untrimmedsequences wacv 20208 h kuehne b arslan t serre language actions recovering syntax semantics goaldirectedhuman activities cvpr 201425 ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illiapolosukhin attention need neurips 20179 hilde kuehne juergen gall thomas serre endtoend generative framework video segmentation recognition wacv 201626 sebastian stein stephen j mckenna combining embedded accelerometers vision recognizingfood preparation activities ubicomp 201310 fadime sener angela yao unsupervised learningsegmentation complex activities video cvpr201827 dean huang li feifei juan carlos niebles connectionist temporal modeling weakly supervised actionlabeling eccv 201611 zheng shou jonathan chan alireza zareian kazuyukimiyazawa shihfu chang cdc convolutionaldeconvolutional networks precise temporal action localization untrimmed videos cvpr 201728 ivan laptev marcin marszalek cordelia schmid benjamin rozenfeld learning realistic human actionsmovies cvpr 200812 serena yeung olga russakovsky greg mori li feifei endtoend learning action detection frameglimpses videos cvpr 201629 ozan sener amir r zamir silvio savarese ashutoshsaxena unsupervised semantic parsing video collectionsiccv 201513 peihao chen chuang gan guangyao shen wenbinghuang runhao zeng mingkui tan relation attentiontemporal action localization ieee transactions multimedia 201930 hilde kuehne alexander richard juergen gall weaklysupervised learning actions transcripts cviu 201714 fuchen long ting yao zhaofan qiu xinmei tian jieboluo tao mei gaussian temporal awareness networksaction localization cvpr 201931 alexander richard hilde kuehne juergen gall weaklysupervised action learning rnn based finetocoarsemodeling cvpr 201715 mengmeng xu chen zhao david s rojas ali thabetbernard ghanem gtad subgraph localization temporal action detection arxiv 201932 alexander richard hilde kuehne ahsan iqbal juergen gall neuralnetworkviterbi framework weaklysupervised video learning cvpr 201816 runhao zeng wenbing huang mingkui tan yu rongpeilin zhao junzhou huang chuang gan graph convolutional networks temporal action localizationiccv 201933 jonathan malmaud jonathan huang vivek rathod nickjohnston andrew rabinovich kevin murphy whatscookin interpreting cooking videos text speechvision arxiv 201534 gutemberg guerrafilho yiannis aloimonos language human action 200735 emily b fox michael c hughes erik b sudderth michaeljordan et al joint modeling multiple time seriesbeta process application motion capture segmentation annals applied statistics 201436 xiaolong wang abhinav gupta unsupervised learningvisual representations videos iccv 201537 biagio brattoli uta buchler annasophia wahl martin eschwab bjorn ommer lstm selfsupervision detailed behavior analysis cvpr 201738 vignesh ramanathan kevin tang greg mori li feifei learning temporal embeddings complex video analysis iccv 201539 basura fernando efstratios gavves jose m oramas amirghodrati tinne tuytelaars modeling video evolutionaction recognition cvpr 201540 anoop cherian basura fernando mehrtash harandistephen gould generalized rank pooling activity recognition cvpr 201741 hsinying lee jiabin huang maneesh singh minghsuan yang unsupervised representation learning sorting sequences iccv 201742 alexander richard hilde kuehne juergen gall weaklysupervised action learning rnn based finetocoarsemodeling cvpr 201743 chienyi chang dean huang yanan sui li feifeijuan carlos niebles d3tw discriminative differentiable dynamic time warping weakly supervised action alignmentsegmentation cvpr 201944 jun li peng lei sinisa todorovic weakly supervisedenergybased learning action segmentation iccv2019