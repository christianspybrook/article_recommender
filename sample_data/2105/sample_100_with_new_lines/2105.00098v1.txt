quantum machine learning with squid
jakub filipek1  shihchieh hsu2  alessandro roggero34  and nathan wiebe562
1 paul
g allen school of computer science  engineering university of washington seattle wa 98195 usa
of physics university of washington seattle 98195 usa
3 institute for nuclear theory university of washington seattle wa 98195 usa
4 inqubator for quantum simulation iqus department of physics university of washington seattle wa 98195 usa
5 university of toronto department of computer science toronto on m5g 1v7 canada
arxiv210500098v1 quantph 30 apr 2021
2 department
6 pacific
northwest national laboratory richland wa 99352 usa
april 30 2021
to their classical counterparts hard as potential performance benefits are the main driver of the quantumenhanced machine learning we believe that ease of comparison to classical machine learning should be one of
the priorities in the field
second approach is to classically preprocess data
so it can fit in the limited space defining the quantum model as in eg 8 while this approach allows
for direct comparison to classical performance on the
same data it also requires to factor in what impact
preprocessing had on performance of both algorithms
it requires scientists to carefully prepare experiments to
not give unfair advantages to either quantum or classical algorithms lastly since no two studies will use the
same preprocessing there is additional overhead when
comparing two different quantumenhanced approaches
or performing metaanalysis of the field
to combat these issues we propose a standardized
approach of designing hybrid quantum and classical
models similarly to how tensorflow 9 and pytorch 10 changed classical machine learning field and
increased reproducibility of efforts we propose scaled
quantum identifier squid which is an extensible
framework which can incorporate quantum models as
it is based on top of pytorch it has most of the benefits
of a mature framework when it comes to purely classical architectures for quantum models we provide a
standardized model design where user has to implement
forward and backpropagation functions
by doing so the preprocessing algorithms can be
standard across applications and approaches making
them more directly comparable it also reduces overhead on new researchers as it significantly reduces the
amount of coding required for an experiment such mix
of both worlds also resembles quantuminspired algorithms 1 which also benefit from above points
the article is organized in following manner in section 2 we oultine the framework design and describe the
relevant internal details in section 3 we show an exam
in this work we present the scaled quantum identifier squid an opensource framework for exploring hybrid quantumclassical algorithms for classification problems the classical infrastructure is based on pytorch and we
provide a standardized design to implement a
variety of quantum models with the capability
of backpropagation for efficient training we
present the structure of our framework and provide examples of using squid in a standard
binary classification problem from the popular
mnist dataset in particular we highlight the
implications for scalability for gradient based optimization of quantum models on the choice of
output for variational quantum models
1 introduction
quantum machine learning qml is a rapidly growing emerging field with a diverse set of ideas and applications while there are significant differences in
applications as to where machine learning and where
quantum computing are applied quantumenhanced
machine learning has become one of the dominant subfields 13 the main benefits of such algorithms are
potential quantum speedups 4 5 and the potential
of recognizing statistical patterns hard to learn with
purely classical schemes 6
however machine learning algorithms on nearterm
quantum devices face an issue of constrained resources
while there exist encodings that efficiently use qubits
they still do not allow to load datasets such as mnist in
quantum memory while additionally introducing overhead when encoding and decoding information between
classical and quantum devices to counter that issue
researchers have used two approaches
one was to use synthetic or very small datasets that
could be learned efficiently as in eg 7 this however makes any benchmarks artificial and comparison
ple application of the model using the mnist dataset
and study the impact of including information from single vs all available output qubits we describe the use
of the squid helpers package and possible future extensions in sec 4 finally in section 5 we provide a
summary and perspective
2 the squid framework
our main goal when designing squid is to propose
a framework within which both classical and quantum
machine learning can work in concert to solve a classification problem properly utilizing classical computing
when possible is of great importance because quantum
and classical models for data will often have different
advantages and disadvantages from an architectural
perspective the key innovation that squid allows is
for classical neural networks to be globally trained in
conjunction with a quantum neural network to build optimal encoders and decoders for the classical inputs or
quantum outputs from the hybrid neural network this
ability allows us to in effect learn a feature map that
not only allows us to represent large quantum datasets
in near term devices but also allows us to incorporate
classical information that may be known a priori into
the quantum model
before proceeding with the detailed description of the
model it is important for work like this that hybridizes
between quantum and classical models to discuss the
correspondence between the quantum and classical machine learning models that we implicitly assume in
all these cases we assume that in general our training dataset contains both classical vector and quantum
states and the following form
strain  vclass j vquant ji j  0     ntrain 
figure 1 squid model architecture a shows a current implementation which is a simplified version of b iin  0
in a the nodes can be thought of as neurons in classical machine learning or quantum state vectors the edges represent
transformations applied to these states these typically involve trainable weights but they might as often in case of
quantum model perform a set of transformations defined by
incoming data inputs to the quantum model vclass  in
particular can define both the input state to a quantum system
we  along with the rotations that should be applied to that
input state wv  quantum model should also perform a
measurement of the state md  and return classical amplitude
estimates vout  part b contains also planned future extension
in which quantum features are also allowed to be passed in
ally follows design principles set by pytorch 10 currently there are many competing quantum sdks 11
13 most of which include python interfaces hence a
successful qml package should allow a simple extensible solution which can be adjusted to any specific sdk
squid enables that by providing general classes similar to nnmodule in pytorch one each for quantum
and classical models these satisfy minimal requirements of functions used by the backend mainmodel to
properly propagate the gradient through combinations
of the models
here we assume a classical bitencoding meaning that
we assume that each vclass j  rd  this is the typical
setting in machine learning however it is also possible
to envision that the actual training vectors are distributions over d symbols and the classical values vclass are
given by the probabilities of drawing each symbol from
the distribution we further implicitly assume that the
quantum data vquant ji is provided using an amplitude
encoding by this we mean that the values of the training vectors are stored in the amplitudes of the vquant ji
state we make this choice because it is the most general setting that we can assume as it also subsumes the
case where the quantum training vectors correspond to
distinct quantum bit strings otherwise known as a bitencoding
the squid framework was designed with extensibility and simplicity as its core principles it gener
21 framework design
main component of squid is the mainmodel which
itself accepts three smaller models the first and last
models are currently enforced to be classical while the
middle can be either quantum or classical in case all
three models are classical mainmodel is equivalent to
pytorchs nnsequential with three subcomponents
the complete framework is shown graphically on
fig 1 and the detailed relations between models within
the ensemble are described in the following subsections
22 propagating through the main model
algorithm 1 backpropagation through the
main model
input main model m loss l torch tensor
let m1  m2  m3 refer respectively to encoder
quantum model decoder of m
lbackward
if m2 is quantum then
let be g23 the gradient of l with respect to
input to m3 pytorchprovided
let g12 be the gradient of l with respect to
input to m2  this is achieved by passing g23
through userprovided backward function of
the m2 model
let o12 be the output of m1 and input of
m2 saved from forward iteration
let l0 be sum of all elements of g12 o12 
l0 backward
end
calling the model or calling the forward function is
exactly the same to pytorchs forward pass the only
difference is when middle model is quantum and the
conversion between tensors and numpy arrays 14 is
required the reason for choosing numpy arrays to be
passed into quantum model is due to the fact that many
qml packages accept them as the input and in fact
prefer them even over standard pythonic lists
the backward function offers the only major modification for the user in comparison to pytorch and it is
required to be called explicitly by the user for classical
models standard pytorch backpropagation autograd
is used and exact gradients are calculated in the case
there is a quantum model in the middle the automatic
gradient propagation stops at the end of the second
quantum model this is because there was a conversion tofrom numpy in the forward pass squid uses
the backward function provided by implementation of
the quantum model this both updates any parameters stored within the model and provides the gradient
with respect to the output of the encoder
the conversion from numpy array to pytorch tensor
in backward call requires us to create a fake loss which
we then use to propagate the gradients backward using
backpropagation given encoder forward output o12 
gradient of global loss with respect to that output g12
provided by the quantum model as described above
we define a fake loss l0 
l0 o12  g12  
o12ij g12 ij
algorithm 2 quantum model
input number of qubits n  vector of m
 desired number
quantum parameters 
of outputs q1  1 2 
construct unitary operation corresponding to
quantum circuit
measure q1 output probabilities pk from circuit
use parameter
 shift rule and a total of
nc  min 2n m 2q1 m  1 circuits to
estimate the q1 m dimensional gradients gkw 
output returns result
p and gradgkw
l0 o12  g12 
 g12 ij
o12ij
complexity of the gradient calculation this is especially relevant since the gradients propagated through
the quantum model require statistical sampling or a
quantum technique such as amplitude estimation to
evaluate 1517 here we provide such a complexity analysis with the aim of bounding the scaling of
the number of quantum operations needed to ensure
that the gradients yielded by algorithm 2 are accurate
within bounded error  here in ensuring that the error
is  we mean that the gradient computation detailed in
algorithm 2 outputs an estimate of the gradient ge such
that g  ge2  
let us consider a montecarlo estimate of the gradient the algorithm for generating such an estimate
involves measuring the expectation value of the gradient this expectation value can be evaluated using
hadamard tests to estimate each component of the gradient see appendix a using the empirical frequency
of measurements as an unbiased estimate of the probability we have that if ge is the estimate that returns
after the above calculation a pytorch backward call
is made on l0  which propagates the gradient using
autograd hence gradients with respect to all of the
main models parameters are calculated for clarity the
process is shown in algorithm 1
it is worth mentioning that by using pytorch builtin
autograd procedure any pytorch loss can be used for
example in the section 3 cross entropy loss is used
similarly any optimizer can be used the main caveat
to using various optimizers is that if any parameters
are defined within the quantum model the user has
absolute control over updating them and the overhead
of optimizer implementation falls onto the user
23 complexity of gradient evaluation
a crucial question that needs to be evaluated to assess
the practicality of any qml algorithm is the quantum
from our protocol
g  ee
g 2  0
finally an application of the chernoff bound shows
that if we wish the error to be  with probability at least
1   then we can repeat the experiment a logarithmic
number of times and use majority voting to estimate
the updated parameters this results in
q1 m 2 2
q1 m 2 2
log
 11
n o
as there are m different parameters and q1 outputs
yielded by the quantum model we further have that
eg  ge22  
egk2  2g gek  gek 2  
gk  5
where o
denotes an asymptotic upper bound with
polylogarithmic multiplicative factors suppressed on
a future faulttolerant quantum device it would also be
possible to obtain a quadratic speedup in both  and m
at the cost of a longer circuit depth see eg 15 18
since the variance of the sum is the sum of the variances
and we are using the sample mean for our estimates of
the probability this means that if n samples are used
per point then
gk  
24 available classical models
vgk 
there are two builtin classical models linear and convolutional both accept arguments which specify numbers of neurons per layer and activation functions between them
however since classicalmodel is a subclass of
nnmodule from pytorch it is straightforward for a
user to implement their own model this is recommended unless configuration files from squid helpers
are utilized see sec 41
therefore we have that the mean square error of ge is at
most 2 if
q1 m
q1 m
max
note that if the variances are small then the number of
samples required will be further reduced then from
markovs inequality the number of samples needed to
estimate the gradient within error  with probability
greater than 23 will be simply given by 3 times the
estimate in eq 7 above
if a learning rate of  is used for the gradient ascent
then the error in the quantum parameters as quantified
by the euclidean distance is with probability greater
than 23 at most  we denote by  the error introduced by using the noisy estimator ge for the parameter
update
eij1 hj 
j1 hj
eie
25 available quantum models
in this section we provide details on the implementation of quantum models within squid the approach
we follow in this preliminary study is to construct the
most general models on a given set of qubits by expressing the quantum circuits as layers of structured operations acting in nearestneighbors only this allows for
both generality and a direct connection with realworld
implementations on nearterm devices with limited connectivity despite this choice the framework is general
and can be easily extended to accommodate quantum
models with a different structure
the common construction for a variational quantum
classifier see eg 6 7 19 is to start by considering
the encoding of an input ddimensional feature vector
v  0 1d into the quantum state of a register containing n  dlog2 de qubit by introducing an encoding
unitary operation we as
with k  k2 the induced euclidean norm the generators
hj used in squid see sec 25 have unit norm this
allows us to bound the error as
    
 v i  we v  0i 
therefore it follows from the fact that the error in the
  pk e
  o
output probabilities p satisfies pk 
that the value of n needed to ensure that the maximum
error  in pk after a single step of gradient ascent with
probability at least 23 obeys
q1 m 2 2
n o
with 0i a reference state in the computational basis
the encoded state i is then modified by acting with
a second unitary wv defined in terms of a set of nv
variational parameters   0 nv  the final state of
the quantum register right before measurement is then
  v i  wv w
 e v  0i  13
 v    wv 
the output of the quantum models we consider here
are the probabilites of measuring each one of the computational basis states in the state i which can be
estimated by collecting statistics over a large number
of circuit executions given that the number of possible outcomes scales exponentially in the register size a
small subset of probabilities is tipically selected in order
for the overall scheme to be scalable
the decomposition of the total unitary operation
mapping 0i to i as a function an the encoding unitary we and a variational unitary wv is however artificial and does not necessarily lead to the most efficient
scheme this is especially true in the squid framework where a classical network is devoted to optimally
determine an encoding of the classical data into a quantum state the approach we take in squid is to consider instead the m dimensional output from the classical encoder as the parameters describing a global unitary w in the quantum register without artificially distinguishing between encoding parameters and variational parameters this hibridization of the standard
approach described above is still completely general and
the global network can adjust to effectively reproduce
a factorized form w  wv we if there is a measurable
advantage for the data under analysis
we note that a generic unitary operation on n qubits
can depend on at most 4n  1 parameters which implies that we need to choose m  4n  1 for the
output of the classical encoder in practice this is not a
limitation since unitary operations which can be decomposed efficiently into a polynomial number of one and
two qubit operations will depend at most on a polynomially large number of parameters
in this first exploratory study we use a simple but
general parametrization of w in terms of a one qubit
unitary uk1    and a two qubit unitary ujk
both parametrized by 3 real parameters taking values
in 0  the unitary u1 can be written as
actly represented with the following circuit see 20 21
u01 0  0  0 
u11 1  1  1 
u01 2  2  2 
u11 3  3  3 
requiring 1 application of u01
and 4 applications of uk1
on qubit 0 and 1 for a total of 15 parameters this
construction can be readily extended to larger systems
applied
by interleaving layers of uk1 with layers of ujk
alternatively on even or odd partitions for instance
with 4 qubits we consider circuits of the following form
u01
u11
u21
u31
u01
u23
u01
u11
u21
u12
u11
u21
u31
u01
u23
u01
u11
u21
u31
note that in the costruction above we didnt include
the first and last singlequbit operations in the third u 1
layer this allows to remove redundancy in the parameters since we replace the product of two u 1 operations
with a single u 1  this simplification results in enhanced
stability in the training
3 example applications
as an initial application of our framework we present
here results for binary classification on the mnist
database 22 using digits 3 and 7 this is a standard benchmark for classification algorithms and analysis with a quantum model is made possible by the ability
of squid to compress the input features into data with
the appropriate dimensions
the input feature vectors for this dataset are real vectors of dimension 784 representing a grayscale 28  28
pixel picture in this section we will compare results
obtained with the architecture displayed in fig 2 composed by a single layer encoder with m0 units a blackbox classifier to be specified below and a single layer
decoder with m1 units the classical blackbox classifier used in this section consists of a simple single layer
network with 2 units panel b of fig 2 and we take
m1  2 for its output in the following subsections we
will also consider different implementations of quantum
classifiers with the general structure displayed in panel
c of the same figure
all of the calculations classical and quantum presented in this section were obtained using an adam optimizer as implemented in pytorch and using the hyperparameters reported in tab 1 in all case we use
uk1     exp iyk  exp izk  exp iyk   14
and we recognize the parameters    to be the euler angles in the y zy decomposition in the expression
above zk yk  denote the pauli z matrix y matrix for
qubit k and we will denote xk similarly in the follow2
ing the twoqubit unitary ujk
actin on the qubit pair
j k is instead defined as
ujk
    eixj xk iyj yk izj zk 
u01
the usefulness of these choices comes from the possibility to represent a generic unitary by applying appropriately layers of uk1 and uk2 operations for instance
a general su 4 transformation for 2 qubits can be ex5
accuracy
0995
099
median accuracy
90 boundary  val
90 boundary  train
0985
210
410
number of parameters
epochs
100
learning rate
00001
train size
9916
0988
0992
0996
accuracy
figure 3 results for the accuracy achieved on mnist with the
classical model described in the text panel a shows the accuracy as a function of the number of parameters in the model
green solid circles the other two sets of points show the location of the 90 accuracy percentile for both the validation
set red squares and the training set blue diamonds  panels
b and c show the histogram of achieved accuracies for the
smallest m0  3 and largest m0  60 models respectively
figure 2 pictorial representation of the hybrid classifier model
used for mnist classification panel a shows the complete
network including an encoder with m0 units and a decoder with
m1 units panel b shows the implemented classical classifier
composed by two units and panel c shows a schematic of the
quantum models input parameters coming from the encoder
determine the unitary w while the output is obtained upon
measurement of the qubits
batch size
bootstrap runs
bootstrap runs
ktot  in order to understand this point better we show
in the left panels the estimated histograms for accuracy
reached in our set of 48 bootstrap runs for the smallest
classical model panel b and the largest model panel
c as expected from the results in the main panel
most of the density is in the same location but for the
larger models the tails are more important
note that the dispersion in the accuracy around the
median reported in the main panel of fig 3 is relatively
small of the order of  0002 this is caused in large
part by the simplicity of the classification problem as
we can see in fig 4 obtained for a medium sized model
with m0  40 the accuracy quickly converges to a narrow interval around the mean for both the training set
and the validation set with more than 80 epochs the
accuracy for the training data set is able to reach 100
when the inner model is replaced by a quantum subroutine as depicted in panel c of fig 2 the output dimension for a quantum circuit over n qubits is
bounded by m1  2n  in the following sections we will
consider two limiting situation the maximum possible
dimension m1  2n indicated as full below and the
minimum one m1  2 indicated as min below and
corresponding to the probability of measuring a single
basis state here we choose 0i 
val size
2480
table 1 hyperparameters used for the results on mnist
48 independent optimization runs that were performed
in order to estimate the variance in the attained accuracy in the following we will refer to this ensemble as
bootstrap runs
we present the results obtained with the classical network in fig 3 the full data is available in in tab 2 of
appendix b we can see from the evolution of the median accuracy green circles that the classical network
is able to achieve classification accuracies above 99
but the increase in the number of hidden units at the
level of the input model doesnt seem to provide a statistically significant improvement on the final accuracy
the displayed error bars are 68 confidence intervals
extracted from our finite population sample
in the main panel we also show the location of the
90 accuracy percentile ie the boundary value for
which 10 of bootstrap runs provide a higher accuracy
for both the validation set red squares and the training setblue diamonds these results are consistent
with the expectation that as the number of parameters ktot in the model increases the training set can
be described almost exactly by the network while at
the same time we see that the distribution of accuracies
for the validation set does not evolve significantly with
31 separable quantum models
the first class of quantum models we consider here are
separable models with a single layer of u 1 unitaries and
are therefore fundamentally classical in that entanglement plays no role in shaping the output probabilities
validation set
0995
0995
099
099
0985
0985
6 qubit  full
median cl
median qm  min
median qm  full
0995
0985
90 accuracy boundary
accuracy
accuracy
099
098
098
098
0975
training set
training epochs
110
210
6 qubit  min
099
number of parameters
0975
100
0975
100
097
training epochs
110
210
310
410
number of parameters
figure 4 example of training of the classical model described in
the main text the left panel shows the increase in classification
accuracy for the training set as a function of the number of
epochs the right panel shows the same for the validation set
all bands are 90 confidence intervals with averages indicated
by the solid lines
510
bootstrap runs
098 0985 099 0995
bootstrap runs
accuracy
figure 5 results for the accuracy achieved on mnist with the
classical model green points and the separable quantum models described in the text red and blue points panel a shows
the accuracy as a function of the number of parameters for the
classical model green solid circles the full quantum separable
models blue solid circles and the separable quantum models
with a single output variable m1  1 red solid circles indicated by min the inset panel b shows the location of the
90 accuracy percentile for the classical green squares full
separable quantum model blue squares and minimal separable quantum model red squares panels c and d show the
histogram of achieved accuracies for the 6 qubit models with
either the full number of possible output variables m1  64
top panel and the single output bottom panel
of the quantum model the results are shown in fig 5
and the full data is available in tab 3 of appendix b
in this case at least for the models with m1  2n  we
can see a mild increase of the final accuracy as a function of the number of parametersqubits in the model
the largest model is however outperformed by classical networks with a smaller size see the blue circles
in fig 5a the models with a latent space corresponding to the restricted output for the quantum layer
shown as red circles in fig 5a show instead a deterioration as we increase the number of qubitsparameters
this effect is especially clear looking at the histograms
of achieved accuracy in the 48 bootstrap runs displayed
in the right panels of fig 5 for the largest model with
n  6 qubits employing a large output vector from
the quantum layer produces results with better than
99 for more than half of the runs while restricting the
output to a single probability prevents most runs from
reaching this threshold strikingly this is true even
for the training set not shown where only a single
bootstrap run achieved an accuracy above 99 this is
a first clear sign of the importance to supplement the
quantum classifier with a rich decoder at the possible
expense of a larger sample complexity
and there doesnt seem to be any measurable advantage
in increasing the number of parameters interestingly
for the models with restricted output size red circles
denoted qm  min in panel a of fig 6 we see that
the optimization procedure is struggling to find a good
set of parameters for the larger models and the accuracy
decreases almost monotonically with size it is possible
that better results could be obtained using directly the
accuracy as cost function instead of the crossentropy
in order to clarify that the effect we are seeing is not
coming from overfitting of the training set but really
from difficulties in exploring efficiently the energy surface we show on the left panels the evolution of the 90
accuracy percentile as a function of the number of parameters for both the validation and training set panels
b and c respectively for the 3 networks considered
here the classical feedforward network considered before green squares and the quantum models with entanglement either with the full output red squares or
the restricted output model blue squares as can be
clearly seen in panel c the optimizer is not able to find
a good parameter set for large models and the accuracy
in training decreases
32 quantum models with entanglement
we now turn to consider more general quantum models
that are capable of creating entanglement in the quantum register through the use of the twoqubit unitary
u 2 defined in sec 25 the resulting median accuracy
shown in panel a of fig 6 show a similar trend to
the simpler separable models above the accuracy of
the quantum model never exceeds the classical accuracy
these results highlight the importance of supplementing the quantum classifier with a non trivial output
decoder in order to achieve a good efficiency a possibil7
median cl
median qm  full
median qm  min
0995
0995
099
099
0985
0985
0995
098
0975
099
90 percentile cl
90 percentile qm  full
90  percentile qm  min
110
210
310
training
410
number of parameters
210
410
90 percentile
accuracy
of data to ensure that dimensions of models inputs and
outputs match this is done due to failfirst and failfast
principle a single forward pass of data is faster than
a pass of a single batch and in perspective of training
it is very cheap the code then runs a typical training
forloop with an additional call to backward function
of main model as explained in section 22 in the end
all of the results as well as configuration is saved to a
single location if bootstrap was used along with the
results for each run there is a folder with aggregated
results created for simpler analysis
helpers extension also provides very basic plotting
utilities for the results however those are meant as
a example of processing the output folders since plots
are highly dependent on studies performed
validation
90 percentile
0985
number of parameters
figure 6 results for the accuracy achieved on mnist with
the classical model green points and the quantum models
with entanglement described in the text red and blue points
panel a shows the accuracy as a function of the number of
parameters for the classical model green solid circles the full
quantum models blue solid circles and the quantum models
with a single output variable m1  1 red solid circles indicated by min the left panels show the location of the 90
accuracy percentile for the classical green squares full quantum blue squares and minimal quantum model red squares
in either the validation set panel b or the training set panel
42 future work
there is a vast amount of possible extensions to squid
some of which can be included directly in a main
project while others can be used as standalone packages the main advantage of the squid is that it allows
for abstracting communication with a specific backend
to do so however custom quantummodel subclasses interfacing with the backend api will be needed
additionally as of right now squid allows only for
a classification and regression tasks more advanced
scenarios would require changes both in squid as well
as to a larger extent squid helpers
another and much sooner addition to squid will be
to start supporting various quantum computing frameworks for example qiskit has created a great package
for optimization of quantum circuits 12 this would
fit perfectly into squid ecosystem with a translation
layer such addition would allow the user to implement
hybrid models without the explicit definition of circuits
for training provided the circuit gradient are correctly
passed by the backward function
there are also others frameworks that have either already similar behavior or plans for optimization packages modularity of squid would allow code to be
backend agnostic and work uniformly across multiple
types of quantum devices
ity that is available only if we choose to measure more
than a single qubit from the quantum device
4 squid extensions and future work
as mentioned is section 2 squid is designed with extensibility in mind this means that it should be easy to
write additional packages on top of it as well that additional features should involve changing at most few
modules in the following two sections we describe
the squid helpers package designed to allow the use
of squid using userprovided configuration files and
comment on possible extensions of the framework
41 squid helpers package
to show how such extension could function but also
to streamline the workload for use cases we provide
squid helpers extension it allows user to use yaml
configuration files to run squid code as a result bootstrapping multiple runs of multiple test cases and aggregation of the results is much easier
the conversion from configuration files to squid is
done when a file is first read and if a bootstrap option
is provided then random seed is changed during every
iteration the change of the seed is deterministic and
hence all of the results are exactly reproducible at the
initialization step there is a single batch forward pass
5 conclusions
the great success of classical machine learning algorithms in tasks such as classification together with the
expectation that quantum computers will allow us to explore algorithms in a larger complexity class than their
classical counterparts makes the exploration of the connections between these ideas a fertile ground for the
discovery of novel approaches to automated inference
in this work we have presented squid as a computational framework which allows to explore efficiently the
possible advantages of quantum computing for machine
learning purposes this is achieved by embedding the
quantum algorithm part in a more general multilayer
architecture that allows to interface classical and quantum networks while enjoying efficient optimization by
using the automatic differentiation engine provided by
pytorch while there are similar packages notably
xanadus strawberry fields 23 ibms qiskit 12
they do not offer as much flexibility as squid for example strawberry fields offers much more complete
experience with many examples and models as well
as ability to run code on quantum computer and not
a simulator however for the same reason creating a
custom model is much easier in squid similar argument can be made about qiskit with an addition that
it does not contain bindings to pytorch this generalized frameworks provides several advantages over an
either purely classical or purely quantum approach it
allows for a seamless dimensionality reduction of the
inference problem a step that would be necessary to
explore high dimensional datasets on small quantum
devices while at the same time allowing for automatic
tuning of the measurement settings needed to extract
information from the quantum state produced by the
algorithm this latter feature implemented in squid
by using a classical network as decoder after the central model as shown in fig 1 is extremely important
in order to reach high precisions the use of an explicit decoder at the output of the quantum model allows for a more careful optimization of the tradeoff
between measuring only vanishingly small fraction of
the possible output probabilities on one hand with the
drawback that entanglement can start to be detrimental
due to information scrambling see eg 24 25 and a
full measurement of the probability distribution on the
computational basis states which will require an exponential number of repetitions in this work we used a
simple classification problem from the mnist database
to show the effect of this tradeoff for a concrete highdimensional problem
been identified with a simplified approach as the one
currently implemented in squid a successive study of
the sample complexity along the lines of the derivation
presented in sec 23 will be needed to assess the practical viability of the algorithm extensions to implement the effect of finite statistics together with more
advanced effects such as models of decoherence for a
specific target device can be added efficiently within
squid and we plan to explore their impact on classification problems in future work
acknowledgements
the work of a roggero was supported by the inqubator for quantum simulation under us doe grant no
desc0020970 and by the institute for nuclear theory
under us department of energy grant no defg0200er41132 the work of j filipek was supported in
part by a washington research foundation fellowship
at the university of washington the work of sc hsu
is supported by the us department of energy office
of science office of early career research program under award number desc0015971 support for nathan
wiebe was provided by the laboratory directed research and development program at pacific northwest
national laboratory a multiprogram national laboratory operated by battelle for the us department of
energy release no pnnlsa157287 and the theoretical work on this project by nw was supported by the
us department of energy office of science national
quantum information science research centers codesign center for quantum advantage under contract
number desc0012704 additional support for nathan
wiebe was provided by google research award
references
1 vedran dunjko and peter wittek a nonreview
of quantum machine learning trends and explorations quantum views 432 march 2020
doi 1022331qv2020031732 url https
doiorg1022331qv2020031732
2 nathan wiebe key questions for the quantum machine learner to ask themselves new
journal of physics 229091001 sep 2020
doi 10108813672630abac39 url https
doiorg10108813672630abac39
3 wen guan gabriel perdue arthur pesah maria
schuld koji terashi sofia vallecorsa and jeanroch vlimant quantum machine learning in
high energy physics machine learning science
and technology 21011003 mar 2021 issn
thanks to the generality of the architecture developed in this work future explorations of algorithms
with entanglement but with a classically efficient representation such as tensor network states with polynomially large bond dimension see eg 26 27 could be
carried out within squid with only minimal modifications to the code we expect the added flexibility in
interchanging classical and quantum components in a
global classifier to prove valuable in identifying promising datasets and inference problems where the presence
of entanglement and quantum correlations can provide
important accuracy gains once these problems have
26322153 doi 10108826322153abc17d url
httpdxdoiorg10108826322153abc17d
patrick rebentrost masoud mohseni and seth
lloyd quantum support vector machine for big
data classification phys rev lett 113130503
sep 2014 doi 101103physrevlett113130503
url
httpslinkapsorgdoi101103
physrevlett113130503
jacob biamonte peter wittek nicola pancotti
patrick rebentrost nathan wiebe and seth
lloyd quantum machine learning nature 549
195202 2017 doi 101038nature23474 url
httpsdoiorg101038nature23474
maria schuld and nathan killoran
quantum machine learning in feature hilbert
spaces
phys rev lett 122040504 feb
2019
doi 101103physrevlett122040504
url
httpslinkapsorgdoi101103
physrevlett122040504
vojtch havlek antonio d crcoles kristan
temme aram w harrow abhinav kandala
jerry m chow and jay m gambetta supervised learning with quantumenhanced feature
spaces nature 5677747209212 mar 2019
issn 14764687 doi 101038s4158601909802
url httpdxdoiorg101038s4158601909802
evan peters joo caldeira alan ho stefan leichenauer masoud mohseni hartmut
neven panagiotis spentzouris doug strain and
gabriel n perdue machine learning of high dimensional data on a noisy quantum processor
arxiv 210109581 2021
martn abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg s
corrado andy davis jeffrey dean matthieu
devin sanjay ghemawat ian goodfellow andrew
harp geoffrey irving michael isard yangqing
jia rafal jozefowicz lukasz kaiser manjunath
kudlur josh levenberg dandelion man rajat monga sherry moore derek murray chris
olah mike schuster jonathon shlens benoit
steiner ilya sutskever kunal talwar paul
tucker vincent vanhoucke vijay vasudevan fernanda vigas oriol vinyals pete warden martin
wattenberg martin wicke yuan yu and xiaoqiang zheng tensorflow largescale machine
learning on heterogeneous systems 2015 url
httpswwwtensorfloworg software available from tensorfloworg
adam paszke sam gross francisco massa
adam lerer james bradbury gregory chanan
et al
pytorch an imperative style highperformance deep learning library in h wallach
h larochelle a beygelzimer f dalchbuc
e fox and r garnett editors advances in
neural information processing systems 32 pages
80268037 curran associates inc 2019 url
httppapersnipsccpaper9015pytorchanimperativestylehighperformancedeeplearninglibrarypdf
robert s smith michael j curtis and william j
zeng a practical quantum instruction set architecture 2016
hctor abraham et al qiskit an opensource
framework for quantum computing 2019
quantum ai team and collaborators cirq october 2020
url httpsdoiorg105281
zenodo4062499
charles r harris k jarrod millman stefan j
van der walt ralf gommers pauli virtanen
david cournapeau et al array programming
with numpy nature 5857825357362 september 2020 doi 101038s4158602026492 url
httpsdoiorg101038s4158602026492
gilles brassard peter hyer michele mosca and
alain tapp quantum amplitude amplification
and estimation quantum computation and information page 5374 2002 issn 02714132
doi 101090conm30505215 url http
dxdoiorg101090conm30505215
yohichi suzuki shumpei uno rudy raymond
tomoki tanaka tamiya onodera and naoki yamamoto amplitude estimation without phase estimation quantum information processing 192
jan 2020 issn 15731332 doi 101007s1112801925652 url httpdxdoiorg101007
s1112801925652
dmitry grinko julien gacon christa zoufal and
stefan woerner iterative quantum amplitude estimation npj quantum information 71 mar
2021 issn 20566387 doi 101038s41534021003791 url httpdxdoiorg101038
s41534021003791
andrs gilyn srinivasan arunachalam and
nathan wiebe optimizing quantum optimization
algorithms via faster quantum gradient computation in proceedings of the thirtieth annual acmsiam symposium on discrete algorithms pages
14251444 siam 2019
maria schuld alex bocharov krysta m svore
and nathan wiebe
circuitcentric quantum
classifiers
phys rev a 101032308 mar
2020
doi 101103physreva101032308
url
httpslinkapsorgdoi101103
physreva101032308
g vidal and c m dawson
universal
quantum circuit for twoqubit transformations
a gradient evaluation for variational
quantum models
with three controllednot gates
phys rev
a 69010301 jan 2004 doi 101103physreva69010301 url httpslinkapsorg
doi101103physreva69010301
farrokh vatan and colin williams optimal quantum circuits for general twoqubit gates phys
rev a 69032315 mar 2004 doi 101103physreva69032315 url httpslinkapsorg
doi101103physreva69032315
y lecun l bottou y bengio and p haffner
gradientbased learning applied to document
recognition proceedings of the ieee 86112278
2324 1998 doi 1011095726791
nathan killoran josh izaac nicols quesada
ville bergholm matthew amy and christian
weedbrook strawberry fields a software platform for photonic quantum computing quantum 3129 mar 2019
issn 2521327x
doi 1022331q20190311129 url http
dxdoiorg1022331q20190311129
huitao shen pengfei zhang yizhuang you and
hui zhai information scrambling in quantum neural networks phys rev lett 124200504 may
2020
doi 101103physrevlett124200504
url
httpslinkapsorgdoi101103
physrevlett124200504
carlos ortiz marrero mria kieferov and
nathan wiebe entanglement induced barren
plateaus 2021
ding liu shiju ran peter wittek cheng
peng raul blzquez garca gang su and
maciej lewenstein machine learning by unitary tensor network of hierarchical tree structure
new journal of physics 217073059 jul 2019
doi 10108813672630ab31ef url https
doiorg10108813672630ab31ef
chase roberts ashley milsted martin ganahl
adam zalcman bruce fontaine yijian zou jack
hidary guifre vidal and stefan leichenauer tensornetwork a library for physics and machine
learning 2019
k mitarai m negoro m kitagawa and k fujii
quantum circuit learning
physical review a 983 sep 2018
issn 24699934
doi 101103physreva98032309 url http
dxdoiorg101103physreva98032309
maria schuld ville bergholm christian gogolin
josh izaac and nathan killoran evaluating analytic gradients on quantum hardware physical review a 993 mar 2019 issn 24699934
doi 101103physreva99032331 url http
dxdoiorg101103physreva99032331
in this appendix we provide a derivation of the scheme
employed in the main text to evaluate gradients with
respect to the parameters of the quantum models following the discussion in sec 25 of the main text a
generic variational quantum model is defined by a unitary transformation w w
 dependent on m real parameters w
 taking values in 0  the general construction employed in this work consists in decomposing the full unitary operation w w
 into a combination
of one and two qubit unitaries denoted by uk1   
and ujk
   respectively the subscript indices indicate the qubit or pairs of qubits the unitary acts
upon given the simple structure of a generic circuit as
the one depicted in eq 25 one can obtain a closed
form expression for every component of the gradient by
looking at the individual gradients of the basic unitaries
u 1 and u 2 directly here we will use as a concrete example the generic su 4 unitary transformation from
eq 25 which we reproduce here for convenience
u01 0  0  0 
u11 1  1  1 
u01
u01 2  2  2 
u11 3  3  3 
note that in this case m  15 in the following we
will assume without loss of generality that this unitary
operation is applied to the initial state 00i of the two
qubit register and denote the resulting state by
 wi
  w w
 0i 
the classical output generated by a measurement on
the qubit register can be completely characterized by
the 4 probabilities to find the system in each one of the
possible basis states
pk  t r k  wih
where we have introduced explicitly the projectors
0 00ih00 1 01ih01
2 10ih10 3 11ih11 
computing the derivatives with respect to the 12 angles corresponding to the 4 one qubit su 2 operations
is straightforward by recalling the definition eq 14 of
u 1 in terms of exponentials of pauli operators as an
example the derivative with respect to 0 of any of the
probabilities in eq 17 can be written explicitly as
model
pk 
h00w  w
 k w w00i
ih00w  w
 k w wy
 0 00i
 ih00y0 w  w
 k w w00i
  2i h00w w
 k w wy
 0 00i
w w
2r h00w  w
00i 
tr90
09966
09973
09980
09994
vr90
09951
09947
09967
09952
09950
09964
rekm  r hkw w
 0m 00i
eikm  i hkw w
 0m 00i 
where we used the compact notation w w
 0m to indicate
the derivative with respect to the mth parameter this
requires a total of 2km independent circuit executions
for a total of 2km  1 observables the gradient can
then be computed as
2  2  2  3  3  3
accuracy
099154
0990320
0988144
098958
0991112
0989116
with ki the computational basis state associated to the
projector k  these expectation values can be estimated using an hadamard test with one additional ancilla qubit and require the execution of 2k independent
circuits one each for real and imaginary part
for each one of the m parameters we then use additional 2k hadamard tests to estimate the expectation
values associated with the shifted unitaries
with a new set of parameters given by
 0 0   0  0  0   1  1  1 
ktot
2366
4727
9449
18893
31485
47225
table 2 results for the classical feedforward models described
in the main text
where i r indicating the imaginary real part note
that for all of the 12 parameters characterizing the single qubit transformations the derivative of the full variational circuit unitary w can be expressed in terms of
the same parametrized unitary with the appropriate angle angle shifted by 2 for the case of 0 considered
above we have for instance
w w
 iw wy
 0  w w
 0 0 
pk 2r h00w  w
 k w w
 0m 00i
2 rk rekm  ikeikm 
using the optimal implementation for the more general
su 4 transformation derived in ref 21 see fig6
there one can show that we have the same property
for the 2 qubit unitary ujk
 this property is usually
referred to as the parameter shift rule 28 29
in order to estimate the expectation values in the
last line of eq 19 we can employ two strategies if
the required number of output probabilities k is the
maximum possible one with n qubits ie k  2n  it
is convenient to first decompose the projectors in the
computational basis states into a linear combination of
k  2n diagonal operators obtained by considering all
the possible tensor products of identities and pali z and
then to evaluate each one of the resulting expectation
values using a single hadamard test each the total
number of separate circuits required for this approach
is then km  with m the total number of parameters
in the more realistic situation where k fi 2n instead
the strategy just described will still require an exponential number of measurement in the size of the qubit register a more efficient alternative can be obtained by
evaluating explicitly the k pairs of expectation values
rk  r h00w  wki
ik  i h00w  wki
an alternative approach to reduce the number of independent circuits needed for gradient evaluation is to
use expectation values of unitary operators instead of
projectors this extension can be easily implemented
within the squid framework
b additional information on the mnist
benchmark
we report in tab 2 the parameters and results for the
classical models used in the mnist classification discussed in sec 3 and corresponding to the results presented in fig 3 on the main text the last two columns
in tab 2 denoted by tr90 and vr90 show the boundary value for the 90 accuracy percentile the latter
refers to the validation data while the former to the
training set the estimated errors correspond to a 68
confidence interval
the parameters of the separable quantum models
considered in the main text together with the results
accuracy
0983730
0984732
0985416
0985926
0983926
0986924
09812538
tr90
09898
09920
09921
09948
09905
09948
09881
training set
vr90
09872
09889
09887
09891
09868
09904
09858
ktot
7070
7068
11780
11778
23567
23553
30632
30618
44762
44748
35390
35328
accuracy
0987912
0986522
0987518
0985924
0988928
0984724
0988524
0983318
0988322
0981024
0989518
0982332
tr90
09940
09933
09960
09926
09987
09897
09984
09867
09993
09863
09995
09871
098
098
096
096
094
094
full output
092
092
table 3 results for the first set of separable quantum models
described in the text the parenthesis in the label for the
quantum models indicates the number of qubits n employed
model
qe2
qe1 2
qf 2
qf1 2
qg4
qg1 4
qh4
qh1 4
qi4
qi1 4
ql6
ql1 6
validation set
098
098
096
096
single output
094
vr90
09899
09892
099
09893
09924
09879
09915
09854
09911
09840
09920
09867
094
training epochs
100
training epochs
figure 7 example of training of the largest quantum model
described in the main text the left panels show the increase
in classification accuracy for the training set as a function of
the number of epochs the right panels shows the same for
the validation set the top panels are for the full model with
the maximum number of outputs m1  16 while the bottom
panels are for the minimal model with a single output the
dashed black line indicates 99 accuracy all bands are 90
confidence intervals with means indicated by the solid lines
table 4 results for the set of quantum models described in
the text the parenthesis in the label for the quantum models
indicates the number of qubits n employed
median cl
median qm  full
median qm  min
0995
4 qubit max  full
obtained from training on the mnist classification
problem are presented in tab 3 the models with a
latent space corresponding to the restricted output for
the quantum layer are indicated with a subscript 1 in
the table
the same convention is used in tab 4 where we
present the parameters and results for the quantum
models with entanglement described in sec 32 we
also show in fig 7 the evolution of the accuracy for both
the training set left panels and validation set right
panels the top two panels are obtained with models
with maximal output size m1  16 in this case while
the bottom panels show the results using a restricted
output model with m1  1
finally we present in fig 8 an extension of the results
presented in fig 6 where in the left panels we show the
accuracy histograms for the largest model considered in
this work for both the full output model panelc and
the restricted output model paneld
0985
098
0975
097
0965
90 accuracy boundary
accuracy
099
210
410
4 qubit max  min
0995
099
0985
number of parameters
110
210
bootstrap runs
ktot
2358
4715
4713
9437
9423
14195
14133
310
410
number of parameters
bootstrap runs
accuracy
accuracy
model
qa1
qb2
qb1 2
qc4
qc1 4
qd6
qd1 6
0975 098 0985 099 0995 1
accuracy
figure 8 results for the accuracy achieved on mnist with
the classical model green points and the quantum models
with entanglement described in the text red and blue points
panel a shows the accuracy as a function of the number of
parameters for the classical model green solid circles the full
quantum models blue solid circles and the quantum models
with a single output variable m1  1 red solid circles indicated by min the inset panel b shows the location of
the 90 accuracy percentile for the classical green squares
full quantum blue squares and minimal quantum model red
squares with a single output panels c and d show the histogram of achieved accuracies for the largest quantum model
considered using n  4 qubits with either the full number of
possible output variables m1  16 top panel and the single
output bottom panel
