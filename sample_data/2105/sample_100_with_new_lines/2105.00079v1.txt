improving response quality with backward reasoning
in opendomain dialogue systems
ziming li
julia kiseleva
maarten de rijke
zliuvanl
university of amsterdam
amsterdam the netherlands
juliakiselevamicrosoftcom
microsoft
redmond united states
mderijkeuvanl
university of amsterdam  ahold
delhaize
amsterdam the netherlands
arxiv210500079v1 cscl 30 apr 2021
abstract
being able to generate informative and coherent dialogue responses
is crucial when designing humanlike opendomain dialogue systems encoderdecoderbased dialogue models tend to produce
generic and dull responses during the decoding step because the
most predictable response is likely to be a noninformative response
instead of the most suitable one to alleviate this problem we propose to train the generation model in a bidirectional manner by
adding a backward reasoning step to the vanilla encoderdecoder
training the proposed backward reasoning step pushes the model
to produce more informative and coherent content because the forward generation steps output is used to infer the dialogue context
in the backward direction the advantage of our method is that
the forward generation and backward reasoning steps are trained
simultaneously through the use of a latent variable to facilitate
bidirectional optimization our method can improve response quality without introducing side information eg a pretrained topic
model the proposed bidirectional response generation method
achieves stateoftheart performance for response quality
ccs concepts
 information systems  chat question answering
keywords
opendomain dialogue system response generation
acm reference format
ziming li julia kiseleva and maarten de rijke 2021 improving response
quality with backward reasoning in opendomain dialogue systems in
proceedings of the 44th international acm sigir conference on research and
development in information retrieval sigir 21 july 1115 2021 virtual
event canada acm new york ny usa 5 pages httpsdoiorg101145
34048353463004
introduction
recently developed endtoend dialogue systems are trained using
large volumes of humanhuman dialogues to capture underlying
interaction patterns 4 10 14 16 29 35 38 a commonly used
permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page copyrights for components of this work owned by others than the
authors must be honored abstracting with credit is permitted to copy otherwise or
republish to post on servers or to redistribute to lists requires prior specific permission
andor a fee request permissions from permissionsacmorg
sigir 21 july 1115 2021 virtual event canada
 2021 copyright held by the ownerauthors publication rights licensed to acm
acm isbn 97814503803792107   1500
httpsdoiorg10114534048353463004
hi how are you
context
im good
whats the weather tomorrow
query
i dont know response a
reply
its cloudy
response b
figure 1 a more informative response response b in the
figure can provide information that helps to infer the query
content given the dialogue context
approach to designing datadriven dialogue systems is to use an
encoderdecoder framework feed the dialogue context to the encoder and let the decoder output an appropriate response building
on this foundation different directions have been explored to design
dialogue systems that tend to interact with humans in a coherent
and engaging manner 2 15 19 31 34 37 however despite significant advances there is still room for improvement in the quality
of machinegenerated responses
an important problem with encoderdecoder dialogue models
is their tendency to generate generic and dull responses such
as i dont know or im not sure 2 9 14 15 there are two
types of methods for dealing with this problem the first introduces updating signals during training such as modeling future
rewards eg ease of answering by applying reinforcement learning 15 19 or bringing variants or adding constraints to the decoding step 2 14 31 the second type holds that by itself the dialogue
history is not enough for generating highquality responses and
side information should be taken into account such as topic information 34 35 or personal user profiles 37 solutions relying on
large pretrained language models such as dialogpt 38 can be
classified into the second family as well
in this paper we propose to train dialogue generation models
bidirectionally by adding a backward reasoning step to the vanilla
encoderdecoder training process we assume that the information
flow in a conversation should be coherent and topicrelevant given
the dialogue history neighboring turns are supposed to have a tight
topical connection to infer the partial content of one turn given
the previous turn and vice versa inferring the next turn given
the previous conversation history and the current turn is the
traditional take on the dialogue generation task we extend it by
adding one more step given the dialogue history and the next
turn we aim to infer the content of the current turn we call the
latter step backward reasoning we hypothesize that this can push
the generated response to be more informative and coherent it is
unlikely to infer the dialogue topic given a generic and dull response
in the backward direction an example is shown in figure 1 given
the dialogue context and query1 we can predict the reply following
a traditional encoderdecoder dialogue generation setup in contrast
we can infer the content of query given the context and reply
as long as the reply is informative inspired by zheng et al 39
we introduce a latent space as a bridge to simultaneously train
the encoderdecoder model from two directions our experiments
demonstrate that the resulting dialogue generation model called
mirror benefits from this bidirectional training process
overall our work provides the following contributions
c1 we introduce a dialogue generation model mirror for generating high quality responses in opendomain dialogue systems
c2 we define a new way to train dialogue generation models bidirectionally by introducing a latent variable and
c3 we obtain improvements in terms of dialogue generation performance with respect to human evaluation on two datasets
related work
conversational scenarios being considered today are increasingly
complex going beyond the ability of rulebased dialogue systems 30
ritter et al 22 propose a datadriven approach to generate responses building on phrasebased statistical machine translation
neural networkbased models have been studied to generate more
informative and interesting responses 23 26 29 serban et al 24
introduce latent stochastic variables that span a variable number
of time steps to facilitate the generation of long outputs deep reinforcement learning methods have also been applied to generate
coherent and interesting responses by modeling the future influence of generated responses 15 19 retrievalbased methods are
also popular in building dialogue systems by learning a matching
model between the context and predefined response candidates
for response selection 7 20 28 33 our work focuses on response
generation rather than selection
since encoderdecoder models tend to generate generic and dull
responses li et al 14 propose using maximum mutual information
as the objective function in neural models to generate more diverse
responses xing et al 35 consider incorporating topic information
into the encoderdecoder framework to generate informative and
interesting responses to address the dullresponse problem baheti
et al 2 propose incorporating side information in the form of
distributional constraints over the generated responses su et al
27 propose a new perspective to diversify dialogue generation by
leveraging nonconversational text recently pretrained language
models such as gpt2 21 bert 5 xlnet 36 have been proved
effective for a wide range of natural language processing tasks
several authors make use of pretrained transformers to attain
performance close to humans both in terms of automatic and human
evaluation 6 32 38 though pretrained language models can
perform well for general dialogue generation they may become
less effective without enough data or resources to support these
1 we
use query to distinguish the current dialogue turn from the context and the
response query is not necessarily a real query or question as considered in search or
questionanswering tasks
models pretraining in this work we show the value of developing
dialogue generation models with limited data and resources
the key distinction compared to previous efforts 2 14 is our
work is the first to use the original training dataset through a differentiable backward reasoning step without external information
3 method mirror
31 problem setting
in many conversational scenarios the dialogue context is relatively
long and contains a lot of information while the reply response is
short and from a different speaker this makes it difficult to predict
the information in the context by only relying on the response in
the backward direction therefore we decompose the dialogue
context into two different segments the context c and query x
figure 1 assuming that we are predicting the response at turn t
in a dialogue the context c will consist of the dialogue turns from
t  m to t  2 and the query x corresponds to turn t  1 here
we use the term query to distinguish the dialogue turn at time
step t  1 from the context c and response y as explained before
the term query should not be confused with a query or question
as in search or questionanswering tasks the value m indicates
how many dialogue turns we keep in the context c we use call to
represent the concatenation of c and x which is also the original
context before being decomposed our final goal is to predict the
response y given dialogue context c and query x
mirrorgenerative dialogue generation
shen et al 25 propose to maximize the conditional log likelihood
of generating response y given context call  log p y  call  and
they introduce a latent variable z to group different valid responses
according to the context call  the lower bound of log p y  call  is
given as
log p y  call   ezq z call y log p y  call  z 
d kl q z  call  yp z  call 
in eq 1 q z  call  y is the posterior network while p z  call 
is the prior one
instead of maximizing the conditional log likelihood log p y 
call  we propose to maximize log p x y  c representing the
conditional likelihood that x y appears together given dialogue
context c the main assumption underlying this change is that in
a conversation the information flow between neighboring turns
should be coherent and relevant and this connection should be
bidirectional for example it is not possible to infer what the query
is about when a generic and noninformative reply i dont know is
given as shown in figure 1 by taking into account the information
flow from two different directions we hypothesize that we can build
a closer connection between the response and the dialogue history
and generate more coherent and informative responses therefore
we propose to optimize log p x y  c instead of log p y  call 
following 12 25 we choose to maximize the variational lower
bound of log p x y  c which is given as
log p x y  c  ezq z cxy log p x y  c z 
d kl q z  c x yp z  c
enc
dec
rec
net
prior
net
enc
dec
dec
dec
figure 2 the main architecture of our model mirror it consists of three steps information encoding latent variable
generation and target decoding
enc
where z is a shared latent variable between context c query x and
response y next we explain how we optimize a dialogue system by
maximizing the lower bound shown in eq 2 from two directions
321 forward generation in dialogue generation with respect to
the forward dialogue generation we interpret the conditional likelihood log p x y  c z in the forward direction
log p x y  c z  log p y  c z x  log p x  c z
therefore we can rewrite eq 2 in the forward direction as
log p x y  c
 ezq z cxy log p y  c x z  log p x  c z
 d kl q z  c x yp z  c
we introduce q z  c x y as the posterior network also referred
to as the recognition net and p z  c as the prior network
322 backward reasoning in dialogue generation as in the forward
direction if we decompose the conditional likelihood log p x y 
c z in the backward direction we can rewrite eq 2 as
log p x y  c
 ezq z cxy log p x  c y z  log p y  c z
 d kl q z  c x yp z  c
323 optimizing dialogue systems bidirectionally since the variable z is sampled from the shared latent space between forward
generation and backward reasoning steps we can regard z as a
bridge to connect the training in two different direction and this
opens the possibility to train dialogue models effectively by merging eq 4 and eq 5 we can rewrite the lower bound eq 2 as
log p x y  c  ezq z cxy
log p x  c z y
 log p y  c z  log p y  c z x
 log p x  c z  d kl q z  c x yp z  c
 lc x y  
which is the final loss function for our dialogue generation model
324 model architecture the complete architecture of the proposed joint training process is shown in figure 2 it consists of
three steps 1 information encoding 2 latent variable generation
and 3 target decoding with respect to the information encoding
step we utilize a context encoder encctx to compress the dialogue
context c while an utterance encoder encutt is used to compress
the query x and response y respectively to model the latent variable z we assume z follows the multivariate normal distribution
the posterior network q z  c x y  n   2 i  and the prior
network p z  c  n    2 i  then by applying the reparameterization trick 12 we can sample a latent variable z from the
estimated posterior distribution n   2 i  during testing we use
the prior distribution n    2 i  to generate the variable z the
kldivergence distance is applied to encourage the approximated
posterior n   2 i  to be close to the prior n    2 i  according
to eq 6 the decoding step in the right side of figure 2 consists
of four independent decoders dec 1  dec 2  dec 3  and dec 4  corresponding to log p y  c z x log p x  c z log p x  c z y and
log p y  c z respectively decoder dec 1 is used to generate the
final response during the testing stage to make full use of the variable z we attach it to the input of each decoding step since we have
the shared latent vector z as a bridge training for the two directions is not independent and updating one direction will definitely
improve the other direction as well in the end both directions will
contribute to the final dialogue generation process
4 experimental setup
41 datasets
we use two datasets first the movietriples dataset 23 has been developed by expanding and preprocessing the moviedic corpus 3
of film transcripts and each dialogue consists of 3 turns between
two speakers we regard the first turn as the dialogue context while
the second and third one as the query and response respectively
in the final dataset there are around 166k dialogues in the training
set 21k in the validation set and 20k in the test set in terms of the
vocabulary table size we set it to the top 20k most frequent words
in the dataset
second the dailydialog dataset 18 is a highquality multiturn
dialogue dataset we split the dialogues in the original dataset into
shorter dialogues by every three turns as a new dialogue the last
turn is used as the target response and the first as the context and
the third one as the query after preprocessing we have 65k 6k and
6k dialogs in the training testing and validation sets respectively
we limit the vocabulary table size to the top 20k most frequent
words for the dailydialog dataset
baselines
seq2seqatt this is a lstmbased 8 dialogue generation model
with attention mechanism 1
hred this method 23 uses a hierarchical recurrent encoderdecoder to sequentially generate the tokens in the replies
vhred this extension of hred incorporates a stochastic latent
variable to explicitly model generative processes that possess
multiple levels of variability 24 this is also the model trained
with eq 1
mmi this method first generates response candidates on a seq2seq
model trained in the direction of contexttotarget p y  c x
then reranks them using a separately trained seq2seq model
in the direction of targettocontext p x  y to maximize the
mutual information 14
training details
we implement our model mirror 2  with pytorch in the opennmt
framework 13 the utterance encoder is a twolayer lstm 8
and the dimension is 1000 the context encoder has the same architecture as the utterance encoder but the parameters are not shared
the four decoders have the same design but independent parameters and each one is a twolayer lstm with 1000 dimensions
in terms of the dimension of the hidden vector z we set it to 160
for the dailydialog dataset while 100 for movietriples the word
embedding size is 200 for both datasets we use adam 11 as the
optimizer the initial learning rate is 0001 and learning rate decay
is applied to stabilize the training process
evaluation
we conduct a human evaluation on amazon mturk guided by 17
for each twoway comparison of dialogue responses against mirror we ask annotators to judge which of two responses is more
appropriate given the context for each method pair mirror baseline and each dataset we randomly sample 200 dialogues from the
test datasets each pair of responses is annotated by 3 annotators
2 codebase
httpsgithubcomcszmlimirrorsigir
wins
losses
ties
mirror vs seq2seqattn
mirror vs hred
mirror vs vhred
mirror vs mmi
mirror vs dc
mirror vs dcmmi
053
041
045
048
050
039
037
040
038
042
033
035
010
019
017
010
017
026
mirror vs seq2seqattn
mirror vs hred
mirror vs vhred
mirror vs mmi
mirror vs dc
mirror vs dcmmi
050
049
048
040
045
047
026
032
037
034
038
035
024
019
015
026
017
018
the effectiveness of maximizing mutual information in improving
the response quality the mirror method can be treated as a way to
maximize mutual information implicitly the advantage is that we
can train dialogue models in two directions simultaneously
table 2 example generated responses by different models
when the dialogue context is given
context
speaker a here  s my license 
speaker b i  m afraid i  m going to have to ask you to
remain in the apartment  the narcotics squad will be arriving
any moment now  they want to ask you a few questions 
response
results and analysis
in table 1 we show performance comparisons between mirror and
other baselines on two different datasets according to table 1top
it is somewhat unexpected to see that hred can achieve such close
performance compared to mirror on dailydialog given its main
architecture is a hierarchical encoderdecoder model we randomly
sample some dialogue pairs for which hred outperforms mirror
to see why annotators prefer hred over mirror for many of these
cases mirror fails to generate appropriate responses while hred
returns generic but still acceptable responses given the context
when we have the back reasoning step in mirror we expect that it
will lead to more informative generations still it also increases the
risk of generating responses with incorrect syntax or relevant but
inappropriate responses a possible reason for the latter is that the
backward reasoning step has dominated the joint training process
which can degenerate the forward generation performance
the performance gap between mirror and all approaches including hred is large on the dailydialog dataset see table 1bottom
due to space limitations we only present one dialogue example in
table 2 the example is a typical case of why the response generated
by dc has high embedding scores but the human evaluation result
is not promising in this example the response from dc has high semantic similarity with the context because of words like ask you
apartment and questions however it cannot be regarded as an
appropriate and meaningful response in the given context comparing mirror with methods that have use mmi mmi dcmmi
the performance gap is relatively small this is evidence showing
method pair
a movietriple
table 1 human evaluation using the movietriple and dailydialog datasets
b dailydialog
dc this method incorporates side information in the form of distributional constraints including topic constraints and semantic
constraints 2
dcmmi this method is a combination of mmi and dc where
the decoding step takes into account mutual information together
with the proposed distribution constraints in the method dc
reference squad  what do they want with me 
i don  t even use aspirin 
seq2seq no 
hred i don  t think so 
vhred oh  i  m sorry 
mmi i  m sorry  i  m sorry  i don  t know what you  re
talking about  i don  t know what i  m afraid of 
dc i  m not going to ask you  but he will be in the apartment for
a moment  and we can have some questions with that one of them 
dcmmi i  m going to ask you 
mirror well  i  m sure they  ll have to wait 
conclusion and future work
we have presented a novel approach to generating informative
and coherent responses in opendomain dialogue systems called
mirror first we reformulate the original response generation task
from two sides context and response to three sides context query
and response given the dialogue context and query predicting the
response is exactly like the traditional dialogue generation setup
thus mirror has one more step inferring the query given the dialogue context and response by incorporating the backward reasoning step we implicitly push the model to generate responses that
have closer connections with the dialogue history by conducting
experiments on two datasets we have demonstrated that mirror
improves the response quality compared to several competitive
baselines without incorporating additional sources of information
which comes with additional computational costs and complexity for future work mirrors bidirectional training approach can
be generalized to other domains such as taskoriented dialogue
systems and questionanswering tasks
references
1 dzmitry bahdanau kyunghyun cho and yoshua bengio 2014 neural machine translation by jointly learning to align and translate arxiv preprint
arxiv14090473 2014
2 ashutosh baheti alan ritter jiwei li and bill dolan 2018 generating more
interesting responses in neural conversation models with distributional constraints arxiv preprint arxiv180901215 2018
3 rafael e banchs 2012 moviedic a movie dialogue corpus for research and
development in proceedings of the 50th annual meeting of the association for
computational linguistics short papersvolume 2 association for computational
linguistics 203207
4 siqi bao huang he fan wang hua wu and haifeng wang 2019 plato pretrained dialogue generation model with discrete latent variable arxiv preprint
arxiv191007931 2019
5 jacob devlin mingwei chang kenton lee and kristina toutanova 2018 bert
pretraining of deep bidirectional transformers for language understanding
arxiv preprint arxiv181004805 2018
6 sergey golovanov rauf kurbanov sergey nikolenko kyryl truskovskyi alexander tselousov and thomas wolf 2019 largescale transfer learning for natural
language generation in proceedings of the 57th annual meeting of the association
for computational linguistics 60536058
7 jiachen gu tianda li quan liu zhenhua ling zhiming su si wei and
xiaodan zhu 2020 speakeraware bert for multiturn response selection in
retrievalbased chatbots in proceedings of the 29th acm international conference
on information  knowledge management 20412044
8 sepp hochreiter and jrgen schmidhuber 1997 long shortterm memory neural
computation 9 8 1997 17351780
9 shaojie jiang and maarten de rijke 2018 why are sequencetosequence models
so dull understanding the lowdiversity problem of chatbots in proceedings
of the 2018 emnlp workshop scai the 2nd international workshop on searchoriented conversational ai acl
10 chandra khatri behnam hedayatnia anu venkatesh jeff nunn yi pan qing liu
han song anna gottardi sanjeev kwatra sanju pancholi et al 2018 advancing
the state of the art in open domain dialog systems through the alexa prize
arxiv preprint arxiv181210757 2018
11 diederik p kingma and jimmy ba 2014 adam a method for stochastic optimization arxiv preprint arxiv14126980 2014
12 diederik p kingma and max welling 2013 autoencoding variational bayes
arxiv preprint arxiv13126114 2013
13 guillaume klein yoon kim yuntian deng jean senellart and alexander m
rush 2017 opennmt opensource toolkit for neural machine translation in
proc acl
14 jiwei li michel galley chris brockett jianfeng gao and bill dolan 2015 a
diversitypromoting objective function for neural conversation models arxiv
preprint arxiv151003055 2015
15 jiwei li will monroe alan ritter michel galley jianfeng gao and dan jurafsky
2016 deep reinforcement learning for dialogue generation arxiv preprint
arxiv160601541 2016
16 jiwei li will monroe tianlin shi sbastien jean alan ritter and dan jurafsky
2017 adversarial learning for neural dialogue generation in proceedings of the
2017 conference on empirical methods in natural language processing 21572169
17 margaret li jason weston and stephen roller 2019 acuteeval improved
dialogue evaluation with optimized questions and multiturn comparisons
arxiv preprint arxiv190903087 2019
18 yanran li hui su xiaoyu shen wenjie li ziqiang cao and shuzi niu 2017
dailydialog a manually labelled multiturn dialogue dataset arxiv preprint
arxiv171003957 2017
19 ziming li julia kiseleva and maarten de rijke 2019 dialogue generation from
imitation learning to inverse reinforcement learning in proceedings of the aaai
conference on artificial intelligence vol 33 67226729
20 lisong qiu yingwai shiu pingping lin ruihua song yue liu dongyan zhao
and rui yan 2020 what if bots feel moods in proceedings of the 43rd international acm sigir conference on research and development in information
retrieval 11611170
21 alec radford jeffrey wu rewon child david luan dario amodei and ilya
sutskever 2019 language models are unsupervised multitask learners openai
blog 1 8 2019 9
22 alan ritter colin cherry and william b dolan 2011 datadriven response
generation in social media in proceedings of the conference on empirical methods
in natural language processing association for computational linguistics 583
593
23 iulian vlad serban alessandro sordoni yoshua bengio aaron c courville and
joelle pineau 2016 building endtoend dialogue systems using generative
hierarchical neural network models in aaai vol 16 37763784
24 iulian vlad serban alessandro sordoni ryan lowe laurent charlin joelle
pineau aaron courville and yoshua bengio 2017 a hierarchical latent variable encoderdecoder model for generating dialogues in thirtyfirst aaai
conference on artificial intelligence
25 xiaoyu shen hui su yanran li wenjie li shuzi niu yang zhao akiko aizawa
and guoping long 2017 a conditional variational framework for dialog
generation arxiv preprint arxiv170500316 2017
26 alessandro sordoni michel galley michael auli chris brockett yangfeng ji
margaret mitchell jianyun nie jianfeng gao and bill dolan 2015 a neural
network approach to contextsensitive generation of conversational responses
arxiv preprint arxiv150606714 2015
27 hui su xiaoyu shen sanqiang zhao xiao zhou pengwei hu randy zhong
cheng niu and jie zhou 2020 diversifying dialogue generation with nonconversational text arxiv preprint arxiv200504346 2020
28 chongyang tao wei wu can xu wenpeng hu dongyan zhao and rui yan
2019 multirepresentation fusion network for multiturn response selection
in retrievalbased chatbots in proceedings of the twelfth acm international
conference on web search and data mining 267275
29 oriol vinyals and quoc le 2015 a neural conversational model arxiv preprint
arxiv150605869 2015
30 joseph weizenbaum 1966 elizaa computer program for the study of natural
language communication between man and machine commun acm 9 1 1966
3645
31 sam wiseman and alexander m rush 2016 sequencetosequence learning as
beamsearch optimization arxiv preprint arxiv160602960 2016
32 thomas wolf victor sanh julien chaumond and clement delangue 2019
transfertransfo a transfer learning approach for neural network based conversational agents arxiv preprint arxiv190108149 2019
33 yu wu wei wu chen xing ming zhou and zhoujun li 2016 sequential
matching network a new architecture for multiturn response selection in
retrievalbased chatbots arxiv preprint arxiv161201627 2016
34 chen xing wei wu yu wu jie liu yalou huang ming zhou and weiying
ma 2016 topic augmented neural response generation with a joint attention
mechanism arxiv preprint arxiv160608340 2 2 2016
35 chen xing wei wu yu wu jie liu yalou huang ming zhou and weiying ma
2017 topic aware neural response generation in thirtyfirst aaai conference
on artificial intelligence
36 zhilin yang zihang dai yiming yang jaime carbonell ruslan salakhutdinov
and quoc v le 2019 xlnet generalized autoregressive pretraining for language
understanding arxiv preprint arxiv190608237 2019
37 saizheng zhang emily dinan jack urbanek arthur szlam douwe kiela and
jason weston 2018 personalizing dialogue agents i have a dog do you have
pets too arxiv preprint arxiv180107243 2018
38 yizhe zhang siqi sun michel galley yenchun chen chris brockett xiang
gao jianfeng gao jingjing liu and bill dolan 2019 dialogpt largescale
generative pretraining for conversational response generation arxiv preprint
arxiv191100536 2019
39 zaixiang zheng hao zhou shujian huang lei li xinyu dai and jiajun chen
2019 mirrorgenerative neural machine translation in international conference
on learning representations
