unsupervised discriminative embedding for subaction learning in complex
activities
arxiv210500067v1 cscv 30 apr 2021
sirnam swetha  hilde kuehne  yogesh s rawat  mubarak shah
center for research in computer vision university of central florida orlando fl
mitibm watson lab cambridge ma
abstract
action recognition and detection in the context of long
untrimmed video sequences has seen an increased attention
from the research community however annotation of complex activities is usually time consuming and challenging
in practice therefore recent works started to tackle the
problem of unsupervised learning of subactions in complex activities this paper proposes a novel approach for
unsupervised subaction learning in complex activities the
proposed method maps both visual and temporal representations to a latent space where the subactions are learnt
discriminatively in an endtoend fashion to this end we
propose to learn subactions as latent concepts and a novel
discriminative latent concept learning  dlcl  module aids
in learning subactions the proposed dlcl module lends
on the idea of latent concepts to learn compact representations in the latent embedding space in an unsupervised
way the result is a set of latent vectors that can be interpreted as cluster centers in the embedding space the
latent space itself is formed by a joint visual and temporal
embedding capturing the visual similarity and temporal ordering of the data our joint learning with discriminative
latent concept module is novel which eliminates the need
for explicit clustering we validate our approach on three
benchmark datasets and show that the proposed combination of visualtemporal embedding and discriminative latent
concepts allow to learn robust action representations in an
unsupervised setting
figure 1 overview of the proposed approach given videos of a
complex activity the proposed model learns subactions as latent
concepts in an endtoend manner the latent concept assignment
for each input video segment feature forms subaction prediction
shown as initial predictions which is then refined using viterbi
to generate final predictions sample results for activity make
chocolate milk it can be seen that the latent concepts are able
to group subactions the subaction pourmilk includes lifting
bottle and pouring milk the jitter can be associated to the confusion when either a chocolatemilk bottle is lifted
temporal action segmentation 9 10 17 18 19 20 where
each frame of the video is associated with a respective subaction class as it requires to identify subactions and also
temporally localize them
existing works on temporal action segmentation mainly
explore supervised approaches where framelevel annotations are required for all the subactions 21 8 12 9
22 11 23 however complex activities are usually longranged and obtaining framelevel annotation is arduous and
expensive a new line of research focuses on learning these
subactions from videos of a complex activity in an unsupervised setting 4 5 19 10 24  in the unsupervised setting the problem is even more challenging as it requires i
breaking down a complex activity video into semantically
meaningful subactions and ii capturing the temporal relationship between the subactions most approaches tackle
this problem in two stages where during the first stage an
1 introduction
recent years have seen a great progress in video activity analysis however most of this research is focused on
the classification of short video clips with atomic or shortrange actions 1 2 3 this is a relatively easier task when
compared with analysis of untrimmed and complex video
sequences 4 5 6 7 8 9 10 11 12 in untrimmed video
analysis the focus is either on the problem of temporal action localization 13 14 15 16 where only a set of key
actions is considered in untrimmed videos or on the task of
embedding based on visual andor temporal information is
learned and in the second stage clustering is applied on this
embedding space this limits the learning ability by preventing the embedding to actually learn from clustering at
the same time performing explicit clustering which is independent of embedding learning makes the model less efficient and does not utilize endtoend learning
to address this problem we propose an endtoend approach where subactions are learned by combining embedding and latent concepts here the embedding space is
trained jointly with the latent concepts leading to an effective subaction discovery as shown in figure 1 to allow
for such a joint training we propose a novel discriminative
latent concept learning dlcl module which combines latent concept learning with a contrastive loss to ensure that
the subactions learnt in the latent embedding are distant
the resulting latent concept vectors can be interpreted as
cluster centers removing the need for explicit clustering at
a later stage
as the subactions are softly bound to the temporal position of each activity incorporating temporal ordering is
crucial recent works 10 19 incorporated temporal embedding either by predicting the discrete temporal entities
or by learning continuous temporal embedding with shallow
mlp architectures in those cases the temporal information
is only given by a discrete or continuous scalar value and
the joint embedding space is constructed by predicting this
value from the input to learn better spatiotemporal representations we propose to use temporal position encoding 25 instead of scalar values and learn the respective
embedding space by jointly reconstructing both visual and
temporal representations this embedding is further trained
jointly with constrastive loss of the latent concept module
so that the embedding is also guided by and contributes to
overall clustering
we evaluate our method on three benchmark datasets
breakfast 8 50salads 26 and youtube instructions 5
for the evaluation at test time we follow the protocol from
19 and employ the viterbi algorithm to decode the initial
subaction predictions into coherent segments based on the
ordered clustering of the subaction latent concepts a detailed analysis shows the impact of the proposed elements
the reconstruction and well as the latent concept learning
in summary we propose a novel endtoend unsupervised approach for subaction learning in complex activities we make the following contributions in this work
 we propose an unsupervised endtoend approach for
subaction learning in complex activities by jointly
learning an embedding which simultaneously incorporates visual and temporal information
 our method improves the stateoftheart on three
benchmark datasets
2 related work
recently there has been a lot of interest in learning with
less supervision this is essential for both action 3 1 2
and complex activity understanding 12 27 11 as supervised approaches require a large number of frames to be
annotated in videos which is expensive tedious and cannot
be scaled to large datasets weakly supervised approaches
use a video and readily available information like accompanying text narration or audio some works 28 29 use associated text narrations or scripts for learning actions in the
video another line of work with weaksupervision include
the works where it is assumed that the order of subactions
is known 17 30 31 32 however the perframe annotations between video and subactions are not known during
training authors in 33 propose to use combination of audio text and video to identify steps in instructional videos
in kitchen setting the performance of the above methods
is highly dependent on both the availability and quality of
the textaudio alignment to video which is not guaranteed
and heavily limit their application
there have been some works in which the assumption of weak supervision have been removed in learning
of action classes one of the first works with no supervision addressed the problem of human motion segmentation 34 based on sensorymotor data and proposed an
application of a parallel synchronous grammar to learn
simple action representations similar to words in language
later a bayesian nonparametric approach to concurrently
model multiple sets of time series data was proposed in
35 however this work only focuses on motion capture data 36 37 benefit from the temporal structure of
videos to finetune networks without any labels additionally 38 39 40 41 also leverage the temporal structure of
videos to learn feature representation to learn actions
recently unsupervised approaches have been proposed
to learn subactions in complex activity 10 19 24 propose unsupervised approaches for temporal segmentation
of complex activity into subactions while 4 proposes
to solve a variant of the problem where the goal is to detect event boundaries ie event boundary segmentation for
complex activities this does not focus on identifying subactions instead it learns to identify boundaries across multiple subactions in long videos a selfsupervised predictive
learning framework is proposed to solve by utilizing the difference between observed and predicted frame features to
determine event boundaries in complex activities
in this work we focus on solving the temporal segmentation of complex activity into subactions as shown
in 10 19 24 in 10 an iterative approach is proposed
that alternates between discriminative learning and gener
 we learn discriminative latent concepts using contrastive loss thus integrating clustering as part of latent
embedding learning
figure 2 overview of the proposed model given videos for a complex activity we extract visual features xnm  and compute positional
encoding vectors nm  which are fed to the encoder to map them to a joint latent embedding for learning subaction clusters to learn
these subaction clusters as latent concepts yb  an attention block d is used which takes in randomly initialized vectors y  along with
nm and learns the latent concepts we use contrastive loss to learn yb discriminatively in b here  znm and ybk represent attention
latent vector for input xnm and kth latent concept respectively
pendent of embedding learning in this work we present an
endtoend model where clustering is incorporated in embedding learning using a constrastive loss to incorporate
temporal ordering we propose to use positional encodings
and we also propose an effective way to unify visual and
temporal representations to learn a visualtemporal embedding by jointly reconstructing visual and temporal representations the proposed latent embedding is not only better at
capturing visual  temporal representations but also clustering friendly we demonstrate later in this paper the usefulness of the proposed model both qualitatively and quantitatively
ative modeling for discriminative learning they map the
visual features into latent space using a linear transformation and compute the weight matrix which minimizes the
pairwise ranking loss for temporal ordering they use generalized mallows model which models distributions over
permutations as they formulate complex activity as a sequence of permutable subactions in 19 the model incorporates the continuous temporal ordering of frames in
a joint embedding space this is achieved by training a
regressor to predict the timestamp of frames in a video
the hidden layer representations are used as the embedding
for clustering and the clusters are ordered with respect to
their time stamp we refer to this model as cte continuous temporal embedding in 24 twostage embedding
pipeline is proposed where a next frame prediction unet
model in stage one is combined with with temporal discriminator in stage two followed by clustering the temporal
embedding model employed is similar to 19
latent embedding learning is crucial for unsupervised
learning recently 7 formulated learning graph based latent embedding using latent concepts for supervised classification of complex activities the intuition was to model
long range videos using latent concepts as graphical nodes
for complex activity recognition inspired by their ideology
of latent concept learning to model latent space we propose
dlcl as an unsupervised latent learning module with joint
embedding learning to model subactions
most of the above works in unsupervised learning involve two stage process which does not utilize endtoend
learning making them less efficient as clustering is inde
3 proposed model
31 overview
given a set of n videos vn n
n1  for a complex activity we divide each video into segments and for each
segment we extract i3d features 1  and compute positional encoding vectors 25 as described in section 32
each video is represented by mn features where xnm represent the mth feature in the nth video and its corresponding positional encoding is represented by nm  the task
is to learn the subactions and their ordering for each activity ie by predicting sequence of a subaction labels
lnm  1 2  k for each feature xnm for each video
the number of subactions labels k for each activity is the
maximum number of possible subactions as they occur in
that activity
overview of our proposed model is shown in figure 2
first we learn an encoded representation of xnm and nm
shown as nm  which is passed as input feature to the
attention block shown as d in figure 2 to learn the
latent conceptsclusters which are representative of subactions the attention block learns the latent concepts
yb  yb1  yb2   ybk  discriminatively where each input
feature nm  is assigned to only one latent concept we
use a combination of reconstruction loss and constrastive
loss to learn the embeddingshown as b in figure 2 we
believe that using a combination of both visual and temporal
information in conjunction with latent concept learning to
learn a latent embedding shown as block b in fig 2 makes
our model more robust we evaluate the performance of our
model based on latent concept assignments for each input
feature which forms initial predictions then we model
the subaction transitions and perform viterbi decoding to
estimate optimal subaction ordering
note that unlike previous works 19 we do not perform explicit clustering instead our model learns to cluster
features in latent space as discussed in section 32  33
thus eliminating the need for all the data to be available at
once our model can learn the latent concepts incrementally
the resulting subaction latent concepts are temporally ordered and then each video is decoded wrt the above ordering given initial subaction probability assignments for each
clip to each latent concept as described in section 35
put and reconstructed positional encoding  is a hyperparameter and l is a loss function penalizing x0nm and 0nm for
being dissimilar from xnm and nm respectively namely
mean squared error a combination of latent visual feature
representation and the positional encodings becomes input
to the attention block in order to ensure that the learnt
clusters are representative of subactions the clusters have
to be distant in the latent space which is described in the
next section
33 discriminative latent concept learning
the idea behind having this module is to learn the subaction clusters discriminatively in the latent space in an endtoend fashion eliminating the need for explicit clustering
the attention block is inspired by 7 which takes an input
feature nm  and randomly initialized latent vectors y 
which is analogous to cluster center initialization as shown
in figure 2 the latent concepts yb  are learnt using an
mlp with weight w and bias b ie it transforms the
random latent vector initializations y  to latent concepts
yb  as yb  w  y  b though latent vector initialization y  is fixed w  b are learnable parameters making
the latent concepts yb  learnable in the latent space these
latent concepts which represent cluster centers are learned
by minimizing the contrastive loss by moving features in
the latent space closer to the latent concepts the similarity between input feature nm  and latent concepts yb  is
measured with the dot product  then activation function  is applied on the similarities to compute activation
values  ie   nm  yb t  finally the attended latent vector representation is computed as znm   yb 
which captures how much each latent concept is related to
the given input feature however these latent concepts tend
to learn similaroverlapping concepts which is not what we
intend to learn our objective in learning the latent concepts is to cluster the latent representations discriminatively
we achieve this with a contrastive loss where the similarity between latent vectors of the same subaction with the
maximum confident latent concept is maximized while the
similarity wrt other latent concepts is minimized as shown
in eq 2
esimznm yk 
lossd znm  yb   log p
bk 
simznm y
k6k e
where ybk represents the latent concept associated with k th
subaction sim denotes cosine similarity and k  represents
the latent concept with maximum confidence probability for
znm as shown below
k   argmaxp kznm 
32 joint visual and temporal latent embedding
in unsupervised learning the approach to learn clusters
in latent space plays a critical role in learning semantically
meaningful clusters we employ an encoderdecoder model
to obtain the latent representation skipconnections are included between encoder and decoder shown as c in figure 2 as they help to preserve commonality of an action
and reduce redundant information like background in latent
representation
for incorporating temporal ordering in our model we
employ the positional encodings inspired by 25 we divide the video segment sequence into g equal groups and
then use the ordering index to compute positional encoding vectors quantizing temporal index of the video clip
and using a positional encoding not only captures relative
positioning but also makes it easy to generalize for highly
varying video lengths the idea of learning a mapping
from features to joint visual and temporal embedding with
an encoderdecoder aids in grouping clips into subactions
in the latent space the reconstruction loss for the autoencoder is composed of visual features and positional encoding as shown below
lossr  lxnm  x0nm     lnm  0nm 
where p kznm   simznm  ybk  represents the confidence probability of latent vector znm for the latent concept
ybk   is softmax activation
where xnm  x0nm respectively represent input and reconstructed visual feature nm  0nm respectively represent in4
figure 3 qualitative comparison of initial predictions wo
viterbi and after viterbi predictions of our approach for activity
make tea it can be seen that our model init is able to group
subactions and also learn the ordering of subactions for an activity the jitter in subaction predictions occurs during transition
from one subaction to next which is expected during transition
finally using transition modeling viterbi decoding smoothness
the jitter between subaction transitions
figure 4 brief overview of transition modeling and viterbi decoding each latent concept is color coded best viewed in color
the latent concepts are ordered wrt the mean time shown as t 
and each video is decoded into coherent segments using viterbi
algorithm based on the ordered subaction latent concepts
34 overall loss
to get consistent latent set assignments in a video by maximizing
 lmn  argmax
p lm lm1   p lm znm 
total loss for learning the proposed embedding is composed of losses from section 32 and 33 as loss   
lossr    lossd
l1 lmn m1
35 temporal segmentation
where l1   lm  1 2  k represent the set label sequence for nth video p lm  kznm  is the probability
that znm belongs to the k th latent set as described in section 33  l1mn is the set label sequence for the maximum
likelihood for nth video
initial predictions at test time we first assign each feature
in video to its respective closest latent concept vector using
eq 3 this gives initial predictions directly based on the
embedding shown as predictions in figure 1 for ease of
understanding we refer to those as latent sets analogous to
clusters from here on
transition modeling and viterbi decoding
figure 4 represents a brief outline for transition modeling
and viterbi decoding to allow for a temporal decoding
the global ordering of the latent sets needs to be estimated
we follow here the protocol proposed by 19 and compute
the mean timestamp for each set shown as t in figure 4
and sort them in ascending order the last set in the sorted
ordering becomes the terminal state and using this ordering
the subaction state transition probabilities from subaction
i to j are defined as p ji given
05 if j immediately follows i
05 if i  j
p ji 
10 if i  j  j is terminal state
otherwise
4 experiments
for our experiments we define a segment as a sequence
of 8 frames the video segment sequence is divided into
128 equal groups and then the ordering index is used to
compute positional encoding 25 for each segment we extract i 3 d features layer mixed 5c which is fed to the encoder our embedding dimension is 1024 we use a 3layer
encoderdecoder with adam optimizer and the learning rate
is set to 1  104  we evaluate our approach on 3 datasets
breakfast dataset comprises of 10 complex activities of
humans performing common kitchen activities there are
a total of 48 subactivities in 1 712 videos with varying
lengths based on activity and preparation style with variations in subaction orderings
50salads dataset contains videos of duration 45 hours for
a single complex activity making mixed salad it is a multimodal dataset as it includes rgb frames depth maps and
accelerometer data however we only use rgb frames the
videos in this dataset are much longer with average frame
length of 10k frames and provides annotations at multiple
granularity levels
youtube instructions dataset has 5 activities and 150
videos with 47 subactions these videos are taken from
decoding finally we use the ordering and transition
probabilities to compute the best path for the set ordering
given the input features xnm and nm  using eq 3 we compute the probability of each embedded input feature znm 
belonging to the latent set k we maximize the probability
of the input sequence following the order defined by eq 4
method
cte
cte
a make cereals
19 with f v
19 with i 3 d
ours with i 3 d
moc
moc
wo viterbi
w viterbi
209
248
401
368
375
469
table 2 comparison of moc mean over class of all activities
on breakfast dataset before and after applying viterbi f v represents fisher vectors
b make chocolate milk
figure 5 illustrative comparison with stateoftheart by com
metrics our model predicts a sequence of cluster labels
 1 2  k for each video without any correspondence
to the k groundtruth class labels to map groundtruth and
prediction label correspondences inline with 5 10 19
for each activity we use the hungarian method to find a onetoone mapping for each cluster to exactly one subaction
and report performance after this mapping in this work we
use mean over frames mof as used by 10 19 as well
as f1score used by 5 in addition we report mean over
class moc accuracy as it averages the accuracy for each
activity class therefore giving equal weights to all classes
irrespective of the underlying data distribution mof is the
percentage of correct predictions computed across all activity classes together which can be affected by the underlying
activity classes distribution and biased towards dominant
activity class for f1score similar to previous methods
we report the mean score over all activities for stateoftheart comparisons we also evaluate our method for the
task of event boundary segmentation following the protocol in 4 and compare our method to 4  indicated as
videobased hungarian matching
paring with cte init and ours init we show that our approach
learns to model subactions with very few intermittent subaction
transitions leading to effective grouping of subactions then
viterbi decoding helps to smoothen the intermittent jitters in predictions we show that our method provides coherent subaction
predictions and is able to capture the orderings for subactions
method
f1score
mof
weakly supervised
rnnfc 42
tcfpn 17
nnvit 32
d3tw 43
cdfl 44
333
384
457
502
unsupervised
mallow 10
cte 19
jvt 24
264
299
346
418
481
ours
319
474
429
746
lstm  al
ours
41 comparison to stateoftheart
here we compare the proposed method to stateoftheart approaches we present the accuracy comparison with
recent works on breakfast dataset in table 1 and present the
performance on new metric moc in table 2 our approach
achieves 474 mof and 319 f1score which is 2 gain
over stateoftheart as shown in table 1 we show qualitative evaluation of the proposed approach in figure 3  5 in
figure 5 we show that our approach models the subactions
coherently with very less intermittent subaction transitions
along with learning ordering of subactions for complex activity for example in figure 3 our model predicts stirtea
with intermittent transitions after pourwater this occurs
when the person dips the tea bag in water which closely
resembles to the subaction stirtea as shown in last image in figure 3 and then it correctly predicts background
once the dip action ends there is no annotation for dipping teabag in ground truth indicating the goodness of the
proposed subaction learning the intermittent transitions
indicate that the model confuses to assign latent concept
table 1 comparison of the proposed method to stateoftheart
on breakfast dataset here  denotes results with videobased
hungarian matching for the task event boundary segmentation
youtube directly and have background segments where
there is no subaction the frequency and spread of background varies based on activity as well as on the person
performing the task hence the background segments neither have similar appearance nor have a temporal ordering
therefore the background segments would be assigned to
the latent concepts with very less confidence probability
following protocol in 19 we consider  percent of clips
with least confidence as background only the foreground
labeled segments along with latent concepts assignments
form our initial predictions we report results for background ratio of 60
method
cte
jvt
ours
lstm  al 4
ours
f1score
mof
method
f1score
mof
355
306
frankwolfe 5
mallow 10
cte 19
jvt 24
244
270
283
299
346
278
390
282
344
422
606
702
ours
296
438
397
454
lstm  al 4
ours
table 3 comparison of the proposed method to stateoftheart
unsupervised approaches on 50salads dataset at granularity eval
here  denotes results with videobased hungarian matching for
the task event boundary segmentation
table 4 comparison of the proposed method to stateoftheart
unsupervised methods on youtube instructions dataset here 
denotes results with videobased hungarian matching for the task
event boundary segmentation
based on single feature and viterbi aids in generating more
coherent subaction segments for the sequence as shown
additionally we evaluate our method for the task of event
boundary segmentation and compare with the stateoftheart approaches our approach outperforms the stateoftheart mof by a margin of 31 on breakfast dataset indicating
the effectiveness of the proposed method to temporally segment meaningful subactions
for 50salads dataset we perform evaluation on granularity level eval and provide stateoftheart comparison in
table 3 our method outperforms 19 by 667 and 24
by 116 with an f1score of 3437 we further evaluate our method for the task of event boundary segmentation and perform stateoftheart comparison in table 3
we show 10 gain over stateoftheart 4 mof indicating
our method is effective in subaction learning for complex
events
for youtube instructions dataset we follow protocol in
5 10 19 and report the performance of our approach without considering the background frames we achieve 42
moc  438 mof as shown in table 4 this is a 48
gain in mof over stateoftheart method with comparable
f1score note that 4 reported f1score with background
frames included on youtube instructions dataset we follow the same procedure and compare our method to 4 in
table 4 indicated with  it can be seen that our method
outperforms the stateoftheart for event boundary segmentation task showing the subaction learning capability to
identify better event boundaries
performance is due to the effectiveness of the approach and
not with using i 3 d features we train 19 using i 3 d features by keeping the embedding dimension same as ours and
compare the performance as shown in table 2 the moc
wo viterbi improves by 4 by using i 3 d features on cte
while the moc with viterbi drops by 3 with 1 increase
in f1score however our approach still outperforms the
baseline with same embedding dimension by huge margin
indicating our approach effectiveness
besides dataset level comparisons we also show activity
level comparison with cte 19 figure 6 a shows that our
joint embedding outperforms cte on all activities indicating the significance of our joint embedding we see a drop
in performance for activity making cereals after viterbi
decoding from figure 6b this can be attributed to the
ordering of the subactions takebowl and pourcereals
for many samples in making cereals the subaction takebowl does not occur impacting the ordering of both subactions leading to drop in performance
43 ablation experiments
we perform the below ablation studies on the breakfast
dataset
effect of loss components to begin with we first examine the influence of lossr and lossd on our model and the
performances are presented in table 5 it can be seen that
having all loss components leads to best performance
effect of discriminative learning the use of constrastive
loss lossd  helps the clusters to move apart in the latent
space this helps in obtaining more discrete boundaries in
the latent space as shown in table 5 the accuracy drastically reduces to 358 11  indicating the importance
of discriminative learning
effect of positional encoding positional encoding plays
a crucial role in our model it helps to temporally group
the video clips in the latent space as subactions are softly
bound to the temporal position for each activity removing
42 evaluation of the embedding
to demonstrate the impact of the proposed embedding
we compare our joint embedding with continuous temporal embedding in 19 in table 2 from table 2 moc wo
viterbi it can be seen that the proposed joint embedding
outperforms the continuous temporal embedding by a huge
margin of 166 it can be seen that our moc wo viterbi
is closer to the cte moc w viterbi suggesting that our
embedding is very effective to emphasize that our gain in
a mof wo viterbi
figure 7 mof vs subactions for all activities in breakfast
dataset k represents the number of subactions from groundtruth
we vary the subactions for each activity and report mof
port the performance in table 6 it can be seen that wo
skipconnections the accuracy drops considerably indicating that the skipconnections help in learning better representations
b final mof
figure 6 activity level mof comparison on breakfast dataset
moc
f1score
with cte 19 last column represents the average moc for all
activities a represents mof for each activity without viterbi ie
the mof is computed based on the learnt cluster assignments our
method outperforms the baseline on all activities b represents
mof for each activity after applying viterbi
lossr
lossd
wo pe
wo sc
full
409
203
357
287
469
319
table 6 ablations experiments to evaluate the effect of pe and
sc on breakfast dataset wo without pe positional encoding
sc skipconnections
moc
257
336
358
402
401
469
effect of subactions cluster size for all the above evaluations the subaction cluster size k is defined as mentioned in section 31 to analyze the impact of subaction
cluster size we vary the number of subactions from k  2
to k2 where k is the number of subactions as per ground
truth and evaluate performance figure 7 shows the mof vs
number of subactions for each activity in breakfast dataset
for 6 out of 10 activities we see that having k subactions
leads to best performance
table 5 ablation experiments for the loss components are performed on the breakfast dataset lossr and lossd represents
reconstruction loss and contrastive loss respectively lf and lp
denote the reconstruction loss for feature and positional encoding
respectively
5 conclusion
in this work we proposed an endtoend approach for
unsupervised learning of subactions in complex activities
the main motivation behind this approach is to design a latent space to incorporate visual as well as positional encoding together this latent space is learned via jointly training this embedding space in conjunction with a contrastive
learning for clustering we show that this allows for a robust learning that on its own already results in a reasonable clustering of subactions we then predict optimal subaction sequence by employing the viterbi algorithm which
outperforms all the other methods our evaluation shows
the impact of the proposed ideas and how they are able to
improve the performance on this task compared to existing
methods
reconstruction loss for positional encoding is expected to
deteriorate the model performance we observe the similar trend in table 5 additionally we perform an ablation
by removing the pe component branch and train our model
endtoend as expected there is a significant reduction in
accuracy and f1score as shown in table 6 indicating the
significance of using positional encoding
effect of skipconnections to assess the effectiveness
of skipconnections we report performance by removing
the skipconnections and train model endtoend we re8
references
17 li ding and chenliang xu weaklysupervised action segmentation with iterative soft boundary assignment in cvpr
2018
1 joao carreira and andrew zisserman quo vadis action
recognition a new model and the kinetics dataset in cvpr
2017
18 alexander richard hilde kuehne and juergen gall action
sets weakly supervised action segmentation without ordering constraints in cvpr 2018
2 christoph feichtenhofer haoqi fan jitendra malik and
kaiming he slowfast networks for video recognition in
iccv 2019
19 anna kukleva hilde kuehne fadime sener and jurgen
gall unsupervised learning of action classes with continuous temporal embedding in cvpr 2019
3 karen simonyan and andrew zisserman twostream convolutional networks for action recognition in videos in
neurips 2014
20 yazan abu farha and jurgen gall mstcn multistage
temporal convolutional network for action segmentation in
cvpr 2019
4 sathyanarayanan n aakur and sudeep sarkar a perceptual prediction framework for self supervised event segmentation in cvpr 2019
21 yazan abu farha and jurgen gall mstcn multistage
temporal convolutional network for action segmentation in
cvpr 2019
5 jeanbaptiste alayrac piotr bojanowski nishant agrawal
ivan laptev josef sivic and simon lacostejulien unsupervised learning from narrated instruction videos in cvpr
2016
22 hilde kuehne alexander richard and juergen gall a hybrid rnnhmm approach for weakly supervised temporal action segmentation ieee transactions on pattern analysis
and machine intelligence 2018
6 noureldien hussein efstratios gavves and arnold wm
smeulders timeception for complex action recognition in
cvpr 2019
23 colin lea michael d flynn rene vidal austin reiter and
gregory d hager temporal convolutional networks for action segmentation and detection in cvpr 2017
7 noureldien hussein efstratios gavves and arnold wm
smeulders videograph recognizing minuteslong human
activities in videos arxiv 2019
24 rosaura g vidalmata walter j scheirer anna kukleva
david cox and hilde kuehne joint visualtemporal embedding for unsupervised learning of actions in untrimmed
sequences in wacv 2020
8 h kuehne a b arslan and t serre the language of actions recovering the syntax and semantics of goaldirected
human activities in cvpr 2014
25 ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia
polosukhin attention is all you need in neurips 2017
9 hilde kuehne juergen gall and thomas serre an endtoend generative framework for video segmentation and recognition in wacv 2016
26 sebastian stein and stephen j mckenna combining embedded accelerometers with computer vision for recognizing
food preparation activities in ubicomp 2013
10 fadime sener and angela yao unsupervised learning and
segmentation of complex activities from video in cvpr
2018
27 dean huang li feifei and juan carlos niebles connectionist temporal modeling for weakly supervised action
labeling in eccv 2016
11 zheng shou jonathan chan alireza zareian kazuyuki
miyazawa and shihfu chang cdc convolutionaldeconvolutional networks for precise temporal action localization in untrimmed videos in cvpr 2017
28 ivan laptev marcin marszalek cordelia schmid and benjamin rozenfeld learning realistic human actions from
movies in cvpr 2008
12 serena yeung olga russakovsky greg mori and li feifei endtoend learning of action detection from frame
glimpses in videos in cvpr 2016
29 ozan sener amir r zamir silvio savarese and ashutosh
saxena unsupervised semantic parsing of video collections
in iccv 2015
13 peihao chen chuang gan guangyao shen wenbing
huang runhao zeng and mingkui tan relation attention
for temporal action localization ieee transactions on multimedia 2019
30 hilde kuehne alexander richard and juergen gall weakly
supervised learning of actions from transcripts cviu 2017
14 fuchen long ting yao zhaofan qiu xinmei tian jiebo
luo and tao mei gaussian temporal awareness networks
for action localization in cvpr 2019
31 alexander richard hilde kuehne and juergen gall weakly
supervised action learning with rnn based finetocoarse
modeling in cvpr 2017
15 mengmeng xu chen zhao david s rojas ali thabet and
bernard ghanem gtad subgraph localization for temporal action detection arxiv 2019
32 alexander richard hilde kuehne ahsan iqbal and juergen gall neuralnetworkviterbi a framework for weakly
supervised video learning in cvpr 2018
16 runhao zeng wenbing huang mingkui tan yu rong
peilin zhao junzhou huang and chuang gan graph convolutional networks for temporal action localization in
iccv 2019
33 jonathan malmaud jonathan huang vivek rathod nick
johnston andrew rabinovich and kevin murphy whats
cookin interpreting cooking videos using text speech and
vision arxiv 2015
34 gutemberg guerrafilho and yiannis aloimonos a language for human action computer 2007
35 emily b fox michael c hughes erik b sudderth michael i
jordan et al joint modeling of multiple time series via the
beta process with application to motion capture segmentation the annals of applied statistics 2014
36 xiaolong wang and abhinav gupta unsupervised learning
of visual representations using videos in iccv 2015
37 biagio brattoli uta buchler annasophia wahl martin e
schwab and bjorn ommer lstm selfsupervision for detailed behavior analysis in cvpr 2017
38 vignesh ramanathan kevin tang greg mori and li feifei learning temporal embeddings for complex video analysis in iccv 2015
39 basura fernando efstratios gavves jose m oramas amir
ghodrati and tinne tuytelaars modeling video evolution
for action recognition in cvpr 2015
40 anoop cherian basura fernando mehrtash harandi and
stephen gould generalized rank pooling for activity recognition in cvpr 2017
41 hsinying lee jiabin huang maneesh singh and minghsuan yang unsupervised representation learning by sorting sequences in iccv 2017
42 alexander richard hilde kuehne and juergen gall weakly
supervised action learning with rnn based finetocoarse
modeling in cvpr 2017
43 chienyi chang dean huang yanan sui li feifei and
juan carlos niebles d3tw discriminative differentiable dynamic time warping for weakly supervised action alignment
and segmentation in cvpr 2019
44 jun li peng lei and sinisa todorovic weakly supervised
energybased learning for action segmentation in iccv
2019
