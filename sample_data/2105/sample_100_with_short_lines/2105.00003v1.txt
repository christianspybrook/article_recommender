1

nuspan a proximal average network for
nonuniform sparse model  application to
seismic reflectivity inversion
arxiv210500003v1 physicsgeoph 1 may 2021

swapnil mache  praveen kumar pokala  kusala rajendran and chandra sekhar seelamantula senior
member ieee

abstractwe solve the problem of sparse signal deconvolution
in the context of seismic reflectivity inversion which pertains to
highresolution recovery of the subsurface reflection coefficients
our formulation employs a nonuniform nonconvex synthesis
sparse model comprising a combination of convex and nonconvex
regularizers which results in accurate approximations of the
0 pseudonorm the resulting iterative algorithm requires the
proximal average strategy when unfolded the iterations give rise
to a learnable proximal average network architecture that can be
optimized in a datadriven fashion we demonstrate the efficacy
of the proposed approach through numerical experiments on
synthetic 1d seismic traces and 2d wedge models in comparison
with the benchmark techniques we also present validations
considering the simulated marmousi2 model as well as real 3d
seismic volume data acquired from the penobscot 3d survey off
the coast of nova scotia canada

is defined as the product of density  and pwave velocity
v  2 given by
ri 

i1 vi1  i vi

i1 vi1  i vi

1

solving the illposed linear inverse problem of estimating
the subsurface reflectivity through the classical leastsquares
formulation 5 leads to nonuniqueness issues arising out of a
convolution with a bandlimited wavelet and loss of low and
highfrequency information 2 6 7 the nonuniqueness
aspect can be tackled through regularization 8 for example
by imposing a sparsity prior on the solution through the 1 norm 2 5
zhang and castagna 9 solved the 1 norm constrained
index termsgeophysics inverse problems seismology seis reflectivity inversion problem through basispursuit inversion
mic reflectivity inversion geophysical signal processing deep bpi 10 using a wavelet dictionary of odd and even
learning neural networks algorithm unrolling nonconvex opreflectivity pairs the fast iterative shrinkagethresholding
timization sparse recovery
algorithm fista 11 has been employed for reflectivity
inversion 12 along with an amplitude recovery boost through
i i ntroduction
debiasing steps of leastsquares inversion 13 and adding back
eflectivity inversion which is a seismic deconvo the residual 14 the 1 norm although convex is not the best
lution problem in reflection seismology is the means by sparsity constraint for reflectivity inversion and the accurate
which one can characterize and image the layered subsurface estimation of the sparsity of seismic reflections is challenging
structure a recorded seismic trace y  rn is modeled as 15 16 further 1 norm regularization underestimates the
the convolution of the source pulse h assumed to be a highamplitude components and introduces a bias in the
ricker wavelet 1 and the subsurface reflectivity x  rn  estimate of the sparse code x 17 18 19 both 13 and
given by y  h  x  n where n is the noise and  14 observed the attenuation of reflectivity magnitudes due to
denotes convolution reflectivity is modeled as a sparse vector the 1 norm regularization term and adopted a postprocessing
comprising subsurface layers each with a constant impedance debiasing step 20
giving rise to a piecewiseconstant impedance model 2 3
nonconvex regularization strategies have been adopted to
figure 1 illustrates the model with two shale layers sandwiching overcome the shortcomings of 1 regularization in sparse
a wet sand layer figure 1a 4 the reflectivity figure 1c recovery problems 19 21 22 nonconvex penalties
convolved with the source figure 1d is recorded at the such as the smoothly clipped absolute deviation scad
receiver as a seismic trace figure 1e
23 and minimax concave penalty mcp 21 have been
the subsurface geology figure 1a is related to reflectivity shown to be superior to 1 and nonconvex regularization
figure 1c through acoustic impedance figure 1b which approaches in the context of inverse problems in the context
of reflectivity inversion 15 introduced a datadriven p loss equal contribution
q regularization p  2 0  q  1 with an adaptive
swapnil mache is with the centre of excellence in advanced mechanics
of materials indian institute of science iisc bangalore india and with the approach for choosing the optimal q recently 24 and 25
department of electrical engineering iisc bangalore he was formerly with developed proximal gradientdescent methods based on the
the centre for earth sciences iisc bangalore email machesanjayiiscacin
proximal average strategy 26 27 by considering composite
praveen kumar pokala and chandra sekhar seelamantula are with the
department of electrical engineering iisc bangalore email praveenku regularization which is a convex combination of several
marpokalagmailcom chandrasekharieeeorg
sparsityinducing regularizers
kusala rajendran is with the centre of excellence in advanced mechanics
a new class of deep neural network dnn architectures was
of materials iisc bangalore she was formerly with the centre for earth
sciences iisc bangalore email kusalarajgmailcom
introduced by 28 which are inspired by unrolling iterative

r

2

fig 1

an ideal threelayer subsurface model the operator  denotes convolution

algorithms into learnable networks 29 gregor and lecun of weighted counterparts of three sparsitypromoting penalties
28 proposed the learned iterative shrinkage and thresholding the 1 norm mcp 21 and scad 23 penalties
algorithm lista based on unfolding the update steps of
the contributions of this paper are stated below
ista 30 into the layers of a neural network this class of
1 we consider the problem of seismic reflectivity inversion
modelbased architectures 31 has been demonstrated to be
based on a datadriven prior as opposed to a preeffective in solving sparse linear inverse problems 29 32
designated prior within the framework of deepunfolding
33 34 35 36 37 38 39 40 41
to the best of our knowledge datadriven priors have
recently learningbased approaches have also been emnot been explored for solving the problem of seismic
ployed for solving inversion problems in geophysics including
reflectivity inversion
seismic reflectivity inversion 42 43 4 16 44 45
2 we propose an optimization framework for seismic reflec43 employed an elementary feedforward neural network and
tivity inversion based on a composite sparseprior which
observed superior support recovery compared to a leastsquares
comprises multiple weighted regularizers the weights
approach but poor amplitude recovery yuan and su 16
are allowed to be different for each component such a
and earlier 42 employed sparse bayesian learning sbl
model is commonly referred to as a nonuniform sparse
for recovering the sparse reflection coefficients by maximizmodel
ing the marginal likelihood either by using the expectation3 we develop the nonuniform proximalaveraged thresholdmaximization algorithm sblem 46 16 or by sequening algorithm nupata and its deepunfolded version the
tially updating the sparsitycontrolling hyperparameters through
nonuniform sparse proximal average network nuspan
a sequential algorithmbased approach 42 the former 46
to solve the problem under consideration
16 was demonstrated as being more robust to noise having
4 we demonstrate the efficacy of the proposed network
higher accuracy and preserving lateral continuity in the seismic
nuspan over synthetic 1d and 2d datasets and
profile
simulated and real datasets
ii m otivation and c ontribution

iii o rganization of the paper and n otations
estimating the sparsity of the reflection data is difficult
this paper is structured as follows in section iv we
15 16 and the recovery of support is prioritized 1 introduce the nonuniform sparse model for sparse seismic
one can resort to datadriven approaches which outperform reflectivity inversion we develop an optimization algorithm
conventional techniques especially in support recovery 43 and the corresponding unrolled network for solving the problem
when the knowledge about the underlying geology is limited under consideration section v explains a generalized variant
with an added computational benefit 4 further as opposed of the problem formulation discussed in section iv in
to an elementary feedforward neural network approach for section vi we present experimental results and demonstrate the
seismic reflectivity inversion 43 one can employ modelbased efficacy of the proposed proximal average network nuspan
learning frameworks 31 such as deepunrolled architectures in comparison with baselines conclusions are provided in
29 such deep neural networks where the architecture is section vii
informed by the inverse problem itself can provide insights
the notational conventions used in this paper are listed in
into the problem as well as model interpretability that is table i
critical to gaining physical insights into the system under
consideration 44 in our previous work 47 we demonstrated
iv p roximal average n etwork for r eflectivity
the efficacy of firmnet 48 and listalike 28 formulations
i nversion  n onuniform s parse m odel t ype 1
in solving the seismic reflectivity inversion problem comparing
the approaches with baselines such as bpi 10 9 fista
the penalties used in this study are shown in table ii of
11 12 and sblem 46 16 here we expand the these the 1 penalty is convex whereas mcp and scad are
work by constructing and learning from the data 44 a not we propose weighted counterparts of regularizers given
composite nonuniform sparse prior from a convex combination in table ii which are proven to be good in approximating

3

table i
n otations

notation
x
x
kxkp
xk
xi
x0
1
h
h
i
sgn


h i

rnt

description
scalar lowercase letter
vector lowercase bold letter
p  norm of x
x at the k th iteration of an algorithm
ith element of x
flipped version of x
vector of all ones
matrix uppercase bold letter
pseudoinverse of h
identity matrix
signum function
gradient operator
convolution operator
innerproduct
elementwise sum
elementwise product
set of vectors in rn with entries greater than t

the 0 pseudonorm as atoms to construct a learnable sparsityprior further the chosen penalty functions have closedform
proximal operators the weighted counterparts of the penalty
functions and their proximal operators are given in figure 2
denotes
1 g1 x  k xk1  where   rn0  and
elementwise product rn0 denotes the set of vectors in
rn containing positive entries
pn
2 g2 x is weighted mcp defined as g2 x  j1 g2 xj 
3 gp
3 x is weighted scad which is defined as g3 x 
n
j1 g3 xj 
the parameters of g are learned in a datadriven setting with
the trainable parameters of the proposed sparsityprior being
    rn0 and   rn1  a  rn2  rn1 denotes the
set of vectors in rn with entries greater than 1
a problem formulation  type1
the sparse reflection coefficients x  rn are recovered from
the observed noisy seismic trace y  rn via optimization of
the following compositeregularized cost
m
n
o
x
1
2
arg min j x  kh  x  yk2 
i gi x
x i 
2
z
 i1

z

f x
2
gx
m
x
subject to
i  1 0  i  1 i

p3 j is bounded from below ie inf j  

given a set of training samples y p  xp  p 
1 2 3     n  our objective is to design a deepunrolled
network that solves the optimization problem stated in eq 2
our algorithm is referred to as the nonuniform proximalaveraged thresholding algorithm nupata1 which relies
on majorizationminimization mm 49 and the proximal
average strategy 26 27 40 further we unfold the
nupata1 iterations into a learnable network called nonuniform
sparse proximal average network nuspan1

b optimization algorithm  type1
due to p1 there exists   1l such that f x is upper
bounded locally by a quadratic expansion about x  xk as
follows


f x  q x xk 
3
1
kx 
where qx xk   f xk   hf xk  x  xk i  2
k 2
k
x k2  the majorizer to j at x is

j x  qx xk   gx 

z


4

hxxk 

the update for x at the k  1th iteration is given by
xk1  arg min hx xk 
x


1
x  xk  x f xk 
 arg min
x 2
 gx

2
2

5

the above problem does not have a closedform solution
hence we consider an approximate variant of the problem
corresponding to eq 5 based on the proximal average
strategy
m

 2
x
i
xk1  arg min
x  xk  x f xk 
x
2
2
i1


m
x

i gi x

i1



m
x
i1

i pgi uk 

6

where uk  xk  x f xk  x f xk   h0 
y  h  xk  where h0 is the flipped version of h and
pgi represent the proximal operators corresponding to gi cf
table ii the optimization procedure is listed in algorithm 1

i1

where m is the number of regularizers and i  are the weights
assigned to the regularizers
the composite objective function j in 2 regularizer g
and datafidelity term f satisfy the following properties which
are essential for minimizing j 
p1 f  rn    is proper closed and lsmooth ie
kf x  f yk2  lkx  yk2 
p2 g is lower semicontinuous

c nonuniform sparse proximal average network  type1
nuspan1
the update in 6 involves convolutions followed by nonlinear activation and can therefore be represented as a layer
in a neural network we unfold the iterations of algorithm 1
to obtain the deepunrolled architecture namely nonuniform
sparse proximal average network nuspan1 for solving the

4

table ii
s parsity promoting regularizers used in this study and corresponding proximal operators 

name

penalty function

1  norm   0
mcp 21   0   1

scad 23   0 a  2

fig 2

proximal operator

g1 x   x


2
x


 x 
 for x  
2
g2 x 

 2 


for x  
2

 x 
for x  




 x2  2a x   2
 for  x  a
g3 x 
21  a


2

a

1



for x  a
2

pg1 x  sgnx max x   0


0
for x  




pg2 x  sgn x
x    for   x  

1


x
for x  

sgn x max x   0  for x  2



a  1 x  sgn x a
pg3 x 
 for 2  x  a

a2


x
for x  a

the penalty functions and proximal operators corresponding to table ii

reflectivity inversion problem in eq 2 the structure for each
layer in nuspan1 is given by
xk1 

m
x
i1

i pgi wy  sxk 

7

where w  1l h0 and s  i  1l h0  h 28 are
initialized as toeplitz matrices in the learning stage they
are dense and unstructured given training data that consists
of a large number of independent and identically distributed
examples xp  y p n
p1 and nuspan1 with a fixed number of
layers we optimize the smooth 1 cost computed between the
true reflectivity x and the prediction x

w and s the parameters of gi      rn0pand  
m
rn1  a  rn2  and the weights i  0  i  1 i1 i 
1 the nuspan1 algorithm for kmax layers is listed in
algorithm 2

we enhance the amplitude recovery of nuspan1 and
nuspan2
in section v during the inference phase by ren

1 x
2
estimating
the
amplitudes over the supports given by nuspan1
min
 kx  xk1  1   kx  xk2 

 n
as
x

h
sx
sx y where x is the sparse vector estimated
i1
by nuspan1 sx is the support ie non zero locations of
where 0    1 we set   1 it is not a trainable parameter
x  and hsx is the pseudoinverse of the toeplitz matrix h
the parameters  that need to be learned are the matrices
of the kernel h over sx

5

algorithm 1 nonuniform sparse proximalaveraged
thresholding algorithm  type1 nupata1

algorithm 2 nonuniform sparse proximal average
network  type1 nuspan1

2

2

input y h l  khk2  kp
max 
m
i  0  i  1 i1 i  1
n
    r0    rn1  a  rn2 
initialize x0  0
while k  kmax do

z k1  xk  12l h0  y  h  xk 
m

p
xk1 
i pgi z k1

input y l  khk2  kmax
pm
i  0  i  1 i1 i  1
    rn0    rn1  a  rn2 
m
p
i pgi wy
initialize w s x0 
i1

while k  kmax do
ck1  wy  sxk
m
p
i pgi ck1 
xk1 

i1

k k1

i1

output x  x

k k1

kmax 

v p roximal average n etwork for r eflectivity
i nversion  n onuniform s parse m odel t ype 2
the generalized variant of the compositeregularization
problem given in eq 2 is given as follows
m
x
1
2
kh  x  yk2 
h i  qi xi
x  i  2
i1

arg min

st

m
x
i1

 i  1 0   i  1 i

8

t

where qi x  gi x1  gi x2      gi xn   gi is the scalar
penalty provided in table ii h i denotes innerproduct 0
is the null vector 1 is the vector of all ones and  i 
i1  i2      in t 
a optimization algorithm  type2
we can follow the strategy mentioned in section ivb for
optimizing the cost given in eq 8 the update for x at
k  1th iteration is given by


xk1  arg min h x xk 
x

 2
1
x  xk  x f xk 
 arg min
x 2
2
m
x

h i  qi xi
9
i1

since the above problem is complex and nonconvex we solve
the approximate variant of the problem corresponding to eq 9
based on the proximal average strategy 27
m

 2
x
i
xk1  arg min
x  xk  x f xk 
x
2
2
i1


m
x
h i  qi xi



i1

i

pgi uk 

b nonuniform sparse proximal average network  type2
nuspan2
representing the nupata2 update in 10 as a layer in
a neural network we obtain the nonuniform sparse proximal
average network nuspan2 the structure for each layer in
nuspan2 is given by
xk1 

m
x
i1

i

pgi wy  sxk 

11

where w and s definitions are provided in section ivc
similar to nuspan1 we optimize the learnable parameters
of nuspan2 subject to the constraints given in eq 8 by
minimizing the smooth 1 cost defined between the true reflectivity x and prediction x where  denotes the learnable
parameters such as the matrices w and s the parameters of
gi  i     rn0 p
and   rn1  a  rn2  and the
m
weights  i  0  i  1 i1  i  1 algorithm 4 gives
the nuspan2 algorithm for kmax layers and figure 3 gives
the structure of layers in nuspan2
vi e xperimental r esults
we demonstrate the efficacy of the proposed networks
namely nuspan1 and nuspan2 on both synthetic and
simulated datasets as well as on real data in comparison with
the benchmark techniques such as bpi 10 9 fista 11
12 and sblem 46 16 the performance of the proposed
approaches is quantified based on objective metrics computed
between groundtruth sparse vector x and the predicted sparse
vector x listed in the following section
a objective metrics

i1

m
x

xsx  hsx y
output x  xkmax 

10

algorithm 3 illustrates the nupata2 formulation for solving
the optimization problem 8 using the proximal average
strategy as in the previous section we unfold the update
in 10 into a layer of a neural network

we evaluate the performance in terms of amplitude and
support recovery metrics which are crucial to estimate the
amplitudes and locations of reflection coefficients
 correlation coefficient cc 51
cc  r

hx xi  hx 1ihx 1i


2
2
2
2
kxk2  hx 1i
kxk2  hx 1i

6

 3

1
latexit sha1base646zsnmc7p8lwsdxbg1qtpl5drkzeaaabnicbvdlssnafl2pr1pfqs7ddbbbvumkqcspuhfzwt6gcweynbrdjw9mjkqjrq3lhrx65e482ctflo64gbwzn3cs8cpfmksv6nipr6xubw9xt2s7u3v6bwtsytgvhhzjzgmx8lgknew0q5jidjaiikof074vsn8gmvksxrvzol1a3xogibi1hpytprmrninfgdzolmwj57lc9swe1rdrrk7ji0oethm7cuuzskeakcczl0lys5wzykey4zwtokmmcyrsp6vdtciduutk8eo5ottjcqsz0ixsaq783mhxkoqt9pvnklmteif7ndvmvxlkzi5ju0ygsdguprypgrq9oxaqlis80wuqwnrwrcraykn1wtzdgl395lfratfui2bo7b7svyzqqcawncay2xeibbqedxsdwcmwcmgkfivbsfi9gkue4cwr8ynz9hpqklatexit

2

latexit sha1base64amrt3kefts0hrxok15usl3hkreaaacbhicbvdlssnafj34rpuvddnnybeqqklefowkblxwsa9oy5hmj3qystmtiqyzohgx3hjqhg3foq78zjm4w2hrhwoode7r0nsbivyngraxlldw19djgexnre2fx3ttvyzgvmlrwzglrdzakjhlsulqx0k0eqvhasccyxd54eiswnpyyj8si05dskgckjxalhye1wojpzubroadllvxtfgje5z5dtwpo1pareiwpaoknh37qzicrorrjbduvzcj1gerkjrzehw7qesjaip0zd0douoitlt0ycyegsuaqxjyyorofvt2gustmjatoznyznvvz8zulkrz0novjqgjhs0vhyqckyz4ihfbbsgitqxaw1nwk8qgjhjxjrwxccodfxitt07p7xnduz6qnqykoeqiaq1adlrgadxadmqafmhgezavvflp1ov1bn3mwpesyuyaih1qnrdzfnlatexit

pgk1
1

latexit sha1base64aj3ueq1vbwwnlldqapdlzqsq4mwaaabnicbvdlssnafl3xwesr1awbybfclaskupkcg5cv7aoaecbtstt0mgkze6xefiobf4q49uvctdo2iy09cda4zx7uwdokdaqlw1gyurasbm5wt6vbo7t6wtvoyjgvmhrwzglrd5akjhlsuvqx0k8eqvhascy3br74eiswnr6yj8si04jskgcktwytcyokxkgyue0xzxpzdfrdsoewvomtknqukltm1ummzprljcdek5coxeerksimjg8qqbspigpeejmtcuo4hil5tfz60trqytmbb6cwxn1n8bgyqkneabnixyykwvepzbqkkr7ym8irvhop5otblloqtogdrsaxbik01qvhqndxcyyqqvrqtqi7bwfzymuk2g85fo3l3xm9dl3vu4aio4rqcuiqw3eiboodhez7hfd6mjpfedc5qmrrrlzch9gfp4agaqujqlatexit

3

latexit sha1base641txyw2wl3ikhpmp27wvpbrp8c4aaabhicbvbns8naen34wethox69lbbbu0lkuy8veby1ln5bg8pmu22xbjzhdylw0fixymixv0p3vw3btsctpxbwoo9gwbmzhgghzn21pb39jc2s7szhf39g9y9ufru4exoqxbqxgqtk80e1yybnaqrb0prgjfsjyvpn5rqemna9lhsyr8wiylhzakqej9excf9gjjjxqbe26xqlne3bekthz4fxipispulr79le3h9i4ybkoifp3xccclyekobvsmu3gmkwejsmqdqyvjgdasahtgzufp4ecptevbct2rkedrsecbzodasc97merxpd4mpluixiyjiufg1igshesxrwnytgquwmivrxcyumi6iibznv1otglr8sprfgntrkn6x8uvsgkcgnabtdi5cdink6a5vuqnrfknn9irercfrxxq3phata1y6c4zwpr8atiekr8latexit

latexit sha1base64ssbgw4qtp9vmacoldm8bfpr17skaaacahicbvbns8naen3ur1qoh48eakwwvnjqqgnkxjxwmfqbpczrtpl242yxciljclf8wlb0w8jo8wctdlo64obx3szzmwles4u2pa3uvlzxvvfqg7wtrz3dvfmyouilnjaifepjb9acvkmaadymbpp5eurwgnvwbywi9ryovi8udtbpqrxgkwmgibi355pelja9p5kyyxkgyue0xy3p3dfrdsoewvomtknqqetbn7cyuzsiaoghcs1cowevaxlyittvoamiiaytpcidjqvoklky2yp5napvozwgetdaqyznsiw5fs0yjqncwdaterxp8qqrhtzcxkarabzkvclnuqwwvavhdjikbptuee8n0rryzy4kj6mxqogrn8evl0m02nmtg86i3rop46iiy3sczpcdrlal3ae26icccvsmxtgb8ws8gogx7y1ypqzhgpjm8fpv6wzwlatexit

 2

latexit sha1base642hqwvv0iqwipujrtcapyac85g4caaabnicbvdlssnafl2pr1pfqs7dbivgqirf1juu3lisybqhdczttqhk0mymsgl5lpcufdervizr9x0mahrqcgdufcyz1zgorrqwz726isrw9sblw3azu7eshzv2wjnuynlfmyvfiecsmmpjv1hfycarbeubi1gelp4qciji35vzolxivqmnoqyqs05jv1zi2qmgrh5nymnm99xzcbdtoew1oltkkaukljm1ukmzprljcdek5doxeerksimjg8pqbspigpevjmtsuo4hil5thz61tryysmbb6cwxn1d8bgyqknewbnixyymwvepzhqkkr7ym8irvhopfotblloqtogdrraxbis00qvhqndxceyqqvrqtmi7bwf7ykum1ms5fs3v33mhfl3vu4rho4awcuiq23eihuodhez7hfd6mjpfedcfqmvo9w5gj8wpn8afqkuiwlatexit

latexit sha1base64fhv2t2sbofoygagc2cvgko4za4aaabnicbvbns8naen3ur1quj16crbbu0lkuyfiqghw6vf0iay2w7bpztn2j2ojfanepggifditfjds2b219mpb4b4azev7imqlbjzsk6tr6xvpzczw9s7unpndb6ggkotwscad2fkwopwjwgcgnlzcsbhvcdr0ruwp37ynurfa1gacutfha8h6jgdqutfmdoaqly9q1xflgtxlztj18zzexsga5k4ccmhbnwudxpbstyqqdcsvjtxw7bjbeerjidzdqroiemizygbu0f9qly49npetykz2rh0hdaqyznsixr5sy9tnt6govr0pujxjucrkbmxfgqawzlph3ilamuzg9zikbphye0wk07dazigljqdtyugqnmwxl0mjkhdo84xbyq5utojio0n0he6qg85qcv2ikqojgh7qm3pfb8at8wk8gxz1psrzbygpzafwbja5nklatexit

latexit sha1base64oiblpybagiwtcecoup4kwcwmvyaaacahicbvdlssnafj34rpuvdehczwarxjwkilqsghuxfewdmhamk5t26otbzeqoirtxy0lrdz6ge78gydtftp64mlhnhu59x45uwqyo2vlbx1jc2a1v17z3dvx3z4lank0xq6nkej2lgewmcxdbvtheypaji5hpo5pb0u8gpasirunau3iqoyhywspsxpphyu4whktktu2a9zpznmreg1plnhna0z8dkxk9jaftqeeuecc0iibxlrmqhbaxkzylqjhio6k4misv0qkyw1dqmeug3nz1q4dotbdhmhk5y4zn6eyinkzttyned5z1y0svf7xhpsjrn2dxmimi6xxrmhgselymgqmmgco1yrqwfstmi6jiftpzoo6bhvx5wxsazxty2brqlrvqniqketdiroky2uubvdoq7qiook9ixe0zvxzlwy78bhvhxfqgao0b8ynz872pbolatexit

 1

projection
operator

latexit sha1base64za2ye0edqv0nke2oyrnaxmmvfhuaaacahicbvdlssnafj34rpuvdehczwarxjwkilqsghuxfewdmhamk0k7dpjg5kyoirtxy0lrdz6ge78gydtftp64mlhnhu59x4fvybzx0bk6tr6xubta369s7u3r55cnhtssyp69jejhlge8uej1kxoag2scujks9y35cln7kunfkgbpilzizkkecgpas155redxaqsdyicyzmnc6yf4vne2bdaloz4gviv6sbknq888sjeppflayqifjd20rbzyketgur6k6mwerohizyunoyrey5eybap9pjcbhinxfggfq74mcrepni193lneqra8uoggytxbs7jnamw0mimbmyelymgqmugqux1yrqyfwtmi6jjbr0znudgr348jlptzr2zbn1f9fo31rx1najokxnyezxqi3uuad1euufekav6m14ml6mdnj3rpivdnh6amzx86vpbnlatexit

latexit sha1base64jtc63m6dmcubml1afdxxbpfs5puaaab3icbvblswmxgmzwv62vvcgll2arkklzlaiecx70wmeof1lnpu2odlksbjiwffgxhiqrgvg1vhuz7r60ohayzhwfmywfmaq043xzhyxfpewv4mppbx1jc8ve3mkpeutmmlgwitsuorrtpqaaky6ksqo9blpolzgfeamo4dd6ehevrenobxqjbasvdfzbqvujdrxcpejpxxce0o7dtlppmaf8snydlkkprtz97gcbxsljgdcnvdz1iewmsmmjg0livvirceiygpgsoryfrxjlnn8jdowrwikq5xmopnmjqahkiprjeomrmvcy8tvgvbuzdqhswacdx7abazqaxmyoablqrrnjeeyulnvohhsckstwulu4i7wpfwruqdv5qkxlm6yicfxaaksafz6aorkadnaegdajvibx69f6tt6s99lowcp3dsevwbf4l6wawlatexit

latexit sha1base64lcjlt3skb1imu5u7kkn2dbjyaaacanicbvdlssnafj3uv62vqctxm1ieuimjilosunblbfuanobjdnionuzczeqoq3djr7hxoyhbv8kdfokzujbd1w4nhmv994tjixk5tjfvmlpewv1rbxe2djc2t6xdfamk4fji0cs1h0ayqjo5y0ffwmdbnbubqw0gngv7nfesbc0pjfqulcvagnoq0prspivn3qj5aaycr0mp10ndult3r2vgk82qu3emgivelugvfgj69ldeom0ilxhhqtsuu6ipi2eopirrnjpjukqhqmh6rnkuuskp6cvzpdykamyxsiuv3cqp7qkjjyegwmmz9yznu5jxs1v46wnkk1qrjmelwprbfcm8dziggmdfjoyglki5feireggrk1rfhodov7xi2qd197zu3j5vg9dfhgvwci5adbjgajtadwicfsdgetydvbmpvkv1rv1mwstwcxmpvgd6mhhogxhqlatexit

pgk
1

y

latexit sha1base64zyandonfv0uthhcswr2vwdmh6gaaacbhicbvdlssnafj3uv62vqmtubotqeupsxmeu4mzlbfuanobjdnoonuzczeqoqxzubu3lhrx60e482ctflo64elh3pu5d57gphrqrzn2yqsrk6tbxq3s1vbo7t79v5bw0ajwksfixajboakyzstlqkkkw4scaodrjrb5drzow9esbrxozwnireieaddipeykmxyfsy4yybqahvm6nqb3ujo5du9s3644nwcguezcnfrajqzvfuheu5cwhvmsmqe68tk00goihljs1ekhjhcrqrnqechur6evzeconmoddsjjics7u3xmahvjow8b0ziflrs8tn6irpeepryofge4miyckgimcwcbxqqbbiu0mqfttccveycysvya1kqnaxx14m7xrnpa85t2evxluerxguwrgoahdcgaa4au3qahg8gmfwct6sjvferc5q0fk585bh9gff4abqox6alatexit

pgk1
2

w
latexit sha1base64kbfcbj1ebjoxjgfndkpoiautwioaaab8xicbvdlssnafl3xweur6tlnybfclurexrbdukxgh9igmpnetemnkzazeurox7hxoyhb8adfokzujbdwwczrmxofceiedauo63s7k6tr6xwdoqbs7u1xdg5bok4vwyalraw6aduoumsm4uzgj1fio0bgoxjf5n77czxmsxwwkwt9ia4ldzmjxkqpvyiaurbm7wmunvr7gxkmxgfqukbrry1rveli1qgiao1l3ptyyfuwu4ezgt91kncwvjossupzjgqp1slnhktq0yiggs7jogzntfgxmntj5egz3mepflxf87qpca9jmsknsjzkmwfctejdfdlhczsteesout1kjg1ffmbellw0j3uljy6r1xvmua79rbvu9rrgmm4gtpw4arqcacnaaidccwcmodl6cddjprrifdth8afo5wmupealatexit

latexit sha1base64vn0zkareo15sltgkyhxqnlybgaaab9xicbvdlsgmxfl1tx7wqi7dbivgqsyiqmuig5cv7apaswqymty0kwxjrild8onc0xciubsz7sy09udi4zx7yckjes60cd1vp7syuraud6sbg3v7o5v9wawqak0barxkpugdxltncwyybtbqiojgnoo8h4jvc7j1rpjsw9mstujfqsigrbkz00a8kduktlc2mq6qnbfuzocwiveqghrodqpfvcsnkbcei617nluyvwmk8mip9nkp9u0wwsmh7rnqcax1x42sz1fj1yjussvpckgmfp7i8oxzqpzyribkv70cver5ea6mrpmehsqwwzpxslhbmj8gpqybqlhk8swuqxmxwrevaygftuxzbglx55mbtp6t5f3b07rzwuizrkcathcaoexeidbqejlscg4ble4c15cl6cddjplpyip1dapn8wdwd5mplatexit

xk2

latexit sha1base64852kzogv7nsvczxahxjinsbaj0qaaab3icbvblswmxgmzwv62vvcgll2arkklzfvgprs8ek9ghtgvjztm2njsssvys6x78k148kolvvhnf2o23yo2doqmm99hjunhjcrton9wywfxaxmlufpaw9y3lk3d5pkxbktbhzmylapfgguk4ammpf2jakkfuzaugq81v3rcoqk0er8ql0ydtpsvig6ln73v9wqi1ds2vpkr3swv07b6lpbvsvj0j4dxxc1igoeo96sbcbyhhgvmkfid14m0lycpkwyklxvjrsker2haoozyfblljzp8ktw0sgd7qprdnzyovzcsfkosopkmkr6qws8tm6se5feanluawjx9ohjgdwscsdbhqsbbmy0mqltrkhxiijmlavfyyjbizx54nzzoqe1z1bk7ltcu8jilybwegalxwdmrggtrba2dwcj7bk3iznqwx6936mi4wrhxnfyb9fkd3bevqlatexit

xk1

latexit sha1base644syegt8aryqwrc5ncnoorwvb8waaacanicbvdlssnafj34rpuvdsvubotqnyupoi4llnrzwt6gjweynbrdj5mwmxhkenz4k25ckolwr3dn3zhps9dwaxco59zlvfcecansoc63tbs8srq2xtoob25t7zaettgacckxaowsy6azkeuu5aiipguokgkaoy6qtjq9zvpbahaczv1cqhxosgniyui2uk3z7sr0inmgk6mfl66ot6lt3r6vg082ku3omgivelugffgj69ldeom0ilxhhqtsuu6ipi2eopirrnxpjukqhqmh6rnkuuskp6cvzpdekamyxsiuv3cqp7qkjjyegwmmz9yznu5jxs1v46wnkk1qrjmelwprbfcm8dziggmdfjoyglki5feireggrk1rzhodov7xi2vwae15zbs8qjesijhi4asegclxwarrgbjrbc2dwcj7bk3iznqwx6936mluuwcxmafgd6mhia2xhglatexit

y

w

pgk
2

xk

s

pgk
3

latexit sha1base64vn0zkareo15sltgkyhxqnlybgaaab9xicbvdlsgmxfl1tx7wqi7dbivgqsyiqmuig5cv7apaswqymty0kwxjrild8onc0xciubsz7sy09udi4zx7yckjes60cd1vp7syuraud6sbg3v7o5v9wawqak0barxkpugdxltncwyybtbqiojgnoo8h4jvc7j1rpjsw9mstujfqsigrbkz00a8kduktlc2mq6qnbfuzocwiveqghrodqpfvcsnkbcei617nluyvwmk8mip9nkp9u0wwsmh7rnqcax1x42sz1fj1yjussvpckgmfp7i8oxzqpzyribkv70cver5ea6mrpmehsqwwzpxslhbmj8gpqybqlhk8swuqxmxwrevaygftuxzbglx55mbtp6t5f3b07rzwuizrkcathcaoexeidbqejlscg4ble4c15cl6cddjplpyip1dapn8wdwd5mplatexit

latexit sha1base64kbfcbj1ebjoxjgfndkpoiautwioaaab8xicbvdlssnafl3xweur6tlnybfclurexrbdukxgh9igmpnetemnkzazeurox7hxoyhb8adfokzujbdwwczrmxofceiedauo63s7k6tr6xwdoqbs7u1xdg5bok4vwyalraw6aduoumsm4uzgj1fio0bgoxjf5n77czxmsxwwkwt9ia4ldzmjxkqpvyiaurbm7wmunvr7gxkmxgfqukbrry1rveli1qgiao1l3ptyyfuwu4ezgt91kncwvjossupzjgqp1slnhktq0yiggs7jogzntfgxmntj5egz3mepflxf87qpca9jmsknsjzkmwfctejdfdlhczsteesout1kjg1ffmbellw0j3uljy6r1xvmua79rbvu9rrgmm4gtpw4arqcacnaaidccwcmodl6cddjprrifdth8afo5wmupealatexit

latexit sha1base64rzmovh86kwoismp0tji4hbmpswaaacbhicbvdlssnafj3uv62vqmtubotqeuqi4mnxcooygn1aw8nkommhtizhzikuiqs3oobf4q49spctdo2iy09ccfwzn3cu89fsyovi7zbrwwlldw14rrpy3nre0de3evjaneynleeytex0esmmpju1hfsccwbiui21fj357qciji34nzrepbiiacbxugzybplvrcpeuzmn1jpdz19mqb3ujodo9sz644nwckuejcnfrajoznfugeu5cwhvmsmqu68sqr5fqfdoslnqjjdhcyzqkxum5cons6ktktw0ygagktdffzyqvyc0cqwchl7pze6w814mud1exvc9jxlcaiix7nfqckgimcwcbxqqbbie0mqfttccveicysvya1kqndnx14krzoae15zbs8q9as8jiiogwnqbs64ahvwaxqgctb4bmgfbxzt9al9w59zfolvj6zd7avwbbpgx6qlatexit

pgk1
3

s
latexit sha1base64faazxddd5qui0cbv4f7p9kw1fsqaaab8xicbvdlsgmxfl1tx7wqi7dbivgqsyiqmuig5cv7qpbujjppg3nzibkjlcgoubf4q49wctdm2llo64ha4zx7ybnhj6uw6lrftmfldw19o7hz2tre2d0r7x80tzroxhsskpfu9rwkrrvoedj27hmnpqlbnjm8xvpxftrkqecblzxkihsgscubtsyzekopkd9h7al1fcqjsdwszetiqqo94vf3uheutcrpbjakzhc2pspvsjyjjps93e8jiymr3yjqwkhtz00lniktmxyoaekbzpizmpvzdsghozcx07msu0i14mud1egyueqlqcyjcsflhqsijriq7nwye5gzlxblktlbzcrtrtrnakkq2bgx5gxspkt6f1x37rxsu87rkmirhmmpehajnbifojsagyjneiu3xzgvzrvzmr8topnoifyb8kdxqaqalatexit

latexit sha1base64n9bpnccgyx78cgpvixpujmg91yaaabxicbva7t8mwghr4lvikj43fokiqs5ugbiwvlixfogpdzxjok1vx45sb1giilcwgbcrpwpnv4ntpsbwk6yflr7pvl8fsyo0o7zbs0sli2vrjbwyusbm1vb9s5us4leytleggnz8zeijhls1fqz0oklqzhpsnsfxev55irqw1eoyebeacbpsjlsrvzzxcsuopixolddpdwr8dz3644nwccoecglragubfuofaicr4rozpftxdwltpuhqihnjyr1ekrjherqqrqecrur56sr9bomesbqsho4hhp190akipuhnjmr0km16xif1430egfl1iej5pwph0otbjuauzvwibkgjubg4kwpcyrxemkedamslipwz398jxpndtcs5pzc1qpxxz1lmaboarv4ijzuafxoagaainh8axewzv1zl1y79bhdhtbknb2wb9ynz4i5wnlatexit

latexit sha1base64insky93spcken2m5o1mbr9dbnyuaaacanicbvdlssnafj34rpuvdsvubotqnyvruzcff7qsybqxjczttqhk0mymqhlcg78ftcufhhrv7jzb5y0wwjrgquhc7l3nuchfgphofbwlhcwl5zla2v1zc2t7btnd2wjfobsrphlbadaenckcdnrrujnuqqfawmtiprve63h4iqnoz3apwql0idtkokktksb3iqsggdhdyhw98pvplt3r6ug482ku3mmgppelugffgj49levhm0ilxhhqtsuk6ipi2eopirrnxljukqhqeb6rrkuuskpycvzpdikh0yxsiuv3cip7qkjjyhawmmz9yznq5jxtvv46wnkk1qrjqelwprbfcm8d9ingmdfxoyglki5feiheggrk1rzhodovjxpwic197zm3j5v6tdfhcvwaa5bfbjgattbdwiajsdgetydvbmpvkv1rv1mw1dsiqzpfah1ucpizmxhwlatexit

latexit sha1base64faazxddd5qui0cbv4f7p9kw1fsqaaab8xicbvdlsgmxfl1tx7wqi7dbivgqsyiqmuig5cv7qpbujjppg3nzibkjlcgoubf4q49wctdm2llo64ha4zx7ybnhj6uw6lrftmfldw19o7hz2tre2d0r7x80tzroxhsskpfu9rwkrrvoedj27hmnpqlbnjm8xvpxftrkqecblzxkihsgscubtsyzekopkd9h7al1fcqjsdwszetiqqo94vf3uheutcrpbjakzhc2pspvsjyjjps93e8jiymr3yjqwkhtz00lniktmxyoaekbzpizmpvzdsghozcx07msu0i14mud1egyueqlqcyjcsflhqsijriq7nwye5gzlxblktlbzcrtrtrnakkq2bgx5gxspkt6f1x37rxsu87rkmirhmmpehajnbifojsagyjneiu3xzgvzrvzmr8topnoifyb8kdxqaqalatexit

softmax
softmax
axis1
axis1
latexit sha1base64ujhl03vs2xbvmhevwbcaw5paq4yaaab3icbvdlssnafj3uv42vwjdubotqnyxpqt0ilyk4esv2bu0okmkhtqzhjmjtitihsxirj1r9z5n07bllt1wixdofdy7z1zkhutv1t5nbwnza38tvmzu7eof1wgjjkbgynhheithxkssmctjuvdhsiqvboc9i2x9dzz2exgsrryhjjhxqjtgnkayks31rmlju3jrtzxxbnug1n55zz1rkjdtueaq8tjsbfkqpesl7cf4sqkxgggpow6dqy8falfmsnt000kireeoqhpasprsksxzmfwlot9geqcv1cwbn6eyjfozst0nedivjdueznxp8bqkcsylpe4u4xixkegyvbgcbqh7vbcs2eqthaxvt0i8rajhpemydqjo8surpfupodl6fsrfayoplggjyaendabaicw1ahtydbgdydvbmti0x4934wltmjgzmcpyb8fkdl6siglatexit

 2 
 1 
 3

latexit sha1base64jmaiy1wwx5dycffauscg1js8caaacchicbvdlssnafj3uv62vqesxbovgqirf1gxrjcsk9gfncjpjbtt0kgkze6geln34k25ckolwt3dn3zhps9dwa8mczrmxe8jekalsu1vo7kyuraud2sbw3v7o6zwddyvnboem446ifyammxtbrvdhojwjwfddobzobwu89gjcuxdqmoax4vfmh5rgpsxfphyvzsfkbsbzkkerjkxrzdcee5nzdw363bdnsfajk5j6qhe2ze3jctnijyeyalhdh2orwmc0ujg7zmphistcz4bannyxyb9llzibl1qpxqgnkhx6ysmfq7i8orljbulrfwy7nofej3ibvwysvo3gskojjfnawzzbivpgkfvibrlgpjpgiqne1ybgltjtorqzdcbzpxibdzso5anh35xwdrlhfr2he3sghhsjwugwtvehefsinterejoejbfj3fiyl1amsucqyhxqngpjrflatexit

latexit sha1base64al2gzsmjqsfuyoab0t7kaotv2zqaaacchicbvbns8naen3ur1qoh49gcycp5kiqmeif48vbc00iww2k3bpjht2n0ijoxrxr3jxoihxf4i342bngdtfbds470zzuyfkans2fa3uvtaxlldq683nja3tnfm3b2e5jkg0cwccdepsargegqqhj0uwe4dhjcbpr0r9acept7ujauvxsoerprgpsxfphqvzshkbsbzkcexnkxxzderehntugbtbtlt2eteqcitvsh45tfbshjfkoicmnsdhw7vv6ohakeqdfwmwkpjmm8higmcy5bevn0kmi61kporvzolyhrqv7uyhesyyv1zyzvsm57pfifn8hudonlnekzbqmzdyoyzilulalyirvafjtogomgeleljldaronsgjoez7krdi7btnnlfv2rnmqukoown0he6qgy5qg92gduoigh7rm3pfb8at8wk8gxz0ppr9eyjpzafwbet5relatexit

latexit sha1base64diyrumvphrg2vkjyohkgb5glmiaaacchicbvdlssnafj3uv62vqesxbovgqiqq6rloxmuf4amlmnkph06yyszivbclm78ftcufhhrj7jzb5y0wwjrgweo59zlvff4cans2fa3uvlaxlldq67xnja3tnfm3b2o5kkg0caccdhzsqrgy2grqhj0ege48hl0ffn4xcfqejk43s1sccl8dcmisvyawlghrqksgay1cskjnif5nlixjipb9kznarnsnewprktglqamsryh55qacpbheijaszdxevlwchkgoq1n5wqydlgqhrgumipjdnd8mty60evsiffrgypurvjgxhslhsv0zyjes8v4jef1uhvderumkvrct2aawzzbivpgkfvabrlgjjpgiqne1yagltjtorqzdcozpxisd04zz0bdvzuvn6zkokjpargeoegsndetaqe2iugrpanx9gy8gsguexk60yzc8gpj8wdhwzrglatexit

latexit sha1base64ujhl03vs2xbvmhevwbcaw5paq4yaaab3icbvdlssnafj3uv42vwjdubotqnyxpqt0ilyk4esv2bu0okmkhtqzhjmjtitihsxirj1r9z5n07bllt1wixdofdy7z1zkhutv1t5nbwnza38tvmzu7eof1wgjjkbgynhheithxkssmctjuvdhsiqvboc9i2x9dzz2exgsrryhjjhxqjtgnkayks31rmlju3jrtzxxbnug1n55zz1rkjdtueaq8tjsbfkqpesl7cf4sqkxgggpow6dqy8falfmsnt000kireeoqhpasprsksxzmfwlot9geqcv1cwbn6eyjfozst0nedivjdueznxp8bqkcsylpe4u4xixkegyvbgcbqh7vbcs2eqthaxvt0i8rajhpemydqjo8surpfupodl6fsrfayoplggjyaendabaicw1ahtydbgdydvbmti0x4934wltmjgzmcpyb8fkdl6siglatexit

latexit sha1base64l9wx7xhxk7kzizaq9aox1muiymaaacahicbvc7tsmwfhxkq5rxgigbxajcyqoshicxgowxspqhnvhkoe5r1y4j20gqoiz8cgsdclhygwz8du6bavqozpnonht17z1hyqjsjvnt1vzw19y36punre2d3t176cnrcyx6wlbhbyesbfge9lvvdmyscvbpgskh05us7sksiinnq05t4hi0sglomtjec8glbyvuljsv9wqni1qeuvsedtnpotpazejwpakqdal7y4sezjhjngziqahrpnrpkdqum1i0veyrfoejgpghoqnirpn57iacnholgrgq5iuazttfhtniqtzrvhkkx2rrk8xvggm42sp0maazlgaa4y1alwkybiyoj1mxqcmksml0hhiojsdaznuwi7uljy6r33nivw879rbn9u8vrb8fgbjwbf1ybnrgdhdafgbtggbycnvjerhery95ac2qeg7bh1ifp5vjlw4latexit

1 2 3
latexit sha1base64kuio5lodp4oygopbzmsmzy6ksaaacahicbvdlssnafj34rpuvdehctbairkpsrf0w3bisybqhdcztnqh8wgze6gebpwvny4ucetnupnvnlrzaoubyq7n3mu990qpjuq77re1srq2vrfz26pv7zu7dshhz0lmolwfwkq5cccclpccvcttfeglriyiojnlktf4jlooiqcnkq4yhhgseas1kul72i8ejdwums3bcmjwir5qwjthtt0z3cwiverbqjqce0vpxyoy5hrrkfsq89ndzbdqqmiukj7mciprbm4wkndowrybfnsgmi5m0rsjekax7uzu3935jcpckdtyaaeq0wvfpzhplorooc8dttmkp5ocsjjhzomyyte4mrplndijle7oqgmzqqaznz3ytglz68thqtpnfzdo8vgu2bko4aoagn4bx44aq0wr3ogc5aoadp4bw8wuwivufcxlv6yq5wj8gfx5a51olw8latexit

latexit sha1base64sy2rfhkvnuixszihmyn2i9eila0aaacahicbvc7tsmwfhxkq5rxgigbxajcyqosqmbywcjyjeornvhkoe5r1bej20gqoiz8cgsdclhygwz8du6bavqozpnonht17z1hyqjsjvnt1zawv1bx6uunjc2t7r17d9biuxi0swccdkpkskmctlvvdpstyvbschilxzflh7vkuhfbbxk5t4crpyglomtjec8albyvujdff7omedfer5gdfydedljmfxcrurzqgqiewv7xi4cwhxgoglbq4tqr9helnmsnfw8susreeoyezgmprqpsftw8o4lfrihglar7xckr7shrosodtwwc9ejne6x4nzfidhzl55snmsyczwbfgynawdingffjsgytqxcw1owk8qhjhlxjrgfccodpxiqppy33ouxcntfb11ucdxaijsajcmelainb0afdgeebnsereloerbfr3fqyldasqmcfih1qoe05cqlatexit

fig 3
structure of layers in nuspan2 the matrices w s and  1  2  3  and i  which are the parameters of gi      rn
0 and
n
  rn

1 a  r2  corresponding to the penalties in table ii are shared across layers of nuspan2 the projection operator refers to the constraints
imposed according to table ii through the pytorch 50 clamp function the operators  and
represent sum and product all considered elementwise

algorithm 3 nonuniform sparse proximalaveraged
thresholding algorithm  type2 nupata2
2

input y h l  khk2  kmax
pm
 i  0  i  1 i1  i  1
    rn0    rn1  a  rn2 
initialize x0  0
while k  kmax do

z k1  xk  12l h0  y  h  xk 
m

p
xk1 
 i pgi z k1
i1

k k1
output x  x

2

input y l  khk2  kmax
p m
 i  0  i  1 i1  i  1
    rn0    rn1  a  rn2 
m
p
initialize w s x0 
 i pgi wy
i1

while k  kmax do
ck1  wy  sxk
m
p
xk1 
 i pgi ck1 
i1

kmax 

relative reconstruction error rre and signaltoreconstruction error ratio srer in db

2
2
kx  xk2
kxk2
rre 
 srer  10 log10

2
2
kxk2
kx  xk2


algorithm 4 nonuniform sparse proximal average
network  type2 nuspan2

k k1
xsx  hsx y
output x  xkmax 

profile contains 200 samples with amplitudes ranging from
10 to 10 padded with zeros before convolution with the
wavelet
to have the same length as the seismic trace 4 the
 probability of error in support pes is given by
sparsity
factor which is the ratio of the number of nonzero

t 
1 x maxsxi   sxi   sxi   sxi 
elements to the total number of elements is set to 005 we

pes 
t i1
maxsxi   sxi 
also report results for sparsity factors 010 015 and 020 in
the supplementary document the reflectorspike locations are
where    denotes the cardinality and s denotes the support chosen uniformly at random without any minimum spacing
of the argument
constraint between any two spikes reflectivity values are then
assigned to these locations picked randomly from the amplitude
b training phase
range of 10 to 10 the amplitude increment sampling
the synthetic training data that appropriately represents interval wavelet frequency and the initial hyperparameters
observed seismic data is generated as recommended by 43 vary depending on the dataset the optimum number of layers
we generate synthetic training data of size 5  105 seismic also varies with the dataset and we consider 10 15 or 20
traces each consisting of 300 samples obtained by convolving layermodels in our experiments our models are trained with
1d reflectivity profiles with a ricker wavelet each reflectivity a batch size of 200 use the adam optimizer 52 and with

amplitude

7

15
10
05
00
05
10

trueseis

noisyseis

amplitude

a

15
10
05
00
05

b

trueref

bpi

amplitude

c

15
10
05
00
05

d

fista

sblem

amplitude

e

15
10
05
00
05

f

nuspan1

0

50

100

nuspan2

150 200
time ms

250

g

300

0

50

100

150 200
time ms

250

300

h

fig 4
sample results for a synthetic 1d seismic trace a true seismic trace b noisy seismic trace c true reflectivity dh recovered reflectivity
blue circles compared with true reflectivity red crosses the proposed techniques nuspan1 g and nuspan2 h distinguish between closelyspaced
spikes around 150 ms the benchmark techniques bpi fista and sblem df predict a single reflector instead

the learning rate set to 1  103  and input measurement snr
set to 10 db  to ensure robustness against noisy testing
data 43 we consider models trained on 1d data to operate
tracebytrace on all the datasets synthetic 1d data synthetic
2d wedge models 53 simulated marmousi2 data 54 and
real data

nuspan1 and nuspan2 resolve closelyspaced reflection
coefficients whereas the benchmark techniques bpi fista
and sblem predict a single reflector from table iii observe
that nuspan1 and nuspan2 nonuniform sparse proximal
average network type 1 and 2 recover amplitudes with higher
accuracy than the benchmark methods and nuspan1 exhibits
superior support recovery the low computation time of the
proposed models is crucial for processing the large volume of
c testing phase  synthetic 1d  2d data
data in reflection seismic processing
1 synthetic 1d traces we validate the performance of
2 synthetic 2d wedge models synthetic 2d wedge
the proposed models over 1000 realizations of synthetic 1d models are considered to evaluate resolving capability on
traces with a 30 hz ricker wavelet 1 ms sampling interval thin beds 53 a wedge model typically consists of two
02 amplitude increments and the minimum spacing between interfaces one horizontal and another inclined with the polarity
reflection coefficients spikes 1 ms comparisons across several n negative p positive of reflection coefficients same or
methods are reported in figure 4 and table iii figure 4 shows opposite giving four possible types of wedge models here
the results for a sample synthetic 1d trace out of the 1000 we report results from an odd wedge model np with negative
test realizations for which the results are reported in table iii polarity on the upper horizontal interface n and positive

8

table iii
p erformance evaluation in terms of objective metrics averaged
over 1000 test realizations of synthetic 1d traces  t he
proposed nuspan1 and nuspan2 show superior amplitude
recovery in terms of cc rre and srer and significantly
lower computation time  while nuspan1 outperforms in support
recovery in terms of pes t he best performance is highlighted
in boldface  t he second best is underlined 

method
bpi
fista
sblem
nuspan1
nuspan2

cc

rre

srer

pes

time s

05499
05473
05501
05979
06050

07290
07203
07682
06354
06274

18687
18391
18276
22038
22508

09704
08112
09704
07104
09563

5038519
332539
10761833
01778
01870

on the lower inclined interface p and an even nn wedge
model while those for the remaining two variants are given
in the supplementary in our experimental setup each model
consists of 26 traces with separation between reflectors of
the two interfaces increasing from 0 ms to 50 ms in 2 ms
increments the amplitudes of the reflectors are fixed as  05
based on the polarity
results for the np odd wedge model are given in table iv
and figure 5 nuspan1 outperforms other benchmark techniques in terms of the objective metrics namely cc and rre
and shows superior support recovery measured in terms of
pes figure 5 highlights the fact that bpi fista and sblem fail to resolve the locations of closelyspaced reflectors
which is evident from the divergence observed below the tuning
thickness of 13 ms wedge thickness between 56 m 55
table v and figure 6 give the results for the nn even
wedge model for the even wedge model the baselines perform
marginally better than nuspan1 in terms of amplitude
recovery measured through cc rre and srer but nuspan1 outperforms in terms of support recovery table v figure 6
shows that the baselines and the proposed networks recover
the reflectivity profile well for the even wedge model
although nuspan1 outperforms the baselines in all the
objective metrics except srer we observe a drop in the
performance of nuspan1 and nuspan2 in the synthetic
2d case over the 1d case in section vic1 especially in
terms of srer we hypothesize that this could be attributed
to the mismatch between the training and testing conditions
further we observe a disparity between the performance of
the proposed networks in the case of the odd np vs even
nn wedge model this discrepancy could be attributed to
the suboptimal amplitude recovery of reflection coefficients in
the region where destructive interference is observed between
the two interfaces of the wedge models as highilghted in
figure 6 nonetheless the support recovery evaluated in terms
of pes remains comparable in the case of both wedge models
compare tables iv and v however these aspects need further
investigation
d testing phase  marmousi2 model
the marmousi2 model 54 is widely used in reflection
seismology to calibrate algorithms in structurally complex

table iv
m etrics for a synthetic 2d odd np wedge model  nuspan1
outperforms in amplitude recovery in terms of cc and rre and
support recovery in terms of pes t he best performance is
highlighted in boldface  t he second best is underlined 

method
bpi
fista
sblem
nuspan1
nuspan2

cc

rre

srer

pes

07865
07786
08064
08736
07858

02919
03008
02693
02553
03441

123197
120683
101708
84383
53164

09933
07090
09933
05994
09896

table v
r esults for a synthetic 2d odd nn wedge model  nuspan1
offers superior support recovery in terms of pes t he
benchmark techniques bpi fista and sblem show higher
amplitude recovery accuracy quantified in terms of cc rre
and srer

method
bpi
fista
sblem
nuspan1
nuspan2

cc

rre

srer

pes

08504
08791
08519
08208
07591

02831
02524
03280
03963
04605

124768
117202
107475
90663
55752

09933
06681
09933
05885
09896

settings the model width  depth 17 km  35 km an
expanded version of the original marmousi model width 
depth 92 km  3 km 56 has a 2 ms sampling interval with
traces at an interval of 125 m we obtained the reflectivity
profile from the pwave velocity and density models 1 and
convolved them with a 30 hz ricker wavelet to generate the
measurement
figure 7 shows the result for a region of the model with a gascharged sand channel 54 nuspan1 and nuspan2 preserve
the lateral continuity better evident from an observation of the
insets in figure 7 bpi and fista introduce false interfaces due
to the interference of multiple events at the ends of the channel
the testing times mentioned in table iii for synthetic data
when computed for the marmousi2 model further highlight
the advantage of the proposed approach for seismic processing
table vi
e testing phase  real data
we validate on real data from the field a 3d volume from
the penobscot 3d survey off the coast of nova scotia canada
57 we pick a portion of the 3d volume with 201 inlines
from inline 11501350 and 121 xlines between 10401160
chosen such that the region includes two wells wells l30
and b41 58 the sampling interval for the dataset is 4 ms
and the region chosen includes 800 samples between 03196
ms along the timedepth axis a 25 hz ricker wavelet fits the
data well also observed by 58
the recovered reflectivity profiles are shown in figure 8
along with the reflectivity profiles calculated from the sonic
logs of well l30 overlaid in black figure 8 shows that inverted
reflectivity for bpi and fista is smooth and missing details
with relatively poor amplitude recovery following convention

9

fig 5
results for a synthetic 2d odd np wedge model true a seismic traces and c reflectivity b noisy seismic traces with input snr 10 db
dh recovered reflectivity profiles show that nuspan1 g and nuspan2 h resolve reflectors  5 m thickness whereas the baselines df fail to do
so evident from the diverging interfaces highlighted by the rectangle in black

table vi
m etrics for a portion of the m armousi2 model  nuspan2
outperforms in terms of cc while nuspan1 shows superior
support recovery in terms of pes t he reported time is for the
complete model  showing significantly lower computation time
for nuspan1 and nuspan2 t he best performance is
highlighted in boldface  t he second best is underlined 

method
bpi
fista
sblem
nuspan1
nuspan2

cc

rre

srer

pes

time h

09473
09407
09549
09376
09591

00875
01017
00684
01032
00715

148024
138579
181890
142592
150973

09724
07146
09724
03693
09662

162671
55636
1012659
00737
01024

red indicates positive reflectivity and blue negative sblem results provide a more detailed image for characterization
by recovering the sparse reflectivity profiles but the method
fails to remove the noise the nuspan1 and nuspan2
recovered reflectivity profiles show better amplitude recovery
while removing the noise from the observations further the
nuspan variants also preserve lateral continuity especially
for closely spaced interfaces as seen around 11 s on the time

axis in figure 8
vii c onclusions
we considered the problem of seismic reflectivity inversion
based on systematically learned nonconvex sparseprior we
proposed a nonuniform sparse model based on composite
regularization and solved it based on a deepunrolled architecture for prior learning given training data using the
proposed framework we solved the problem of seismic
reflectivity inversion where the challenge lies in improving
the resolution to characterize the subsurface the proposed
techniques outperform benchmark reconstructions given by
stateoftheart techniques on synthetic simulated and real
datasets in terms of objective metrics
acknowledgments
this work is supported by ministry of earth sciences
government of india centre of excellence in advanced
mechanics of materials indian institute of science iisc
bangalore and science and engineering research board
serb india

10

fig 6
results for a synthetic 2d even nn wedge model true a seismic traces and c reflectivity b noisy seismic traces with input snr 10 db
dh recovered reflectivity the underperformance of nuspan1 and nuspan2 reported in table v could be due to the poor amplitude recovery as a
result of the destructive interference between the two interfaces in the region highlighted in black

r eferences
1 p m shearer introduction to seismology cambridge university press
2009
2 d w oldenburg t scheuer and s levy recovery of the acoustic
impedance from reflection seismograms geophysics vol 48 no 10
pp 13181337 1983
3  yilmaz seismic data analysis processing inversion and interpretation of seismic data society of exploration geophysicists 2001
4 b russell machine learning and geophysical inversion  a numerical
study the leading edge vol 38 no 7 pp 512519 2019
5 h l taylor s c banks and j f mccoy deconvolution with the 1
norm geophysics vol 44 no 1 pp 3952 1979
6 a j berkhout leastsquares inverse filtering and wavelet deconvolution geophysics vol 42 no 7 pp 13691383 1977
7 h w j debeye and p van riel p norm deconvolution geophysical
prospecting vol 38 no 4 pp 381403 1990
8 a tarantola inverse problem theory and methods for model parameter
estimation society for industrial and applied mathematics 2005
9 r zhang and j castagna seismic sparselayer reflectivity inversion
using basis pursuit decomposition geophysics vol 76 no 6 pp r147
r158 2011
10 s s chen d l donoho and m a saunders atomic decomposition
by basis pursuit siam review vol 43 no 1 pp 129159 2001
11 a beck and m teboulle a fast iterative shrinkagethresholding
algorithm for linear inverse problems siam journal on imaging
sciences vol 2 no 1 pp 183202 2009
12 d o prez d r velis and m d sacchi inversion of prestack
seismic data using fista mecnica computacional vol 31 no 20
pp 32553263 2012

13 d o prez d r velis and m d sacchi highresolution prestack
seismic inversion using a hybrid fista leastsquares strategy geophysics vol 78 no 5 pp r185r195 2013
14 c li x liu k yu x wang and f zhang debiasing of seismic
reflectivity inversion using basis pursuit denoising algorithm journal
of applied geophysics vol 177 p 104028 2020
15 f li r xie wz song and h chen optimal seismic reflectivity
inversion datadriven p lossq regularization sparse regression ieee
geoscience and remote sensing letters vol 16 no 5 pp 806810
2019
16 c yuan and m su seismic spectral sparse reflectivity inversion
based on sblem experimental analysis and application journal
of geophysics and engineering vol 16 no 6 pp 11241138 2019
17 e j cands m b wakin and s p boyd enhancing sparsity
by reweighted 1 minimization journal of fourier analysis and
applications vol 14 no 56 pp 877905 2008
18 t zhang analysis of multistage convex relaxation for sparse regularization journal of machine learning research vol 11 no 3 2010
19 i selesnick sparse regularization via convex analysis ieee transactions on signal processing vol 65 no 17 pp 44814494 2017
20 s j wright r d nowak and m a figueiredo sparse reconstruction
by separable approximation ieee transactions on signal processing
vol 57 no 7 pp 24792493 2009
21 ch zhang nearly unbiased variable selection under minimax concave
penalty the annals of statistics vol 38 no 2 pp 894942 2010
22 j woodworth and r chartrand compressed sensing recovery via
nonconvex shrinkage penalties inverse problems vol 32 no 7 p
075004 2016
23 j fan and r li variable selection via nonconcave penalized likelihood

11

fig 7
pwave velocity profile for the a marmousi2 model and b the portion corresponding to table vi c true reflectivity ground truth and dh
predicted reflectivity the inset plots are zoomedin portions of the selected area nuspan1 and nuspan2 preserve lateral continuity better

fig 8
a observed seismic data and bf predicted reflectivity profiles for the inset marked in a for xline 1155 of the penobscot 3d survey 57
the overlaid waveforms in black show the recorded a well seismic and bf reflectivity profiles respectively at well l30 58 nuspan1 and nuspan2
show superior amplitude recovery while also removing noise from the observations

12

24
25
26
27
28
29
30

31
32

33
34
35
36
37

38

39
40

41

42
43
44
45

46

and its oracle properties journal of the american statistical association
vol 96 no 456 pp 13481360 2001
w zhong and j kwok gradient descent with proximal average for
nonconvex and composite regularization proceedings of the aaai
conference on artificial intelligence vol 28 no 1 2014
u s kamilov a parallel proximal algorithm for anisotropic total
variation minimization ieee transactions on image processing vol 26
no 2 pp 539548 2016
h h bauschke r goebel y lucet and x wang the proximal
average basic theory siam journal on optimization vol 19 no 2
pp 766785 2008
y yu better approximation and faster algorithm using the proximal
average advances in neural information processing systems vol 26
pp 458466 2013
k gregor and y lecun learning fast approximations of sparse coding
proceedings of the 27th international conference on machine learning
pp 399406 2010
v monga y li and y c eldar algorithm unrolling interpretable
efficient deep learning for signal and image processing ieee signal
processing magazine vol 38 no 2 pp 1844 2021
i daubechies m defrise and c de mol an iterative thresholding
algorithm for linear inverse problems with a sparsity constraint communications on pure and applied mathematics vol 57 no 11 pp
14131457 2004
n shlezinger j whang y c eldar and a g dimakis modelbased deep learning 2020 arxiv201208405 online available
httpsarxivorgpdf201208405pdf
j zhang and b ghanem istanet interpretable optimizationinspired
deep network for image compressive sensing proceedings of the ieee
conference on computer vision and pattern recognition pp 18281837
2018
m borgerding p schniter and s rangan ampinspired deep networks
for sparse linear inverse problems ieee transactions on signal
processing vol 65 no 16 pp 42934308 2017
h sreter and r giryes learned convolutional sparse coding proceedings of the ieee international conference on acoustics speech and
signal processing pp 21912195 2018
r liu s cheng l ma x fan and z luo deep proximal unrolling
algorithmic framework convergence analysis and applications ieee
transactions on image processing vol 28 no 10 pp 50135026 2019
y li m tofighi j geng v monga and y c eldar efficient and
interpretable deep blind image deblurring via algorithm unrolling ieee
transactions on computational imaging vol 6 pp 666681 2020
p k pokala p k uttam and c s seelamantula confirmnet
convolutional firmnet and application to image denoising and inpainting
proceedings of ieee international conference on acoustics speech and
signal processing pp 86638667 2020
l wang c sun m zhang y fu and h huang dnu deep nonlocal unrolling for computational spectral imaging proceedings of the
ieeecvf conference on computer vision and pattern recognition pp
16581668 2020
b tolooshams s dey and d ba deep residual autoencoders for expectation maximizationinspired dictionary learning ieee transactions
on neural networks and learning systems 2020
d jawali p k pokala and c s seelamantula cornet compositeregularized neural network for convolutional sparse coding proceedings
of the ieee international conference on image processing pp 818822
2020
b tolooshams s mulleti d ba and y c eldar
unfolding neural networks for compressive multichannel blind
deconvolution 2021 arxiv201011391 online available
httpsarxivorgpdf201011391pdf
s yuan and s wang spectral sparse bayesian learning reflectivity
inversion geophysical prospecting vol 61 no 4 pp 735746 2013
y kim and n nakata geophysical inversion versus machine learning
in inverse problems the leading edge vol 37 no 12 pp 894901
2018
k j bergen p a johnson m v de hoop and g c beroza machine
learning for datadriven discovery in solid earth geoscience science
vol 363 no 6433 2019
a adler m arayapolo and t poggio deep learning for seismic
inverse problems toward the acceleration of geophysical analysis
workflows ieee signal processing magazine vol 38 no 2 pp 89119
2021
d p wipf and b d rao sparse bayesian learning for basis selection
ieee transactions on signal processing vol 52 no 8 pp 21532164
2004

47 s mache p k pokala k rajendran and c s seelamantula durin a deepunfolded sparse seismic reflectivity inversion network 2021 arxiv210404704 online available
httpsarxivorgpdf210404704pdf
48 p k pokala a g mahurkar and c s seelamantula firmnet a
sparsity amplified deep network for solving linear inverse problems
proceedings of the ieee international conference on acoustics speech
and signal processing pp 29822986 2019
49 m a t figueiredo j m bioucasdias and r d nowak majorization
minimization algorithms for waveletbased image restoration ieee
transactions on image processing vol 16 no 12 pp 29802991 2007
50 a paszke s gross f massa a lerer j bradbury g chanan
t killeen z lin n gimelshein l antiga a desmaison a kopf
e yang z devito m raison a tejani s chilamkurthy b steiner
l fang j bai and s chintala pytorch an imperative style highperformance deep learning library advances in neural information
processing systems vol 32 pp 80248035 2019
51 d freedman r pisani and r purves statistics international student
edition ww norton  company new york 2007
52 d p kingma and j ba adam a method for stochastic
optimization
2014
arxiv14126980
online
available
httpsarxivorgpdf14126980pdf
53 w hamlyn thin beds tuning and avo the leading edge vol 33
no 12 pp 13941396 2014
54 g s martin r wiley and k j marfurt marmousi2 an elastic
upgrade for marmousi the leading edge vol 25 no 2 pp 156166
2006
55 h chung and d c lawton frequency characteristics of seismic
reflections from thin beds canadian journal of exploration geophysics
vol 31 no 1 pp 3237 1995
56 r versteeg the marmousi experience velocity model determination
on a synthetic complex data set the leading edge vol 13 no 9 pp
927936 1994
57 o s r dgb earth sciences penobscot 3d  survey 2017 data
retrieved from httpsterranubiscomdatainfopenobscot
58 e bianco geophysical tutorial welltie calculus the leading edge
vol 33 no 6 pp 674677 2014

13

s upplementary m aterial
in this document we provide additional experimental validation with results on synthetic and simulated data for the
proposed nuspan1 and nuspan2 in comparison with
the benchmark techniques namely basispursuit inversion
bpi 10 9 fast iterative shrinkagethresholding algorithm
fista 11 12 and expectationmaximizationbased sparse
bayesian learning sblem 46 16

table vii
c omputational time  in seconds  during testing computed over
100 200 500 and 1000 realizations of synthetic 1d traces  t his
comparison shows that the proposed techniques require
significantly lower compute time as compared with the
benchmark methods during inference  t he best performance is
highlighted in boldface  t he second best performance scores
are underlined  bpi basis p ursuit i nversion  fista fast
i terative s hrinkage t hresholding a lgorithm  sblem
e xpectation m aximization  based s parse bayesian l earning 
nuspan1 and nuspan2 n onuniform s parse p roximal average
n etwork  t ype 1 and t ype 2

a experimental results and discussion
in this section we provide additional results to demonstrate
the efficacy of the proposed networks nuspan1 and nuspan2 in comparison with the benchmark techniques bpi fista
and sblem on synthetic 1d seismic traces and 2d wedge
models 53 and the simulated 2d marmousi2 model 54 we
quantify the performance based on objective metrics defined
in section via of the main document namely correlation
coefficient cc relative reconstruction error rre signaltoreconstruction error ratio srer and probability of error
in support pes
1 testing phase  synthetic 1d traces table vii shows
the computational time during testing for nuspan1 and
nuspan2 in comparison with the benchmark techniques
which is computed over 100 200 500 and 1000 test realizations of synthetic 1d traces the proposed networks namely
nuspan1 and nuspan2 require lower computational time
in comparison to other techniques fista the next best to our
techniques requires two orders of magnitude more computation
time than ours low computation times are significant in the
context of reflection seismic processing where the amount
of data to be handled is large we note that nuspan1 and
nuspan2 are trained on a large number of synthetic seismic
traces before testing and have longer training times but a much
shorter testing time
tables viii ix and x compare the performance of the
proposed nuspan1 and nuspan2 with that of the benchmark
techniques for sparsity factor 010 015 and 020 the results
for nuspan1 and nuspan2 for all the tested sparsity factors
are obtained by considering the model trained with sparsity
factor of 020 whereas the parameters of the benchmark
techniques are tuned for each sparsity factor this demonstrates
the robustness of the proposed nuspan to mismatch in sparsity
factor which is a significant advantage in the context of seismic
reflectivity inversion as the accurate estimation of the sparsity
of the seismic reflections is challenging 15 16
2 testing phase  synthetic 2d wedge models in section vic2 of the main document we have shown the results
for one odd np and one even nn wedge model where n
and p denote the polarity negative and positive respectively
of the two interfaces of the wedge models here we present
results for two wedge model variants one odd pn  table xi
and figure 9 and one even pp  table xii and figure 10
3 testing phase  simulated marmousi2 model initial
evaluations on the marmousi2 model 54 showed very low pes
for bpi and sblem table xiii possibly due to the spurious
support estimates introduced by these methods complementing
the lowamplitude spikes  1 of the absolute of the

method
bpi
fista
sblem
nuspan1
nuspan2

time s for  of test realizations
100

200

500

1000

450384
34676
1550183
00420
00493

930857
68286
3132058
00627
00707

2388020
175604
7812321
01505
01536

5038519
332539
10761833
01778
01870

table viii
m etrics averaged over 1000 test realizations of synthetic 1d
traces  for sparsity factor 010 nuspan2 shows superior
amplitude recovery accuracy mentioned in terms of cc rre
and srer nuspan1 and nuspan2 are trained with sparsity
factor 020 and tested for sparsity factor 010 whereas the
benchmark techniques bpi fista and sblem are tuned and
tested for the same sparsity factor of 010

method
bpi
fista
sblem
nuspan1
nuspan2

cc

rre

srer

pes

03764
03988
03827
03874
04042

08590
08383
08520
08472
08340

06749
07770
07036
07258
07950

09424
08880
09424
09130
09131

maximum amplitude in the marmousi2 model when these
spikes were muted the cc rre and srer for all the methods
improved but the pes now evaluated only on significant
interfaces was much higher for bpi and sblem and lower
for nuspan1 table vi in the main document
in table xiii we also provide the training and testing times
of the benchmark methods namely bpi fista and sblem and the proposed nuspan1 and nuspan2 on the
complete marmousi2 model comparing the three learningbased methods ie sblem nuspan1 and nuspan2
shows that the proposed approaches have lower combined
training and testing time than sblem although the combined
time for fista is low nuspan2 achieves higher accuracy
in terms of both amplitude and support recovery after training
on a larger synthetic dataset

14

fig 9
results for a synthetic 2d odd pn wedge model true a seismic traces and c reflectivity b noisy seismic traces with input snr 10 db
df recovered reflectivity signatures for bpi fista and sblem show that these methods fail to resolve reflectors with wedge thickness  5 m evident
from the diverging interfaces highlighted by the rectangle in black gh the recovered reflectivity profiles for nuspan1 and nuspan2 show better
resolution of reflector locations at wedge thickness  5 m

table ix
m etrics averaged over 1000 test realizations of synthetic 1d
traces for sparsity factor 015 t he proposed nuspan1 and
nuspan2 show superior amplitude and support recovery
although they are trained with sparsity factor 020 but tested
on a different sparsity factor  t he best performance is
highlighted in boldface  t he second best performance scores
are underlined 

method
bpi
fista
sblem
nuspan1
nuspan2

cc

rre

srer

pes

03602
03727
03646
03804
03904

08693
08585
08658
08528
08448

06126
06679
06299
06965
07377

09155
08759
09155
08725
08726

table x
m etrics averaged over 1000 test realizations of synthetic 1d
traces for sparsity factor 020 nuspan1 and nuspan2
outperform the benchmark techniques in both amplitude and
support recovery h ere  all the methods are tuned  trained with
the same sparsity factor as they are tested for 

method
bpi
fista
sblem
nuspan1
nuspan2

cc

rre

srer

pes

03550
03619
03240
03750
03812

08735
08687
08999
08574
08523

05910
06144
04602
06724
06984

08895
08781
08899
08339
08340

table xi
r esults for a synthetic 2d odd pn wedge model  nuspan1
shows superior amplitude recovery in terms of cc and
outperforms the benchmark techniques in support recovery
quantified in terms of pes

method
bpi
fista
sblem
nuspan1
nuspan2

cc

rre

srer

pes

08002
07984
08096
08650
07877

02711
02684
02622
02695
03440

130565
128473
103299
81181
53725

09933
07058
09933
06019
09896

table xii
r esults for a synthetic 2d even pp wedge model  nuspan1
outperforms the benchmark techniques in support recovery in
terms of pes whereas bpi fista and sblem show better
amplitude recovery accuracy mentioned in terms of cc rre
and srer

method
bpi
fista
sblem
nuspan1
nuspan2

cc

rre

srer

pes

08586
08791
08491
08037
07516

02608
02274
03205
03689
04540

129680
123515
10559
86339
545024

09933
06673
09933
05680
09896

15

time ms

0

5

wedge thickness m
10
15
20

25

40
60
80
100
trueseis
120

0

5

wedge thickness m
10
15
20

noisyseis
a

40

25

b

time ms

60
80
100 trueref

bpi
c

40

d

time ms

60
80
100 fista

sblem
e

40

f

time ms

60
80
100 nuspan1

nuspan2
g

h

fig 10
results for a synthetic 2d even pp wedge model true a seismic traces and c reflectivity b noisy seismic traces with input snr 10 db
dh recovered reflectivity all methods resolve the even wedge model well nuspan1 does not introduce spurious supports as the other methods seen as
lowamplitude spikes in regions away from the two interfaces in the results of the other methods

table xiii
r esults for a portion of the m armousi2 model corresponding to table vi and f igure 7 in s ection vid of the main document
without muting low amplitude spikes  nuspan2 provides the best amplitude recovery t he spurious supports introduced by bpi and
sblem coincide with some of the low amplitude spikes  leading to very low pes values  t he reported times are for the complete
model  training times on one rtx 2080 t i gpu t raining times are applicable only to nuspan1 and nuspan2 t he best
performance is highlighted in boldface  t he second best performance scores are underlined 

method
bpi
fista
sblem
nuspan1
nuspan2

cc
09472
09406
09548
09375
09590

rre
00878
01021
00686
01035
00718

srer
147626
138227
181117
142261
150579

time h

pes
00181
0912
00181
09650
02021

training

testing

training  testing

98069
86007

162671
55636
1012659
00737
01024

162671
55636
1012659
98806
87031

