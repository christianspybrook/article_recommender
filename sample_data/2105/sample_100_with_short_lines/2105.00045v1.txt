estimation and selection properties of the lad
fused lasso signal approximator
arxiv210500045v1 statme 30 apr 2021

xiaoli gao
department of mathematics and statistics
university of north carolina at greensboro

abstract
the fused lasso is an important method for signal processing when the hidden signals are sparse and blocky it is often used in combination with the
squared loss function however the squared loss is not suitable for heavy tail
error distributions nor is robust against outliers which arise often in practice
the least absolute deviations lad loss provides a robust alternative to the
squared loss in this paper we study the asymptotic properties of the fused
lasso estimator with the lad loss for signal approximation we refer to this
estimator as the lad fused lasso signal approximator or ladflsa we
investigate the estimation consistency properties of the ladflsa and provide sufficient conditions under which the ladflsa is sign consistent we
also construct an unbiased estimator for the degrees of freedom of the ladflsa for any given tuning parameters both simulation studies and real data
analysis are conducted to illustrate the performance of the ladflsa and
the effect of the unbiased estimator of the degrees of freedom
keywords estimation consistency jump selection consistency block selection consistency
degrees of freedom fused lasso least absolute deviations sign consistency

 correspondence

106 petty building greensboro nc 27412 email x gao2uncgedu

1

1

introduction

highdimensional data arise in many fields including signal processing image denoising and genomic and genetic studies when a model is sparse and has certain known structures penalized
methods have been widely used since they can incorporate known structures into penalty functions in a natural way and can do estimation and variable selection simultaneously a biological
example is the analysis of copynumber variations in a human genome in this problem we are
interested in detecting the changes in copy numbers based on data from comparative genomic
hybridization cgh arrays for instance snijders et al 2001 studied a cgh array consisting
of 2400 bacterial artificial chromosome bac clones where the log base 2 intensity ratios at all
clones are measured in figure 1 we plot a sample cgh copy number data on chromosomes 14
from cell line gm 13330 the data set has two characteristics 1 there are only a small number of chromosomal locations where the copy numbers of genes change that is the underlying
signals are sparse 2 the adjacent markers tend to have similar observations ie the signals are
blocky
in a signal approximation model with sample size n the ith observation yi is considered to
be a realization of the true signal 0i plus random noise i 
yi  0i  i  i  1  n

1

in many cases the true signal vector 0  01   0n  is both blocky and sparse meaning that the
intensities of true signals within each block are the same and most blocks consist of zero signals
the goal here is to find a solution not only to recover all the changes points but also to identify
the nonzero blocks we can use the lasso penalty to enforce a sparse solution by penalizing the
1 norm of the signals 1  ni1 i  and use the fusion total variation penalty to enforce a
blocky solution by penalizing tv  ni2 i  i1  the combination of these two penalties
results in the fused lasso fl penalty tibshirani et al 2005
for detecting changing points in copy number variations tibshirani and wang 2008 proposed to use the fused lasso with a squares loss function we refer to this approach as the least
squares fused lasso signal approximator lsflsa for n  1n  2n  the lsflsa seeks to
n2 n   
n2 n  that minimizes
find 
12 n   
n

n

n

yi  i 2  1n  i   2n  i  i1 
i1

i1

2

i2

where 1n and 2n are two nonnegative penalty parameters
recently rinaldo 2009 studied the selection properties of the lsflsa and adaptive lsflsa under the block partition assumption in the underlying signal several authors have also
studied the properties of related procedures for example mammen and van de geer 1997 studied the rate of convergence in bounded variation function classes of the parameter functions harchaoui and lvyleduc 2010 investigated the estimation properties of change points using the
total variation penalty boysen et al 2009 studied the asymptotic properties for jumppenalized
leastsquares regression aiming at approximating a regression function by piecewiseconstant
functions these studies significantly advanced our understanding of lsflsa or fusion penalized ls methods in the context signal detection or nonparametric estimation however all these
2

results are obtained for methods with the least squares loss andor require normality assumption on the errors the lsflsa is easily affected by outliers which arise often in practice for
example in cgh copy number variation data
a more robust fused lasso signal approximator can be constructed by using the lad loss
which we shall refer to as ladflsa for any given n  1n  2n  the ladflsa is defined
as
n

n

n

n n   arg min  yi  i   1n  i   2n  i  i1  

rn

i1

i1

3

i2

n in 3 has been applied to detect copy number variation breakpoints in
the convex minimizer 
gao and huang 2010a however its theoretic properties of have not been studied
in this paper we seek to answer the following questions about the ladflsa 1 how close
1
n can be to the true model 0 asymptotically 2 how accurately 
n1 can recover the true

nonzero blocks with a large probability when 0 is both sparse and blocky 3 what is the
degrees of freedom of ladflsa the contributions of this paper are as follows
 we show that the ladflsa is rate consistent if the number of blocks is relatively small
 we provide sufficient conditions under which the ladflsa is able to recover the block
patterns and distinguish nonzero blocks from zero ones correctly with high probability
that is the ladflsa can determine all the jumps identify all the nonzero blocks and
also distinguish the positive signals from negative ones under some conditions
 in terms of model complexity measures we justify that the number of nonzero blocks
generated from a ladflsa estimate is an unbiased estimator of the degrees of freedom
of the ladflsa
 without the assumption of gaussian or subgaussian random error our studies can be
widely applied for signal detection in signal processing when random noises do not follow
nice distributions or the signaltonoise ratios are large
the rest of the paper is organized as follows we list some notations in section 2 we study
the estimation consistency and sign consistency properties of the ladflsa respectively in
section 3 and 4 in section 5 we derive an unbiased estimator of the degrees of freedom of the
ladflsa in section 6 we conduct simulation studies and real data analysis to demonstrate the
performance of the ladflsa we also verify the effect of unbiased estimator of the degrees of
freedom numerically in this section section 7 concludes the paper with some discussions all
the technical proofs are postponed to the appendix

2

preliminaries

suppose the true model 0  01   0n  includes j0 blocks and there exists a unique vector
 0  10   j00  such that
j0

0i   j0 ii  bj0 
j1

3

4

where b10   bj00  is a mutually exclusive block partition of 1  n generated from 0  based
on the block partition we can rewrite 1  n as i0   i1  1 i1   i2  1  ij0 1   ij0  1
where 1  i0  i1    ij0 1  ij0  1  n and ij1   ij  1 gives the jth block set bj0  we
denote the jump set in the true model as j 0  then j 0  i1   ij0 1  and j0  j 0   1 where
j 0  is the the cardinality of j 0  let k0  k0   1  j  j0  j0  0 be the set of nonzero
blocks of 0 and the number of nonzero blocks k0  k0  we now list the following notations
that will be used throughout the paper some of which are adopted from rinaldo 2009
 for the true model 0 defined in 4 we introduce the following notations iiv
i b0j  bj0  the size of the block set bj0 for 1  j  j0 
ii b0min  min1jj0 b0j  the smallest block size
iii an  minij 0 0i  0i1  the smallest jump
iv n  minjk0 j0  the smallest nonzero signal intensity
n in 3 as follows
 corresponding notations are analogous to a ladflsa estimate 
v   j 
n    j
n  bj  bj 
n  with 
bj  bj  and j  j 
n  for 1  j 

b are the estimated jump set number of blocks block partitions of 1  n with
n 
corresponding block size and the associated unique vector generated from 
  k

  k
vi k
n   1  j    j  0 is the set of estimated nonzero blocks and k

3

estimation consistency of ladflsa estimators

n  we first consider the
in this section we study the estimation consistency of the ladflsa 
following conditions
a1 error assumption random errors i s in model 1 are independent and identically distributed with median 0 and have a density f that is continuous and positive in a neighborhood of 0
a2 block number assumption the true block number j0  m1 n for a constant m1  0 where
n  max16n22n  2n2 21n  n2n  n1n   1 with 22n  2n21n 
in a1 we only require that the random errors have a density that is continuous and positive in
neighborhood of zero and have median zero this is a much weaker condition than the gaussian
random error assumption required in harchaoui and lvyleduc 2010 and rinaldo 2009
indeed a1 allows all heavytail distributions of the errors including the cauchy distribution
whose moments do not exist condition a2 requires that the number of blocks in the underlying
model can increase with n but at a slower rate than on  as a byproduct the tuning parameter
for jumps 2n  grows much faster that the tuning parameter for signals intensities 1n  it is a
reasonable assumption since the true model is blockwise that is the number of nonzero jumps
can be much smaller than the number of the nonzero signals for example if the number of
jumps is ologn12  then we can let 2n  n12 logn14 and 1n  n1 
4

n in 3 we first
in order to study the asymptotic properties of the ladflsa estimator 
investigate its lsflsa approximation
n

n

n

n n   arg min zi  f 012 i 2  1n  i   2n  i  i1  



i1

i1

5

i2

where zi  f 012 0i  i with i  4f 012 sgni  for 1  i  n consist of pseudosignal
data thus all estimates listed in vvi can be analogous to the corresponding ones generated
n  we now provide some rate upper bounds for the number of blocks generated from 
n
from 
n in 3 respectively
in 5 and 
lemma 1 under a1 we have i j  16n22n  2n2 21n   1 provided 22n  2n2 21n and ii
  n2n  n1n   1 provided 2n  n1n  in addition suppose a2 holds we also have iii
j    j0  m1  2n  where both m1 and n are defined in a2
the proof of lemma 1 is given in the appendix lemma 1 gives upper bounds for the number
n and 
n  we can interpret bounds in i and ii as the maximal
of blocks associated with 
n and 
n may belong respectively similarly iii provides
dimension of any linear space where 
n  
n and 0 can
us an unified rate upper bound for the dimension of any linear space to which 
n and 
n  furthermore
belong lemma 1 is useful in obtaining the estimation consistencies of 
it is important to notice that those upper bounds in lemma 1 are mainly affected by the rate of
2n  which is reasonable since the number of jumps in an flsa model is mainly determined by
2n 
denote 2n  ni1 2i n and 22  ni1 2i  below we present the estimation properties of
n in 5

lemma 2 suppose a1a2 hold then there exists a constant 0  c  1 such that
p 
n  0 n  n   n expn log n  1  c2 f 02nn2 

where n is defined in a2 and n  1c f 01n 22n m1 1n n12  furthermore

if we let n  2m2 n log nn12 and choose 1n and 2n such that 1n  22n  c f 0n 
m1  1n n12 for a constant m2  1f 01  c2  then
p 
n  0 n  n   n n1m2 f 01c

2 

n



the proof of lemma 2 is given in the appendix lemma 2 gives us the estimation consistency
n using pseudo data zi s and bounded noises i s it is worthresult for a pseudo lsflsa 
while to point out that even though we only report the consistency result for a pseudo lsflsa
n with bounded noises i s in lemma 2 we can obtain a similar consistency result for
estimator 
the regular lsflsa estimator 2 under the assumption of gaussian noises without much extra
work thus the estimation consistency properties of the ls signal approximator with the total
variation penalty in harchaoui and lvyleduc 2010 can also be obtained from lemma 2 by
taking 2n  0 and n  kmax 
n in lemma 2 plays an important role in deriving the correspondthe consistency result of 
n in the following theorem 1
ing estimation consistency result of 
5

theorem 1 suppose a1 and a2 hold then there exists a constant 0  c  1 such that
p 
n  0 n  n   n expn log n  1  c2 f 08nn2   8f 0n nn2 12 

where n is defined in a2 and n  2c f 01n  22n  m1  1n n12 
12 for a constant m  1f 01  c2  and
furthermore if we let n  8m3 n log nn
3

choose 1n and 2n such that 1n  22n  c f 02n  m1  1n n12  then
2 1
n

p 
n  0 n  n   n nm3 f 01c


 o 1 log n 

n can
the proof of theorem 1 is given in the appendix theorem 1 implies that the ladflsa 
0
12
be consistent for estimating  at the rate of o n log nn  furthermore if the number of
blocks in true signals is bounded the rate of convergence can be stated more explicitly as in the
following corollary 1
corollary 1 suppose a1 holds and there exists jmax  0 such that j0  jmax  then

 j
  jmax   
p maxj
n  0 n  n   jmax nc2m jmax  o 1 log n
for n  8m jmax log nn12 and 1n  22n  c1m jmax log nn12  jmax n12  here
m  11  c2 f 0 is a constant c1m  2m c2 f 012 and c2m  f 0m 1  c2  1
n is consistent for estimating 0 at the rate o jmax log nn12 
corollary 1 says that the 
if the numbers of both true and estimated jumps are bounded above this convergence rate can
be compared to n12  which is argued by yao and yu 1989 to be optimal for ls estimators
of the levels of a step function notice that if limn p  j 0   1 then ni1 
i  0i 2 
j0 0
0
bj 
j  j0 2  b0min jj1

j  j0 2 for large n almost surely thus corollary 1 implies that
j1
for large n
2
p   j 0   
n   0 2  8m jmax log nb0min 12   jmax nf 0m 1c 1jmax 

6

n can converge to  0 in 2 norm at
where the convergence rate is affected by b0min  therefore 
0
12
n can converge faster to the
rate o jmax log nbmin   in other words a block estimator 
true model 0 with larger block size

4

block sign consistency of ladflsa

in this section we study the sign consistency of the ladflsa the sign consistency has been
studied by zhao and yu 2006 and gao and huang 2010b for both the lslasso and ladlasso in highdimensional linear regression settings it is a stronger result than variable selection
consistency since it not only requires that variables to be selected correctly but also their signs
are estimated correctly with high probability
in light of the block structure in the hidden signals we consider the selection consistency and
sign consistency for jumps and blocks separately

6

n is jump selection consistent if
definition 1 
lim p   j0  1jj0 bj  bj0   1

n

n is jump sign consistent if
definition 2 
i1   sgn0i  0i1  i  j 0   1
lim p   j 0  sgn
i  

n

n can partition the signals into blocks correctly with probability
definition 1 requires that 
n finds not only
converging to one definition 2 is a stronger requirement since it requires that 
all the jumps but also the jump directions updown correctly a jump selection consistent
estimator can recover the jumps set j 0 correctly with high probability but does not tell us which
blocks have nonzero intensities in other words there may exist   0 and 1  j  j0 such that
n 
p 
j  0  j0  0   for a jump sign consistent 
we now define the block selection consistency and the block sign consistency in definitions
3 and 4 the latter is a stronger definition since it requires the signs of the signals to be recovered
correctly
n is block selection consistent if
definition 3 
  k0   1
lim p   j 0  k

n

n is block sign consistent if
definition 4 
  k0  sgn
lim p   j 0  k
j   sgnj0  j  j0   1

n

41

jump selection consistency

for 1n  0 a ladflsa becomes a lad signal approximator using only the total variation
penalty ladfsa defined as
n

n

fn 2n   
fl

n 0 2n   arg min  yi  i   2n  i  i1  
i1

7

i2

fn 2n  can do the block partition correctly then we expect to sort out those nonzero
suppose 
blocks by increasing 1n slowly from 0 so we first investigate the jump selection consistency of
fn 2n  below we list some conditions on the smallest value of true jumps and smallest size

of the true blocks in model 1 and 4 for the jump sign consistency recall that b0min and an are
defined in ii and iii in section 2
b1 a 2n   b there exists a   0 such that 2n logn  j0 12  1  2
0
0
12
12
b2 a b
min  an   b there exists   0 such that bmin  logj0  an  31 
 2f 0 for sufficiently large n

b3 2n  f 03b0min an for sufficiently large n
7

here b1 and b3 indicate that 2n increases with n faster than o logn  j0 12  but slower
than ob0min an  b2a requires that either the smallest jump or the smallest size of all blocks in
the true model should be large enough so that 1  n can be partitioned into different blocks
correctly b2b strengthes b1a by providing a lower bound conditions b1b3 provide us
some helpful information in finding an optimal tuning parameter in model 7 when the above
fn 2n  can group all signals into different
conditions are satisfied the ladfsa estimator 
blocks correctly with a large probability
theorem 2 consider the signal approximation model 1 with the true model 4 a ladfsa
fn 2n  is jump sign consistent under a1 and b1b3
estimator 
the proof of theorem 2 is postponed to the appendix theorem 2 tells us that we can apply a
ladfsa approach to recover not only the true jumps but also their signs correctly with high
probability if the true hidden signal vector is blocky and the tuning parameter 2n is chosen
appropriately

42

block selection consistency

we have seen that a ladfsa solution can be jump selection consistent to the blocky hidden
signal vector under some conditions in many cases the true signal vector includes some zero
blocks which cannot be separated from nonzero ones using the ladfsa approach since the total variation penalty only shrinks adjacent differences but not signals themselves the additional
lasso penalty of flsa can force the estimates of some block intensities to be exactly zero we
are interested in finding a ladflsa solution to not only recover the true jumps but also find
the zero blocks and keep only the nonzero ones with a large probability eventually we need to
study how to choose tuning parameters 1n and 2n appropriately such that the ladflsa is
block selection consistent
when the true block model in 4 is also sparse we need the following additional conditions
to separate nonzero blocks from zero ones
c1 a 1n b0min
12   when n   b there exists   0 such that 1n b0min  logj0 
k0 12  4 21  
c2 2n b0min  1n 8 for sufficiently large n
0
0
12
12
c3 a
 n bmin    when n   b there exists   0 such that n bmin  logk0  
2 21  f 0

c4 2n b0min  f 0n 3 for sufficiently large n
c5 1n  f 0n 2 for sufficiently large n
here condition c1 and c2 indicate that either 1n or the smallest block size b0min should grow
with n with a lower bound provided in c1b since 2n grows with n from b1 especially
if 1n is relatively small as seen in c5 b0min must be large enough c4 and c5 provide us
a lower bound for the smallest nonzero signal n when n is large above interpretations are
consistent with c3a which requires either the block size or the true nonzero signal intensities
8

should be large enough such that the nonzero blocks can be separated from zero ones in other
words if n is relatively smaller it becomes harder to separate nonzero ones from zero ones
however it is not impossible for us to distinguish those nonzero blocks if we have larger enough
block size since more observations can be used to estimate j0 within jth block c3b provides
us an upper bound of the number of nonzero blocks it is worthwhile to point out that even though
these conditions seem to be complicated some of them can be redundant for instance c2 and
c5 can be used to derive a smaller upper bound than the one in c4 thus if both c2 and
c5 are satisfied c4 can be redundant one can see that c3a can be also redundant if c5
and c1a hold
theorem 3 under a1 b1b3 and c1c5 a ladflsa solution is block sign consistent
theorem 3 tells us that the ladflsa can first recover the block patterns of hidden signals by
detecting all the true jumps and then rule out those nonzero blocks furthermore with a very
large probability those nonzero blocks are identified correctly to have either positive or negative
signals thus the ladflsa is justified to be a promising approach for signal processing when
the true hidden signal vector is both blocky and sparse and the observed data are contaminated
by outliers the proof of theorem 3 is provided in the appendix
remark 1 the block assumption of the true model in 4 is crucial in our study if the
model is grouped but not blocky fused lasso might be misleading since the fusion term is used
to generate the blockwise solution some other techniques such as group lasso yuan and lin
2006 or smooth lasso hebiri 2008 can be more useful to generate the corresponding group
sparsity structure
remark 2 the relaxation of gaussian or subgaussian random error assumption is important since it is very common to see some contaminated data in signal processing especially when
repeated measurements are not available some normalization methods such as loess have been
used in preprocessing the real data in order to improve the robustness of lsflsa however
those techniques may oversmooth the data and then generate some false negatives

43

additional remarks on asymptotic properties

we will provide two additional comments on the asymptotic results obtained in section 3 and 4
remark 3 an ladflsa may not reach the estimation consistency and sign consistency simultaneously
the rate estimation consistency in theorem 3 holds for 1n  22n  olognn12  however from b1b and c2 we know one of the sufficient conditions for the sign consistency in
theorem 3 requiring kn  ologn12 for k  1 2 so an ladflsa may not be able to reach
both the estimation consistency and sign consistency simultaneously however this claim is not
theoretically justified since all conditions assumed in both theorem 3 and 3 are sufficient
remark 4 the weak irrepresentable condition is not necessary for the jump point detection
consistency in theorem 2
to understand remark 4 we will transform the signal approximation model in 7 into a
lasso representation consider a linear regression model
p

yi   xij j  i 
j1

9

1  i  n

8

where yi  xi1   xip  and   1   p  represent the observed data and coefficients vector a
lasso solution tibshirani 1996 of  is
p

n

p



 arg min 12 yi   xij j 2    j  
i1

j1

j1

if we further divide the coefficients vector   1  2   where 1 include those nonzero coefficients and 2 includes zeros only and correspondingly we can write x  x1  x2  and
s1  sgn1  consist of sign mappings of nonzero coefficients in the true model then the weak
irrepresentable condition of the designed matrix x means
x2 x1 x1 x1 1 s1   1

9

where 1 is a vector with element being 1 naturally we can write the ladfsa in 7 into a
lasso solution of   1   n  
n

nf 2n   arg min y  z2  2n  i  


10

i2

where 1  1  i  i  i1 for 2  i  n and z is the low triangular design matrix with nonzero
items being 1 zhao and yu 2006 proved that the weak irrepresentable condition is a necessary
condition for a lasso solution in 8 to be sign consistent under two regularity conditions we
list the result in the following lemma 3
lemma 3 zhao and yu 2006 suppose two regularity conditions are satisfied for the designed
matrix x 1 there exists a positive definite matrix c such that the covariance matrix x xn 
c as n   and 2 max1in xi xi n  0 as n   then lasso is general sign consistent

limn p   0 sgn
 sgn0   1 only if there exists n so that x satisfies the weak
irrepresentable condition holds for n  n  here 0 is the true coefficient vector
unfortunately it is easy for us to verify that the design matrix z in 10 does not satisfy the weak
irrepresentable condition for example if we consider a signal approximation data with only five
observations where 1  2  3  4  5  then the first row vector of z2 z1 z1 z1 1 is 0 0 1
thus 9 is violated however there is no contradiction between the sign consistency result in
theorem 2 and lemma 3 since both two regularity conditions in lemma 3 are violated for
design matrix z suppose 1  n are eigenvalues of z zn we know that a 1  13n  0
and n  4n12   when n   and in addition b max1in zi zi n  1

5

degrees of freedom of ladflsa

it is crucial to seek appropriate 1n and 2n in 3 large 1n will generate all zero coefficients
while large 2n will generate all zero jumps conditions on 1n and 2n in section 3 and 4 provide
us some guidance in choosing the rates of two tuning parameters to obtain a wellbehaved ladflsa estimate this section helps us to choose two optimal tuning parameters from the model
selection point of view
10

for given 1 and 2  a ladflsa approach is a modeling procedure including both model
selection and model fitting the complexity of a modeling procedure is defined as the generalized
degrees of freedom df and measured by the sum of the sensitivity of the predicted values see
ye 1998 and gao and fang 2011 for the discussion on the df for a modeling procedure under
i y 1  2  be a ladflsa
both the 2 and 1 loss functions respectively for 1  i  n let 
fitted value of yi for any given 1 and 2  the degrees of freedom of a ladflsa approach
n

df1  2    e
i y 1  2 yi 

11

i1

in theorem 4 we provide an unbiased estimator of df1  2  in 11 for a ladflsa modeling
procedure
theorem 4 consider a ladflsa modeling procedure defined for 1 3 and 4 for any
fixed positive 1 and 2  we have
 1  2   df1  2 
ek

12

 1  2  is an unbiased estimatheorem 4 indicates that the number of nonzero blocks k
tor of the degrees of freedom of a ladflsa modeling procedure with any given 1 and 2 
we provide both the numerical demonstration and theoretical proof are provided in section 63
and the appendix respectively in fact such an unbiased estimator in 12 can be also observed from theorem 2 of li and zhu 2008 for example if 1  0 for any 2  0 the
ladflsa reduces to a ladlasso solution of w wi  i  i1  for 2  i  n then
n
yi 0 2 yi  0 2  suppose 2  0 is fixed the block partition is decided then
i1 
for 1  0 the ladflsa becomes a lasso model of   is the block intensity vector
 2 
therefore ni1 
yi 1  2 yi  k
results in theorem 4 can be used to choose two optimal tuning parameters from the model
selection point of view let yi0 s denote new observations generated from the same mechanism
y 1  2  is defined as
generating yi s the prediction error of 
n

e0  
i 1  2   yi0 

13

i1

where e0 is taken over yi0 s from theorem 4 we can estimate the prediction error 13 by
n

 1  2 
i 1  2   k
 yi  
i1

thus some existing model selection criteria can be modified to choose an optimal combination of
tuning parameters for instance we can extend aicr ronchetti 1985 bic schwarz 1978
and gcv wahba 1990 to the ladflsa as follows
 1  2 
i 1  2   k
aicr  ni1 yi  
n
 1  2  logn2
i 1  2   k
bic  i1 yi  
n
 1  2 n
i 1  2 1  k
gcv  i1 yi  
11

14

6

numerical studies

in this section we first use some simulation studies and real data analysis to demonstrate the
performances of the ladflsa approach in recovering the true hidden signals then we verify
theorem 4 numerically using a sample copy number data

61

recovery of hidden signals

we illustrate the performance of the ladflsa by modifying the block example studied in both
donoho and johnstone 1995 and harchaoui and lvyleduc 2010 where the signal vector is
only blocky but not sparse we choose t  1 23 65 76 9 and h  15 3 43 31 2
and round j hj 1  sgnin  tj 2 to the nearest integers to get 0i for 1  i  n then the
generated true hidden signal vector
0  0p1 2p2  2p3 3p4 0p5 2nq 
is blocky and sparse with four nonzero blocks and two zero ones here q  p1    p5  the
observed data are generated from model 1 by simulating i s from 1 normal distribution with
mean 0 and standard deviation  2 double exponential distribution with center 0 and standard
deviation  and 3 standard cauchy distribution with a multiplier 01 similar to harchaoui and
lvyleduc 2010 we consider weak mild and strong noises by setting   01 05 and 1 in all
three types of distributions in figure 4 we plot a sample data set generated from 2 where the
observed data the true hidden signals and the ladflsa estimates are plotted using gray black
and red colors respectively
the data is standardized as yi sy and analyzed using the ladflsa approach in 3
where sy is the standard deviation of y1   yn   we choose optimal 1 and 2 by minimizing bic in 14 for 0  1  05 with increments of 001 and n logn12  2n  n12
with increments of 01 respectively to demonstrate the robust properties of the ladflsa
approach we also report the simulation results from the lsflsa approach in 2 for each
0
model we illustrate the variable selection effect using cfr6 the ratio of either recovering 
correctly or overfitting the model by including six additional noises over 1000 replicates we
choose six here since we have six blocks in the true model we also report jump the average
number with standard deviation of jumps over 1000 replicates a jump is counted only if the
adjacent differences is at least 01 we demonstrate the estimation effects by computing the least
absolute relative error lare as follows
lare
n  0  

n
i  0i 
i1 

n
i1 i 

15

the simulation results for sample size n  1000 and 5000 are reported in table 1 where we
can see that the ladflsa approach has much better performance than the lsflsa for both
strong and mild signal noises when the signal noises are weak the ladflsa still has some
advantages over the lsflsa especially when the data is contaminated by cauchy distributed
noises for example for cauchy error and   01 the ladflsa recovers the true signal vector
exactly at a ratio of 87 for n  1000 and 92 for n  5000 while the lsflsa only recovers
the true model exactly 49 and 78 of the time
12

62

bac array

in section 1 we introduced a sample bac cgh data where the observation of each entry for
cell line gm 13330 is the log 2 fluorescence ratios from all 23 chromosomes resulted from the
bac experiment sorted in the order of the clones locations on the genome the purpose of the
study is to detect the locations where there are some significant deletions or amplifications as a
demonstration of the effect of the ladflsa applied to copy number analysis we only analyze
the data from chromosome 14 with 129 67 83 and 167 markers respectively since the log
2 ratios at many markers are observed to be around 0 and the data may also have some spatial
dependence properties it is reasonable to assume the true hidden signals to be both sparse and
blocky we analyze each chromosome independently by using both the ladflsa in 3 and
lsflsa in 2 tuning parameters are chosen the same as in section 61 the final estimates
from both methods on all 4 chromosomes are plotted together in figure 1 the ladflsa
estimates top panel provides four blocks with one amplification region in chromosome 1 and
one deletion region in chromosome 4 besides the two variation regions detected by the ladflsa the lsflsa estimates bottom panel also show an amplification at a single point in
chromosome 2 which is not confirmed by spectral karyotyping in snijders et al 2001

63

effect of unbiased estimator of the degrees of freedom

we now conduct some simulations based upon the sample bac array studied in section 62 to
examine theorem 4 numerically to illustrate the effect of the unbiased estimator of the degrees
of freedom we only take chromosome 1 with 129 locations as an example one can see the
sample data from figure 1
we generate 500 monte carlo simulations based on the same hypothetical model
yi0  yi  0i  i  1  129
where yi s are observations at 129 locations and 0i s are independent normal with center 0 and
standard deviation 01   where   is the standard deviation of y for each combined 1  2 
 1  2  and compute the true df1  2 
 1  2  from k
with 0  1  2  1 we record df
defined in 11 using the monte carlo simulation from algorithm 1 in ye 1998 in figure 2
 1  2  of the ladflsa estimate for every combination of 0  1  2  1 with the
we plot df
 1  2  over 500 repetitions
increment of 005 respectively the averages of df1  2  and k
are reported in figure 3 those simulation results show that the number of estimated nonzero
 1  2  is a promising estimate to the df1  2  numerically especially when the
blocks k
number of estimated nonzero blocks is not deviated from the true one seriously

7

concluding remarks

in this paper we study the asymptotic properties of the lad signalapproximation approach
using the fused lasso penalty by assuming the true model to be both blocky and sparse we
investigate both the estimation consistency and sign consistency of the ladflsa estimator in
terms of estimation consistency the consistency rate is optimal up to an logarithmic factor if the
13

dimension of any linear space where the true model and its estimates belong is bounded from
above in terms of sign consistency we justify that a ladflsa approach can not only recover
the true block pattern but also distinguish those nonzero blocks from the zero ones correctly with
high probability under reasonable conditions in fact those jump selection and block selection
consistency results can be made stronger by matching the corresponding signs correctly with a
large probability thus by choosing two tuning parameters 1 and 2 properly we can reach a
wellbehaved ladflsa estimate to recover the true hidden signal vector under some random
noises the consistency results in this paper extend the theoretical properties of the lsfsa
in harchaoui and lvyleduc 2010 and the lsflsa in rinaldo 2009 to the lad signal
approximation which amplify the study of signal approximation using linear regression when
the random error does not follow a gaussian distribution furthermore we demonstrate that the
number of estimated nonzero blocks is an unbiased estimator of the degrees of freedom of the
ladflsa thus the existing model selection criteria can be extended to the ladflsa for
choosing the tuning parameters
as in many recent studies our results are proved for penalty parameters that satisfy the conditions as stated in the theorems it is not clear whether the penalty parameters selected using
datadriven procedures satisfy those conditions however our numerical study shows a satisfactory finitesample performance of the ladflsa particularly we note that the tuning parameters selected based on the bic seem sufficient for our simulated data this is an important and
challenging problem that requires further investigation but is beyond the scope of the current
paper also a basic assumption required in our results is that the random error terms i in 1 are
independent since the observations y1      yn are in a natural order in this model for example
copy number variation data based on genetic markers are ordered according to their chromosomal
locations it would be interesting to study the behavior of the ladflsa allowing for certain
dependence structure in the error terms

appendix
proof of lemma 1
i  w
i 1n  2n   
i  
i1 and w
i  w
i 1n  2n   
i  
i1 be the lsflsa and ladlet w
flsa estimates of ith jump in 5 and 3 using the karushkuhntucker kkt conditions
5 we get
i
n
k


j   1n  sgn w
j   2n sgnw
i  if w
i  0
2 f 0zi  f 0  w
j1

ki

j1

then
n

22n j  8f 0 zi 



i

j 2  221n n2 j
f 0  w
j1

i1

thus
n

j  8f 022n  2n2 21n 1  zi2  16f 0n22n  2n2 21n 
i1

14

16

using the kkt equations of 3 we have
i

n

k

j   1n  sgn w
j   2n sgnw
i  if w
i  0
sgnyi   w
j1

ki

j1

then
  n2n  1n n

17

finally combining with 16 17 and a2 iii holds which completes the proof of lemma 1

proof of lemma 2
n in 5 we have
from the definition of 


n
n
i1 zi  f 0i 2  i1 zi  f 00i 2
i1 
1n ni1 0i   
i   2n ni2 0i  0i1   
i  

18

from the triangle inequality 18 becomes
n
f 0 
i  0i 2
i1 
i1 
 2f 0 ni1 i 
i  0i   1n ni1 
i  0i   2n ni2 0i  0i1   
i  
n
n
0
0
i  i   1n  22n  i1 
i  i 
 2 f 0 i1 i 

19

the rest of the proof is similar to the proof of proposition 2 in harchaoui and lvyleduc 2010
for   rn  we define
n

g  2 f 0  i i  0i   0 2 
i1

thus 19 becomes
n

n  0 2  g
n 
n  0 2 
f 0 
i  0i 2  1n  22n  n
i1

then



f 0
n  0 2  1n  22n  n  g
n 

20

n may belong from
let sk  be a collection of any kdimensional linear space to which 
lemma 1 1  k  n  from 20 for any n  0


p
n  0 2  n   pg
n   f 0n  1n  22n  n


21
n
 k1
nk p supsk g  f 0n  1n  22n  n 
notice that eg  0 and varg  1 as a consequence of cirelson ibragimov and
sudakovs 1976 inequality
p sup g  e  sup g  z  expz 2 2 for some constant z  0
sk

sk

15

22

consider the collection sk  let  be the ddimensional space to which   0 belongs and
1   d be its orthogonal basis

2 f 0 ni1 i i

supsk g  sup
 nnn
2 f 0 i1 i d
j1 aj ji 
 supard

d
n j1 aj j n
23

n
d
2 f 0 j1 aj i1 i ji 
 supard
12
d
j1 aj 

12
n
2

 2 f 0 d
j1 i1 i ji  
where the last  is obtained using the cauchyschwarz inequality from a2 and i in lemma
1 there exists m1  0 such that d  m1  1n  then by taking expectations on both sides of
23 we have

2 12
n
esupsk g  2 f 0e d

j1 i1 i ji  

2 12
n

2 f 0 d
j1 e i1 i ji  
 d  m1  1n 12 

24

combining 22 and 24 we get
p  sup g  m1  1n 12  z  expz 2 2

25

sk

let 0  c  1 such that


c f 0n  1n  22n  n  m1  1n 12 


then we can choose a positive z  f 0n  1n  22n  n  m1  1n 12 in 25
combining 21 and 25


p
n  0 2  n   n expn log n  12 f 0n  1n  22n  n  m1  1n 2 
26
 n expn log n  121  c2 f 0n2 

for n  n  n we have
p
n  0 n  n   n expn log n  121  c2 f 0nn2 
thus the first part of lemma 2 holds furthermore if we also have n  2m2 n log nn12 
then

2
p
n  0 n  2m2 n log nn  n n1m2 f 01c n 
which completes the proof 
proof of theorem 1
16

define

n

n

n

n

ln   n1  yi  i    yi  0i   1n  i   2n  i  i1 
i1

i1

i1

i2

and
n

mn   n

1

n

f 0 i  0i 2
i1

n
n
0
  sgni i  i   1n  i   2n  i
i1
i1
i2

 i1  

n  arg minln  and 
n  arg minmn  define rni  rni i  i   i  i 
then 
0i   i   sgni i  0i  and ni  rni  erni  following gao and huang 2010b we can
verify
n

ln   mn   ni n  n  0 

27

i1

n 2   sd   
where n  0   o  0 2n  for any   0 we define s      
0
0
n 2   and s       2   we define
  
n   sup ln   mn 
s

and
hn   infd mn   mn 
n 
s

we have
n 2n  2n1 f 0ni1 i  
i 
mn   mn 
n   f 0  
i  0i   n1 ni1 sgni 
i  i 
28
n
n
1
1
i1 
n 1n i1 i   
i   n 2n i2 i  i1   
i  
since mn i i i  0
i   0
2n1 f 0
i  0i   n1 sgni   n1 2n sgn
i  i1   sgni1  
i  on both sides and take sums then 29 becomes
multiply i  
n 2n  n1 1n ni1 
mn   mn 
n   f 0  
i   i sgn
i   
i   i 
n
1
i i  
i   sgn
n 2n i2 sgni1  
i  i1 
i  i 
i1 
n1 2n ni2 i  i1   
i  
n 2n
 f 0  
i i  
i   sgn
n1 2n ni2 sgni1  
i  i1 
i  i 
n
1
i1 
n 2n i2 i  i1   
i  
then for any n  0 we have
hn n   f 02n n
from the convex minimization theorem in hjort and pollard 1993 we have
n 2  n 
p
n  
 pn n   hn n 2
29
n
1
2
0
2
 p supsn n i1 ni   f 0n 2n  p supsn n     f 0n 2n 
17

suppose rn  o1 and n  n 
n  0   rn 
n  0 2n  then
limn p supsn n  0   f 02n 2n
n  0 2n  f 02n 4n
 limn p supsn 2rn 
n 2n  f 02n 4n
 limn p supsn 2rn   
n  0 2n  f 02n 8n 
 limn p supsn rn 

30

combining 27 and 29
2  n   p supsn ni1 ni n  f 02n 2n
p

n  0 2n  f 02n 8n  o1
p supsn rn 
 p sups 0

n

 ni1 ni n



n
f 02n 2n  p 

 0 2

31
 n  

where n  2n  let the ui s be a rademacher sequence and 1   d be the orthogonal basis
of a ddimensional space to which   0 belongs using the contraction theorem in ledoux
and talagrand 1991 and also the cauchyschwarz inequality we have
e sups 0  ni1 ni n  8ne sups 0  ni1 ui i  0i 


n

n

n
 8ne supard sups 0  d
j1 aj i1 ui ji 
n

n
2 12
2 12 
 8ne supard sups 0 d
d
j1 aj 
j1 i1 ui ji  
n

2 12
 8n sups 0 dd
j1 aj 
n

 8n sups 0 d  0 2
n

 16 n n n

p sups 0  ni1 ni n  f 02n 2n  e sups 0  ni1 ni n f 02n 2n
n
n
32


 32 n f 0n   8 n f 0n 

let n  n  n combining 31 and 32
n 2  n 2  p
p  0 n  n   p
n  
n  0 2  n 2
n 2  n 2  p
 p
n  0 2  n 2
n  

 32n f 0n n  2p 
n  0 n  n 2

 32 n f 0n n  2n expn log n  1  c2 f 0nn2 8

the last  is from 32 and lemma 2 by choosing n  2c f 01 1n  22n  m1 
1n n12  thus the first part of theorem 2 holds furthermore if we let 1n  22n 
12
2
12
2c2 f
0m3 n log
nn12 m
 1 1n  for m3  11c f 0 and n  8m3 n log nn 
then n f 0n n  o1 log n thus

p
n  0 n  n   o1 log n  2n expn log n1  m3 1  c2 f 0
18


proof of corollary 1
n  
n or 0
if we replace the upper bound of maximal dimension of any linear space where 
belong by jmax in the proof of theorem 1 we can obtain the consistency result in corollary 1
we do not repeat the proof here 
proof of theorem 2
suppose vector  has j blocks and b1   bj  is the corresponding unique block partition
let j be the intensity at jth block for 1  j  j from lemma a1 in rinaldo 2009 the
subdifferential of the total variation penalty

2n sgnj1  j 
j1



 2n  j  j1    2n sgnj1  j   sgnj  j1  1  j  j 


j2

jj
 2n sgnj  j1 
j

33

where sgnx  1 0 1 when x  0  0  0 respectively we define c0j and 
cj as the subdif0
n scaled by the corresponding block sizes in other words we
ferentials 33 at both  and 
have
0

2n sgnj1
 j0 b0j 
j1



0
0
0
0
0
0
cj   2n sgnj1  j   sgnj  j1 bj  1  j  j0

 2n sgn 0   0 b0 

j  j0

j
j1
j

34


2n sgn
j1  j 
bj 
j1





cj   2n sgn
j1  j   sgn
j  j1 bj  1  j   



j  j1 
bj 
j  
 2n sgn

35

and

n  we let bji be the block estimate where i stays that is 
i are all the same
for an estimate 
0
0
for i  bji  let 
bji  bji  be the size of bji  then bji bji  is the corresponding block
0
set size from notations i and v in section 2 we have b0ji  bji
 for 1  j  j0  from the
f is a ladfsa solution if and only if
kkt conditions 


i   
bji
cji
 kbji sgnyk  


i   22n

sgnyk  

 kbji

if i  

if i  

36

i and 0i satisfy
let 


  0i  2f 0b0ji 1 kb0 sgnk   b0ji c0ji  
hi 
 
ji
 i

i  
i1

 

i  j 0

i  j 0 

37

here 
hi is the remainder term with the stochastically equicontinuity more specifically
b0ji 12
hi   op 1 1  i  n
19

38

0
0
i 0i sgnk 
ki with rki  sgnk 
in fact 
hi  2f 0b0ji 
i 0i  kbji
erki  kbji
0
 define the difference vector w  w1   wn   with w1  1
and ki  rki  e rki  for k  bji
n in 37
i   sgnwi0  i  j 0  then 36 holds for 
and wi  i  i1 for 2  i  n if sgnw
n is a ladfsa solution define
thus 

if   sgnwi0  i  j 0 
r2n    j 0   sgnw
then r2n holds if

i   sgnwi0 

sgnw

i r  22n
r kb sgnyk  


ji


i  j 0
if i  j 0

39a
39b

i   sgnwi0  i  j 0 holds if
it is easy to verify that sgnw
i wi0  w
i   wi0  for i  j 0
sgnw

40

n 37 into 40 and 39b and then use the triangle inequality we know that r2n holds if
plug 
0
0
maxij 0 b0ji 1 kbji
sgnk   b0ji1 1 kbji1
sgnk wi0 
maxij 0 b0 1
hi  b0
1
hi1 w0  maxij 0 c0  c0
w0

ji

i

ji1

ji

ji1

i

41

 2f 0
and
max
sgni   sgni1   
hi  
hi1   42n 
0
ij

42

we have
esgni   sgni1   0
and
varsgni   sgni1   2 for 2  i  n
and for 2  il  i2  n
covsgni1   sgni1 1  sgni2   sgni2 1   1 0 for i1  i2   1 otherwise
suppose di are independent copies of n 0 2 then we have
pi4   pmaxij 0 sgni   sgni1   22n 
 pmaxij 0 di   22n 
 2 exp422n  log j0c 
where we get the first  using slepians inequality the second  using chernoffs bound
then pi4   o1 if conditions in b1 hold define
xi  2f 0b0ji 1  sgnk   2f 0b0ji1 1
0
kbji


0
kbji1

20

sgnk  i  j 0 

then exi   0 and maxij 0 varxi   2f 0b0ji 1  consider independent copies xi 
n 0 2f 0b0ji 1  i  j 0  we have
0
0
pi1   pmaxij 0 b0ji 1 kbji
sgnk   b0ji1 1 kbji1
sgnk   2f 0an 3
 pmaxij 0 xi   an 3  2 exp2b0min f 2 0a2n 9  log j 0 

thus pi1   o1 if conditions in b2 holds since maxij 0 c0ji  c0ji1   22n b0min  from
b3
pi2   pmax
c0ji  c0ji1   2f 0an 3  0
0
ij

furthermore we have
pi5   pmax

hi  
hi1   22n   0
0
ij

from 38 we have
pi3   pmaxij 0 wi0 b0ji 1
hi  wi0 b0ji1 1
hi1   2f 03
0
0
12
12


 pmaxij 0 bji  hi  bji1  hi1   2f 0b0min 12 an 3
 o1
then from 41 and 42 we get
prc2n   pi1   pi2   pi3   pi4   pi5   0 when n  

proof of theorem 3
from theorem 2 we know that if a1 and b1b3 hold the ladflsa can choose all jumps
with probability 1 thus we can prove the main results based on the true block partition by the
n is a ladflsa solution if and only if
kkt 


bj 
cj  1n
bj sgn
j  if j  0
 kbj0 sgnyk  j   



0 sgnyk  
j   
cj   1n
bj
if j  0
bj 


 kbj

43

let j and j0 satisfy


 j  j0  2f 0b0j 1 ibj0 sgni   b0j c0j  1n b0j sgnj0   
hj 



 j  0

 j  k0

 j  k0

44

here by abuse of notation 
hj is the remainder term with the stochastically equicontinunity
b0j 12
hj   op 1 1  i  n

45

in fact 
hj  2f 0b0j 
j  j0   ibj0 erij   ibj0 ij  with rij  sgni  
j  j0   sgni  and
ij  rij  e rij  for i  bj0 and 1  j  j0  1 if sgn
j   sgnj0   j  k0  then n in 44
satisfies kktflsa and therefore is a ladflsa solution define an event
  k0   sgn
rn  r1n  2n   k
j   sgnj0   j  k0 
21

then rn holds if


sgn
j   sgnj0 
 j  k0

 kbj0 sgnyk  j   
bj 
cj   1n
bj  j  k 0

46

we can verify that sgn
j   sgnj0  j  k0 holds if sgn
j j0  j   j0  for j  k0 
therefore from 44 and 46 rn holds if


 ib0 sgni   b0j c0j  1n b0j sgnj0   
hj   2f 0b0j j0   j  k0

 ib0 sgni   b0j c0j  
hj   1n b0j
 j  k0

47

thus we have
prc   pmaxjk0  ibj0 sgni   2f 0 minjk0 b0j minjk0 j0 4
pmaxjk0 c0j   2f 0 minjk0 j0 4
pmaxjk0 1n sgnj0   2f 0 minjk0 j0 4
pmaxjk0 
hj b0j j0   f 02
pmaxjk0  ibj0 sgni   1n minjk0 b0j 3
pmaxjk0 c0j   1n 3
pmaxjk0 
hj b0j   1n 3
 ps1   ps2   ps3   ps4   ps5   ps6   ps7 

48

let zj  ibj0 sgni b0j  then ezj   0 and varzj   1b0j  then zj s are independent
subgaussian from c3 we have
p s1   2k0 expb0min f 2 02n 8  o1
we can verify p s2   o1 from c4 p s3   o1 from c5 and p s6   o1 from c2
from c1
p s5   2j0  k0  expb0min 21n 32  o1
furthermore we have p s7   o1 and p s4   o1 from 48 we have p r  1 when
n   which completes the proof 
the rest of the appendix are presented to prove theorem 4
recall that w is the jump coefficients vector with wi  i  i1 for 2  i  n and  
1   j  is the block coefficients factor from proposition 3 in rosset and zhu 2007 we
know that the following results of the ladflsa solution
lemma 4

i for any 1  0 there exists a set of values of 2 
0  20  21    2m2  2m2 1  

 2k  for 1  k  m2 is not uniquely defined the set of optimal solutions of
such that w0
 2  is
each 2k is a straight line in rn  and for any 2  2k  2k1  the solution w0
constant
22

ii for above 2k  1  k  m2  there exists a set of values of 1 
0  10  11    1m2  1m2 1  
1j  2k  for 1  j  m1 is not uniquely defined the set of optimal solutions
such that 
1  2k 
of each 1j is a straight line in rn  and for any 1  1j  1j1  the solution 
is constant
in lemma 4 if we define 2k  1  k  m2 from i as the transition points for w and n02
as the set of y  rn such that 2 is a transition point for w then the jumps set j 0 2  only
changes at those 2k s furthermore let 2  2k for some 1  k  m2  if we can also define
1j  1  j  m1 from ii as the transition points for  and n1 2 as the set of y  rn such that
1 is a transition point for  then the set of nonzero blocks k1  2  only changes at 1j s or
2k s and n1 2  is in a finite collection of hyperplanes in rn  from lemma 4 we know that
1  2  is fixed and then 1 2  n is divided into two sets
for any given y  rn n1 2  
c
ey1 2 and ey1 2  where ey1 2  1  i  n  yi  ji  0 ji  0 and ji specifies the
i stays thus we have
block where 
ey1 2   k1  2 

49

lemma 5 for any 1  0 and 2  0 if y  rn n1 2  1  2  y is a continuous function of
y and then ey1 2 is locally constant
proof of lemma 5
let l y denote the function
n

n

n

l y   yi  i   1  i   2  i  i1 
i1

i1

i2

 does not change from lemma 4 for any y0  rn n1 2  and for any
since y  rn n1 2  
1  2  ym   
1  2  y0  it
sequence ym  such that ym   y0  we want to prove that 
1  2  ym   
1  2  y0  because 
is equivalent to prove 
1  2  y1  
0 0 y1 
1  2  y is bounded thus we only need to check that for every converging subsequence
y1  
1  2  ymk   
1  2  y0  suppose that 
1  2  ymk  
of ym  say ymk  we have 
1  2  when mk   let  y y   l y  l y  on the one hand we have
l
1  2  y0  y0 
 l
1  2  y0  ymk   
1  2  y0  y0  ymk 
 l
1  2  ymk  ymk   
1  2  y0  y0  ymk 
 l
1  2  ymk  y0   
1  2  ymk  ymk  y0   
1  2  y0  y0  ymk 
on the other hand we have

 ymk  ymk  y0   
1  2  y0  y0  ymk 
n

i 1  2  ymk   yi0  
i 1  2  ymk 
 yimk  
i1

i 1  2  y0   yimk  
i 1  2  y0 
yi0  
n

 2  yimk  yi0   0 when k  
i1

23

1  2  y0 
thus l
1  2  y0  y0   limk l
1  2  ymk  y0   l1  2  y0  y0  since 
1  2  y0  
is the unique minimizer of l y0  we have 1  2  y0   
proof of theorem 4
from 49 and lemma 5 there exists   0 such that y  bally  ey1 2 stays the same
when neither 1 and 2 is a transitional point thus 
ji 1  2 yi  1 if i  ey1 2 and
ji 1  2 yi  ey1 2  

ji 1  2 yi  0 if i  ey1 2  overall we have ni1 
 1  2  for y  n   since n  is in a collection of finite hyperplanes we can obtain the
k
1 2
1 2
conclusion by taking the expectation 

references
boysen l kempe a liebscher v munk a ad wittich ol 2009 consistencies and rates
of convergence of jumppenalized least squares estimators annals of statistics 37 157183
cirelson b s ibragimov i a and sudakov v n 1976 norm of gaussian sample function in proceedings of the third japanussr symposium on probability theory springer
lecture notes in mathematics 550 2041 springer berlin
donoho dl and johnstone im 1995 adapting to unknown smoothness via wavelet
shrinkage journal of the american statistical association 90 12001224
gao xl and fang yx 2011 generalized degrees of freedom under the l1 loss function
journal of statistical planning and inference 141 677686 
gao xl and huang j 2010a a robust penalized method for the analysis of noisy dna copy
number data bmc genomics 11517
gao xl and huang j 2010b asymptotic properities of ladlasso in highdimensional
settings statistic sinica 20 14851506
harchaoui z and lvyleduc c 2010 multiple changepoint estimation with a total variation
penalty journal of the american statistical association 105 14801493
hebiri m 2008 regularization with the smoothlasso procedure preprint laboratoire de
probabilits et modles alatoires
hjort n l and pollard d 1993 asymptotics for minimisers of convex processes statistical
research report university of oslo
kato k 2009 on the degrees of freedom in shrinkage estimation journal of multivariate
analysis 100 13381352
ledoux m and talagrand m 1991 probability in branch spaces isoperimetry and processes
springer verlag new york
li y and zhu j2008 l1norm quantile regression journal of computational  graphical
statistics 17 163185
mammen e and van de geer s 1997 locally adaptive regression splines annals of statistics 25 387413
rinaldo a 2009 properties and refinements of the fused lasso annals of statistics 37 2922
2952
24

ronchetti e 1985 robust model selection in regression statistics  probability letters 3
2123
snijders am nowak n segraves r blackwood s brown n conroy j hamilton
g hindle ak huey b kimura k law s myambo k palmer j ylstra b yue
jp gray jw jain an pinkel d and albertson d 2001 assembly of microarrays for
genomewide measurement of dna copy number nature genetics 29 263264
schwarz ge 1978 estimating the dimension of a model annals of statistics 6 461464
tibshirani r 1996 regression shrinkage and selection via the lasso journal of the royal
statistical society series b 58 267288
tibshirani r saunders m rosset s zhu j and knight k 2005 sparsity and smoothness
via the fused lasso journal of the royal statistical society series b 67 91108
tibshirani r and wang p 2008 spatial smoothing and hot spot detection for cgh data using
the fused lasso biostatistics 91 1829
wahba g 1990 spline models for observational data philadelphia society for industrial and
applied mathematics
yao y and au s t 1989 leastsquares estimation of a step function sankhya the indian
journal of statistics series a 51 370381
ye j 1998 on measuring and correcting the effects of data mining and model selection journal of american statistist association 93 120131
yuan m and lin y 2006 model selection and estimation in regression with grouped variables
journal of royal statistical scociety series b 68 4967
zhao p and yu b 2006 on model selection consistency of lasso journal of machine
learning research 7 25412563
zou h hastie t and tibshirani r 2007 on the degrees of freedom of the lasso annals
of statistics 35 21732192

25

figure 1 copy number data set from the gm 13330 bac cgh array the top and bottom
panels give outputs from the ladflsa and lsflsa respectively both observed data gray
dots and estimates dark solid lines from chromosome 14 are plotted  data from different
chromosomes are separated by gray vertical lines

26

figure 2 the estimated degrees of freedom of ladflsa for every combined 1 and 2 for the
chromosome 1 data from the gm 13330 bac array

figure 3 hypothetical model from 500 monto carlo simulations of 129 markers for chromosome
1 data from the gm 13330 bac array it shows that the estimated number of nonzero blocks
k1  2  is very close to the true dm1 2  using the 45 degree line

27

figure 4 example of observed data grey with true hidden signals black and ladflsa
estimates red there are 6 blocks with 4 nonzero ones random
noise i s are generated from

double exponential distributions with center 0 and scale 05 2
table 1 simulation results for section 61

i



normal

10
05

double exp

01
10
05
01

cauchy

10
05
01

model
ladflsa
lsflsa
ladflsa
lsflsa
ladflsa
lsflsa
ladflsa
lsflsa
ladflsa
lsflsa
ladflsa
lsflsa
ladflsa
lsflsa
ladflsa
lsflsa
ladflsa
lsflsa

lare1
0197
0035
0098
0016
0019
0013
0154
0031
0077
0016
0015
0013
0048
0239
0028
0120
0007
0029

n  1000
cfr62
jump3
8917
712132
183
782147
9732 559 075
4813
568074
10093 500000
10093 500000
88 22 742154
120
742142
9734
595090
5712
573078
10097 500000
10097 500000
8756
612107
174
1637538
9970
556086
3917 1067362
9592
518046
9478
630134

lare
0173
0021
0087
0007
0017
0003
0128
0021
0064
0007
0013
0003
0029
0275
0015
0132
0003
0023

n  5000
cfr6
8215
50
9622
577
10094
10094
8925
31
10041
6219
10090
10089
8245
20
8766
153
9687
8749

jump
724133
77148
554072
561074
500000
500000
718135
731143
574086
568087
500000
500000
614095
59411438
559078
3205851
517040
1014313

note 1 lare is the least absolute relative ratio defined in 15
0 correctly or plus at most six additional false posives
note 2 cfr6 is the ratio of recovering 
correctly fitted ratio
note 3 jump is the average number standard deviation of the number of jumps

28

