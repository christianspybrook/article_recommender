selfsupervised augmentation consistency
for adapting semantic segmentation

arxiv210500097v1 cscv 30 apr 2021

1

nikita araslanov1
stefan roth12
department of computer science tu darmstadt

abstract

miou

1 introduction
unsupervised domain adaptation uda is a variant of
semisupervised learning 6 where the available unlabelled data comes from a different distribution than the annotated dataset 4 a case in point is to exploit synthetic
data where annotation is more accessible compared to the
costly labelling of realworld images 59 60 along with
some success in addressing uda for semantic segmentation 67 69 80 91 the developed methods are growing
increasingly sophisticated and often combine style transfer networks adversarial training or network ensembles
39 46 68 77 this increase in model complexity impedes
reproducibility potentially slowing further progress
in this work we propose a uda framework reaching
stateoftheart segmentation accuracy measured by the
intersectionoverunion iou without incurring substantial
training efforts toward this goal we adopt a simple semisupervised approach selftraining 12 42 91 used in recent works only in conjunction with adversarial training or
code is available at httpsgithubcomvisinfdasac

hessianai

mean iou on cityscapes val after adaptation from gta5 with vgg16
499

50

we propose an approach to domain adaptation for semantic segmentation that is both practical and highly accurate in contrast to previous work we abandon the use
of computationally involved adversarial objectives network
ensembles and style transfer instead we employ standard
data augmentation techniques  photometric noise flipping
and scaling  and ensure consistency of the semantic predictions across these image transformations we develop
this principle in a lightweight selfsupervised framework
trained on coevolving pseudo labels without the need for
cumbersome extra training rounds simple in training from
a practitioners standpoint our approach is remarkably effective we achieve significant improvements of the stateoftheart segmentation accuracy after adaptation consistent
both across different choices of the backbone architecture
and adaptation scenarios

2

selftraining
 number of rounds
adversarial training

48

ensemble

46

436

2019
44

1

423
3

424
2

pit
53

fda
80

tir
39

422

42
390
40
38
36

372
1

4

1

style transfer
465
6

ours
449

438

sai2i
55 cdam
78
ldr fada
77 70
1

lse 65
2020

2021

pycda 47

figure 1 results preview unlike much recent work that combines multiple training paradigms such as adversarial training and
style transfer our approach retains the modest singleround training complexity of selftraining yet improves the state of the art for
adapting semantic segmentation by a significant margin

network ensembles 17 39 54 70 80 87 86 by contrast
we use selftraining standalone compared to previous selftraining methods 9 43 65 91 92 our approach also
sidesteps the inconvenience of multiple training rounds as
they often require expert intervention between consecutive
rounds we train our model using coevolving pseudo labels
endtoend without such need
our method leverages the ubiquitous data augmentation
techniques from fully supervised learning 11 85 photometric jitter flipping and multiscale cropping we enforce
consistency of the semantic maps produced by the model
across these image perturbations the following assumption formalises the key premise
assumption 1 let f  i  m represent a pixelwise
mapping from images i to semantic output m denote
  i  i a photometric image transform and similarly 0  i  i a spatial similarity transformation
where  0  p are control variables following some predefined density eg p  n 0 1 then for any image
i  i f is invariant under  and equivariant under 0 
ie f  i  f i and f 0 i  0 f i
next we introduce a training framework using a momentum
network  a slowly advancing copy of the original model

to appear in proceedings of the ieeecvf conference on computer vision and pattern recognition cvpr virtual 2021
 2021 ieee personal use of this material is permitted permission from ieee must be obtained for all other uses in any current or future media including
reprintingrepublishing this material for advertising or promotional purposes creating new collective works for resale or redistribution to servers or lists or
reuse of any copyrighted component of this work in other works

the momentum network provides stable yet recent targets
for model updates as opposed to the fixed supervision in
model distillation 15 87 86 we also revisit the problem
of longtail recognition in the context of generating pseudo
labels for selfsupervision in particular we maintain an
exponentially moving class prior used to discount the confidence thresholds for those classes with few samples and
increase their relative contribution to the training loss our
framework is simple to train adds moderate computational
overhead compared to a fully supervised setup yet sets a
new state of the art on established benchmarks cf fig 1

features

pit
53

adversarial training
1round training
sotavgg
sotaresnet

3

ldr
77

sai2i
55

iast
54

rpt
83

3

3

3

3

3
3

6
3

3

3

3

3

ours

3
3
3

table 1 relation to state of the art previous work reaches the
state of the art in terms of iou either with vgg16 sotavgg
or resnet101 sotaresnet our framework uses neither adversarial training nor multiple training rounds given in parentheses yet outperforms the state of the art consistently in both cases

2 related work
selftraining on pseudo labels as a more computationally lightweight approach selftraining seeks highquality
pseudo supervision coming in the form of class predictions
with high confidence our work belongs to this category
most of such previous methods precompute the labels offline used subsequently to update the model and repeat
this process for several rounds 43 65 91 92 more recent frameworks following this strategy have a composite
nature they rely on adversarial pretraining 14 20 86
style translation 17 80 or both 54 46 39 70 73
training on coevolving pseudo labels can be computationally unstable hence requires additional regularisation
chen et al 13 minimise the entropy with improved behaviour of the gradient near the saturation points using
fixed representations be it from a frozen network 15 86
a fixed set of global 53 or selfgenerated local labels
47 68 81 further improves training robustness
overconfident predictions 28 have direct consequences
for the quality of pseudo labels zou et al 92 attain some
degree of confidence calibration via regularising the loss
with prediction smoothing akin to temperature scaling 28
averaging the predictions of two classifiers 87 or using
dropoutbased sampling 7 88 achieves the same goal

most of the work on scene adaptation for semantic segmentation has been influenced by a parallel stream of work
on domain adaptation da and semisupervised learning
for image classification 23 24 27 45 50 the main idea
behind these methods is to formulate an upper bound on the
target risk using the socalled hhdivergence 3 in a
nutshell it defines the discrepancy between the marginals
of the source and target data by means of a binary classifier
in the following we briefly review implementation variants
of this idea in the context of semantic segmentation
learning domaininvariant representations
adversarial feature alignment follows the gan framework 24 26 and minimises the gap between the source
and target feature representations in terms of some distance
eg wasserstein in 41 the discriminator can be employed at multiple scales 15 67 77 and use local spatial priors 83 it can be conditional 33 and classspecific
22 52 or align the features of hard and easy target
samples 56 often selfsupervised losses such as entropy
minimisation 69 or a conservative loss 90 assist in this
alignment
the alternative to adversarial feature alignment are more
interpretable constraints such as feature priors 51 bijective sourcetarget association 37 or aligning the domains
directly in the image space with style transfer 89 used either alone 74 or most commonly jointly with adversarial feature alignment 8 16 25 55 78 79 82 one issue with style translation is to ensure semantic consistency
despite the changes in appearance to address this hoffman et al 32 use semantic and cycleconsistency losses
while yang et al 77 reconstruct the original image from
its labelspace representation
these methods tend to be computationally costly and
challenging to train since they require concurrent training
of one or more independent networks eg discriminators or
style transfer networks although yang and soatto 80 obviate the need for style networks by incorporating the phase
of a fouriertransformed target image into a source sample
multiple networks have to be trained each with its own predefined phase band

spatial priors different from da for classification the
characteristic feature of adaptation methods for segmentation is the use of spatial priors local priors have been
enforced patchwise 15 47 68 and in the form of precomputed superpixels 81 83 although global spatial
priors have also been used 91 their success hinges on the
similarity of the semantic layout in the current benchmarks
relation to our approach as shown in table 1 our work
streamlines the training process first we do not use adversarial training as feature invariance alone does not guarantee label invariance 36 84 second we train our model
with coevolving pseudo labels in one round our framework bears resemblance to the noisy mean teacher 76 and
combines consistency regularisation 2 61 64 75 with
selfensembling 40 66 similar approaches have been explored in medical imaging 44 58 and concurrent uda
work 71 albeit limited in the scope of admissible aug2

masks
momentum net
multiscale
crops  flips

multiscale
fusion

input sample

input batch

output masks

fused prediction

pseudo labels

momentum updates

gradient
photometric

noise

segmentation net
masks

lt
target loss

a framework overview

b multiscale crops and flips

c multiscale fusion

figure 2 overview the segmentation network in our framework a maintains a slow copy of itself the momentum network which
provides stable targets for selfsupervision in addition to encouraging semantic invariance wrt the photometric noise we facilitate
consistent predictions across multiple scales and flips by first b feeding random multiscale crops and flips to the momentum network and
then c fusing the predictions by simple averaging to produce the pseudosupervision targets

mentations we leverage photometric invariance scale and
flip equivariance 72 to extract highfidelity pseudo supervision instead of more computationally expensive sampling
techniques 38 contrary to 65 we find that scale alone
is not predictive of the label quality hence we average the
predictions produced at multiple scales and flips this parallels uncertainty estimation using testtime augmentation
1 but at training time 5

put to the networks fig 2b demonstrates this process following the noisy student model in image classification 76
the input to the segmentation network additionally undergoes a photometric augmentation we add random colour
jitter and smooth the images with a gaussian filter at random the momentum network on the other hand receives
a clean input ie without such augmentations this is to
encourage model invariance to photometric perturbations

33 selfsupervision

3 selfsupervised augmentation consistency

multiscale fusion we reproject the output masks from
the momentum network back to the original image canvas
of size h  w as illustrated in fig 2c for each pixel the
overlapping areas average their predictions note that some
pixels may lie outside the crops hence contain the result
of a single forward pass with the original image we keep
these predictions intact the merged maps are then used to
extract the pseudo masks for selfsupervision
a short longtail interlude handling rare classes ie
classes with only a few training samples is notoriously difficult in recognition 29 for semantic segmentation we
here distinguish between the classes with low imagelevel
eg truck bus and pixellevel eg traffic light
pole frequency while generating selfsupervision we
take special care of these cases and encourage i lower
thresholds for selecting their pseudo labels ii increased
contributions to the gradient with a focal loss and iii employ importance sampling we describe these in detail next
samplebased moving threshold most previous work
with selftraining employs multiround training that requires interrupting the training process and regenerating
the pseudo labels 43 46 54 65 91 one of the reasons is the need to recompute the thresholds for filtering
the pseudo labels for supervision which requires traversing the predictions for the complete target dataset with the
model parameters fixed in pursuit of our goal of enabling
endtoend training without expert intervention we take a
different approach and compute the thresholds onthego
as the main ingredient we maintain an exponentially moving class prior in detail for each softmax prediction of the
momentum network we first compute a prior estimate of

31 framework overview
shown in fig 2a our framework comprises a segmentation network which we intend to adapt to a target domain
and its slowly changing copy updated with a momentum
a momentum network to perform selfsupervised scene
adaptation we first supply a batch of random crops and
horizontal flips from a sample image of the target domain
to both networks for each pixel we average the predictions
ie semantic masks from the momentum network after the
appropriate inverse spatial transformation we then create
a pseudo ground truth by selecting confident pixels from
the averaged map using thresholds based on running statistics which are capable of adapting to individual samples
finally the segmentation network uses stochastic gradient
descent to update its parameters wrt these pseudo labels
our approach closely resembles the mean teacher framework 23 66 and temporal ensembling 35 40 however
as we will show empirically the ensembling property itself plays only an auxiliary role more importantly akin
to the critic network in reinforcement learning 48 and the
momentum encoder in unsupervised learning 30 our momentum network provides stable targets for selfsupervised
training of the segmentation network this view allows us
to focus on the targetgenerating process detailed next

32 batch construction
for each sampled target image we generate n crops
with random scales flips and locations but preserving the
aspect ratio we rescale the crops as well as the original image to a fixed input resolution hw and pass them as the in3

cn

the probability that a pixel in sample n belongs to class c as
1 x
cn 
mcnij 
1
hw ij

08

02
0

2

0

002

003

004

c

34 training

4

pretraining with sourceonly loss following 47 83
we use adaptive batch normalisation abn 45 to jumpstart our model on the segmentation task by minimising the
crossentropy loss on the source data only in our experiments we found it unnecessary to recompute the mean
and the standard deviation only at the end of the training
instead in pretraining we alternate batches of source and
target images but ignore the loss for the latter for a target
batch this implies updating the running mean and the standard deviation in the batch normalisation bn 34 layers
and leaving the remaining model parameters untouched
importance sampling our loss function in eq 6 accounts for longtail classes with a high image frequency
eg traffic light pole and may not be effective for
the classes appearing in only few samples eg bus
train to alleviate this imbalance we use importance
sampling 21 and increase the sample frequency of these
longtail classes we minimise the expected target loss by
resampling the target images using the density pt 


min enpt ltn  
7

fig 3 plots eq 3 as a function of the moving class prior c
for a selection of  for predominant classes eg road
the exponential term has nearly no effect the threshold is
static wrt the peak class confidence ie cn  mcn 
however for longtail classes such that c   the threshold is lower than this upper bound hence more pixels for
these classes are selected for supervision to obtain the
pseudo labels we apply the threshold cn to the peak predictions of the merged output from the momentum network

c
mc nij  cn
5
mnij 
ignore otherwise
where c  arg maxc mcnij is the dominant class for that
pixel note that the pixels with confidence values lower than
the threshold as well as nondominant predictions will be
ignored in the selfsupervised loss
focal loss with confidence regularisation our loss function incorporates a focal multiplier 49 to further increase
the contribution of the longtail classes in the gradient signal unlike previous work 49 65 however our moving
class prior c regulates the focal term
ltn m m    mcn 1  c  logmcn 

001

figure 3 samplebased moving threshold our thresholding
scheme has two hyperparameters  and  in this example
mcn  1 and   075 predominant classes eg road have
c fl 0 hence their threshold approximates mcn  longtail
classes eg traffic light have c  0 and their thresholds are
further reduced with a steepness controlled by  see eq 3

where  and  are hyperparameters and mcn is the predicted peak confidence score for class c ie
ij

  0005
  001
  002

04

our samplebased moving threshold cn takes lower values
when the moving prior c  0 ie for longtail classes but
is bounded from above as c  1 we define it as

cn   1  ec  mcn 
3

mcn  max mcnij 



06

where mcn is the mask prediction for class c with resolution h  w we keep an exponentially moving average
after each training iteration t with a momentum   0 1
t1
  tc  1   cn 
c

m
cn

1



to obtain pt  we use our pretrained segmentation network
and precompute cn  the class prior estimate for each image n using eq 1 at training time we i sample a semantic class c uniformly and then ii obtain a target sample
l with probability
cl
cl  p

8
n cn

6

where m is the prediction of the segmentation network with
parameters  the pseudo label c derives from m in eq 5
and  is a hyperparameter of the focal term recall that low
values of c signify a longtail category hence should have
a higher weight high values of  ie  1 increase the relative weighting on the longtail classes while setting   0
disables the focal term note that we also regularise our loss
with the confidence value of the momentum network mc n
eq 4 in case of an incorrect pseudo label we expect this
confidence to be low and to regularise the training owing to
its calibration with the multiscale fusion we minimise the
loss in eq 6 applied for each pixel wrt 

this twostep sampling process ensures that all images have
nonzero sample probability owing to the prevalent classes
for which cl  0 for all l eg road in urban scenes
joint targetsource training we train the segmentation
network with stochastic gradient descent using the crossentropy loss for the source and our focal loss for the target
4

a input

b segmentation net output

c momentum net output

d fused prediction

e pseudo labels

figure 4 selfsupervision example in this image sample a and its crops the segmentation network b tends to mistake the motorcycle
for a bicycle the momentum network c improves on this prediction but may still produce an inconsistent labelling averaging the
predictions over multiple scales d corrects this inconsistency allowing to produce highprecision pseudo labels e for selfsupervision

data sampled from pt  as defined by eqs 6 and 7 fig 4
illustrates the synthesis of pseudo labels we periodically
update the parameters  of the momentum network as
t1   t  1   

and report the results on the validation split we measure
the segmentation accuracy with perclass intersectionoverunion iou and its average the mean iou miou

9

41 implementation details

where  are the parameters of the segmentation network 
regulates the pace of the updates low values result in faster
but unstable training while high  leads to a premature
and suboptimal convergence we keep  moderate but
update the momentum network only every t iterations

we implement our framework in pytorch 57 we adopt
deeplabv2 10 as the segmentation architecture and evaluate our method with two backbones resnet101 31 and
vgg16 63 following recent work 39 67 68 69 73
both backbones initialise from the models pretrained on
imagenet 19 we first train the models with abn 45
cf sec 34 implemented via syncbn 57 on multiscale crops resized to 640  640 and a batch size of 16
next training proceeds with the selfsupervised target loss
cf sec 33 and the batchnorm layers 34 frozen the
batch size of 16 comprises 8 source images and 8 target images at resolution 1024  512 which is a common practice
70 80 the target batch contains only two image samples
along with 3 random crops each ie n  3 in sec 32
downscaled up to a factor of 05 as the photometric noise
we use colour jitter random blur and greyscaling see appendix b for details the optimisation uses sgd with a
constant learning rate of 25  104  momentum 09 and
weight decay of 5104  we accumulate the gradient in alternating sourcetarget forward passes to keep the memory
footprint in check since the focal term in eq 6 reduces
the target loss magnitude wrt the source loss we scale it
up by a factor of 5 2 for vgg16 we train our vggbased framework on two titan x gpus 12gb while
the resnetbased variant requires four this is a substantially reduced requirement compared to recent work eg

4 experiments
datasets in our experiments we use three datasets the
cityscapes dataset 18 contains 2048  1024 images from
realworld traffic scenes split into 2975 images for training and 500 for validation the gta5 dataset 59 contains 24 966 synthetic scenes with resolution 1914  1052
and pixelwise annotation aided by the gta5 game engine
we also use the synthiarandcityscapes subset
of the synthia dataset 60 which contains 9400 synthetic images with resolution 1280  760 and a semantic
annotation compatible with cityscapes
setup we adopt the established evaluation protocol from
previous work 47 67 69 the synthetic traffic scenes
from gta5 59 and synthia 60 serve as the source
data and the real images from the cityscapes dataset as the
target obviously ignoring the available semantic labels
this results in two domain adaptation scenarios depending
on the choice of the source data gta5  cityscapes and
synthia  cityscapes as in previous work at training
time we only use the training split of the cityscapes dataset
5

method

road sidew build wall fence pole light sign veg

terr

sky pers ride

car truck bus train moto bicy

miou

cycada 32
advent 69
cbst 91
pycda 47
pit 53
fda 80
ldr 77
fada 70
cdam 78
sai2i 55

852
869
904
867
862
861
901
923
901
911

313
264
251
289
318
303
375
326
414
409

607
702
708
588
819
736
814
853
789
823

769
815
769
804
804
817
830
835
831
812

00
16
286
62
12
240
269
152
278
337

354
361
361
372
418
422
436
438
449
465

baseline ours
sac ours

815 286
900 531

795 232 211 313 282 185 756 149 722 580 171 811 197 263 137 129 21
862 338 327 382 460 403 842 264 884 658 280 856 406 529 173 137 238

371
499

pycda 47
cdam 78
fada 70
ldr 77
fda 80
sai2i 55
pit 53
iast 54
rpt 83

905
913
925
908
925
912
875
938
892

844
845
851
847
824
852
788
851
861

493
422
395
381
464
378
499
396
568

474
492
492
495
505
504
506
515
526

baseline ours
sac ours

802 293
904 539

768 238 219 377 354 211 798 213 750 595 175 835 224 334 130 307 123
866 424 273 451 485 427 874 401 861 675 297 885 491 546 98 266 453

408
538

backbone vgg16

372
287
508
248
350
351
412
511
467
464

765
787
720
809
821
806
822
837
827
829

218
285
183
214
311
308
303
331
342
332

150
252
95
273
221
204
213
291
253
279

238
171
272
302
232
275
183
285
213
206

229
203
86
266
294
300
335
280
330
290

215
109
141
211
285
260
230
210
220
282

805
800
824
866
793
821
841
826
844
845

505
471
426
532
521
525
542
552
555
524

90
84
145
179
232
217
243
288
258
244

171
260
59
188
295
240
276
244
249
218

282
172
125
224
269
305
320
374
314
448

45
189
12
41
307
299
81
00
206
315

98
117
140
97
205
146
297
211
252
265

backbone resnet101

363
460
475
414
533
433
434
578
433

324
344
376
351
265
386
312
395
395

287
297
328
275
276
259
302
267
299

346
326
334
312
364
347
363
262
402

364
358
338
380
406
413
399
431
496

315
364
184
328
389
410
420
347
331

868
845
853
856
823
855
792
849
874

379
432
377
421
398
460
371
329
385

785
830
835
849
780
865
793
880
860

623
600
632
596
626
617
654
626
644

215
322
397
344
344
338
375
290
251

856
832
875
850
849
855
832
873
885

279
350
329
428
341
344
460
392
366

348
467
478
527
531
487
456
496
458

180
00
16
34
169
00
257
232
239

229
337
349
309
277
361
235
347
365

  denotes the use of pspnet 85 instead of deeplabv2 10

table 2 perclass iou  comparison on gta5  cityscapes adaptation evaluated on the cityscapes validation set

gta5  cityscapes table 2 our method achieves a
clear improvement over the best published results 55 83
of 34 and 12 using the vgg16 and resnet101
backbones respectively note that rpt 83 and sai2i
55 have a substantially higher model complexity rpt
83 uses pspnet 85 which has a higher upper bound
than deeplabv2 in a fully supervised setup eg 57
iou on pascal voc 85 it requires extracting superpixels and training an encoderdecoder lstm thus increasing the model capacity and the computational overhead
sai2i 55 initialises from a stronger baseline bdl 46
and relies on a style transfer network and adversarial training while both rpt 83 and sai2i 55 require multiple
rounds of training 3 and 6 from bdl 46 respectively
we train with the target loss in a single pass notably compared to the previous best approach for vgg with a resnet
evaluation sai2i 55 our improvement with resnet101
is substantial 34 and is comparable to the respective
margin on vgg16

fada 70 requires 4 tesla p40 gpus with 24gb memory note that the momentum network is always in evaluation mode has gradient tracking disabled hence adds only
around 35 memory overhead for the momentum network we fix   099 and t  100 in all our experiments for the other hyperparameters we use   099
  075   103 and   3 appendix c2 provides
further detail on hyperparameter selection as well as a sensitivity analysis of our framework wrt  and  the inference follows the usual procedure of a single forward pass
through the segmentation network at the original image resolution without any postprocessing

42 comparison to state of the art
we compare our approach to the current state of the
art on the two domain adaptation scenarios gta5 
cityscapes in table 2 and synthia  cityscapes in
table 3 for a fair comparison all numbers originate
from singlescale inference in both cases our approach
denoted as sac selfsupervised augmentation consistency substantially outperforms our baseline ie the
sourceonly loss model with abn see sec 34 and in
fact sets a new state of the art in terms of miou importantly while the ranking of previous works depends on the
backbone choice and the source data we reach the top rank
consistently in all settings

synthia  cityscapes table 3 here the result is
consistent with the previous scenario our approach attains
stateoftheart accuracy for both backbones improving by
76 and 14 with vgg16 and resnet101 backbones
over the best results previously published 55 83 again
our method with resnet101 outperforms the previous best
method with full evaluation pycda 47 by 59 iou
6

method

miou13

miou









359
381
395
405
408
411
415

335 119 183 664 704 521 161 646 155 115 264
382 413 279 808 830 643 212 783 385 326 621

391
562

344
491

04
03
07




00
46
05

259
218
327




343
323
385

330
313
320
439
511
447
457
278
527
561




524
525
531
551




412
440
467




452
498
512

02
13

369 76 200 729 755 467 167 745 158 208 217
430 455 320 871 893 636 254 869 356 304 530

410
593

363
526

road sidew build wall fence pole light sign

veg

sky

pers ride

car

bus moto bicy

pycda 47
pit 53
fada 70
fda 80
cdam 78
ldr 77
sai2i 55

806
817
804
842
730
737
791

266
269
359
351
311
296
340

745
784
809
780
771
776
783

20
63
25
61
02
10
03

01
02
03
04
05
04
06

181
198
304
270
270
260
267

808
767
818
772
812
806
810

710
741
836
796
810
818
811

480
475
489
555
590
572
555

723
760
777
748
750
761
772

225
217
311
249
263
276
235

baseline ours
sac ours

607
779

269
386

671 83
835 158

00
15

advent 69
pit 53
pycda 47
cdam 78
fda 80
ldr 77
sai2i 55
fada 70
iast 54
rpt 83

856
831
755
825
793
851
877
845
819
889

422
276
309
422
350
445
497
401
415
465

797 87
815 89
833 208
813 
732 
810 
816 
831 48
833 177
845 151

baseline ours
sac ours

639
893

259
472

710 110
855 265

backbone vgg16

137
134
79
85
113
147
159

142
174
223
221
274
266
295

190
224
168
199
256
245
219

121
196
135
143
101
136
118

181
277
179
407
474
466
475

backbone resnet101

54
264
273
183
199
164
193
201
309
395

81
338
335
159
240
152
185
272
288
301

804
764
847
806
617
801
811
848
834
859

841
788
850
835
826
848
837
840
850
858

579
642
641
614
614
594
587
535
655
598

238
276
254
332
311
319
318
226
308
261

733
796
850
729
839
732
733
854
865
881

364
312
452
393
408
410
479
437
382
468

142
310
212
266
384
326
371
268
331
277

  denotes the use of pspnet 85 instead of deeplabv2 10 miou13 is the average iou over 13 classes ie excluding wall fence and pole

table 3 perclass iou  comparison on synthia  cityscapes adaptation evaluated on the cityscapes validation set


miou

configuration

80
64
39
26
24
19
17
16
15
06

419
435
460
473
475
480
482
483
484
493

no augmentation consistency
no momentum net   0 t  1
no photometric noise
no multiscale fusion
no focal loss   0
min entropy fusion vs averaging
no classbased thresholding   0
no confidence regularisation
no importance sampling
no horizontal flipping

499

full framework vgg16

00

setting with the vgg16 backbone we independently
switch off each component and report the results in table 4
we find that two components augmentation consistency
and the momentum network play a crucial role disabling
the momentum network leads to a 64 iou decrease
while abolishing augmentation consistency leads to a drop
of 80 iou recall that augmentation consistency comprises three augmentation techniques photometric noise
multiscale fusion and random flipping we further assess
their individual contributions training without the photometric jitter deteriorates the iou more severely by 39
compared to disabling the multiscale fusion 26 or
flipping 06 we hypothesise that encouraging model
robustness to photometric noise additionally alleviates the
inductive bias inherited from the source domain to rely on
strong appearance cues eg colour and texture which can
be substantially different from the target domain
following the intuition that highconfidence predictions
should be preferred 65 we study an alternative implementation of the multiscale fusion for overlapping pixels instead of averaging the predictions we pool the prediction with the minimum entropy the accuracy drop by
19 is somewhat expected averaging predictions via data
augmentation has previously been shown to produce wellcalibrated uncertainty estimates 1 this is important for
our method since it relies on the confidence values to select the predictions for use in selfsupervision importance
sampling contributes 15 iou to the total accuracy this

table 4 ablation study we use the gta5  cityscapes setting
with the vggbased model to study the effect of the components
of our framework by individually removing each we report the
mean iou for the cityscapes validation split

remarkably in both settings our approach is more accurate or competitive with many recent works 65 70 77
even when using a weaker backbone ie vgg16 instead
of resnet101 this is significant as these improvements
are not due to increased training complexity or model capacity in contrast to these previous works additional results including the evaluation on cityscapes test are shown
in appendices c and d

43 ablation study
to understand what makes our framework effective we
conduct an ablation study using the gta5  cityscapes
7

ground truth
adapted  vgg16  baseline
adapted  resnet101  baseline

a gta5  cityscapes

b synthia  cityscapes

figure 5 qualitative examples our approach rectifies an appreciable amount of erroneous predictions from the baseline

is surprisingly significant despite that our estimates cl are
only approximate cf sec 34 but the overall benefit is in
line with previous work 29 recall from eq 3 that our
confidence thresholds are computed per class to encourage
lower values for longtail classes disabling this scheme
is equivalent to setting   0 in eq 3 which reduces
the mean iou by 17 this confirms our observation that
the model tends to predict lower confidences for the classes
occupying only few pixels similarly the loss in eq 6
without the focal term   0 and confidence regularisation mcn  1 are 24 and 16 iou inferior this is a
surprisingly significant contribution at a negligible computational cost

align well with the object boundaries in the image although
our framework has no explicit encoding of spatial priors
which was previously deemed necessary 15 68 81 83
we believe that enforcing semantic consistency with data
augmentation makes our method less prone to the contextual bias 62 often blamed for coarse boundaries

5 conclusion
we presented a simple and accurate approach for domain
adaptation of semantic segmentation with ordinary augmentation techniques and momentum updates we achieve
stateoftheart accuracy yet make no sacrifice of the modest training or model complexity no components of our
framework are strictly specialised they build on a relatively
weak and broadly applicable assumption cf sec 1 although this work focuses on semantic segmentation we are
keen to explore the potential of the proposed techniques for
adaptation of other dense prediction tasks such as optical
flow monocular depth panoptic and instance segmentation
or even compositions of these multiple tasks

44 qualitative assessment
fig 5 presents a few qualitative examples comparing our approach to the naive baseline ie sourceonly
loss with abn particularly prominent are the refinements
of the classes road sidewalk and sky but even
smallscale elements improve substantially eg person
fence in the leftmost column this is perhaps not surprising owing to our multiscale training and the thresholding technique which initially ignores incorrectly predicted
pixels in selfsupervision as they initially tend to have low
confidence remarkably the segment boundaries tend to

acknowledgements
this work has been cofunded by the
loewe initiative hesse germany within the emergencity
center calculations for this research were partly conducted on
the lichtenberg high performance computer of tu darmstadt

8

references

16 yunchun chen yenyu lin minghsuan yang and jiabin huang crdoco pixellevel domain transfer with crossdomain consistency in cvpr pp 17911800 2019 2
17 jaehoon choi taekyung kim and changick kim selfensembling with ganbased data augmentation for domain
adaptation in semantic segmentation in iccv pp 6829
6839 2019 1 2
18 marius cordts mohamed omran sebastian ramos timo
rehfeld markus enzweiler rodrigo benenson uwe
franke stefan roth and bernt schiele the cityscapes
dataset for semantic urban scene understanding in cvpr
pp 32133223 2016 5
19 jia deng wei dong richard socher lijia li kai li
and feifei li imagenet a largescale hierarchical image
database in cvpr pp 248255 2009 5
20 jiahua dong yang cong gan sun yuyang liu and xiaowei xu cscl critical semanticconsistent learning for
unsupervised domain adaptation in eccv vol viii pp
745762 2020 2
21 arnaud doucet nando de freitas and neil j gordon an
introduction to sequential monte carlo methods in sequential monte carlo methods in practice pp 314 springer
2001 4
22 liang du jingang tan hongye yang jianfeng feng xiangyang xue qibao zheng xiaoqing ye and xiaolin
zhang ssfdan separated semantic feature based domain
adaptation network for semantic segmentation in iccv pp
982991 2019 2
23 geoffrey french michal mackiewicz and mark h fisher
selfensembling for visual domain adaptation in iclr
2018 2 3
24 yaroslav ganin evgeniya ustinova hana ajakan pascal germain hugo larochelle franois laviolette mario
marchand and victor lempitsky domainadversarial training of neural networks j mach learn res 1712096
2030 2016 2
25 rui gong wen li yuhua chen and luc van gool dlow
domain flow for adaptation and generalization in cvpr
pp 24772486 2019 2
26 ian goodfellow jean pougetabadie mehdi mirza bing
xu david wardefarley sherjil ozair aaron courville and
yoshua bengio generative adversarial nets in nips pp
26722680 2014 2
27 yves grandvalet and yoshua bengio semisupervised
learning by entropy minimization in nips pp 529536
2004 2
28 chuan guo geoff pleiss yu sun and kilian q weinberger
on calibration of modern neural networks in icml vol 70
pp 13211330 2017 2
29 agrim gupta piotr dollr and ross b girshick lvis a
dataset for large vocabulary instance segmentation in cvpr
pp 53565364 2019 3 8
30 kaiming he haoqi fan yuxin wu saining xie and
ross b girshick momentum contrast for unsupervised visual representation learning in cvpr pp 97269735 2020
3

1 murat seckin ayhan and philipp berens testtime data augmentation for estimation of heteroscedastic aleatoric uncertainty in deep neural networks in midl 2018 3 7
2 philip bachman ouais alsharif and doina precup learning with pseudoensembles in nips pp 33653373 2014
2
3 shai bendavid john blitzer koby crammer alex
kulesza fernando pereira and jennifer wortman vaughan
a theory of learning from different domains mach learn
7912151175 2010 2
4 shai bendavid john blitzer koby crammer and fernando
pereira analysis of representations for domain adaptation
in nips pp 137144 2006 1
5 david berthelot nicholas carlini ian j goodfellow nicolas papernot avital oliver and colin raffel mixmatch a
holistic approach to semisupervised learning in neurips
pp 50505060 2019 3
6 avrim blum and tom mitchell combining labeled and unlabeled data with cotraining in colt pp 92100 1998
1
7 minjie cai feng lu and yoichi sato generalizing hand
segmentation in egocentric videos with uncertaintyguided
model adaptation in cvpr pp 1438014389 2020 2
8 weilun chang huipo wang wenhsiao peng and weichen chiu all about structure adapting structural information across domains for boosting semantic segmentation
in cvpr pp 19001909 2019 2
9 liangchieh chen raphael gontijo lopes bowen cheng
maxwell d collins ekin d cubuk barret zoph hartwig
adam and jonathon shlens naivestudent leveraging
semisupervised learning in video sequences for urban scene
segmentation in eccv vol ix pp 695714 2020 1
10 liangchieh chen george papandreou iasonas kokkinos
kevin murphy and alan l yuille deeplab semantic image segmentation with deep convolutional nets atrous convolution and fully connected crfs ieee trans pattern
anal mach intell 404834848 2018 5 6 7
11 liangchieh chen yukun zhu george papandreou florian
schroff and hartwig adam encoderdecoder with atrous
separable convolution for semantic image segmentation in
eccv vol vii pp 833851 2018 1
12 minmin chen kilian q weinberger and john blitzer cotraining for domain adaptation in nips pp 24562464
2011 1
13 minghao chen hongyang xue and deng cai domain adaptation for semantic segmentation with maximum
squares loss in iccv pp 20902099 2019 2
14 yihsin chen weiyu chen yuting chen bocheng tsai
yuchiang frank wang and min sun no more discrimination cross city adaptation of road scene segmenters in
iccv pp 20112020 2017 2
15 yuhua chen wen li and luc van gool road reality oriented adaptation for semantic segmentation of urban scenes
in cvpr pp 78927901 2018 2 8

9

31 kaiming he xiangyu zhang shaoqing ren and jian sun
deep residual learning for image recognition in cvpr pp
770778 2016 5
32 judy hoffman eric tzeng taesung park junyan zhu
phillip isola kate saenko alexei a efros and trevor darrell cycada cycleconsistent adversarial domain adaptation in icml vol 80 pp 19942003 2018 2 6
33 weixiang hong zhenzhen wang ming yang and junsong
yuan conditional generative adversarial network for structured domain adaptation in cvpr pp 13351344 2018
2
34 sergey ioffe and christian szegedy batch normalization
accelerating deep network training by reducing internal covariate shift in icml vol 37 pp 448456 2015 4 5
35 pavel izmailov dmitrii podoprikhin timur garipov
dmitry p vetrov and andrew gordon wilson averaging
weights leads to wider optima and better generalization in
uai pp 876885 2018 3
36 fredrik d johansson david a sontag and rajesh ranganath support and invertibility in domaininvariant representations in aistats vol 89 pp 527536 2019 2
37 guoliang kang yunchao wei yi yang yueting zhuang
and alexander g hauptmann pixellevel cycle association
a new perspective for domain adaptive semantic segmentation in neurips pp 35693580 2020 2
38 alex kendall and yarin gal what uncertainties do we need
in bayesian deep learning for computer vision in nips pp
55745584 2017 3
39 myeongjin kim and hyeran byun learning texture invariant representation for domain adaptation of semantic segmentation in cvpr pp 1297212981 2020 1 2 5
40 samuli laine and timo aila temporal ensembling for semisupervised learning in iclr 2017 2 3
41 chenyu lee tanmay batra mohammad haris baig and
daniel ulbricht sliced wasserstein discrepancy for unsupervised domain adaptation in cvpr pp 1028510295
2019 2
42 donghyun lee pseudolabel the simple and efficient
semisupervised learning method for deep neural networks
in icml workshops vol 3 2013 1
43 guangrui li guoliang kang wu liu yunchao wei and
yi yang contentconsistent matching for domain adaptive
semantic segmentation in eccv vol xiv pp 440456
2020 1 2 3
44 xiaomeng li lequan yu hao chen chiwing fu and
phengann heng semisupervised skin lesion segmentation via transformation consistent selfensembling model in
bmvc 2018 2
45 yanghao li naiyan wang jianping shi xiaodi hou and
jiaying liu adaptive batch normalization for practical domain adaptation pattern recognition 80109117 2018 2
4 5
46 yunsheng li lu yuan and nuno vasconcelos bidirectional
learning for domain adaptation of semantic segmentation in
cvpr pp 69366945 2019 1 2 3 6
47 qing lian lixin duan fengmao lv and boqing gong
constructing selfmotivated pyramid curriculums for cross

48

49

50

51

52

53

54

55

56

57

58

59

60

61

62

10

domain semantic segmentation a nonadversarial approach
in iccv pp 67576766 2019 1 2 4 5 6 7
timothy p lillicrap jonathan j hunt alexander pritzel
nicolas heess tom erez yuval tassa david silver and
daan wierstra continuous control with deep reinforcement
learning in iclr 2016 3
tsungyi lin priya goyal ross b girshick kaiming he
and piotr dollr focal loss for dense object detection ieee
trans pattern anal mach intell 422318327 2020 4
mingsheng long zhangjie cao jianmin wang and
michael i jordan conditional adversarial domain adaptation in neurips pp 16471657 2018 2
yawei luo ping liu tao guan junqing yu and yi yang
significanceaware information bottleneck for domain adaptive semantic segmentation in cvpr pp 67786787 2019
2
yawei luo liang zheng tao guan junqing yu and yi
yang taking a closer look at domain shift categorylevel
adversaries for semantics consistent domain adaptation in
cvpr pp 25072516 2019 2
fengmao lv tao liang xiang chen and guosheng lin
crossdomain semantic segmentation via domaininvariant
interactive relation transfer in cvpr pp 43334342 2020
1 2 6 7
ke mei chuang zhu jiaqi zou and shanghang zhang instance adaptive selftraining for unsupervised domain adaptation in eccv vol xxvi pp 415430 2020 1 2 3 6
7
luigi musto and andrea zinelli semantically adaptive
imagetoimage translation for domain adaptation of semantic segmentation in bmvc 2020 1 2 6 7
fei pan inkyu shin franois rameau seokju lee and
in so kweon unsupervised intradomain adaptation for semantic segmentation through selfsupervision in cvpr pp
37633772 2020 2
adam paszke sam gross and francisco massa et al pytorch an imperative style highperformance deep learning
library in neurips pp 80248035 2019 5
christian s perone pedro l ballester rodrigo c barros and julien cohenadad unsupervised domain adaptation for medical imaging segmentation with selfensembling
neuroimage 194111 2019 2
stephan r richter vibhav vineet stefan roth and vladlen
koltun playing for data ground truth from computer
games in eccv vol ii pp 102118 2016 1 5
germn ros laura sellart joanna materzynska david
vzquez and antonio m lpez the synthia dataset
a large collection of synthetic images for semantic segmentation of urban scenes in cvpr pp 32343243 2016 1
5
mehdi sajjadi mehran javanmardi and tolga tasdizen
regularization with stochastic transformations and perturbations for deep semisupervised learning in nips pp 1163
1171 2016 2
rakshith shetty bernt schiele and mario fritz not using the car to see the sidewalk  quantifying and controlling
the effects of context in classification and segmentation in
cvpr pp 82188226 2019 8

77 jinyu yang weizhi an sheng wang xinliang zhu
chaochao yan and junzhou huang labeldriven reconstruction for domain adaptation in semantic segmentation in
eccv vol xxvii pp 480498 2020 1 2 6 7
78 jinyu yang weizhi an chaochao yan peilin zhao and junzhou huang contextaware domain adaptation in semantic
segmentation in wacv pp 514524 2021 1 2 6 7
79 yanchao yang dong lao ganesh sundaramoorthi and stefano soatto phase consistent ecological domain adaptation
in cvpr pp 90089017 2020 2
80 yanchao yang and stefano soatto fda fourier domain
adaptation for semantic segmentation in cvpr pp 4084
4094 2020 1 2 5 6 7
81 yang zhang philip david and boqing gong curriculum domain adaptation for semantic segmentation of urban
scenes in iccv pp 20392049 2017 2 8
82 yiheng zhang zhaofan qiu ting yao dong liu and tao
mei fully convolutional adaptation networks for semantic
segmentation in cvpr pp 68106818 2018 2
83 yiheng zhang zhaofan qiu ting yao chongwah ngo
dong liu and tao mei transferring and regularizing prediction for semantic segmentation in cvpr pp 96189627
2020 2 4 6 7 8
84 han zhao remi tachet des combes kun zhang and geoffrey j gordon on learning invariant representations for
domain adaptation in icml vol 97 pp 75237532 2019
2
85 hengshuang zhao jianping shi xiaojuan qi xiaogang
wang and jiaya jia pyramid scene parsing network in
cvpr pp 62306239 2017 1 6 7
86 zhedong zheng and yi yang unsupervised scene adaptation
with memory regularization in vivo in ijcai pp 1076
1082 2020 1 2
87 zhedong zheng and yi yang rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic
segmentation int j comput vis 129411061120 2021
1 2
88 qianyu zhou zhengyang feng guangliang cheng xin
tan jianping shi and lizhuang ma uncertaintyaware consistency regularization for crossdomain semantic segmentation arxiv200408878 cscv 2020 2
89 junyan zhu taesung park phillip isola and alexei a
efros unpaired imagetoimage translation using cycleconsistent adversarial networks in iccv pp 22422251
2017 2
90 xinge zhu hui zhou ceyuan yang jianping shi and
dahua lin penalizing top performers conservative loss
for semantic segmentation adaptation in eccv vol vii
pp 568583 2018 2
91 yang zou zhiding yu bvk vijaya kumar and jinsong
wang unsupervised domain adaptation for semantic segmentation via classbalanced selftraining in eccv vol
iii pp 297313 2018 1 2 3 6
92 yang zou zhiding yu xiaofeng liu bvk vijaya kumar
and jinsong wang confidence regularized selftraining in
iccv pp 59815990 2019 1 2

63 karen simonyan and andrew zisserman very deep convolutional networks for largescale image recognition in iclr
2015 5
64 kihyuk sohn david berthelot nicholas carlini zizhao
zhang han zhang colin raffel ekin dogus cubuk alexey
kurakin and chunliang li fixmatch simplifying semisupervised learning with consistency and confidence in
neurips pp 596608 2020 2
65 muhammad subhani and mohsen ali learning from scaleinvariant examples for domain adaptation in semantic segmentation in eccv vol xxii pp 290306 2020 1 2 3
4 7
66 antti tarvainen and harri valpola mean teachers are better
role models weightaveraged consistency targets improve
semisupervised deep learning results in nips pp 1195
1204 2017 2 3
67 yihsuan tsai weichih hung samuel schulter kihyuk sohn minghsuan yang and manmohan chandraker
learning to adapt structured output space for semantic segmentation in cvpr pp 74727481 2018 1 2 5
68 yihsuan tsai kihyuk sohn samuel schulter and manmohan chandraker domain adaptation for structured output
via discriminative patch representations in iccv pp 1456
1465 2019 1 2 5 8
69 tuanhung vu himalaya jain maxime bucher matthieu
cord and patrick prez advent adversarial entropy
minimization for domain adaptation in semantic segmentation in cvpr pp 25172526 2019 1 2 5 6 7
70 haoran wang tong shen wei zhang lingyu duan and
tao mei classes matter a finegrained adversarial approach to crossdomain semantic segmentation in eccv
vol xiv pp 642659 2020 1 2 5 6 7
71 kaihong wang chenhongyi yang and margrit betke
consistency regularization with highdimensional nonadversarial sourceguided perturbation for unsupervised domain adaptation in segmentation aaai 2021 2
72 yude wang jie zhang meina kan shiguang shan and
xilin chen selfsupervised equivariant attention mechanism for weakly supervised semantic segmentation in
cvpr pp 1227212281 2020 3
73 zhonghao wang mo yu yunchao wei rogrio feris jinjun xiong wenmei hwu thomas s huang and honghui
shi differential treatment for stuff and things a simple
unsupervised domain adaptation method for semantic segmentation in cvpr pp 1263212641 2020 2 5
74 zuxuan wu xintong han yenliang lin mustafa gkhan
uzunbas tom goldstein sernam lim and larry s davis
dcan dual channelwise alignment networks for unsupervised scene adaptation in eccv vol v pp 535552 2018
2
75 qizhe xie zihang dai eduard h hovy thang luong and
quoc le unsupervised data augmentation for consistency
training in neurips pp 62566268 2020 2
76 qizhe xie minhthang luong eduard h hovy and
quoc v le selftraining with noisy student improves imagenet classification in cvpr pp 1068410695 2020 2
3

11

selfsupervised augmentation consistency
for adapting semantic segmentation
 supplemental material 

1

nikita araslanov1
stefan roth12
department of computer science tu darmstadt

a overview
in this appendix we first provide further training and
implementation details of our framework we then take a
closer look at the accuracy of longtail classes before and
after adaptation next we discuss our strategy for hyperparameter selection and perform a sensitivity analysis we
also evaluate our framework using another segmentation architecture fcn8s 98 finally we discuss the limitations
of the current evaluation protocol and propose a revision
based on the best practices in the field at large

1
2
3
4

import
import
import
import

2

hessianai

random
pil
torchvision  transforms as tf
torchvision  transforms  functional as f

5
6
7

 load the image
image  pil  image  open 

8
9
10
11
12
13

 gaussian blur
 with a randomly sampled radius
radius  random  uniform 1 2
gaussian  pil  imagefilter  gaussianblur  radius 
image  image  filter  gaussian 

14

b further technical details

15

photometric noise recall that our framework uses random gaussian smoothing greyscaling and colour jittering
to implement the photometric noise we reuse the parameters for these operations from the mocov2 framework 93
in detail the kernel radius for the gaussian blur is sampled uniformly from the range 01 20 note that this does
not correspond to the actual filter size1 the colour jitter
applied with probability 05 implements a perturbation of
the image brightness contrast and saturation with a factor
sampled uniformly from 06 14 while the hue factor is
sampled uniformly at random in the range of 09 11 we
convert a target image to its greyscale version with probability 02 fig 6 demonstrates an example implementation
of this procedure in python

17

16

18
19
20
21
22

 colour jitter
 with probability 05
if 05  random  random  
jitter  tf  colorjitter  brightness 04 
contrast 04 
saturation 04 
hue 01
image  jitter  image 

23
24
25
26
27

 convert to greyscale
 with probability 02
if 02  random  random  
image  f  tograyscale  image 

figure 6 python implementation of the photometric noise

training schedule our framework typically needs 150 
200k iterations in total ie including the sourceonly pretraining until convergence as determined on a random subset of 500 images from the training set see our discussion in
appendix d below this varies slightly depending on the
backbone and the source data used this schedule translates
to approximately 3 days of training with standard gpus
eg titan x pascal with 12 gb memory for both vgg16 and resnet101 backbones recall that we used 4 gpus
for our resnet version of the framework hence its training
time is comparable to the vgg variant which uses only 2
gpus all our experiments use a constant learning rate for
simplicity but more advanced schedules such as cyclical
learning rates 35 the cosine schedule 93 95 or rampups
40 may further improve the accuracy of our framework

constraintfree data augmentation similarly to the
multiscale cropping of the target images we scale the
source images randomly with a factor sampled uniformly
from 05 10 prior to cropping however we do not enforce the semantic consistency for the source data since the
ground truth of the source images is available for both
the target and source images we also use random horizontal
flipping we additionally experimented with moderate rotation both with and without semantic consistency but did
not observe a significant effect on the mean accuracy
1 the pillow library 94 internally converts the radius r to the box

length as l  3  r2  1

i

cbt

is

fl

3

3

3

sky pers ride

car

truck bus train moto bicy

miou

881

410

857 308 306 331 370 229 866 368 907 671 271 868 344 304

85

75

00

445

894
900
893

523
471
390

860 340 326 385 433 306 852 309 885 667 280 857 356 396 00
856 313 249 323 389 282 873 398 894 677 286 881 401 500 73
851 332 261 324 418 252 863 274 904 664 282 875 329 454 110

66
99
76

00
22
00

460
468
450

3
3

893
893
897

526
522
451

860 334 300 380 449 343 869 353 880 654 273 862 376 440 209 96 65
861 342 315 370 434 363 852 307 866 662 303 853 362 439 292 68 86
856 296 283 317 419 275 872 374 898 669 292 875 373 316 247 119 202

482
484
475

3

900

531

862 338 327 382 460 403 842 264 884 658 280 856 406 529 173 137 238

499

3
3

terr

3
3

3
3

road sidew build wall fence pole light sign veg

table 5 perclass iou  on cityscapes val using a vgg16 backbone in the gta5  cityscapes setting we study three components
in more detail classbased thresholding cbt importance sampling is and the focal loss fl the miou of the settings in the last
four rows are reproduced from the main text here we elaborate on the perclass accuracy in a broader context of the supplementary
experiments in the first four rows

c additional experiments

on the iou of the other classes therefore the benefits of
individual framework components have to be understood in
the context of their aggregated effect on multiple classes
eg using the mean iou for instance consider the class
train for which is appears to also decrease the iou while
cbt together with fl achieve 292 iou adding is decreases the iou to 173 however the iou of other classes
increases eg motorcycle bicycle as does the mean
iou furthermore only few classes reach their maximum
accuracy when we enable all three longtail components
yet it is the setting with the best accuracy tradeoff between the individual classes ie with the highest mean iou
overall the longtail components improve our framework
by 54 mean iou combined a substantial margin

c1 a closer look at longtail adaptation
recall that our framework features three components to
attune the adaptation process to the longtail classes classbased thresholding cbt importance sampling is and
the focal loss fl which we summarily refer to as the longtail components in the following disabling the longtail
components individually is equivalent to setting   0 for
cbt using uniform sampling of the target images instead
of is or assigning  to 0 for the fl here we extend our ablation study of the gta5  cityscapes setup with vgg16
cf table 4 from the main text and experiment with different combinations of the longtail components table 5
details the perclass accuracy of the possible compositions
we observe that the ubiquitous classes  road building vegetation sky person and car  are hardly
affected it is primarily the longtail categories that change
in accuracy furthermore our longtail components are mutually complementary the mean iou improves when one
of the components is active from 445 to up to 468
it is boosted further with two of the components enabled
to 484 and reaches its maximum for our model 499
when all three components are in place
we further identify the following tentative patterns fl
tends to improve classes wall fence and pole cbt
increases the accuracy of the traffic light category which
has high image frequency and occupies only a few pixels
but also rare classes such as rider bus and train
benefit from cbt especially in conjunction with is is
also enhances the mask quality of the classes bicycle and
motorcycle nevertheless we urge caution against interpreting the results for each class in isolation despite such
widespread practice in the literature todays semantic segmentation models do not possess the notion of an ambiguous class prediction and each pixel receives a meaningful
label by the pigeons hole principle this implies that the
changes in the iou of one class have an immediate effect

c2 hyperparameter search and sensitivity
to select  and  we first experimented with a few reasonable choices   07 08   00001 0012 using
a more lightweight backbone mobilenetv2 97 to measure performance we use the mean iou on the validation
set 500 images from cityscapes train as in the main text
here we study our frameworks sensitivity to the particular choice of  and  to make the results comparable to
our previous experiments we use vgg16 and report the
mean iou on cityscapes val in table 7 we observe moderate deviation of the iou wrt  a more tangible drop
in accuracy with   001 is expected as it leads to lowconfidence predictions which are likely to be inaccurate to
be included into the pseudo label we note that while a suboptimal choice of these hyperparameters leads to inferior
results with a standard deviation of 14 miou even
the weakest model with   08 and   001 did not fail
to considerably improve over the baseline by 85 iou cf
table 2 in the main text
2 while

 may seem more interpretable the maximum confidence
threshold a reasonable range for  can be derived from c for the longtail classes which is simply the fraction of pixels these classes tend to
occupy in the image see eq 3

ii

method

road sidew build wall fence pole light sign veg

terr

sky pers ride

car truck bus train moto bicy

miou

gta5  cityscapes

baseline ours
sacfcn ours

767 282
863 456

744 127 190 272 287 122 770 180 706 548 206 796 190 192 206 279 112
844 303 271 248 428 352 869 397 880 623 321 841 284 437 319 294 458

367 371
499 499

609 18
814 198

316 344
468 491

synthia  cityscapes

baseline ours
sacfcn ours

507 238
747 342

01 277 105 157 601
19 272 348 272 800




724 501 160 665
863 615 208 825




137
312




85 268
320 539

table 6 perclass iou  on cityscapes val using vgg16 with fcn8s for reference the numbers in parentheses in the last column
report the mean iou of the deeplabv2 architecture cf tables 2 and 3 from the main text


07
075
08



  00001
479
486
482

0001

001

d towards bestpractice evaluation

488
499
498

467
463
456

the current strategy to evaluate domain adaptation da
methods for semantic segmentation is to use the ground
truth of 500 randomly selected images from the cityscapes
train split for model selection and to report the final model
accuracy on the 500 cityscapes val images 47 in this
work we adhered to this procedure to enable a fair comparison to previous work however this evaluation approach
is evidently in discord with the established best practice in
machine learning and with the benchmarking practice on
cityscapes 18 in particular
the test set is holdout data to be used only for an unbiased performance assessment eg segmentation accuracy
of the final model 96 while it is conceivable to consult
the test set for verifying a number of model variants such
access cannot be unrestrained this is infeasible to ensure
when the test set annotation is public as is the case with
cityscapes val however benchmark websites traditionally
enable a restricted access to the test annotation through impartial submission policies eg limited number of submissions per time window and user and cityscapes officially
provides one3
we therefore suggest a simple revision of the evaluation protocol for evaluating future da methods as before
we use cityscapes train as the training data for the target
domain naturally without the ground truth for model selection however we use cityscapes val images with the
groundtruth labels the holdout test set for reporting
the final segmentation accuracy after adaptation becomes
cityscapes test with the results obtained via submitting
the predicted segmentation masks to the official cityscapes
benchmark server
an additional advantage of this strategy is a clear interpretation of the final accuracy in the context of fully supervised methods that routinely use the same evaluation setup
also note that cityscapes val contains images from different cities than cityscapes train which are also different
from cityscapes test therefore it is more suitable for detecting cases of model overfitting on particularities of the
city since the validation set was previously a subset of the

table 7 mean iou  on gta5  cityscapes val with varying  and  our framework maintains strong accuracy under different settings of  and  even with a poor choice eg   08
  001 it fares well wrt the state of the art and outperforms
many previous works cf table 2 from the main text

c3 vgg16 with fcn8s
a number of previous works eg 55 77 80 used
the fcn8s 98 architecture with vgg16 as opposed to
deeplabv2 10 adopted in other works eg 39 70
and ours such architecture exchange appears to have been
dismissed in previous work as minor which used only one
of the architectures in the experiments however the segmentation architecture alone may contribute to the observed
differences in accuracy of the methods and more critically to the improvements otherwise attributed to other aspects of the approach to facilitate such transparency in
our work we replace deeplabv2 with its fcn8s counterpart in our framework with the vgg16 backbone and
repeat the adaptation experiments from sec 4 ie using
two source domains gta5 and synthia and cityscapes
as the target domain we keep the values of the hyperparameters the same with an exception of the learning rate
which we increase by a factor of 2 to 5  104  table 6 reports the results of the adaptation which clearly show that
our framework generalises well to other segmentation architectures despite the fcn8s baseline model sourceonly
loss with abn achieving a slightly inferior accuracy compared to deeplabv2 eg 316 vs 344 iou for synthia  cityscapes our selfsupervised training still attains a remarkably high accuracy 468 iou vs 491
with deeplabv2 this is substantially higher than the previous best method using fcn8s with the vgg16 backbone sai2i 55 34 on gta5  cityscapes and
53 on synthia  cityscapes

3 httpswwwcityscapesdatasetcom

iii

method

road sidew build wall fence pole light sign veg terr sky pers ride car truck bus train moto bicy

miou

gta5  cityscapes

sacfcn ours
sacvgg ours
sacresnet ours

875 452
915 539
918 543

850 292 264 233 442 320 883 526 912 652 350 860 244 328 314 369 405
866 341 315 368 472 369 851 380 911 687 319 874 310 467 226 242 240
874 362 302 437 497 421 893 543 905 718 349 898 388 473 249 383 438

504 499
510 499
557 538

669 259
704 297
874 410

808 121
836 116
855 175

458 468
483 491
527 526

979 813
974 784

904 488 474 496 579 673 919 694 942 798 598 937 565 675 575 577 688
892 349 442 474 601 650 914 693 939 771 514 926 353 486 465 516 668

synthia  cityscapes

sacfcn ours
sacvgg ours
sacresnet ours

20 244 371 275 788  889 639 250 847
18 342 412 292 810  871 679 254 759
26 405 447 344 879  912 680 310 893





274
343
332





369 502
425 575
386 499

fully supervised cityscapes

deeplabresnet 10
fcnvgg 98

704
653

table 8 perclass iou  on cityscapes test in the last column the numbers in parentheses report the mean iou on cityscapes val
from the previous evaluation scheme cf tables 2 and 3 from the main text for reference sacfcn denotes our vggbased model with
fcn8s 98 from appendix c3

training images
for future reference we evaluate our framework both
the deeplabv2 and fcn8s variants in the proposed setup
and report the results in table 8 to ease the comparison
we juxtapose our validation results reported in the main
text from table 6 for fcn8s4 as we did not finetune
our method to cityscapes val following the previous evaluation protocol we expect the test accuracy on cityscapes
test to be on a par with our previously reported accuracy
on cityscapes val the results in table 8 clearly confirm
this expectation the segmentation accuracy on cityscapes
test is comparable to the accuracy on cityscapes val synthia  cityscapes or even tangibly higher gta5 
cityscapes we remark that the remaining accuracy gap
to the fully supervised model is still considerable 704
vs 557 iou achieved by our best deeplabv2 model and
653 vs 510 iou compared to our best fcn8s variant
which invites further effort from the research community
we hope that future uda methods for semantic segmentation will follow suit in reporting the results on cityscapes

test owing to the regulated access to the test set we believe this setting to offer more transparency and fairness to
the benchmarking process and will successfully drive the
progress of uda for semantic segmentation as it has done
in the past for the fully supervised methods

references
93 xinlei chen haoqi fan ross girshick and kaiming he
improved baselines with momentum contrastive learning
arxiv200304297 cscv 2020
94 alex clark pillow pil fork documentation 2015
95 ilya loshchilov and frank hutter sgdr stochastic gradient descent with warm restarts in iclr 2017
96 brian d ripley pattern recognition and neural networks
cambridge university press 1996
97 mark sandler andrew g howard menglong zhu andrey
zhmoginov and liangchieh chen mobilenetv2 inverted
residuals and linear bottlenecks in cvpr pp 45104520
2018
98 evan shelhamer jonathan long and trevor darrell fully
convolutional networks for semantic segmentation ieee
trans pattern anal mach intell 394640651 2017

4 to our best knowledge no previous work published their results in
this evaluation setting before

iv

