arxiv210500039v1 csdc 30 apr 2021

gpu acceleration of 3d agentbased
biological simulations
ahmad hesam

lukas breitwieser

fons rademakers

zaid alars

abs group
delft university of technology
delft netherlands
ashesamtudelftnl

cern openlab
cern
geneva switzerland
lukasbreitwiesercernch

cern openlab
cern
geneva switzerland
fonsrademakerscernch

abs group
delft university of technology
delft netherlands
zalarstudelftnl

abstractresearchers in biology are faced with the tough
challenge of developing highperformance computer simulations
of their increasingly complex agentbased models biodynamo
is an opensource agentbased simulation platform that aims
to alleviate researchers from the intricacies that go into the
development of highperformance computing through a highlevel interface researchers can implement their models on top of
biodynamos multithreaded core execution engine to rapidly
develop simulations that effectively utilize parallel computing
hardware in biological agentbased modeling the type of operations that are typically the most computeintensive are those
that involve agents interacting with their local neighborhood
in this work we investigate the currently implemented method
of handling neighborhood interactions of cellular agents in
biodynamo and ways to improve the performance to enable
largescale and complex simulations we propose to replace the
kdtree implementation to find and iterate over the neighborhood
of each agent with a uniform grid method that allows us
to take advantage of the massively parallel architecture of
graphics processing units gpus we implement the uniform
grid method in both cuda and opencl to address gpus
from all major vendors and evaluate several techniques to
further improve the performance furthermore we analyze the
performance of our implementations for models with a varying
density of neighboring agents as a result the performance of the
mechanical interactions method improved by up to two orders of
magnitude in comparison to the multithreaded baseline version
the implementations are opensource and publicly available on
github
index termsagentbased modeling simulation gpu coprocessing biological models acceleration

i i ntroduction
agentbased simulation abs is a powerful tool for conducting research on complex biological systems in abs a
biological system is composed of a number of agents that
individually are modeled to follow a fixed set of often simple
rules agents can interact with neighboring agents or respond
to external stimuli although the individual behavior of agents
is often trivial the emerging behavior that comes forth from
the biological system as a whole can give researchers valuable
insights 13
as the complexity and scale of biological agentbased
models increases so does the demand for computational power
and efficiency 2 agentbased simulations are inherently
parallelizable in their execution as the agents states can be
modified independently of each other modernday hardware is

becoming increasingly more parallelized as a result of dennard
scaling 4 and the stagnation of moores law 5 as pointed
out in 6 moreover generalpurpose computing on graphics
processing units gpus is an attractive solution to improve the
computational efficiency of abs applications in particular 7
8 and parallel applications in general 9 10 by porting
applications to either fully or partially run on gpus it is
possible to observe speedups of several orders of magnitude in
comparison to the cpuonly execution 11 although several
abs frameworks exist that achieve significant speedups using
gpus in the field of abs there is still significant room for
improvement which we wish to address in this article
biodynamo 6 is an opensource software platform for life
scientists for simulating biological agentbased models each
agent in biodynamo is programmed to follow a specified set
of rules imposed by the modeler that can trigger specified
actions affecting itself or other agents agents in biological
systems often interact with their local environment and their
behavior can be influenced by other agents that reside within
a certain range an example is the mechanical interactions
a cellular agent undergoes when it physically collides with
another agent local interactions are an extremely important
concept in biological systems since it is the driving force
behind key biological processes such as tissue development
12
biodynamo is fully parallelized using openmp and its
performance scales with the number of cpu cores available
on a system 6 to further enhance the simulations performance we want to investigate the applicability of gpus in
accelerating computeintensive operations in biodynamo in
this work we present the following contributions
 redesign the neighborhood search in biodynamo from
a kdtree method to a uniform grid method to profit from
the parallel architecture of gpus
 port the uniform grid implementation to gpu code using
opencl and cuda to address all major gpu vendors
 improve the gpu kernels based on domainspecific aspects of biological agentbased models
 benchmark the runtime and analyze the performance
gains that are obtained
the organization of the paper is as follows section ii

r2
p2

fig 1 spheresphere collision force diagram projected as
circles for simplicity

discusses related work in section iii we define the problem
in more detail in section iv we describe the methodology of
our approach section v describes the hardware and software
setup in section vi we present the results and finally in
section vii we draw the conclusions of this work
ii r elated w ork
there are several frameworks and software packages that
make it possible to simulate agentbased models for biological
systems there are many more specialized software solutions
but these generally focus on one biological process or a
few closely related biological processes some of the more
general abs frameworks for biological systems biocellion
13 physicell 14 timothy 15 and chaste 16 focus
among other things on computational efficiency but do not
support gpu acceleration in this work we demonstrate that
gpu acceleration is possible for generalpurpose agentbased
platforms
in the works of 7 and 8 the authors present cellular
agentbased simulation abs programs that run entirely on
a gpu the authors report speedups of several orders of
magnitude over abs frameworks that are only targeted for
cpus although the findings are impressive the fact that
the simulation runs entirely on the gpu has two major
drawbacks first it puts a lot of pressure on minimizing
memory consumption as gpu memory is a nonexpandable
and limited resource there is a limit to the complexity of the
agents state and the scale of the model in this work we
offload the most computeintensive operation to gpu which
requires only a subset of the agents state data to be present
on the gpu memory second operations that are independent
of the agents such as extracellular substance diffusion are
integral to biological systems and are absent from these works
with biodynamo we can simulate the extracellular substance
diffusion efficiently on a multicore cpu independently from
the gpu operations 6
iii p roblem d efinition
the mechanical interaction operation is one of the most
computeintensive operations in any cellular agentbased
model each cell ie agent interacts with all other cells within
a certain interaction radius for cells that are physically in
contact with each other we need to compute the collision
forces and the resulting displacement in biodynamo cellular

fig 2 a visualization of the cell division module in biodynamo crosssectional view the colors represent the diameter
of the cells

agents can be physically modeled as spherical objects for
the scope of this paper we shall consider only spheresphere
interactions as illustrated in fig 1 projected as circles equation 1 17 shows the calculations involved in determining
the mechanical force
 
r 
f 

r1  r2  kp1  p2 k
r1 r2
r1 r2

     



1
r   

p1 p2
kp1 p2 k



where r1 and r2 are the radii of the spheres p1 and p2 their
position vectors  the repulsion coefficient  the attraction
coefficient and f the resulting collision force vector after the
collision force has been computed we determine whether it is
strong enough to break the adherence of the cell in question
if that is the case then we integrate over the collision force
to compute the final displacement the length of the final
displacement vector is generally limited by an upper bound
to quantify the impact of improving this operation for
biodynamo we run one of the available benchmarks that
use all default operations cell division module in this
benchmark a 3d grid of 262144 cells of the same volume are
spawned and proliferate for 10 iterations once the cells are
instantiated in each iteration the same operations are executed
1 cell proliferation 2 neighborhood lookup and 3 resolving
the mechanical forces a visualization of cell proliferation in
biodynamo with fewer cells and a longer runtime is shown in
fig 2 we profile this benchmark to get a better understanding
of the computational bottlenecks in biodynamo
from fig 3 we observe that the mechanical interactions
operation highlighted in blue is the most timeconsuming
in the benchmark by a large margin since this operation
requires iterating over all agents and in turn over all of
their neighboring agents this observation matches our prior
expectation 51 of the benchmarks runtime is spent on
the mechanical force calculations as described in 1 and
36 is spent on updating the neighborhood list of each
agent updating the neighborhood is executed in two steps 1

mech force comp 51
neighborhood 36
cell division 2
remainder 11

box
grid
 boxes  vectorbox
 boxlength  uint32t
 griddimensions  arrayuint32t 6
 successors vectorsohandle

 start  atomicsohandle
 length  atomicuint16t
 addobjectsohandle

sohandle

51

36

11

fig 3 runtime profile of the cell division benchmark in
biodynamo

fig 4 finding the neighborhood of an agent using the uniform
grid method displayed in 2d for simplicity

 foreachneighborwithinradiusf func
 initialize
 updategrid
 cleargrid

 typeidx  uint16t
 elementidx  uint32t
 gettypeidx  uint16t
 getelementidx  uint32t

fig 5 uml diagram of the class created for the uniform grid
method

grid approach in biodynamo as a c class as illustrated
in fig 5 as a uml diagram for every simulation timestep
we reconstruct the uniform grid to take into account the
addition deletion and movement of agents each voxel ie
box keeps track of the number of agents it contains and the
last object that was added through the use of a linked list
gridsuccessors we can iterate through all objects
inside a single box the exact implementation details can be
found in our github repository1 
b gpu implementation

building a kdtree and 2 searching all the agents neighbors
within a specified radius
a kdtree is one of the many methods that can be used
for a radial neighborhood search considering that we want
to offload this mechanical interactions operation to gpu a
more appealing method could be a uniform grid method the
uniform grid method allows us to apply different techniques
to improve the gpu version of the mechanical interactions
operation which we will discuss in this paper
iv m ethodology
in this section we will go over the implementation of
the various improvements that were made on the existing
mechanical interactions operation in biodynamo we use
biodynamo v0098b3d6c7 as the baseline version which
allows us to benefit more from gpu acceleration than the latest
version presented in 6 as the data are stored in a structsofarrays format rather than arraysofstructs
a uniform grid method
the uniform grid method imposes a regularlyspaced 3d
grid within the simulation space each voxel of the grid
contains only the agents that are confined within its subspace
finding the neighboring agents of a particular agent can be
done by only taking into account the voxels surrounding that
particular agent as illustrated in 2d in fig 4 the agent that
we want to find the neighborhood for is colored red and its
interaction radius is highlighted in red we only consider the
agents in the 9 surrounding voxels 27 in 3d around which
a red line is drawn in the figure we implement the uniform

we implement the uniform grid solution on the gpu using
both cuda and opencl to target gpus from all major
vendors to minimize the amount of cpu and gpu context
switches we decided to port the uniform grid algorithm as well
as the mechanical force computation as a single gpu kernel
each gpu thread handles the mechanical interaction of one
cell by 1 finding the cells neighborhood and 2 computing
the mechanical forces between the cell and all the cells in its
neighborhood the state data of all the agents in biodynamo
are stored as structsofarrays eg the position data of all
agents are store contiguously in memory this allows us to
copy the required state data for the mechanical interaction
operation from the host dram to the gpu dram without
first having to coalesce the data for all agents
c improvement i reduction in floatingpoint precision
biodynamo uses doubleprecision floating points fp64
data types for all its floatingpoint data however most consumer gpus perform stronger in singleprecision floatingpoint fp32 operations this is a manifestation of the fact that
gpu vendors primarily target the gaming industry and the field
of artificial intelligence game engines and machine learning frameworks rely mostly on singleprecision floatingpoint
operations so gpu manufacturers designed their consumer
gpus with more fp32 logic units than their doubleprecision
counterparts some gpu vendors have dedicated cards for
highperformance scientific computing that offer more fp64
logic units for agentbased simulations other factors such
1 httpsgithubcomsenuibiodynamotreepaperfloats

a data required by
thread x

b data required by
thread y

c data required by
thread z

fig 7 exploiting the reuse of neighboring simulation object
data for the usage of shared memory resources on gpu
fig 6 the path of a zorder curve in 2d adapted from 18
as choosing the correct runtime parameters for a model eg
initial agent attribute values number of simulation steps etc
generally far outweigh the accuracy of the final results in
comparison to the imprecision that could come forth from
reducing the floatingpoint precision from double to single
biodynamo has an extensive set of unit tests and integration
tests that we can use to verify whether or not the reduction to
fp32 affects the results moreover fp32 data types are half
the size of fp64 data types in memory which reduces the size
of the buffers that need to be copied back and forth from the
host to the device leading to a potentially significant increase
in throughput and thus performance
d improvement ii spacefilling curve sorting
cuda and opencl organize threads in groups of threads
called blocks and workgroups respectively the execution of
the threads on the actual hardware is done in warps generally
in groups of 32 threads with each warp executing the same
instruction but on different data ie simt execution model
biodynamo lays down the agents data in memory in the order
that the c objects were instantiated each thread requires the
data of the neighborhood of the simulation object it processes
which is not contiguous in memory but rather scattered consequently each thread performs numerous scattered memory
accesses which will in most cases end up fetching the data
from dram which can degrade the performance significantly
this could have been prevented if the data of agents that are
close to each other in space are also laid down close to each
other in memory this is where spacefilling curves come in
more specifically the zorder curve 19 a spacefilling curve
describes a path in multidimensional space that passes through
the data points in consecutively local order as illustrated in
fig 6 a function that implements a spacefilling curve can
map multidimensional data such as 3d cartesian coordinates
to a onedimensional array where consecutive elements of
that array are spatially local to each other for a zorder
curve the zvalue of each data point can be computed by
binary interleaving its coordinate values and represents the
index of the resulting onedimensional array with regards
to biodynamo this would imply calculating the zvalues of
all the agents and sorting their state data accordingly we
anticipate that the cache line for accessing an agent will

also contain the data of the agents in its neighborhood and
therefore reduces the number of fetches to dram a reduced
number of fetches to dram should lead to a less datastarved
execution pipeline and therefore a higher throughput and thus
a reduction in the execution time for each simulation step
e improvement iii using shared memory
most gpus feature different types of onchip memory
such as texture memory or shared memory in certain cases
storing data on onchip memory drastically reduces the latency
for fetching data during a gpu kernel execution and could
therefore improve the overall performance in biodynamo
the concept of letting each gpu thread handle the mechanical
interactions of one agent leaves little room for the shared
memory resources of gpus to be used the reason is that
there is no reuse of data for threads within the same cuda
block or opencl workgroup the kernel parallelizes the for
loop over all agents so each thread works on data that are
independent of the threads in the same block to make use of
shared memory we need to create a kernel that allows multiple
threads to work on mostly the same data it is here where
we can reap the benefits of the uniform grid method that we
implemented as an alternative to the kdtree method we can
exploit the fact that cells in the same voxel of the ug grid
share the same neighboring voxels and thus share the same
simulation object candidates for their neighborhood instead of
parallelizing the for loop over all cells we consider a kernel
that would parallelize a loop over all voxels the threads that
process the agents of a single voxel will need to reuse the
neighborhood data which can be stored in shared memory
for lowlatency memory fetches the concept is illustrated in
fig 7 all the state data belonging to the agents that are within
the highlighted region in fig 7 are stored in shared memory
the shared memory objects are built in parallel by appending
state data from agents of multiple voxels within the highlighted
region to avoid race conditions the use of atomic operations
is required in building the shared memory objects in parallel
v e xperimental s etup
the hardware on which the evaluations are done belong
to the cern it department and are tabulated in table i
the cpus of both systems consist of two physical sockets
organized in a nonuniform memory access numa design

table i specifications of the systems used for benchmarking
gpu chip

gpu ram

memory
bandwidth

singleprecision
performance

doubleprecision
performance

system a

nvidia gtx1080 ti

11gb

484 gbs

1134 tflops

0354 tflops

system b

nvidia tesla v100

32gb

900 gbs

157 tflops

78 tflops

baseline serial
baseline 20 threads
ugmethod serial
ugmethod 20 threads
gpu version 0
gpu version i
gpu version ii
gpu version iii

25817
8226
14497
1910
1039
527
199
274

102

103
104
runtime ms

105

cpu chip

cpu cores

intel xeon
e52640 v4
intel xeon
gold 6130

20 2 sockets
40 threads
32 2 sockets
64 threads

baseline serial 1
baseline 20 threads
3
ugmethod serial
2
ugmethod 20 threads
gpu version 0
gpu version i
gpu version ii
gpu version iii
100

cpu dram
256gb
187gb

14
25
49
130
94
101
102
speedup

103

fig 8 the runtime for various implementations of the mechanical interaction operation running benchmark a the gpu
results are obtained from the cuda runtime on system a

fig 9 the speedup with respect to the serial baseline version
as obtained with benchmark a the gpu results are obtained
from the cuda runtime on system a

to mitigate crossnuma effects on some of the benchmark
results we run those benchmarks on only one socket of the
numa domains in practice this was achieved by using the
linux utility tool taskset in section vi we explicitly
mention the benchmarks that were run on a single numa
domain the implementations and benchmarks can be found
on github2 
to profile the gpu kernel and the performance metrics
we made use of nvprof which is part of the cuda sdk
toolkit prior to recording the timing data for profiling gpu
benchmarks we run five iterations of the kernel to warm up
the gpu this measure is necessary for the following reasons
1 the gpu could initially be in a powersaving state and
therefore not perform optimally on the first run 2 justintime compilation of the kernel requires more time on the first
compilation 3 additional time could be taken for transferring
the kernel binary to gpu memory
to quantify the performance of our solutions we perform
three types of analyses first we run the cell division benchmark benchmark a that was introduced in section iii with
this benchmark we will quantify the performance of each
solution in section iv second we created a benchmark
benchmark b to analyze the performance among models
with different local neighborhood densities the cell division
benchmark has a fixed average number of neighboring agents
per agent and therefore only represents models with the same
neighborhood density with the second benchmark we vary
the average neighborhood density by spawning two million

agents on random positions in variablesized simulation space
consequently the average number of neighboring agents per
agent will be greater if the simulation space is smaller to
maintain a constant neighborhood density over the simulated
time we set the maximum displacement value of each agent
to zero the neighboring agents will stay locked in space
and therefore the neighborhood density will stay constant
the timing results of the benchmarks will exclude the model
initialization time creating the agents assigning behaviors
etc and focus on the simulation performance thirdly to
understand the performance limitations of the current gpu
implementation we perform a roofline analysis 20 on the
best performing gpu implementation through this analysis
we will understand how far the current implementation is from
the maximum attainable performance on system b we use the
empirical roofline tool ert 21 to measure the empirical
performance numbers of system b and to generate the roofline
analysis plot we retrieve the performance result in gflops
and the arithmetic intensity flopsbyte of the gpu kernel
with the use of nvprof

2 httpsgithubcomsenuihicomb

benchmarks

vi r esults
fig 8 shows the runtimes obtained from running benchmark
a for the various implementations of the mechanical interaction operation in biodynamo fig 9 shows the obtained
speedups comparison to the serial baseline version note that
the xaxis is scaled logarithmically in both figures the order
of the bar charts follows from the order in which the versions
were introduced in section iv consecutive gpu versions
include the implementation of the prior version so for example

intel xeon 6130
intel xeon 6130
intel xeon 6130
intel xeon 6130
intel xeon 6130
tesla v100

intel
intel
intel
intel
intel

250

speedup

runtime ms

106

300

4 threads
8 threads
16 threads
32 threads
64 threads

105

xeon
xeon
xeon
xeon
xeon

6130
6130
6130
6130
6130

4 threads
8 threads
16 threads
32 threads
64 threads

200

150

104
100
103

1

3

6

11

17

27

35

47

number of neighbors per agent

50

1

3

6

11

17

27

35

47

number of neighbors per agent

fig 10 the runtime of benchmark b for a varying neighborhood density the intel xeon entries represent the baseline
version the tesla v100 entries represent the best performing
gpu implementation the gpu results are obtained from the
cuda runtime on system b

fig 11 the speedups with respect to the baseline version for
various numbers of threads as obtained with benchmark b for
a varying neighborhood density the gpu results are obtained
from the cuda runtime on system b

gpu version ii includes the changes made for gpu version
i the results in fig 9 are obtained from running benchmark
a on system a
the serial uniform grid ug method performs twice as fast
as the serial kdtree method on all 20 cores of the system on
a single numa domain the ug method is 8226
1910  43 times
faster than the kdtree method this can be attributed to the
parallel construction of the uniform grid as opposed to the
serial construction of the kdtree
the initial version of the gpu implementation gpu
version 0 in fig 9 of the ug method already offers an
8226
1039  79 speedup as compared to the multithreaded
1910
baseline version and is 1039
 18 times faster than its
multithreaded cpu version even though the kernel is not yet
optimized due to the massively parallel architecture of the
gpu we are able to attain a significant speedup compared to
the multithread cpu version
from fig 9 we can see about a 1039
527  20 speedup gained
from reducing the data types that define a cells state from
doubles to floats from table i we can see that the fp32
throughput is 32 times greater than the fp64 throughput from
our speedup result it becomes clear that the current gpu
solution is limited by the memory bandwidth since fp32 data
types are 4 bytes and fp64 data types are 8 bytes the expected
speedup of a gpu application that is memory bound and
heavily relies on floatingpoint operations is two we verified
that the correctness of the simulations was not affected as a
result of reducing the floatingpoint precision by running the
unit tests and integration tests that are included in the testing
suite of biodynamo
sorting the agents state data based on a spacefilling curve

proved to reduce the execution time significantly namely
527
199  26 times in comparison to the previous gpu version
this speedup confirms that the gpu kernel enjoys more spatial
data locality when the agents state data is sorted as a result
memory accesses are more coalesced which in turn leads to
an increase in cache hits this reduces the overall latency of
obtaining the required neighborhood data from memory
redesigning the gpu kernel to utilize shared memory
resources appears to worsen the overall performance by 28
one of the reasons we found that causes the kernel performance to deteriorate is the introduction of atomic operations
in the kernel the use of atomics is necessary to build the
shared data structures that were introduced in section ive in
parallel however this causes stalling when multiple threads
try to update the same shared data object moreover the kernel
needs to perform boundary checks on the blocks cuda or
workgroups opencl that are being executed by the gpu
which gives rise to thread divergence
fig 10 and fig 11 summarize the results from running
benchmark b on system b the cpu results up to 32 threads
were obtained by running on a single numa domain on
system b fig 10 shows the runtimes of the multithreaded
baseline version 4 8 16 32 and 64 threads and the best
performing gpu version gpu version ii for a varying
number of neighboring agents per agent from the figure
it becomes clear that increasing the number of threads in
a cpuonly runtime only reduces the runtime marginally
whereas gpu coprocessing shows a significant reduction in
runtime fig 11 shows the speedup of the gpu runtime in
comparison to the multithreaded baseline version we observe
that the speedup in comparison to the baseline version running

vii c onclusion
the goal of this work was to perform a comparative study
of the acceleration potential of gpu coprocessing in biodynamo to enable fast simulation of largescale and complex
biological models to understand what the most effective
way is to improve the performance of simulations such that
largescale and complex models can be implemented we
profiled the simulations that biodynamo is currently capable
of running we discovered that the mechanical interactions
operation was the computational bottleneck by a large margin
due to the required data of local neighboring agents we
implemented a method alternative to the kdtree method the
uniform grid ug method which proved to be an excellent
candidate for exploiting the parallel architecture of gpus for
performance gain not only did the ug method outperform
the kdtree method on cpu but it opened up possibilities to
exploit the advantages that gpus offer the final gpu kernel

v100

fp32 147080 gflops

104

fp64 77193 gflops

performance gflopsec

with 4 threads lies between 160 to 232 depending on
the neighborhood density for the baseline version with 64
threads the speedup lies between 71 to 113 these results
imply that simulations that are densely populated enjoy a
speedup of up to two orders of magnitude when accelerating
their workload with a gpu simulations that would normally
take days on a multicore cpu can be completed in hours
on systems that feature a gpu the significant reduction in
simulation runtime allows researchers in the field of biological
abs to scale out their models and still obtain results rapidly
in fig 11 we notice that the gpu performance gain
stagnates or even decreases as the neighborhood density
increases the gpu kernel parallelizes the mechanical interaction computation for all agents but the loop over all neighboring agents is serial consequently this becomes the bottleneck
for models with a high neighborhood density we would like to
investigate this solution by exploring dynamic parallelism 22
in existing gpu programming models we hypothesize that
parallelizing the serial loop over the neighborhood alleviates
the bottleneck that is manifested in fig 11
from the roofline model analysis in fig 12 we see that
the best performing gpu implementation is still an order of
magnitude away from the maximum attainable singleprecision
floatingpoint performance on system b the data points are
however close to the roof that represents the upper bound of
the device memory bandwidth hbm which indicates that the
kernel is close to being memorybound future improvements
to the kernel must focus on alleviating the strain on data
transfer between the gpu and the gpu memory investigating
other caching methods to bypass the hbm bandwidth roofline
should be the main priority for future improvements we
observe that the kernel is able to attain higher performance
with a higher neighborhood density based on the percentage
of l2 cache reads relative to the number of total l2  hbm
memory reads as obtained by nvprof we believe this to
be the result of increased cache reuse of the neighborhood
state data per agent for n  47 this percentage is 413 for
n  27 it is 406 and for n  6 it is 394

103

7
m
hb

0
82

s
gb
n47
n27
n6
100

101
arithmetic intensity flopsbyte

102

fig 12 gpu roofline model analysis of various neighborhood
densities on system b where n is the number of neighbors
per agent

implementation resulted in speedups between 71 to 232 in
comparison to the multithreaded baseline version depending
on the number of neighboring agents per agent and the number
of threads the baseline is executed with this result enables
researchers of cellular agentbased models to rapidly obtain
biologically insightful simulations with biodynamo
f unding
this work was supported by the cern knowledge transfer
office to lb and cern openlab to ah
r eferences
1

c m macal and m j north agentbased modeling
and simulation in proceedings of the 2009 winter
simulation conference wsc ieee 2009 pp 8698
2 g an q mi j duttamoscato and y vodovotz
agentbased models in translational systems biology
wiley interdisciplinary reviews systems biology and
medicine vol 1 no 2 pp 159171 2009
3 b di ventura c lemerle k michalodimitrakis and
l serrano from in vivo to in silico biology and back
nature vol 443 no 7111 pp 527533 2006
4 g fiori f bonaccorso g iannaccone t palacios
d neumaier a seabaugh s k banerjee and l
colombo electronics based on twodimensional materials nature nanotechnology vol 9 no 10 pp 768
779 2014
5 g e moore et al cramming more components onto
integrated circuits 1965
6 l breitwieser a hesam j de montigny v
vavourakis a iosif j jennings m kaiser m manca
a d meglio z alars f rademakers o mutlu
and r bauer biodynamo a general platform for
scalable agentbased simulation 2021 arxiv 2006 
06775 csce

7

8

9

10

11
12

13

14

15

16

m lysenko and r m dsouza a framework for
megascale agent based model simulations on graphics
processing units journal of artificial societies and
social simulation vol 11 no 4 p 10 2008 issn
14607425 online available httpjassssocsurrey
acuk11410html
p richmond d walker s coakley and d romano
high performance cellular level agentbased simulation with flame for the gpu en briefings in
bioinformatics vol 11 no 3 pp 334347 may 2010
publisher oxford academic issn 14675463 doi 10
1093bibbbp073 online available httpsacademic
oup  com  bib  article  11  3  334  225993 visited on
10082020
s ren k bertels and z alars efficient acceleration of the pairhmms forward algorithm for gatk haplotypecaller on graphics processing units evolutionary
bioinformatics vol 14 p 1 176 934 318 760 543 2018
pmid 29568218 doi 10  1177  1176934318760543
eprint https    doi  org  10  1177  1176934318760543
online available https    doi  org  10  1177 
1176934318760543
g smaragdos g chatzikonstantis r kukreja h
sidiropoulos d rodopoulos i sourdis z alars
c kachris d soudris c i d zeeuw and c strydis
brainframe a nodelevel heterogeneous accelerator
platform for neuron simulations journal of neural
engineering vol 14 no 6 p 066 008 nov 2017 doi
10108817412552aa7fc5 online available https
doiorg10108817412552aa7fc5
j nickolls and w j dally the gpu computing era
ieee micro vol 30 no 2 pp 5669 2010
p van liedekerke m palm n jagiella and d drasdo
simulating tissue mechanics with agentbased models
concepts perspectives and some novel results computational particle mechanics vol 2 no 4 pp 401444
2015
s kang s kahan j mcdermott n flann and i
shmulevich biocellion accelerating computer simulation of multicellular biological system models bioinformatics vol 30 no 21 pp 31013108 2014
a ghaffarizadeh r heiland s h friedman s m
mumenthaler and p macklin physicell an open
source physicsbased cell simulator for 3d multicellular
systems plos computational biology vol 14 no 2
e1005991 2018
m cytowski and z szymanska largescale parallel
simulations of 3d cell colony dynamics computing in
science  engineering vol 16 no 5 pp 8695 2014
g r mirams c j arthurs m o bernabeu r
bordas j cooper a corrias y davit sj dunn
a g fletcher d g harvey et al chaste an open
source c library for computational physiology and
biology plos computational biology vol 9 no 3
e1002970 2013

17

a hauri selfconstruction in the context of cortical
growth from one cell to a cortex to a programming
paradigm for selfconstructing systems phd dissertation eth 2013
18 wikimedia commons filefourlevel zsvg  wikimedia commons the free media repository online
accessed 20august2018 2018
19 g m morton a computer oriented geodetic data base
and a new technique in file sequencing 1966
20 s williams a waterman and d patterson roofline
an insightful visual performance model for multicore
architectures communications of the acm vol 52
no 4 pp 6576 2009
21 c yang r gayatri t kurth p basu z ronaghi
a adetokunbo b friesen b cook d doerfler
l oliker et al an empirical roofline methodology
for quantitatively assessing performance portability in
2018 ieeeacm international workshop on performance portability and productivity in hpc p3hpc
ieee 2018 pp 1423
22 s jones introduction to dynamic parallelism in gpu
technology conference presentation s vol 338 2012
p 2012

