S UBMODULAR M UTUAL I NFORMATION FOR TARGETED DATA
S UBSET S ELECTION
A P REPRINT

arXiv:2105.00043v1 [cs.LG] 30 Apr 2021

Suraj Kothawade ∗
suraj.kothawade@utdallas.edu
Jeff Bilmes ‡
bilmes@uw.edu

Vishal Kaushal †
vkaushal@cse.iitb.ac.in

Ganesh Ramakrishnan †
ganesh@cse.iitb.ac.in

Rishabh Iyer ∗
rishabh.iyer@utdallas.edu

May 4, 2021

A BSTRACT
With the rapid growth of data, it is becoming increasingly difficult to train or improve deep learning
models with the right subset of data. We show that this problem can be effectively solved at
an additional labeling cost by targeted data subset selection (TSS) where a subset of unlabeled
data points similar to an auxiliary set are added to the training data. We do so by using a rich
class of Submodular Mutual Information (SMI) functions and demonstrate its effectiveness for
image classification on CIFAR-10 and MNIST datasets. Lastly, we compare the performance of
SMI functions for TSS with other state-of-the-art methods for closely related problems like active
learning. Using SMI functions, we observe ≈ 20-30% gain over the model's performance before
re-training with added targeted subset; ≈ 12% more than other methods.

1

Introduction

Recent times have seen unprecedented growth in data across modalities such as text, images and videos. This has
naturally given rise to techniques for finding effective smaller subsets of the data for a variety of end-tasks. An example
of this is data subset selection for efficient and/or cost-effective training of machine learning models, wherein we need
to select samples which are most informative for training a model. Training on such smaller subsets of data often
entails significant speedups and reduction in labeling time/cost without sacrificing much on accuracy [Killamsetty et al.,
2021, Kaushal et al., 2019, Wei et al., 2015]. Another flavor of this is targeted data subset selection which focuses on
improving an existing model which is performing poorly is specific cases or improving a dataset which is imbalanced in
certain attributes. Quite often, in these end-tasks, we want to be able to select subsets that align well with a certain
target set.
1.1

Targeted Data Subset Selection

In real-world settings, the training data is often biased. Examples of such biases include distribution shift, imbalance
in classes, presence of rare classes or rare slices, and out of distribution examples in the unlabeled dataset. In such
cases a model's performance can be improved (at a given additional labeling cost) by augmenting the training data
with some most informative samples matching the target distribution (hence called targeted subset) from a large pool
of unlabeled data. One way of achieving this is by assuming access to a clean validation set matching the target set
distribution and using it as a target. Another example is where the target set is a critical slice of the data (e.g., indoor
images of people in the dark or images from specific classes that the user might care about) and we want to improve the
model's performance on the target without sacrificing the overall accuracy and with minimum additional labeling costs
∗

Department of Computer Science, University of Texas at Dallas
Department of Computer Science and Engineering, Indian Institute of Technology Bombay
‡
Department of Electrical & Computer Engineering, University of Washington, Seattle
†

A PREPRINT - M AY 4, 2021

(Fig. 1). Yet another case of this is where the user is aware of a certain rare slice or class in the dataset, and has a few
example images of this (say, either from the labeled set or from a held-out test set) of this rare slice. In this paper, we
will study the problem of targeted subset selection to sample unlabeled data points.

2

Preliminaries

Submodular Functions: We let V denote the groundset of n data points V = {1, 2, 3, ..., n} and a set function
f : 2V →
− <. The function f is submodular [Fujishige,
2005] if it satisfies the diminishing marginal returns,
namely f (j|X ) ≥ f (j|Y) for all X ⊆ Y ⊆ V, j ∈
/ Y.
Facility location, set cover, log determinants, etc. are
some examples [Iyer, 2015]. Due to close connections
between submodularity and entropy, submodular functions can also be viewed as information functions [Zhang
and Yeung, 1998]. Submodularity ensures that a greedy
algorithm achieves bounded approximation factor when
maximized [Nemhauser et al., 1978].

Labelled Training Data
Augment with T

Augmented
Training Data

Targets

Retrain Model

Targeted
Data Subset
Selection
Targeted Subset T

Submodular Mutual Information (MI): Given a set of
items A, B ⊆ V, the submodular mutual information
(MI) [Gupta and Levin, 2020, Iyer et al., 2020] is defined
as If (A; B) = f (A) + f (B) − f (A ∪ B). Intuitively, this
measures the similarity between B and A and we refer to
B as the query set.

Unlabelled dataset

Figure 1: Motivating example for targeted data subset selection (TSS): the night images (target) are under-represented
in training data. TSS mines for night images and augments
the training data to improve the performance of the final
model.
Kaushal et al. [2020] extend MI to handle the case when the target can come from an auxiliary set V 0 different from the
ground set V. For targeted data subset selection, V is the source set of data instances and the target is a subset of data
points (validation set or the specific set of examples of interest). Let Ω = V ∪ V 0 . We define a set function f : 2Ω → <.
Although f is defined on Ω, the discrete optimization problem will only be defined on subsets A ⊆ V. To find an
optimal subset given a query set Q ⊆ V 0 , we can define gQ (A) = If (A; Q), A ⊆ V and maximize the same.
2.1

Examples of SMI functions

We use the MI functions recently introduced in Iyer et al. [2020], Gupta and Levin [2020] and their extensions introduced
in Kaushal et al. [2020]. For any two data points i ∈ V and j ∈ Q, let sij denote the similarity between them.
Graph Cut MI:PThe P
submodular mutual information (SMI) instantiation of graph-cut (GCMI) is defined as:
IGC (A; Q) = 2 i∈A j∈Q sij . Since maximizing GCMI maximizes the joint pairwise sum with the query set, it
will lead to a summary similar to the query set Q. In fact, specific instantiations of GCMI have been intuitively used for
query-focused summarization for videos Vasudevan et al. [2017] and documents Lin [2012], Li et al. [2012].
Facility Location MI
P - V1: In the first variant of FL, we set D to be V . The SMI instantiation of FL1MI can be defined
as: IF L1 (A; Q) = i∈V min(maxj∈A sij , η maxj∈Q sij ). The first term in the min(.) of FL1MI models diversity,
and the second term models query relevance. An increase in the value of η causes the resulting summary to become
more relevant to the query.
Facility Location MI
P - V2: In the V2 variant,
P we set D to be V ∪ Q. The SMI instantiation of FL2MI can be defined
as: IF L2 (A; Q) = i∈Q maxj∈A sij + η i∈A maxj∈Q sij . FL2MI is very intuitive for query relevance as well. It
measures the representation of data points that are the most relevant to the query set and vice versa. It can also be
thought of as a bidirectional representation score.
Log Determinant MI: The SMI instantiation of LogDetMI can be defined as: ILogDet (A; Q) = log det(SA ) −
−1 T
log det(SA − η 2 SA,Q SQ
SA,Q ). SA,Q denotes the cross-similarity matrix between the items in sets A and Q. The
similarity matrix in constructed in such a way that the cross-similarity between A and Q is multiplied by η to control
the trade-off between query-relevance and diversity.

3

A Framework for Targeted Data Subset Selection

We apply SMI functions to the setting of targeted data subset selection for improving a model's accuracy on some
target classes/instances at a given additional labeling cost (k instances) without compromising on the overall accuracy.
Let E be an initial training set of labeled instances and T be the set of examples that the user cares about and desires
2

A PREPRINT - M AY 4, 2021

better performance on. Let U be a large unlabeled dataset. Using appropriate feature representation of the instances,
we compute kernels of similarities of elements within U, within T and between U and T to instantiate a MI function
If (A; T ) and maximize it to compute an optimal subset Â ⊆ U of size k given T as target (query) set. We then
augment E with labeled Â (i.e. L(Â)) and re-train the model to achieve the desired improvement. Through instantiating
a rich class of MI functions including GCMI, FL1MI, FL2MI, COM and LogDetMI, TSS offers a rich treatment to
targeted subset selection. Our framework allows for adding an explicit diversity term γg(A) where γ is the weight and
g is a set function modeling diversity (for eg. total pairwise distance). This is helpful in cases when If itself does not
model diversity (for eg. GCMI). The algorithm is summarized in Algorithm 1. Following Ash et al. [2020], Killamsetty
et al. [2021] we use gradients as feature representation to compute the similarity kernels. The gradients are computed
using model's inference for U and T and similarity is computed using cosine similarity.
Algorithm 1 TSS
Require: Initial Labeled set of Examples: E, large unlabeled dataset: U, A target subset/slice where we want to
improve accuracy: T , Loss function L for learning
1: Train model with loss L on labeled set E and obtain parameters θE
2: Compute the gradients {∇θE L(xi , yi ), i ∈ U} and {∇θE L(xi , yi ), i ∈ T }.
3: Using the gradients, compute the similarity kernels and define a submodular function f and diversity function g
4: Â ← maxA⊆U ,|A|≤k If (A; T ) + γg(A)
5: Obtain the labels of the elements in A∗ : L(Â)
6: Train a model on the combined labeled set E ∪ L(Â)

4

Effectiveness of SMI for TSS

Dataset, Baselines and Implementation details: We demonstrate the effectiveness of TSS in obtaining a targeted
subset for improving image classification accuracy for some target classes on CIFAR-10 and MNIST datasets. To
simulate a real-world setting, we split the available train set into train, validate and a data lake such that (i) the train set
has few labeled instances and poorly represents two randomly picked classes (target), and (ii) data lake is a large set
whose labels we do not use (resembling a large pool of unlabeled data in real-world). The poorly represented classes do
not perform well on the validation set and hold clue to picking up the target of interest. Performance is measured on the
test set from the respective datasets. We then apply TSS (Algorithm 1) comparing MI functions with other existing
approaches. Specifically, for MI functions we use LogDetMI, GCMI, FL1MI, FL2MI, and GCMI + Diversity (equivalent
to an intuitive approach of minimizing average gradient difference with the target) For existing approaches, we compare
with three active learning baselines (uncertainty sampling (US), BADGE, and G LISTER -ACTIVE (GLISTER)) running
them only once as per our setting (i.e. we select the unlabeled subset only once). Since these active learning baselines
do not explicitly have information of the target set, to further strengthen them we also compare against two variants
which are target-aware. The first is 'targeted uncertainty sampling' (TUS) where a product of the uncertainty and the
similarity with the target is used to identify the subset, and second is G LISTER -TSS where the target set is used in the
bi-level optimization. Finally, we also compare with pure diversity/representation functions (Facility Location (FL),
Graph Cut (GC), Log Determinant (LogDet), Disparity-Sum (DSUM)) and random sampling. We train the model
(ResNet-18 [He et al., 2016] for CIFAR-10, LeNet [LeCun et al., 1989] for MNIST) using cross-entropy loss and SGD
optimizer until training accuracy exceeds 99% (Base model). After augmenting the train set with the labeled version of
the selected subset and re-training the model, we report the average gain in accuracy for the target classes and overall
gain in accuracy across all classes on test set, averaged across 10 runs of randomly picking any two classes as target.
We run TSS for different budgets and also study the effect of budget on the performance. Wherever applicable, we
keep the internal parameters at their default values of 1.
Results: In Table 1, we report the results for a budget of 400 for CIFAR-10 and 70 for MNIST. To keep the setting
as realistic as possible, we set the target set to be much smaller than the budget (around 10% of the budget – 10 for
CIFAR-10 and 6 for MNIST). We report the effect of budget on the gain in accuracy of the target classes in Fig. 2.
On both datasets, MI functions yield the best improvement in accuracy on the target classes (≈ 20-30% gain over
the model's performance before re-training with added targeted subset; ≈ 12% more than other methods) while also
simultaneously increasing the overall accuracy by ≈ 2-6%. They consistently outperform BADGE, G LISTER -TSS, US
and TUS across all budgets. Since the SMI functions (LogDetMI, Fl2MI and GCMI+DIV) model both query-relevance
and diversity, they perform better than both a) functions which tend to prefer relevance (GCMI, TUS) and b) functions
which tend to prefer diversity/representation (BADGE, FL, GC, DSUM, LogDet). Also, we observe that across different
budgets, the MI functions outperform other methods by greater margins on the target class accuracy (Fig. 2). This is
expected, as other methods are not effective in considering the target.
3

A PREPRINT - M AY 4, 2021

Figure 2: Comparison of different methods for targeted subset selection for different budgets on CIFAR-10 and MNIST.
X-axis: budgets, Y-axis: gain in model accuracy for target classes on test set. MI based approaches (lines in red)
significantly outperform others across all subset sizes. (Section 4).
Method

CIFAR-10
MNIST
Target
Overall Target
Overall
Base
11.2
42.2
52.76
86.8
Random
+2.75
+1.43
+1.08
-0.032
BADGE [Ash et al., 2020]
+7.245
+2.38
+6.7 +1.659
GLISTER [Killamsetty et al., 2021]
+12.1
+2.27
+14.56
+2.27
GLISTER-TSS
+16.5
+1.78 +22.895
+4.05
US [Settles, 2009]
+3.95
+2.03
+7.56 +1.182
TUS
+10.45
+2.99
+6.21 +1.611
LogDet
+11.85
+1.2
+13.29
+1.89
FL
+15.3
+2.63 +15.025
+2.41
GC
+15.9
+1.79 +10.935
+1.16
DSUM
+10.65
+1.9 +20.515
+3.92
LogDetMI
+26.5
+2.21 +28.035
+5.26
FL2MI
+20.2
+1.7
+34.36
+5.14
FL1MI
+17.1
+2.28
+21.21
+3.83
GCMI
+17.6
+1.48 +29.375
+5.21
GCMI+DIV
+18.2
+3.74
+31.28
+4.21
Table 1: Comparison of TSS (MI functions) with other methods for a budget of 400 (CIFAR-10) and 70 (MNIST). The
numbers are the gain in % accuracy of the target classes (Target) and all classes (Overall) over the Base model after
re-training the model (see text). Highest in blue, 2nd and 3rd highest in red and green respectively.

5

Conclusion

We demonstrate the effectiveness of SMI functions for improving a model's performance by augmenting the training
data with samples that match a target distribution (targeted data subset selection). Through experiments on CIFAR-10
and MNIST datasets, we empirically verify the superiority of SMI functions over existing methods.
References
Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch active learning
by diverse, uncertain gradient lower bounds. In ICLR, 2020.
4

A PREPRINT - M AY 4, 2021

Satoru Fujishige. Submodular functions and optimization. Elsevier, 2005.
Anupam Gupta and Roie Levin. The online submodular cover problem. In ACM-SIAM Symposium on Discrete
Algorithms, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
Rishabh Iyer, Ninad Khargoankar, Jeff Bilmes, and Himanshu Asnani. Submodular combinatorial information measures
with applications in machine learning. arXiv preprint arXiv:2006.15412, 2020.
Rishabh Krishnan Iyer. Submodular optimization and machine learning: Theoretical results, unifying and scalable
algorithms, and applications. PhD thesis, 2015.
Vishal Kaushal, Rishabh Iyer, Suraj Kothawade, Rohan Mahadev, Khoshrav Doctor, and Ganesh Ramakrishnan.
Learning from less data: A unified data subset selection and active learning framework for computer vision. In 2019
IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1289–1299. IEEE, 2019.
Vishal Kaushal, Suraj Kothawade, Ganesh Ramakrishnan, Jeff Bilmes, Himanshu Asnani, and Rishabh Iyer. A unified
framework for generic, query-focused, privacy preserving and update summarization using submodular information
measures. arXiv preprint arXiv:2010.05631, 2020.
Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, and Rishabh Iyer. Glister: Generalization
based data subset selection for efficient and robust learning. In AAAI, 2021.
Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D
Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541–551, 1989.
Jingxuan Li, Lei Li, and Tao Li. Multi-document summarization via submodularity. Applied Intelligence, 37(3):
420–430, 2012.
Hui Lin. Submodularity in natural language processing: algorithms and applications. PhD thesis, 2012.
George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approximations for maximizing
submodular set functions-i. Mathematical programming, 14(1):265–294, 1978.
Burr Settles. Active learning literature survey. Technical report, University of Wisconsin-Madison Department of
Computer Sciences, 2009.
Arun Balajee Vasudevan, Michael Gygli, Anna Volokitin, and Luc Van Gool. Query-adaptive video summarization via
quality-aware relevance estimation. In Proceedings of the 25th ACM international conference on Multimedia, pages
582–590, 2017.
Kai Wei, Rishabh Iyer, and Jeff Bilmes. Submodularity in data subset selection and active learning. In International
Conference on Machine Learning, pages 1954–1963. PMLR, 2015.
Zhen Zhang and Raymond W Yeung. On characterization of entropy function via information inequalities. Information
Theory, IEEE Transactions on, 44(4):1440–1452, 1998.

5

