Most Expected Winner: An Interpretation of Winners over
Uncertain Voter Preferences

arXiv:2105.00082v1 [cs.GT] 30 Apr 2021

Haoyue Ping1 and Julia Stoyanovich1
1
New York University, USA
{hp1326, stoyanovich}@nyu.edu

Abstract
It remains an open question how to determine the
winner of an election given incomplete or uncertain
voter preferences. One solution is to assume some
probability space for the voting profile and declare
the candidates having the best chance of winning
to be the (co-)winners. We refer to this as the Most
Probable Winner (MPW). In this paper, we propose an alternative winner interpretation for positional scoring rules - the Most Expected Winner
(MEW), based on the expected performance of the
candidates. This winner interpretation enjoys some
desirable properties that the MPW does not. We
establish the theoretical hardness of MEW over incomplete voter preferences, then identify a collection of tractable cases for a variety of voting profiles. An important contribution of this work is to
separate the voter preferences into the generation
step and the observation step, which gives rise to
a unified voting profile combining both incomplete
and probabilistic voting profiles.

1

Introduction

Voting is a mechanism to determine winners among the candidates in an election by aggregating the preferences of voters. In classical voting theory, each voter gives a complete
preference (most frequently, a ranking) of all candidates.
How voter preferences are aggregated is determined by a voting rule. A prominent class of voting rules, which assign
scores to candidates based on their positions in the ranking
of each voter and then sum up the scores for each candidate,
are positional scoring rules, on which we focus in this paper.
In practice, voter preferences may well be incomplete and
represented by partial orders. Since voting rules are defined
over (complete) rankings, the solution is to replace each partial order with all of its linear extensions, each of which is
a completion or a possible world1 of the partial order. The
preferences of all voters are referred to as a voting profile.
A voting profile of complete rankings is a complete voting
profile, while that of incomplete preferences is an incomplete
1

We use completion and possible world interchangeably.

voting profile. Voters are assumed to cast their preferences independently. A completion of an incomplete voting profile is
a complete voting profile obtained by replacing each voter's
partial order with one of its completions.
There have been various interpretations of winners proposed for this setting. The most thoroughly studied are the
necessary and possible winners [Konczak and Lang, 2005].
A candidate is an necessary winner (NW) if she wins in every
possible world; she is a possible winner (PW) if she wins in at
least one possible world. The NW and PW semantics are simple but they have shortcomings that make them impractical:
The requirement for NW is so strict that there are often no
winners available in a voting profile under this interpretation,
while the requirement for PW does not differentiate between
a candidate who only wins in one possible world and another
candidate who only loses in one possible world.
[Bachrach et al., 2010] assume that an incomplete voting
profile of partial orders represents a uniform distribution over
its completions, and prefer the candidates who enjoy victory
in more possible worlds. We refer to this winner semantics as
Most Probable Winner (MPW). While this semantics is well
defined under any voting rule, and while it can be extended
in a straight-forward way to incorporate the probability of a
completion of a voting profile into the computation, computing a winner under MPW is known to be intractable already
under plurality [Bachrach et al., 2010].
[Hazon et al., 2012] also investigates MPW but under a different setting, where voter preferences are specified explicitly
by rankings and their associated probabilities. They proved
that it is #P-hard to compute the candidate winning probabilities under plurality, k-approval, Borda, Copeland, and Bucklin, and provided an approximation algorithm.
In this paper, we propose the Most Expected Winner
(MEW) as an alternative winner interpretation for incomplete
voter preferences under positional scoring rules. Like MPW,
it adopts the possible world semantics of incomplete voting
profiles. However, in contrast to MPW that determines a winner by a (weighted) count of the possible worlds in which she
wins, MEW follows the principle of score-based rules that
high-scoring candidates should be favored. Specifically, an
MEW is the candidate who has the highest expected score in
a random possible world.
MEW and MPW are similar in that they both aggregate
election results over all possible worlds and give a balanced

Voter

τ1 =ha, b, ci

τ2 =hb, a, ci

τ3 =hb, c, ai

τ4 =hc, b, ai

Voting rule

rm

x
y

0.7
0

0.3
0

0
0.5

0
0.5

plurality
veto
2-approval
Borda

(1, 0, . . . , 0, 0)
(1, 1, . . . , 1, 0)
(1, 1, 0, . . . , 0, 0)
(m − 1, m − 2, . . . , 1, 0)

Table 1: A probabilistic voting profile for voters x and y. Each
voter casts her vote independently of the other, leading to 4 possible worlds, listed with their probabilities: Pr(τ1 , τ3 )=0.35,
Pr(τ1 , τ4 )=0.35, Pr(τ2 , τ3 )=0.15, and Pr(τ2 , τ4 )=0.15. Under
the plurality rule, candidate a is the MPW with a winning probability of Pr(τ1 , τ3 )+ Pr(τ1 , τ4 )=0.7, while candidate b is the MEW
with expected score Pr(τ2 | x)+ Pr(τ3 | y)=0.3+0.5=0.8.

evaluation of the candidates. Their difference lays in the aggregation methods, which will be discussed in detail in Section 3.2. In practice, MEW and MPW yield the same result often, but not always. Table 1 gives an example where
MEW and MPW select different winners in an election with
two voters and three candidates, under the plurality rule. In
this election, each voter produces a full ranking drawn from
a probability distribution over τ1 = ha, b, ci, τ2 = hb, a, ci,
τ3 = hb, c, ai, τ4 = hc, b, ai, with probabilities of each ranking for each voter given in Table 1. Since the voter preferences are probabilistic distributions of rankings, the corresponding voting profile is named a probabilistic voting profile. Let Pr(τx , τy ) denote the probability that voter x and y
cast ranking τx and τy . Assume there is no correlation between voters x and y, then Pr(τx , τy ) = Pr(τx | x) * Pr(τy |
y), e.g., Pr(τ1 , τ3 ) = 0.7 * 0.5 = 0.35. This voting profile
effectively generates 4 possible worlds as in the caption of
Table 1. The plurality rule rewards a candidate with 1 point
every time she is ranked at the top of a ranking in the profile.
So in the possible world of Pr(τ1 , τ3 ), candidates a obtains 1
point from τ1 and b obtains 1 point from τ3 , and both of them
become (co-)winners in this possible world. After enumerating all 4 possible worlds, we find that candidate a is the MPW
with probability 0.7 to be a (co-)winner, while candidate b is
the MEW with expected score of 0.8 point.
By the way, if applying the Borda rule that rewards a candidate with the number of candidates ranked after her in a
ranking every time, in the possible world of Pr(τ1 , τ3 ), candidate a would obtain 2 points from τ1 , while candidate b
would obtain 1 point from τ1 and 2 points from τ3 . Candidate b would be the only NW of this profile and she would
also be the MEW with an expected score of 2.8 points.
The main technical contribution of this paper is an investigation of computational complexity of MEW. (The problem
statement is given formally in Section 3.3.) Another contribution is the modeling of uncertain voter preference for
two distinct sources. The first is uncertainty in the preferences themselves: a voter may not feel strongly about the
relative ordering of some pair of candidates, or, more generally, their preference may be drawn from some probability
distribution [Marden, 1995]. We refer to this as uncertainty
in preference generation. The second is uncertainty in the
observation: a voting mechanism (e.g., approval ballots or
a ranking of at most k < m candidates) may not allow the
voters to fully reveal their preferences. With this understanding, in Section 2 we will classify voting profiles into prob-

Table 2: Examples of positional scoring rules.

abilistic profiles (uncertainty during generation), incomplete
profiles (uncertainty during observation), and combined profiles. This classification gives us a framework within which
to study the complexity of identifying MEW by computing
expected scores of the candidates. Recall that FP#P is a class
of functions efficiently solvable with an oracle to some #P
problem. A function f is FP#P -hard if there is a polynomialtime Turing reduction from any FP#P function to f . While
it turns out that the MEW problem is FP#P -complete under
plurality, veto, and k-approval rules for the general case of
uncertain profiles (Section 4), we will also identify interesting cases where it is tractable to compute the expected scores
and determine the MEW (Section 5).

2

Uncertain Voting Profiles

We now propose a novel framework for representing uncertainty in voter preferences and introduce a unified voting profile that explicitly models different types of uncertainty that
can arise in preference generation and elicitation.
Voting preliminaries. Let us denote by C = {c1 , . . . , cm }
a set of candidates or items2 , by V = {v1 , . . . , vn } a set of
voters, and by P = (τ1 , . . . , τn ) a complete voting profile
where τi is a ranking over C by voter vi . Ranking τ is a bijection between candidates and ranks where τ (i) is the candidate at rank i and τ −1 (c) is the rank of candidate c in τ .
Positional scoring rules are arguably the most thoroughly
studied voting rules. Let rm = (rm (1), ..., rm (m)) denote
a positional scoring rule where ∀1 ≤ i < j ≤ m, rm (i) ≥
rm (j) and rm (1) > rm (m). It assigns a score rm (i) to the
candidate at rank i. Table 2 lists several popular positional
scoring rules.
The performance of a candidate c is the sumPof her scores
across the entire voting profile: s(c, P ) =
τ ∈P s(c, τ )
where s(c, τ ) = rm (τ −1 (c)). Candidate w is a (co-)winner
if her score is no less than the score of any other candidate.
Uncertainty in voter preferences. In classical voting theory, voters give complete rankings over candidates. However,
in practice only partial preferences may be observed, due to
the voting mechanism (e.g., when approval ballots or a ranking of at most k < m candidates are elicited), the uncertainty
in preferences themselves [Marden, 1995], or both. Figure 1
represents uncertainty as two distinct steps: preference generation (Figure 1a) and preference observation (Figure 1b).
Important special cases of voting profiles are discussed next.
2
Candidates are used in the context of voting, while items are for
general preference analysis. In this paper, they are interchangeable.

Uncertainty in profile generation. The most general form
of voter preferences over rankings is a non-parametric probability distributions such as that given in Table 1. Let P M =
(M1 , . . . , Mn ) denote a probabilistic voting profile where Mi
is the ranking model of voter vi . A possible world of P M is
a complete voting profile P = (τ1 , . . . , τn ) where each τi is
sampled from Mi . It is assumed that the voters
Qn cast their ballots independently, i.e., Pr(P | P M ) = i=1 Pr(τi | Mi ).
So P M represents a probability distribution of its possible
worlds Ω(P M ) = {P1 , . . . , Pz }.
Let s(c, M) and s(c, P M ) denote the scores assigned to
candidate c by model M and the probabilistic voting profile
P M , respectively. Note that they both are random variables.
Partial voting profiles are a special cases of probabilistic
voting profiles, since they are based on the assumption that all
completions are equally likely. Below are a few other cases.
A uniform voting profile, denoted by P U , is a trivial case
where no voter has any preference between any two candidates. Each voter's preferences are sampled from the uniform
distribution over the rankings of all candidates. We list this
special case for convenience of discussion.
There are specific important ranking models that we will
describe in Section 5. The Mallows model [Mallows, 1957]
is the best known, and it is further generalized by the
Repeated Insertion Model (RIM) [Doignon et al., 2004]
and the ranking version of the Repeated Selection Model
(RSM) [Chakraborty et al., 2020]. Voting profiles consisting
of Mallows, RIMs, and RSMs (ranking version) are denoted
by P MAL , P RIM , and P rRSM , respectively. We will see that
when the generation step is based on these models, we have a
computational advantage for computing the MEW.
Uncertainty in profile observation. A partial voting profile, denoted by P PO , consists of partial orders and represents
a uniform distribution of its completions. What follows are
its special cases.
A partitioned voting profile, denoted by P PP , records preferences in the form of partitions: linear orders of item buckets, with no preference among the items in the same bucket.
A partial chain voting profile, denoted by P PC , records
preferences in the form of partial chains. A partial chain is
a linear order of a subset of items, and there is no preference
specified with regard to the remaining items.
A truncated voting profile, denoted by P TR , records preferences in the form of truncated rankings. Let τ (t,b) denote
a truncated ranking with t items at the top and b items at the
bottom. No preference information is specified for the middle part of the ranking. The τ (t,b) is a special case of the
partitioned preferences, with t + b + 1 partitions.
Combined voting profiles. Most research works have assumed that a partial voting profile represents a uniform distribution over its completions. However, the assumption that all
completions are equally likely may not hold in practice. We
propose combined voting profiles P M+P , where each voter
is associated with both her original incomplete preferences
P and a ranking model M. The ranking model M is her
prior preference distribution, e.g., obtained from historical
data. But after observing new evidence P , the probabilities of

PPO
(partial orders)

PM
(ranking models)

PPP
(partitioned preferences)

PRIM
PMAL

PU

PTR

PPC
(partial chains)

(truncated rankings)

P rRSM

(a) Generation step

(b) Observation step

Figure 1: The Venn diagrams of voting profiles. The voter preferences in their mind are the ranking models in the generation step.
Then we observe incomplete voting profiles that are essentially samples of the probabilistic voting profiles.

rankings that violate P collapse to zero, while the rest rankings that satisfy P have the same relative probabilities among
each other. Formally speaking, the preferences of this voter
become the posterior distribution of M conditioned on P .
In another sense, the combined voting profiles are also the
most general form of voting profiles (same as P M ) that unify
all voting profiles so far. For example, a partitioned voting
profile P PP is essentially P U+PP , and a RIM voting profile
P RIM is essentially P RIM+∅ where ∅ means that the partial
orders are the empty ones.
Among the tractable cases in Section 5, there are also combined voting profiles, which means that applying combined
voting profiles not only gains theoretical benefits of more customized ranking distributions, but also becomes practical for
certain combination of profiles.

3

Most Expected Winner

Given a general voting profile P M and a positional scoring
rule rm , the performance of a candidate can be quantified by
the expectation of her score in a random possible world.
Definition 1 (MEW). Given a voting profile P M and a positional scoring rule rm , candidate w is a Most Expected Winner, if and only if, E(s(w, P M )) = maxc∈C E(s(c, P M )).
Let MEW(rm , P M ) be the set of Most Expected Winners.

3.1

Alternative Interpretations

To gain an intuition for Most Expected Winner (MEW), we
will now give two winner definitions that are equivalent to
MEW. (Note that the proofs of all theorems in this section
are presented in the Appendix.)
Least Expected Regret Winner
The MEW can also be regarded as the candidate who minimizes the expected regret in a random possible world. The
concept of regret is borrowed from [Lu and Boutilier, 2011].
Let Regret(w, P ) denote the regret value of choosing w ∈
C as the winner given a complete voting profile P .
Regret(w, P ) = max s(c, P ) − s(w, P )
c∈C

Accordingly, the regret value Regret(w, P M ) over a probabilistic voting profile becomes a random variable.
E(Regret(w, P M )) =

X

Regret(c, P ) * Pr(P | P M )

P ∈Ω(P M )

Candidate w is a Least Expected Regret Winner, if and only
if, she minimizes the expected regret E(Regret(w, P M )).
Theorem 1. The MEW and the Least Expected Regret Winner
are equivalent.
Meta-Election Winner
Recall that a voting profile P M represents a probability distribution of possible worlds Ω(P M ) = {P1 , . . . , Pz }. The
Meta-Election Winner is defined as the candidate who wins a
meta election with a large meta profile PM = (P1 , . . . , Pz )
where rankings in Pi are weighted by Pr(Pi | P M ).
Theorem 2. The MEW and the Meta-Election Winner are
equivalent.

3.2

Comparing MEW and MPW

The difference between MEW and MPW can be interpreted
as different aggregation approaches across possible worlds.
Recall that Ω(P M ) = {P1 , . . . , Pz } is the set of possible
worlds of P M , each possible worldP
Pi is associated with a
z
probability pi = Pr(Pi | P M ) and i=1 pi = 1. Now let's
see how the performance of a candidate c is aggregated across
possible worlds. Let 1() be the indicator function.
Pz
• MEW: E(s(c, P M )) = i=1 s(c, Pi ) * pi
Pz
• MPW: Pr(c wins) = i=1 1(c wins | Pi ) * pi
MEW estimates the average performance of a candidate,
while MPW estimates the probability that she wins. As a
result, MPW ignores the possible worlds if she cannot win.

3.3

Problem Statement

The MEW is determined based on the expected performance
of the candidates. So the winner determination problem of
MEW can be reduced to the problem of calculating the expected scores of candidates. That is, given a voting profile
P M , a positional scoring rule rm , and a candidate c ∈ C,
calculate E(s(c, P M )) the expected score of c. We name this
problem Expected Score Computation (ESC).
Theorem 3. The problem of determining MEW can be reduced to the ESC problem.

4

Hardness of ESC

This section investigates the complexity of the ESC problem.
We first prove the hardness of two closely related problems,
the Fixed-rank Counting Problem and the Rank Estimation
Problem. Then we demonstrate that the ESC problem is hard
as well. The proofs to the theorems and lemmas in this section
are presented in the Appendix.

4.1

Fixed-rank Counting Problem

Counting the number of linear extensions of a partial order
is well-known to be #P-complete [Brightwell and Winkler,
1991]. The Fixed-rank Counting Problem (FCP) is interested
in the number of linear extensions where an item is placed at
a specific rank. Let Ω(ν) denote the set of linear extensions
of ν, and N (c@j | ν) denote the number of linear extensions
in Ω(ν) where item c is placed at rank j.
Definition 2 (FCP). Given a partial order ν over m items,
an item c, and an integer j ∈ [1, m], calculate N (c@j | ν),
the number of linear extensions of ν where item c is placed at
rank j.
[Loof, 2009] discussed this problem in his doctoral dissertation (Section 4.2.1). He proposed both exact and approximate algorithms, but did not provide a proof of complexity.
Theorem 4. The FCP is #P-complete.
The hardness of FCP facilitates the hardness proofs for the
Rank Estimation Problems.

4.2

Rank Estimation Problem

Now we move on to the Rank Estimation Problem (REP).
This problem can be regarded as the probabilistic version
of the FCP. It calculates the probability that a given item is
placed at a specific rank. But the REP is generalized from
partial orders to arbitrary ranking distributions.
Definition 3 (REP). Given a ranking model M over m items,
an item c, and an integer j ∈ [1, m], calculate Pr(c@j | M),
the probability that item c is placed at rank j, by M.
For the convenience of discussions in the rest of this Section, we also define two special cases of the REP where items
are placed at the top and bottom of the linear extensions.
Definition 4 (REP-t). Given a ranking model M of m items
and an item c, calculate Pr(c@1 | M), the probability that
item c is placed at the top of a linear extension, by M.
Definition 5 (REP-b). Given a ranking model M of m items
and an item c, calculate Pr(c@m | M), the probability that
item c is placed at the bottom of a linear extension, by M.
[Lerche and Sørensen, 2003] proposed an approximation
for the REP over partial orders, but did not provide a formal complexity proof. [Bruggemann and Annoni, 2014]
and [De Loof et al., 2011] also worked on a related problem,
calculating the expected rank of an item in the linear extensions of a partial order. These works focused on approximation, lacking complexity proofs as well.
With the help of Theorem 4, it turns out that the REP and
its two variants are all FP#P -complete over partial orders.
Lemma 1. If the ranking model M is a partial order ν
that represents a uniform distribution of Ω(ν), the REP-t is
FP#P -complete.
Lemma 2. If the ranking model M is a partial order ν of
m items that represents a uniform distribution of Ω(ν), the
REP-b is FP#P -complete.
Theorem 5. If the ranking model M is a partial order ν that
represents a uniform distribution of Ω(ν), the Rank Estimation Problem is FP#P -complete.

4.3

Complexity of ESC

The ESC problem is closely related to REP. First of all, ESC
is no harder than REP over general voting profiles (Theorem 6), which lays the foundation of the identification of
tractable cases in Section 5.
Theorem 6. Given a voting profile P M and a positional scoring rule rm , the ESC problem can be reduced to the REP.
Theorem 7. The REP is equivalent to the ESC problem over
a collection of k-approval rules where k = 0, . . . , m.
Theorem 7 provides an insight into the relation between
REP and ESC in terms of computational complexity. There
may be cases where the ESC problem is more tractable than
the REP problem. But if a solver is available for the ESC
problem over a collection of k-approval votes, this solver is
computationally equivalent to the REP solver. Note that Theorem 7 is not limited to partial voting profiles.
Theorem 8. Given a partial voting profile P PO , a distinguished candidate c, and plurality rule rm , the ESC problem
of calculating E(s(c | P PO , rm )) is FP#P -complete.
Theorem 9. Given a partial voting profile P PO , a distinguished candidate c, and veto rule rm , the ESC problem of
calculating E(s(c | P PO , rm )) is FP#P -complete.

Algorithm 1 REP solver for RIM
Input: Item c, RIM(σ, Π) where |σ| = m
Output: {k → Pr(c@k | σ, Π) | k ∈ [1, m]}
1: δ0 := ∅, P0 := {δ0 } and q0 (δ0 ) := 1
2: for i = 1, . . . , m do
3:
Pi := {}
4:
for δ ∈ Pi−1 do
5:
for j = 1, . . . , i do
6:
if σ(i) is c then
7:
δ 0 := {c → j}
8:
else if δ = {c → k} and j ≤ k then
9:
δ 0 := {c → k + 1}
10:
else
11:
δ 0 := δ
12:
end if
13:
Pi .add(δ 0 )
14:
qi (δ 0 ) += qi−1 (δ) * Π(i, j)
15:
end for
16:
end for
17: end for
18: Obtain {k → Pr(c@k | σ, Π) | k ∈ [1, m]} from qm .
19: return {k → Pr(c@k | σ, Π) | k ∈ [1, m]}

Theorem 10. Given a partial voting profile P PO , a distinguished candidate c, and k-approval rule rm , the ESC problem of calculating E(s(c | P PO , rm )) is FP#P -complete.

Theorem 12. Given positional scoring rule rm , a partial
chain voting profile P PC = (ν1PC , . . . , νnPC ), and candidate
w, determining w ∈ MEW(rm , P PC ) is in O(nm2 ).

The three theorems above demonstrate the hardness of
the ESC problem over partial voting profiles, under plurality, veto, and k-approval, respectively. In particular, ESC is
FP#P -complete even under plurality (Theorem 8).

Proof. For any ν PC ∈ P PC and any candidate c, the P r(c@j |
ν PC ) is proportional to the degree of freedom to place the rest
of the candidates, after fixing c at rank j.

• If c 6∈ ν PC , P r(c@j | νiPC ) ∝ m−1
* (m − 1 − K)!
K
where K = |νiPC |.
 m−j 
• If c ∈ ν PC , P r(c@j | ν PC ) ∝ j−1
Kl * Kr * (m − K)!
where Kl = |{c0 | c0 ν PC c}|, Kr = |{c0 | c ν PC c0 }|,
and K = |νiPC |.

5

Tractable Cases

The problem of determining MEW can be reduced to the ESC
problem by Theorem 3, then further reduced to the REP by
Theorem 6. This section will solve the MEW problem for
general positional scoring rules for a variety of voting profiles
by solving the REP.

5.1

Incomplete Voting Profiles

Section 4 has proved that calculating the expected scores of
candidates given partial voting profiles is hard, even under
plurality rule. But it turns out that the MEW problem has efficient algorithms for all special cases of partial voting profiles
in Figure 1b, including partitioned voting profiles (covering
truncated voting profiles) and partial chain voting profiles.
Theorem 11. Given positional scoring rule rm , a partitioned
voting profile P PP = (ν1PP , . . . , νnPP ), and candidate w, determining w ∈ MEW(rm , P PP ) is in O(mn).
Proof. Any ν PP ∈ P PP defines a set of consecutive ranks
in the linear extensions of ν PP for each of its partitions of
candidates. Any candidate is equally likely to be positioned
at these consecutive ranks. So the REP can be solved in O(1)
for any candidate. Thus, the MEW problem can be solved in
O(nm2 ) by calculating the expected scores of all candidates.

It takes O(nm2 ) to obtain the expected scores of all candidates and to determine whether w is a MEW.

5.2

Probabilistic Voting Profiles

While the problem of MEW determination is hard in general,
it is tractable over specific ranking models such as the Mallows [Mallows, 1957] and its generalizations RIM [Doignon
et al., 2004] and RSM [Chakraborty et al., 2020].
RIM, denoted by RIM(σ, Π), is a generative model that
generalizes not only Mallows, but also the Generalized Mallows [Fligner and Verducci, 1986] and multistage ranking
models [Fligner and Verducci, 1988]. It is parameterized by
a reference ranking σ and a probability function Π where
Π(i, j) is the probability of inserting the ith item σ(i) at the
j th position (1 ≤ j ≤ i) during ranking construction. The insertion procedure starts with an empty ranking τ = hi. Then
insert items in order of σ where σ(i) is placed at the j th position of τ with probability Π(i, j).
Example 1. RIM(ha, b, ci, Π) generates τ =hc, a, bi as follows. Initialize τ0 =hi. When i = 1, τ1 =hai by inserting a

into τ0 with probability Π(1, 1). When i = 2, τ2 =ha, bi by inserting b into τ1 at position 2 with probability Π(2, 2). When
i = 3, τ =hc, a, bi by inserting c into τ2 at position 1 with
probability Π(3, 1). Overall, Pr(τ | ha, b, ci, Π)=Π(1, 1) *
Π(2, 2) * Π(3, 1).
Theorem 13. Given positional scoring rule rm , a RIM voting
profile P RIM = (RIM1 , . . . , RIMn ), and candidate w, determining w ∈ MEW(rm , P RIM ) is in O(nm4 ).
Proof. Given any RIM ∈ P RIM and any candidate c, the
P r(c@j | RIM) for j = 1, . . . , m can be calculated by Algorithm 1 in O(m3 ). Algorithm 1 is a variant of RIMDP [Kenig
et al., 2018]. RIMDP calculates the marginal probability of a
partial order over RIM via Dynamic Programming (DP). Algorithm 1 is simplified RIMDP in a sense that Algorithm 1
only tracks a particular item c, while RIMDP tracks multiple
items to calculate the insertion ranges of items that satisfy the
partial order. Note that Algorithm 1 calculates all m different values of j simultaneously. So it takes O(nm * m3 ) =
O(nm4 ) to obtain the expected scores of m candidates over
n RIMs to determine the MEW.
The complexity of MEW determination over RSM voting
profiles is O(nm4 ) as well. Due to space restrictions, please
refer to Section 8.2 in Appendix for more details.

5.3

Combined Voting Profiles

It is usually harder to compute the expected scores over combined voting profiles. Below are some cases where this problem is tractable. The first case is the RIMs combined with
truncated rankings.
Theorem 14. Given positional scoring rule rm , a voting
(t ,b )
(t ,b ) 
profile P RIM+TR = (RIM1 , τ1 1 1 ), . . . , (RIMn , τn n n ) ,
and candidate w, determining w ∈ MEW(rm , P RIM+TR ) is in
O(nm4 ).
Proof. Given any (RIM, τ (t,b) ) ∈ P RIM+TR , candidate c, and
rank j, if c is in the top or bottom part of τ (t,b) , its rank
has been fixed, which is a trivial case; If c is in the middle
part of τ (t,b) , we just need to slightly modify Algorithm 1
to calculate Pr(c@j | RIM, τ (t,b) ). Line 5 in Algorithm 1
enumerates values for j from 1 to i. The constraints made by
τ (t,b) limits this insertion range of item σ(i). If σ(i) is in
the top or bottom part of τ (t,b) , its insertion position has been
fixed by τ (t,b) and the inserted items of the top and bottom
parts of τ (t,b) should be recorded as well by the state δ 0 ; If
σ(i) is in the middle part of τ (t,b) , σ(i) can be inserted into
any position between the inserted top and bottom items.
Theoretically, the algorithm needs to track as many as
(t + b + 1) items. But the (t + b) items are fixed, which makes
c the only item leading to multiple states during DP. The complexity of calculating Pr(c@j | RIM, τ (t,b) ) for all j values is
O(m3 ). It takes O(nm4 ) to calculate the expected scores of
all candidates across all voters to determine the MEW.
Another tractable case is the Mallows combined with partitioned preferences. Let MAL(σ, φ) where 0 ≤ φ ≤ 1 denote a Mallows model [Mallows, 1957]. It defines a probability distribution of rankings: reference ranking σ at the

Profile
PP

P
P PC
P RIM
P rRSM
P RIM+TR
P MAL+PP

Profile (equiv.)
U+PP

P
P U+PC
P RIM+∅
P rRSM+∅
P RIM+TR
P MAL+PP

Complexity
O(nm2 )
O(nm2 )
O(nm4 )
O(nm4 )
O(nm4 )
O(nm4 )

Table 3: Tractability results of MEW under general positional scoring rule for various voting profiles, including partitioned preferences
(PP), partial chains (PC), RIMs, rRSMs, and combined voting profiles. These profiles are also rewritten in the format of combined
voting profiles.

center and other rankings closer to σ having higher probabilities. For a given ranking τ , Pr(τ |σ, φ) ∝ φD(σ,τ ) where
D(σ, τ ) = |(a, a0 )|a σ a0 , a0 τ a| is the Kendall-tau distance counting the number of disagreed preference pairs. If
φ = 1, the Mallows becomes to a uniform distribution. The
MAL(σ, φ) is a special case for both RIM(σ, Π) by Π(i, j) =
φi−j
φj−1
1+φ+...+φi−1 and rRSM(σ, Π) by Π(i, j) = 1+φ+...+φm−i .
Theorem 15. Given positional scoring rule rm , a voting
profile P MAL+PP = (MAL1 , ν1PP ), . . . , (MALn , νnPP ) , and
candidate w, determining w ∈ MEW(rm , P MAL+PP ) is in
O(nm4 ).
Proof. Given any (MAL(σ, φ), ν PP ) ∈ P MAL+PP , candidate
c, and rank j, consider calculating P r(c@j | σ, φ, ν PP ). Let
CP denote the set of candidates in the same partition with c
in ν PP . The relative orders between c and items out of CP
are already determined by ν PP . That is to say, for a nontrivial j value, P r(c@j | σ, φ, ν PP ) is proportional to the
exponential of the number of disagreed pairs within CP . So
we can construct a new Mallows model MAL0 (σ 0 , φ) over
CP . It has the same φ as MAL and its reference ranking σ 0 is
shorter than but consistent with σ. The P r(c@j | MAL0 , ν PP )
for all non-trivial j values can be calculated in O(|CP |3 ) <
O(m3 ) by Algorithm 1.
The MEW problem can be solved in O(nm4 ) by calculating the expected scores of all candidates across all voters to
determine whether w is a MEW.

5.4

Summary

Table 3 summarizes the tractable cases. Combining this table with Figure 1, we can obtain more conclusions for a large
number of specialized voting profiles. For example, the Mallows voting profile P MAL is not listed in the Table, but its
MEW complexity is bounded by O(nm4 ), since it is a special
case of P RIM . An interesting observation is that the MEW
complexity over P RIM+PP is not tractable, but its special case
P MAL+PP has a polynomial complexity. Although this table
demonstrates that evaluating MEW over probabilistic voting
profiles has higher complexities than that over incomplete
voting profiles, e.g., O(nm4 ) for P MAL+PP but only O(nm2 )

for P PP , the tractability results over a collection of probabilistic and combined voting profiles still give the MEW a
computation advantage as a feasible option in practice.

6

Concluding Remarks

We embarked on two tasks: 1) modeling the uncertainty in
voter preferences and 2) determining the winners accordingly.
Distinguishing between uncertainties in preference generation and preference observation provides a powerful framework to describe and unify incomplete and probabilistic voting profiles. Then we proposed the Most Expected Winner for
positional scoring rules, established its theoretical hardness,
and identified a collection of tractable cases.
Our work can be used as a starting point for future studies.
For example, the hardness of ESC is proved over only plurality, veto, and k-approval, which calls for investigation over
other positional scoring rules such as Borda. When the MEW
is intractable, it is often necessary to develop approximate
solvers such as the AMP sampler for Mallows posteriors over
partial orders [Lu and Boutilier, 2014]. The MEW can also
be extended to other score-based rules, such as the Simpson
rule and the Copeland rule.
Another direction is to consider voter preferences represented by additional ranking models [Marden, 1995], such as
the Plackett-Luce (PL) model [Luce, 1959; Plackett, 1975].
[Noothigattu et al., 2018; Zhao et al., 2018] have studied preference aggregation over PL models, which is closely related
to the MEW over voting profiles of PL models, and we plan
to investigate this connection in the future.

7

Acknowledgements

This work was supported in part by NSF Grants No. 1916647
and 1916505.

References
[Bachrach et al., 2010] Yoram Bachrach, Nadja Betzler, and
Piotr Faliszewski. Probabilistic possible winner determination. In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010, Atlanta, Georgia, USA, July 11-15, 2010, 2010.
[Brightwell and Winkler, 1991] Graham R. Brightwell and
Peter Winkler. Counting linear extensions is #p-complete.
In Proceedings of the 23rd Annual ACM Symposium on
Theory of Computing, May 5-8, 1991, New Orleans,
Louisiana, USA, pages 175–181, 1991.
[Bruggemann and Annoni, 2014] Rainer Bruggemann and
Paola Annoni. Average heights in partially ordered
sets. MATCH Commun Math Comput Chem, 71:117–142,
2014.
[Chakraborty et al., 2020] Vishal Chakraborty, Theo Delemazure, Benny Kimelfeld, Phokion G. Kolaitis, Kunal Relia, and Julia Stoyanovich. Algorithmic techniques for
necessary and possible winners. CoRR, abs/2005.06779,
2020.

[De Loof et al., 2011] Karel De Loof, Bernard De Baets, and
Hans De Meyer. Approximation of average ranks in
posets. Match Commun Math Comput Chem, 66:219–229,
2011.
[Doignon et al., 2004] Jean-Paul Doignon,
Aleksandar
Pekeč, and Michel Regenwetter. The repeated insertion
model for rankings: Missing link between two subset
choice models. Psychometrika, 69(1):33–54, 2004.
[Fligner and Verducci, 1986] Michael A Fligner and
Joseph S Verducci. Distance based ranking models.
Journal of the Royal Statistical Society: Series B
(Methodological), 48(3):359–369, 1986.
[Fligner and Verducci, 1988] Michael A Fligner and
Joseph S Verducci. Multistage ranking models. Journal
of the American Statistical association, 83(403):892–901,
1988.
[Hazon et al., 2012] Noam Hazon, Yonatan Aumann, Sarit
Kraus, and Michael J. Wooldridge. On the evaluation of
election outcomes under uncertainty. Artif. Intell., 189:1–
18, 2012.
[Kenig et al., 2018] Batya Kenig, Lovro Ilijasic, Haoyue
Ping, Benny Kimelfeld, and Julia Stoyanovich. Probabilistic inference over repeated insertion models. In Proceedings of the Thirty-Second AAAI Conference on Artificial
Intelligence, (AAAI-18), New Orleans, Louisiana, USA,
February 2-7, 2018, pages 1897–1904, 2018.
[Konczak and Lang, 2005] Kathrin Konczak and Jérôme
Lang. Voting procedures with incomplete preferences. In
Proc. IJCAI-05 Multidisciplinary Workshop on Advances
in Preference Handling, volume 20, 2005.
[Lerche and Sørensen, 2003] Dorte Lerche and Peter B
Sørensen. Evaluation of the ranking probabilities for partial orders based on random linear extensions. Chemosphere, 53(8):981–992, 2003.
[Loof, 2009] Karel De Loof. Efficient computation of rank
probabilities in posets. PhD thesis, Ghent University,
2009.
[Lu and Boutilier, 2011] Tyler Lu and Craig Boutilier. Robust approximation and incremental elicitation in voting protocols. In IJCAI 2011, Proceedings of the 22nd
International Joint Conference on Artificial Intelligence,
Barcelona, Catalonia, Spain, July 16-22, 2011, pages
287–293, 2011.
[Lu and Boutilier, 2014] Tyler Lu and Craig Boutilier. Effective sampling and learning for mallows models
with pairwise-preference data. J. Mach. Learn. Res.,
15(1):3783–3829, 2014.
[Luce, 1959] R.D. Luce. Individual Choice Behavior: A
Theoretical Analysis. Wiley, 1959.
[Mallows, 1957] C. L. Mallows. Non-null ranking models.
Biometrika, 44:114–130, 1957.
[Marden, 1995] John I Marden. Analyzing and modeling
rank data. CRC Press, 1995.

[Noothigattu et al., 2018] Ritesh Noothigattu, Snehalkumar
(Neil) S. Gaikwad, Edmond Awad, Sohan Dsouza, Iyad
Rahwan, Pradeep Ravikumar, and Ariel D. Procaccia. A
voting-based system for ethical decision making. In Proceedings of AAAI, New Orleans, Louisiana, USA, pages
1587–1594, 2018.
[Plackett, 1975] Robin L Plackett. The analysis of permutations. Journal of the Royal Statistical Society: Series C
(Applied Statistics), 24(2):193–202, 1975.
[Zhao et al., 2018] Zhibing Zhao, Haoming Li, Junming
Wang, Jeffrey O. Kephart, Nicholas Mattei, Hui Su, and
Lirong Xia. A cost-effective framework for preference
elicitation and aggregation. In Proceedings of UAI, Monterey, California, USA, pages 446–456, 2018.

8
8.1

Appendix
Additional proofs

T HEOREM 1. The MEW and the Least Expected Regret Winner are equivalent.
Proof. The expected regret of a candidate w can be rewritten
as follows.
E(Regret(w, P M ))
z
X
Regret(c, Pi ) * Pr(Pi | P M )
=
i=1

=
=

z 
X
i=1
z
X
i=1



max s(c, Pi ) − s(w, Pi ) * Pr(Pi | P M )
c∈C

max s(c, Pi ) * Pr(Pi | P M ) − E(s(w, P M ))
c∈C

Pz
The first term i=1 maxc∈C s(c, Pi ) * Pr(Pi | P M ) is
a constant value, when P M and the voting rule are fixed.
Thus, E(Regret(w, P M )) is minimized by maximizing
E(s(w, P M )), the expected score of the candidate w.
T HEOREM 2.
are equivalent.

The MEW and the Meta-Election Winner

Proof. Let P M denote a general voting profile with
Ω(P M ) = {P1 , . . . , Pz }, and PM = (P1 , . . . , Pz ) denote
the large meta profile where rankings in Pi are weighted by
Pr(Pi | P M ). According the definition of the Meta-Election
Winner, s(w, PM ) = maxc∈C s(c, PM ). As a result, for any
candidate c,
X
E(s(c, P M )) =
s(c, P ) * Pr(P | P M ) = s(c, PM )
P ∈Ω(P M )

Her expected score in P M are precisely her score in PM . The
two winner definitions are optimizing the same metric.

T HEOREM 3. The problem of determining MEW can be
reduced to the ESC problem.
Proof. By solving the ESC problem m times for m candidates, the expected scores of all candidates are available to
further determine the MEW.
T HEOREM 4. The FCP is #P-complete.
Proof. First, we prove its membership in #P. The FCP is the
counting version the following decision problem: given a partial order ν, an item c, and an integer j, determine whether ν
has a linear extension τ ∈ Ω(ν) where c is ranked at j. This
decision problem is obviously in NP, meaning that the FCP is
in #P.
Then, we prove that the FCP is #P-hard by reduction. Recall that counting |Ω(ν)|, the number of linear extensions
of a partial order ν, is #P-complete [Brightwell and Win]. This problem can be reduced to the FCP by
kler, 1991P
m
|Ω(ν)| = j=1 N (c@j | ν).
In conclusion, the FCP is #P-complete.
L EMMA 1. If the ranking model M is a partial order ν
that represents a uniform distribution of Ω(ν), the REP-t is
FP#P -complete.
Proof. First, we prove that the REP-t is in FP#P . Recall
that Ω(ν) is the linear extensions of a partial order ν, and
N (c@1 | ν) is the number of linear extensions in Ω(ν) where
candidate c is at rank 1. Then Pr(c@1 | ν) = N (c@1 |
ν)/|Ω(ν)|. Consider that counting N (c@1 | ν) is in #P (Theorem 4) and counting |Ω(ν)| is #P-complete [Brightwell and
Winkler, 1991], so Pr(c@1 | ν) is in FP#P .
In the rest of this proof, we prove that the REP-t is #Phard by reduction from the #P-complete problem of counting
|Ω(ν)|.
Let c∗ denote an item that has no parent in ν. Let ν−c∗
denote the partial order of ν with item c∗ removed. If we are
interested in the probability that c∗ is placed at rank 1, we can
write Pr(c∗ @1 | ν) = N (c∗ @1 | ν)/|Ω(ν)|. The item c∗ has
been fixed at rank 1, so any placement of the rest items will
definitely satisfy any relative order involving c∗ . That is to
say, the placement of the rest items just need to satisfy ν−c∗ ,
which leads to N (c∗ @1 | ν) = |Ω(ν−c∗ )|.
For example, let ν 0 = {c1  c4 , c2  c4 , c3  c4 }. Then
0
N (c1 @1 | ν 0 ) = |Ω(ν−c
)| = |Ω({c2  c4 , c3  c4 })|.
1
Then we re-write Pr(c∗ @1 | ν) = N (c∗ @1 | ν)/|Ω(ν)| =
|Ω(ν−c∗ )|/|Ω(ν)|. The oracle for Pr(c∗ @1 | ν) manages
to reduce the size of the counting problem from |Ω(ν)| to
|Ω(ν−c∗ )|. This oracle should be as hard as counting |Ω(ν)|.
Thus calculating Pr(c∗ @1 | ν) is FP#P -hard.
In conclusion, the REP-t is FP#P -complete.
L EMMA 2. If the ranking model M is a partial order ν
of m items that represents a uniform distribution of Ω(ν), the
REP-b is FP#P -complete.

Proof. This proof adopts the same approach as the proof of
Lemma 1.
For the membership proof that the REP-b is in FP#P ,
let N (c@m | ν) denote the number of linear extensions
in Ω(ν) where candidate c is at the bottom rank m. Then
Pr(c@m | ν) = N (c@m | ν)/|Ω(ν)|. Consider that
counting N (c@m | ν) is in #P (Theorem 4) and counting
|Ω(ν)| is #P-complete [Brightwell and Winkler, 1991], so
Pr(c@m | ν) is in FP#P .
In the proof of Lemma 1, item c∗ is an item with no parent
in the partial order ν. In the current proof, item c∗ is set to be
an item with no child in ν. The ν−c∗ still denotes the partial
order of ν but with item c∗ removed. Then the probability that
item c∗ at the bottom rank m is Pr(c∗ @m | ν) = N (c∗ @m |
ν)/|Ω(ν)| = |Ω(ν−c∗ )|/|Ω(ν)|. The oracle for Pr(c∗ @m |
ν) manages to reduce the size of the counting problem again
from |Ω(ν)| to |Ω(ν−c∗ )|. Thus, this oracle is #P-hard, and
calculating Pr(c∗ @m | ν) is FP#P -hard.
In conclusion, the REP-b is FP#P -complete.

P r(c@j | Mi ) for all m candidates and n voters, which leads
to the complexity of O(n * m * T).

T HEOREM 5. If the ranking model M is a partial order
ν that represents a uniform distribution of Ω(ν), the Rank
Estimation Problem is FP#P -complete.

T HEOREM 8. Given a partial voting profile P PO , a distinguished candidate c, and plurality rule rm , the ESC problem
of calculating E(s(c | P PO , rm )) is FP#P -complete.

Proof. First, we prove that the REP is in FP#P . Recall
that Ω(ν) is the linear extensions of a partial order ν, and
N (c@j | ν) is the number of linear extensions in Ω(ν)
where candidate c is at rank j. Then Pr(c@j | ν) =
N (c@j | ν)/|Ω(ν)|. Consider that counting N (c@j |
ν) is #P-complete (Theorem 4) and counting |Ω(ν)| is
#P-complete [Brightwell and Winkler, 1991] as well. So
Pr(c@j | ν) is in FP#P .
Lemma 1 demonstrates that REP-t, a special case of REP,
is FP#P -hard. Thus REP is #P-hard as well.
In conclusion, REP is FP#P -complete.

Proof. Firstly, we prove the membership of the ESC problem
as a FP#P problem. Consider that the REP is FP#P -complete
over partial orders (Theorem 5), and the ESC problem can be
reduced to the REP (Theorem 6) So the ESC problem is in
FP#P for partial voting profiles.
Secondly, we prove that the ESC problem is FP#P -hard,
even for plurality rule, by reduction from the REP-t that is
FP#P -hard (Lemma 1).
Let ν denote the partial order of the REP-t problem. Recall
that the REP-t problem aims to calculate Pr(c@1 | ν) for a
given item c. Let P ν denote a voting profile consisting of just
this partial order ν. The answer to the REP-t problem is the
same as the answer to the corresponding ESC problem, i.e.,
Pr(c@1 | ν) = E(s(c | P ν , plurality)). So the ESC problem
is FP#P -hard, even for plurality voting rule.
In conclusion, the ESC problem is FP#P -complete, under
plurality rule.

T HEOREM 6. Given a voting profile P M and a positional
scoring rule rm , the ESC problem can be reduced to the REP.
Proof. Recall that the MEW w maximizes the expected
score, i.e.,
s(w, P M ) = max E(s(c, P M ))
c∈C

The voting profile P M contains n ranking distributions
{M1 , . . . , Mn }, so
E(s(c, P M )) =

n
X

E(s(c, Mi ))

i=1

where E(s(c, Mi )) is the expected score of c from voter vi .
E(s(c, Mi )) =

m
X

P r(c@j | Mi ) * rm (j)

j=1

where c@j denotes candidate c at rank j, and rm (j) is the
score of rank j.
Let T denote the complexity of calculating P r(c@j | Mi ).
The original MEW problem can be solved by calculating

T HEOREM 7. The REP is equivalent to the ESC problem
over a collection of k-approval rules where k = 0, . . . , m.
Proof. The ESC problem has be reduced to the REP (Theorem 6). This proof will focus on the other direction, i.e.,
reducing the REP to the ESC problem.
Let Pr(c@j | M) denote the probability of placing candidate c at rank j over a ranking distribution M. Let P M denote
a single-voter profile consisting of only this ranking distribution M. Then the REP can be reduced to solving the ESC
problem twice under j-approval and (j − 1)-approval rules.
Pr(c@j | M) = E(s(c | P M , j-approval))
− E(s(c | P M , (j − 1)-approval))

T HEOREM 9. Given a partial voting profile P PO , a distinguished candidate c, and veto rule rm , the ESC problem of
calculating E(s(c | P PO , rm )) is FP#P -complete.
Proof. This proof adopts the same approach as the proof of
Theorem 8.
Firstly, the membership proof that the ESC is in FP#P is
based on the conclusions that the REP is FP#P -complete over
partial orders (Theorem 5), and that the ESC can be reduced
to the REP (Theorem 6) So the ESC is in FP#P for partial
voting profiles.
Secondly, we prove that the ESC is FP#P -hard, under veto
voting rule, by reduction from the REP-b that is FP#P -hard
(Lemma 2).
Let ν denote the partial order of the REP-b problem. Recall that the REP-b problem aims to calculate Pr(c@m | ν)

Algorithm 2 REP solver for rRSM
Input: Item c, rank k, rRSM(σ, Π)
Output: Pr(c@k | σ, Π)
1: α0 := |{σi |σi σ c}|, β0 := |{σi | c σ σi }|
2: P0 := {hα0 , β0 i} and q0 (hα0 , β0 i) := 1
3: for i = 1, . . . , (k − 1) do
4:
Pi := {}
5:
for hα, βi ∈ Pi−1 do
6:
if α > 0 then
7:
Generate a new state hα0 , β 0 i = hα − 1, βi.
8:
Pi .add(hα0 , β 0 i)
Pα
9:
qi (hα0 , β 0 i) += qi−1 (hα, βi) * j=1 Π(i, j)
10:
end if
11:
if β > 0 then
12:
Generate a new state hα0 , β 0 i = hα, β − 1i.
13:
Pi .add(hα0 , β 0 i)
Pα+1+β
14:
qi (hα0 , β 0 i) += qi−1 (hα, βi) * j=α+2 Π(i, j)
15:
end if
16:
end for
17: end for P
18: return
hα,βi∈Pk−1 qk−1 (hα, βi) * Π(k, α + 1)
for a given item c. Let P ν denote a voting profile consisting of just this partial order ν. The answer to the ESC indirectly solves the REP-b, i.e., Pr(c@m | ν) = 1 − E(s(c |
P ν , veto)). So the ESC problem is FP#P -hard under veto
rule.
In conclusion, the ESC is FP#P -complete, under veto rule.
T HEOREM 10.
Given a partial voting profile P PO ,
a distinguished candidate c, and k-approval rule rm , the
ESC problem of calculating E(s(c | P PO , rm )) is FP#P complete.
Proof. First the proof that the Expected Score Computation
(ESC) is in FP#P is the same as the proof of Theorem 8.
Now we prove that the ESC problem is FP#P -hard, under kapproval rule rm , by reduction from the REP-t problem that
is FP#P -hard (Lemma 1).
Let ν denote the partial order of the REP-t problem. Recall
that the REP-t problem aims to calculate Pr(c@1 | ν) for a
given item c. Let ν+ denote a new partial order by inserting
(k − 1) ordered items d1  . . .  dk−1 into ν such that item
dk−1 is preferred to every item in ν. Such placement of items
{d1 , . . . , dk−1 } is to guarantee that all linear extensions of
ν+ start with d1  . . .  dk−1 and these linear extensions
will be precisely the linear extensions of ν after removing
{d1 , . . . , dk−1 }.
Let P ν+ denote a voting profile consisting of just this partial order ν+ . The answer to the ESC problem for item c is
E(s(c | P ν+ , k-approval)). Since there is only one partial
order ν+ in the voting profile, E(s(c | P ν+ , k-approval)) =
Pk
j=1 Pr(c@j | ν+ ). Recall that the first (k − 1) items in
any linear extension of ν+ starts with d1  . . .  dk−1 , so
∀1 ≤ j ≤ (k − 1), Pr(c@j | ν+ ) = 0, which leads to E(s(c |

P ν+ , k-approval)) = Pr(c@k | ν+ ). Since ν+ is constructed
by inserting (k − 1) items before items in ν, Pr(c@k | ν+ ) =
Pr(c@1 | ν). So E(s(c | P ν+ , k-approval)) = Pr(c@1 | ν).
The answer to the REP-t problem has been reduced to the
ESC problem. So the ESC problem is FP#P -hard, under kapproval rule.
In conclusion, the ESC problem is FP#P -complete, under
k-approval rule.

8.2

MEW Tractability over RSM voting profiles

RSM [Chakraborty et al., 2020] denoted by RSM(σ, p, Π) is
another generalization of the Mallows. It is parameterized by
a reference ranking σ, a probability function Π where Π(i, j)
is the probability of the j th item selected at step i, and a probability function p : {1, ..., m − 1} → [0, 1] where p(i) is the
probability that the ith selected item preferred to the remaining items. In contrast to the RIM that randomizes the item
insertion position, the RSM randomized the item insertion order. In this paper, we will use RSM as a ranking model, i.e.,
p ≡ 1 such that it only outputs rankings. This ranking version
is named rRSM and denoted by rRSM(σ, Π).
Example 2. rRSM(σ, Π) with σ = ha, b, ci generates
τ =hc, a, bi as follows. Initialize τ0 =hi. When i = 1, τ1 =hci
by selecting c with probability Π(1, 3), which making the remaining σ = ha, bi. When i = 2, τ2 =hc, ai by selecting a
with probability Π(2, 1), which making the remaining σ =
hbi. When i = 3, τ =hc, a, bi by selecting b with probability
Π(3, 1). Overall, Pr(τ | σ, Π)=Π(1, 3) * Π(2, 1) * Π(3, 1).
Theorem 16. Given a positional scoring rule rm , a RSM voting profile P rRSM = (rRSM1 , . . . , rRSMn ), and candidate w,
determining w ∈ MEW(rm , P rRSM ) is in O(nm4 ).
Proof. Given any rRSM ∈ P rRSM , candidate c, and rank j,
the P r(c@j | rRSM) is computed by Algorithm 2 in a fashion that is similar to Algorithm 1. This is also a Dynamic
Programming (DP) approach. The states are in the form of
hα, βi, where α is the number of items before c, and β is
that after c in the remaining σ. For state hα, βi, there are
(α + 1 + β) items in the remaining σ. Algorithm 2 only runs
up to i = (k − 1) (in line 3), since item c must be selected at
step k and the rest steps do not change the rank of c any more.
Each step i generates at most (i + 1) states, corresponding to
[0, . . . , i] items are selected from items before c in the original
σ. The complexity of Algorithm 2 is bounded by O(m2 ). It
takes O(nm4 ) to obtain the expected scores of all candidates
and to determine the MEW.
Example 3. Let rRSM(σ, Π) denote a RSM where σ =
hσ1 , σ2 , σ3 , σ4 i, and Π = [[0.1, 0.3, 0.4, 0.2], [0.2, 0.5, 0.3],
[0.3, 0.7], [1]]. Assume we are interested in Pr(σ2 @3 | σ, Π),
the probability of rRSM(σ, Π) placing σ2 at rank 3.
• Before running RSM, there is α0 = 1 item before σ2
and β0 = 2 items after σ2 in σ. So the initial state is
hα0 , β0 i = h1, 2i, and q0 (h1, 2i) = 1.
• At step i = 1, the selected item can be either from {σ1 }
or {σ3 , σ4 }. So two new states are generated here.

– The σ1 is selected with probability Π(1, 1) =
0.1, which generates a new state h0, 2i, and
q1 (h0, 2i) = q0 (h1, 2i) * Π(1, 1) = 0.1.
– An item σ ∈ {σ3 , σ4 } is selected with probability
Π(1, 3) + Π(1, 4) = 0.6, which generates a new
state h1, 1i, and q1 (h1, 1i) = q0 (h1, 2i) * 0.6 = 0.6.
So P1 = {h0, 2i, h1, 1i} and q1 = {h0, 2i 7→
0.1, h1, 1i 7→ 0.6}.
• At step i = 2, iterate states in P1 .
– For state h0, 2i, the selected item must be from
the last two items in remaining reference ranking.
A new state h0, 1i is generated with probability
Π(2, 2) + Π(2, 3) = 0.8.
– For state h1, 1i, the selected item is either the first

or last item in remaining reference ranking. A new
state h0, 1i is generated with probability Π(2, 1) =
0.1, and another state h1, 0i is generated with probability Π(2, 3) = 0.3.
So P2 = {h0, 1i, h1, 0i} and
 q2 (h0, 1i) = q1 (h0, 2i) * 0.8 + q1 (h1, 1i) * 0.1 =
0.1 * 0.8 + 0.6 * 0.1 = 0.14
 q2 (h1, 0i) = q1 (h1, 1i) * 0.3 = 0.6 * 0.3 = 0.18
• At step i = 3, item σ2 must be selected to meet the requirement. For each state hα, βi ∈ P2 , the rank of σ2
is (α + 1) in the corresponding remaining ranking. So
Pr(σ2 @3 | σ, Π) = q2 (h0, 1i) * Π(3, 1) + q2 (h1, 0i) *
Π(3, 2) = 0.14 * 0.3 + 0.18 * 0.7 = 0.168.

